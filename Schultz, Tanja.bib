
@CONFERENCE{Abate20208274,
author={Abate, S.T. and Yifiru Tachbelie, M. and Schultz, T.},
title={Deep Neural Networks Based Automatic Speech Recognition for Four Ethiopian Languages},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2020},
volume={2020-May},
pages={8274-8278},
doi={10.1109/ICASSP40776.2020.9053883},
art_number={9053883},
note={cited By 0; Conference of 2020 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2020 ; Conference Date: 4 May 2020 Through 8 May 2020;  Conference Code:161907},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089239953&doi=10.1109%2fICASSP40776.2020.9053883&partnerID=40&md5=da7ddf2b1e898774e2fc092a765307c4},
affiliation={University of Bremen, Csl, Germany},
abstract={In this work, we present speech recognition systems for four Ethiopian languages: Amharic, Tigrigna, Oromo and Wolaytta. We have used comparable training corpora of about 20 to 29 hours speech and evaluation speech of about 1 hour for each of the languages. For Amharic and Tigrigna, lexical and language models of different vocabulary size have been developed. For Oromo and Wolaytta, the training lexicons have been used for decoding. We achieved relative word error rate (WER) reductions for all the languages by using Deep Neural Networks (DNN) based acoustic models that range from 15.1% to 31.45%. The relative improvement obtained for Wolaytta speech recognition system is much higher (31.45%) than the improvement achieved for the other languages. This attributes to the weaker language model and the bigger size of training speech we used for Wolaytta. © 2020 IEEE.},
author_keywords={Automatic Speech Recognition;  Deep Neural Networks;  Ethiopian Languages;  Out of Vocabulary},
funding_details={Alexander von Humboldt-StiftungAlexander von Humboldt-Stiftung},
funding_text 1={∗The authors would like to thank the Alexander von Humboldt Foundation for research fellowship.},
references={Yifiru Tachbelie, M., Teferra Abate, S., Besacier, L., Using different acoustic, lexical and language modeling units for asr of an under-resourced language-Amharic (2014) Speech Commun., 56, pp. 181-194. , Jan; Teferra Abate, S., Menzel, W., Tafila, B., An Amharic speech corpus for large vocabulary continuous speech recognition (2005) INTERSPEECH 2005, pp. 1601-1604. , Lisbon, Portugal, September 4-8, 2005; Teferra Abate, S., (2006) Automatic Speech Recognition for Amharic, , Ph. D. Thesis, University of Hamburg; Teferra Abate, S., Menzel, W., Automatic speech recognition for an under-resourced language-Amharic (2007) INTERSPEECH 2007, pp. 1541-1544. , Antwerp, Belgium, August 27-31, 2007; Teferra Abate, S., Menzel, W., Syllablebased speech recognition for Amharic (2007) Proceedings of the 2007 Workshop on Computational Approaches to Semitic Languages: Common Issues and Resources, SEMITIC@ACL 2007, pp. 33-40. , Prague, Czech Republic, June 28, 2007; Pellegrini, T., Lamel, L., Experimental detection of vowel pronunciation variants in Amharic (2006) Proceedings of the Fifth International Conference on LREC, pp. 1005-1008. , Genoa, Italy, May 22-28, 2006; Pellegrini, T., Lamel, L., Automatic word decompounding for asr in a morphologically rich language: Application to Amharic (2009) IEEE Trans. Audio, Speech & Language Processing, 17 (5), pp. 863-873; Yifiru Tachbelie, M., Menzel, W., Amharic partof-speech tagger for factored language modeling (2009) RANLP 2009, pp. 428-433. , Borovets, Bulgaria, 2009, 14-16 September; Yifiru Tachbelie, M., (2010) Morphology-based Language Modeling for Amharic, , Ph. D. Thesis, University of Hamburg; Yifiru Tachbelie, M., Teferra Abate, S., Besacier, L., Rossato, S., Syllable-based and hybrid acoustic models for Amharic speech recognition (2012) Third Workshop on SLTU, pp. 5-10. , Cape Town, South Africa, May 7-9, 2012; Yifiru Tachbelie, M., Teferra Abate, S., Effect of language resources on automatic speech recognition for Amharic (2015) AFRICON 2015, pp. 1-5. , Addis Ababa, Ethiopia, September 14-17, 2015; Gelas, H., Teferra Abate, S., Besacier, L., Pellegrino, F., Quality assessment of crowdsourcing transcriptions for african languages (2011) INTERSPEECH 2011, pp. 3065-3068. , Florence, Italy, August 27-31, 2011; Edessa Dribssa, A., Yifiru Tachbelie, M., Investigating the use of syllable acoustic units for Amharic speech recognition (2015) AFRICON 2015, pp. 1-5. , Addis Ababa, Ethiopia, September 14-17, 2015; Abera, H., Hailemariam, S., Design of a Tigrinya language speech corpus for speech recognition (2018) Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing, pp. 78-82. , Santa Fe, New Mexico, USA, Aug. Association for Computational Linguistics; Teferra Abate, S., Yifiru Tachbelie, M., Melese, M., Abera, H., Abebe, T., Mulugeta, W., Assabie, Y., Ephrem, B., Large vocabulary read speech corpora for four ethiopian languages: Amharic, tigrigna, oromo and wolaytta (2020) LREC 2020; Yifiru Tachbelie, M., Teferra Abate, S., Menzel, W., Morpheme-based automatic speech recognition for a morphologically rich language-Amharic (2010) 2nd Workshop on SLTU, pp. 68-73. , Penang, Malaysia, May 3-5, 2010; Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, A., Jaitly, N., Senior, A., Kingsbury, B., Deep neural networks for acoustic modeling in speech recognition (2012) IEEE Signal Processing Magazine, 29; Yifiru Tachbelie, M., Abulimiti, A., Teferra Abate, S., Schultz, T., Dnn-based speech recognition for globalphone languages (2020) LREC 2020; Abera, H., Hmariam, S., Speech recognition for Tigrinya language using deep neural network approach (2019) Proceedings of the 2019 Workshop on Widening NLP, pp. 7-9. , Association for Computational Linguistics, Florence, Italy, Aug; Peddinti, V., Povey, D., Khudanpur, S., A time delay neural network architecture for efficient modeling of long temporal contexts (2015) Sixteenth Annual Conference of the International Speech Communication Association; Povey, D., Cheng, G., Wang, Y., Li, K., Xu, H., Yarmohammadi, M., Khudanpur, S., Semiorthogonal low-rank matrix factorization for deep neural networks. (2018) Interspeech, pp. 3743-3747; Leslau, W., (2000) Introductory Grammar of Amharic, Porta Linguarum Orientalium, , neue Serie, Bd. 21. Harrassowitz, Wiesbaden; Yohannes Tesfay, T., (2002) A Modern Grammar of Tigrinya, , Tipografia U. Detti, Rome; Griefenow-Mewis, C., (2001) A Grammatical Sketch of Written Oromo; Suchomel, V., Rychlý, P., (2016) Oromo Web Corpus, , LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics (ú FAL) Faculty of Mathematics and Physics, Charles University; Stolcke, A., Srilm-an extensible language modeling toolkit (2002) IN PROCEEDINGS of the 7TH INTERNATIONAL CONFERENCE on SPOKEN LANGUAGE PROCESSING (ICSLP 2002, pp. 901-904; Povey, D., Ghoshal, A., Boulianne, G., Burget, L., Glembek, O., Goel, N., Hannemann, M., Vesely, K., The kaldi speech recognition toolkit (2011) IEEE 2011 Workshop on Automatic Speech Recognition and Understanding, , IEEE Signal Processing Society, IEEE Catalog No. : CFP11SRW-USB. Dec; Ko, T., Peddinti, V., Povey, D., Khudanpur, S., Audio augmentation for speech recognition (2015) INTERSPEECH},
sponsors={The Institute of Electrical and Electronics Engineers, Signal Processing Society},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15206149},
isbn={9781509066315},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tachbelie20208269,
author={Tachbelie, M.Y. and Abulimiti, A. and Abate, S.T. and Schultz, T.},
title={DNN-Based Speech Recognition for Globalphone Languages},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2020},
volume={2020-May},
pages={8269-8273},
doi={10.1109/ICASSP40776.2020.9053144},
art_number={9053144},
note={cited By 0; Conference of 2020 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2020 ; Conference Date: 4 May 2020 Through 8 May 2020;  Conference Code:161907},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089225831&doi=10.1109%2fICASSP40776.2020.9053144&partnerID=40&md5=749595dd4e3b369c1f32f0954b9f74b8},
affiliation={University of Bremen, Cognitive Systems Lab, Germany},
abstract={This paper describes new reference benchmark results based on hybrid Hidden Markov Model and Deep Neural Networks (HMM-DNN) for the GlobalPhone (GP) multilingual text and speech database. GP is a multilingual database of high-quality read speech with corresponding transcriptions and pronunciation dictionaries in more than 20 languages. Moreover, we provide new results for five additional languages, namely, Amharic, Oromo, Tigrigna, Wolaytta, and Uyghur. Across the 22 languages considered, the hybrid HMM-DNN models outperform the HMM-GMM based models regardless of the size of the training speech used. Overall, we achieved relative improvements that range from 7.14% to 59.43%. © 2020 IEEE.},
author_keywords={DNN;  Ethiopian Languages;  GlobalPhone},
funding_details={Alexander von Humboldt-StiftungAlexander von Humboldt-Stiftung},
funding_text 1={∗The authors would like to thank the Alexander von Humboldt Foundation for research fellowship.},
references={(2019) Languages of the World, , https://www.ethnologue.com/, Ethnologue Retrieved October 21 2019; Schultz, T., Kirchhoff, K., (2006) Multilingual Speech Processing, , Elsevier Academic Press; Schultz, T., Towards rapid language portability of speech processing systems (2004) Conference on Speech and Language Systems for Human Communication (SPLASH), 1. , Delhi, India, November; (2020) The Online Encyclopedia of Writing Systems and Languages, , https://www.omniglot.com/writing/stats.htm, Retrieved February 12 2020; Schultz, T., Globalphone: A multilingual speech and text database developed at karlsruhe university (2002) Proceedings of the ICSLP, pp. 345-348; (2012) European Language Resources Association Elra, , http://catalog.elra.info, ELRA catalogue. Retrieved November 30 2012; Schultz, T., Thang Vu, N., Schlippe, T., Globalphone: A multilingual text and speech database in 20 languages (2013) ICASSP; McCulloch, W.S., Pitts, W., A logical calculus of the ideas immanent in nervous activity (1943) The Bulletin of Mathematical Biophysics, 5 (4), pp. 115-133; Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, A., Jaitly, N., Senior, A., Kingsbury, B., Deep neural networks for acoustic modeling in speech recognition (2012) IEEE Signal Processing Magazine, 29; Peddinti, V., Povey, D., Khudanpur, S., A time delay neural network architecture for efficient modeling of long temporal contexts (2015) Sixteenth Annual Conference of the International Speech Communication Association; Povey, D., Cheng, G., Wang, Y., Li, K., Xu, H., Yarmohammadi, M., Khudanpur, S., Semi-orthogonal low-rank matrix factorization for deep neural networks. (2018) Interspeech, pp. 3743-3747; Speech and language resources 2012 (2012) Appen Butler Hill Speech and Language Resources 2012-Product Catalogue, , Appen Butler Hill Pty Ltd; (2012) Benchmark Globalphone Language Models, , https://www.csl.unibremen.de/GlobalPhone/, LM-BM Retrieved October 21 2019; Teferra Abate, S., Menzel, W., Tafila, B., An Amharic speech corpus for large vocabulary continuous speech recognition (2005) INTERSPEECH; Teferra Abate, S., Yifiru Tachbelie, M., Melese, M., Abera, H., Abebe, T., Mulugeta, W., Assabie, Y., Ephrem, B., Large vocabulary read speech corpora for four ethiopian languages: Amharic, tigrigna, oromo and wolaytta (2020) LREC2020; (2016) Eager: Automatic Speech Recognition for Uyghur, Award Number 1519164, 2015-2016, , NSF-Funded Project; (1982) IPA, the Principles of the International Phonetic Association, , University College of London, London, UK, 2 edition; Schultz, T., Schlippe, T., Globalphone: Pronunciation dictionaries in 20 languages (2014) LREC; Stolcke, A., Srilm-an extensible language modeling toolkit (2002) Intl. Conf. Spoken Language Processing (ICSLP); Povey, D., Ghoshal, A., Boulianne, G., Burget, L., Glembek, O., Kumar Goel, N., Hannemann, M., Vesely, K., The kaldi speech recognition toolkit (2011) ASRU; Ko, T., Peddinti, V., Povey, D., Khudanpur, S., Audio augmentation for speech recognition (2015) INTERSPEECH; (2020) Kaldi Download Website, , https://github.com/kaldiasr/kaldi/tree/master/egs/wsj/s5/local/chain, Retrieved Febraury 13 2020},
sponsors={The Institute of Electrical and Electronics Engineers, Signal Processing Society},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15206149},
isbn={9781509066315},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Küster2020,
author={Küster, D. and Krumhuber, E.G. and Steinert, L. and Ahuja, A. and Baker, M. and Schultz, T.},
title={Opportunities and Challenges for Using Automatic Human Affect Analysis in Consumer Research},
journal={Frontiers in Neuroscience},
year={2020},
volume={14},
doi={10.3389/fnins.2020.00400},
art_number={400},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084931660&doi=10.3389%2ffnins.2020.00400&partnerID=40&md5=1d32212b0bed4f941763cfdb7463aea4},
affiliation={Cognitive Systems Lab, Department of Mathematics and Computer Science, University of Bremen, Bremen, Germany; Department of Psychology and Methods, Jacobs University Bremen, Bremen, Germany; Department of Experimental Psychology, University College London, London, United Kingdom; Maharaja Surajmal Institute of Technology, Guru Gobind Singh Indraprastha University, New Delhi, India; Centre for Situated Action and Communication, Department of Psychology, University of Portsmouth, Portsmouth, United Kingdom},
abstract={The ability to automatically assess emotional responses via contact-free video recording taps into a rapidly growing market aimed at predicting consumer choices. If consumer attention and engagement are measurable in a reliable and accessible manner, relevant marketing decisions could be informed by objective data. Although significant advances have been made in automatic affect recognition, several practical and theoretical issues remain largely unresolved. These concern the lack of cross-system validation, a historical emphasis of posed over spontaneous expressions, as well as more fundamental issues regarding the weak association between subjective experience and facial expressions. To address these limitations, the present paper argues that extant commercial and free facial expression classifiers should be rigorously validated in cross-system research. Furthermore, academics and practitioners must better leverage fine-grained emotional response dynamics, with stronger emphasis on understanding naturally occurring spontaneous expressions, and in naturalistic choice settings. We posit that applied consumer research might be better situated to examine facial behavior in socio-emotional contexts rather than decontextualized, laboratory studies, and highlight how AHAA can be successfully employed in this context. Also, facial activity should be considered less as a single outcome variable, and more as a starting point for further analyses. Implications of this approach and potential obstacles that need to be overcome are discussed within the context of consumer research. © Copyright © 2020 Küster, Krumhuber, Steinert, Ahuja, Baker and Schultz.},
author_keywords={automatic human affect analysis (AHAA);  consumer research;  dynamic responses;  facial expression;  machine learning;  spontaneous expressions},
keywords={article;  classifier;  consumer;  facial expression;  human;  human experiment;  outcome variable;  physician},
funding_details={UniversitÃ¤t BremenUniversitÃ¤t Bremen},
funding_text 1={We acknowledge support by the Open Access Initiative of the University of Bremen. This work was partially funded by Klaus-Tschira Stiftung.},
references={Ambadar, Z., Schooler, J.W., Cohn, J.F., Deciphering the enigmatic face: The importance of facial dynamics in interpreting subtle facial expressions (2005) Psychol. Sci, 16, pp. 403-410. , 15869701; Anderson, G., (2017) Walmart’s Facial Recognition Tech Would Overstep Boundaries. Forbes, , https://www.forbes.com/sites/retailwire/2017/07/27/walmarts-facial-recognition-tech-would-overstep-boundaries/, (Accessed March 24, 2020), Available online at; Baltrusaitis, T., Robinson, P., Morency, L.-P., OpenFace: An open source facial behavior analysis toolkit (2016) Proceedings of the 2016 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 1-10. , Lake Placid, NY, IEEE; Barrett, L.F., Adolphs, R., Marsella, S., Martinez, A.M., Pollak, S.D., Emotional expressions reconsidered: Challenges to inferring emotion from human facial movements (2019) Psychol. Sci. Public Interest, 20, pp. 1-68. , 31313636; Beringer, M., Spohn, F., Hildebrandt, A., Wacker, J., Recio, G., Reliability and validity of machine vision for the assessment of facial expressions (2019) Cogn. Syst. Res, 56, pp. 119-132; Bradski, G., The opencv library (2000) Dr Dobbs J Softw. Tools, 25, pp. 120-125; Crivelli, C., Fridlund, A.J., Facial displays are tools for social influence (2018) Trends Cogn. Sci, 22, pp. 388-399. , 29544997; Danner, L., Sidorkina, L., Joechl, M., Duerrschmid, K., Make a face! Implicit and explicit measurement of facial expressions elicited by orange juices using face reading technology (2014) Food Qual. Prefer, 32, pp. 167-172; Den Uyl, M.J., Van Kuilenburg, H., The FaceReader: Online facial expression recognition (2005) Proceedings of measuring behavior (Citeseer), pp. 589-590. , Utrecht; Dente, P., Küster, D., Skora, L., Krumhuber, E., Measures and metrics for automatic emotion classification via FACET (2017) Proceedings of the Conference on the Study of Artificial Intelligence and Simulation of Behaviour (AISB), pp. 160-163. , Bath; Dupré, D., Krumhuber, E., Küster, D., McKeown, G.J., Emotion recognition in humans and machine using posed and spontaneous facial expression (2019) PsyArXiv [Preprint]; Durán, J., Reisenzein, R., Fernández-Dols, J.-M., Coherence between emotions and facial expressions (2017) The Science of Facial Expression, pp. 107-129. , Fernández-Dols J.-M., Russell J.A., (eds), Oxford, Oxford University Press; Ekman, P., An argument for basic emotions (1992) Cogn. Emot, 6, pp. 169-200; Ekman, P., Basic Emotions (1999) Handbook of Cognition and Emotion, pp. 45-60. , Power M., (ed), Hoboken, NJ, John Wiley & Sons, Ltd; Ekman, P., Lie catching and microexpressions (2009) The Philosophy of Deception, pp. 118-142. , Martin C., (ed), New York, NY, Oxford University Press; Ekman, P., Friesen, W.V., Nonverbal leakage and clues to deception (1969) Psychiatry, 32, pp. 88-106. , 5779090; Garcia, D., Kappas, A., Küster, D., Schweitzer, F., The dynamics of emotions in online interaction (2016) R. Soc. Open Sci, 3. , 27853586; Garcia-Burgos, D., Zamora, M.C., Facial affective reactions to bitter-tasting foods and body mass index in adults (2013) Appetite, 71, pp. 178-186. , 23994505; Gupta, S., (2018) Facial Emotion Detection using AI: Use-Cases. Medium, , https://itnext.io/facial-emotion-detection-using-ai-use-cases-3507e38da598, (Accessed November 29, 2019), Available online at; Hollenstein, T., Lanteigne, D., Models and methods of emotional concordance (2014) Biol. Psychol, 98, pp. 1-5. , 24394718; Hsu, M., Yoon, C., The neuroscience of consumer choice (2015) Curr. Opin. Behav. Sci, 5, pp. 116-121. , 26665152; Kappas, A., What facial activity can and cannot tell us about emotions (2003) The Human Face: Measurement and Meaning, pp. 215-234. , Katsikitis M., (ed), Boston, MA, Springer; Kappas, A., Küster, D., Dente, P., Basedow, C., Shape of things to come: Facial electromyography vs automatic facial coding via FACET (2016) Proceedings of the Annual Meeting of the Society for Psychophysiological Research (SPR), p. S78. , Minneapolis, MN, Jacobs University Bremen; Kollias, D., Tzirakis, P., Nicolaou, M.A., Papaioannou, A., Zhao, G., Schuller, B., Deep Affect Prediction in-the-Wild: Aff-Wild Database and Challenge (2019) Deep Architectures, and Beyond. Int. J. Comput. Vis, 127, pp. 907-929; Krumhuber, E., Küster, D., Namba, S., Shah, D., Calvo, M., Emotion recognition from posed and spontaneous dynamic expressions: Human observers vs. machine analysis (2019) Emotion, , [Epub ahead of print], 31829721; Krumhuber, E., Manstead, A.S., Cosker, D., Marshall, D., Rosin, P.L., Effects of dynamic attributes of smiles in human and synthetic faces: a simulated job interview setting (2009) J. Nonverbal Behav, 33, pp. 1-15; Krumhuber, E., Manstead, A.S.R., Kappas, A., Temporal aspects of facial displays in person and expression perception: the effects of smile dynamics. Head-tilt, and Gender (2007) J. Nonverbal Behav, 31, pp. 39-56; Krumhuber, E.G., Kappas, A., Manstead, A.S.R., Effects of dynamic aspects of facial expressions: a review (2013) Emot. Rev, 5, pp. 41-46; Krumhuber, E.G., Skora, L., Perceptual Study on Facial Expressions (2016) Handbook of Human Motion, pp. 1-15. , Müller B., Wolf S.I., Brueggemann G.-P., Deng Z., McIntosh A., Miller F., (eds), Cham, Springer International Publishing; Krumhuber, E.G., Skora, L., Küster, D., Fou, L., A review of dynamic datasets for facial expression research (2017) Emot. Rev, 9, pp. 280-292; Kulke, L., Feyerabend, D., Schacht, A., A Comparison of the affectiva imotions facial expression analysis software with EMG for identifying facial expressions of emotion (2020) Front. Psychol, , 32184749; Küster, D., Social effects of tears and small pupils are mediated by felt sadness: an evolutionary view (2018) Evol. Psychol, 16. , 29529867; Küster, D., Kappas, A., Measuring emotions in individuals and internet communities (2013) Internet and Emotions, pp. 62-76. , Benski T., Fisher E., (eds), Abingdon, Routledge; Küster, D., Kappas, A., What could a body tell a social robot that it does not know? (2014) Proceedings of the International Conference on Physiological Computing Systems, pp. 358-367. , Lisbon; Lewinski, P., den Uyl, T.M., Butler, C., Automated facial coding: validation of basic emotions and FACS AUs in faceReader (2014) J. Neurosci. Psychol. Econ, 7, pp. 227-236. , a; Lewinski, P., Fransen, M.L., Tan, E.S., Predicting advertising effectiveness by facial expressions in response to amusing persuasive stimuli (2014) J. Neurosci. Psychol. Econ, 7, pp. 1-14. , b; Littlewort, G., Whitehill, J., Wu, T., Fasel, I., Frank, M., Movellan, J., The computer expression recognition toolbox (CERT) (2011) Proceedings of the Ninth IEEE International Conference on Automatic Face and Gesture Recognition (FG 2011), pp. 298-305. , Santa Barbara, CA, IEEE; Maringer, M., Krumhuber, E.G., Fischer, A.H., Niedenthal, P.M., Beyond smile dynamics: mimicry and beliefs in judgments of smiles (2011) Emotion, 11, pp. 181-187. , 21401238; Masip, D., North, M.S., Todorov, A., Osherson, D.N., Automated prediction of preferences using facial expressions (2014) PLoS One, 9 (e87434). , 24503553; Matsumoto, D., Hwang, H.S., Evidence for training the ability to read microexpressions of emotion (2011) Motiv. Emot, 35, pp. 181-191; McDuff, D., El Kaliouby, R., Cohn, J.F., Picard, R.W., Predicting ad liking and purchase intent: Large-scale analysis of facial responses to ads (2015) IEEE Trans. Affect. Comput, 6, pp. 223-235; McStay, A., Empathic media and advertising: Industry, policy, legal and citizen perspectives (the case for intimacy) (2016) Big Data Soc, 3; McStay, A., (2018) Emotional AI: The Rise of Empathic Media, , Los Angeles, CA, SAGE Publications Ltd; Mehta, D., Siddiqui, M., Javaid, A., Facial emotion recognition: A survey and real-world user experiences in mixed reality (2018) Sensors, 18 (416). , 29389845; Meyer, P., (2010) Understanding Measurement: Reliability, , Oxford, Oxford University Press; Mollahosseini, A., Hasani, B., Mahoor, M.H., AffectNet: a database for facial expression, valence, and arousal computing in the wild (2019) IEEE Trans. Affect. Comput, 10, pp. 18-31; Pantic, M., Bartlett, M.S., Machine analysis of facial expressions (2007) Face Recognition, , Delac K., Grgic M., (eds), London, IntechOpen; Pantic, M., Patras, I., Dynamics of facial expression: recognition of facial actions and their temporal segments from face profile image sequences (2006) IEEE Trans. Syst. Man Cybern. Part B Cybern, 36, pp. 433-449. , 16602602; Peace, V., Miles, L., Johnston, L., It doesn’t matter what you wear: The impact of posed and genuine expressions of happiness on product evaluation (2006) Soc. Cogn, 24, pp. 137-168; Picard, R.W., (1997) Affective Computing, , Cambridge, MA, MIT Press; Porter, S., ten Brinke, L., Wallace, B., Secrets and lies: involuntary leakage in deceptive facial expressions as a function of emotional intensity (2012) J. Nonverbal Behav, 36, pp. 23-37; Ramsøy, T., A Foundation for Consumer Neuroscience and Neuromarketing (2019) J. Advert. Res. Work, pp. 1-32; Recio, G., Schacht, A., Sommer, W., Classification of dynamic facial expressions of emotion presented briefly (2013) Cogn. Emot, 27, pp. 1486-1494. , 23659578; Reisenzein, R., Studtmann, M., Horstmann, G., Coherence between emotion and facial expression: Evidence from laboratory experiments (2013) Emot. Rev, 5, pp. 16-23; Samant, S.S., Chapko, M.J., Seo, H.-S., Predicting consumer liking and preference based on emotional responses and sensory perception: A study with basic taste solutions (2017) Food Res. Int, 100, pp. 325-334. , 28873694; Samant, S.S., Seo, H.-S., Influences of sensory attribute intensity, emotional responses, and non-sensory factors on purchase intent toward mixed-vegetable juice products under informed tasting condition (2020) Food Res. Int, 132; Sato, W., Krumhuber, E.G., Jellema, T., Williams, J.H.G., Editorial: dynamic emotional communication (2019) Front. Psychol, 10 (2836). , 31956318; Sato, W., Yoshikawa, S., Spontaneous facial mimicry in response to dynamic facial expressions (2007) Cognition, 104, pp. 1-18. , 16780824; Schwartz, O., (2019) Don’t Look Now: Why You Should be Worried About Machines Reading Your Emotions. The Guardian, , https://www.theguardian.com/technology/2019/mar/06/facial-recognition-software-emotional-science, (Accessed November 29, 2019), Available online at; Shariff, A.F., Tracy, J.L., What are emotion expressions for? (2011) Curr. Dir. Psychol. Sci, 20, pp. 395-399; Shen, X., Chen, W., Zhao, G., Hu, P., Editorial: recognizing microexpression: an interdisciplinary perspective (2019) Front. Psychol, 10 (1318). , 31214101; Skiendziel, T., Rösch, A.G., Schultheiss, O.C., Assessing the convergent validity between the automated emotion recognition software Noldus FaceReader 7 and facial action coding system scoring (2019) PLoS One, 14 (e0223905). , 31622426; Smidts, A., Hsu, M., Sanfey, A.G., Boksem, M.A.S., Ebstein, R.B., Huettel, S.A., Advancing consumer neuroscience (2014) Mark. Lett, 25, pp. 257-267; Stöckli, S., Schulte-Mecklenbeck, M., Borer, S., Samson, A.C., Facial expression analysis with AFFDEX and FACET: a validation study (2018) Behav. Res. Methods, 50, pp. 1446-1460. , 29218587; Teixeira, T., Picard, R., Kaliouby, R., Why, when, and how much to entertain consumers in advertisements? A web-based facial tracking field study (2014) Mark. Sci, 33, pp. 809-827; Teixeira, T., Wedel, M., Pieters, R., Emotion-induced engagement in internet video advertisements (2012) J. Mark. Res, 49, pp. 144-159; Teixeira, T.S., Stipp, H., Optimizing the amount of entertainment in advertising: what’s so funny about tracking reactions to humor? (2013) J. Advert. Res, 53, pp. 286-296; Tian, Y.-I., Kanade, T., Cohn, J.F., Recognizing action units for facial expression analysis (2001) IEEE Trans. Pattern Anal. Mach. Intell, 23, pp. 97-115. , 25210210; Valstar, M., Pantic, M., Fully automatic facial action unit detection and temporal analysis (2006) Proceedings of the 2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW’06), p. 149. , Piscataway, NJ, IEEE; Valstar, M.F., Mehu, M., Jiang, B., Pantic, M., Scherer, K., Meta-analysis of the first facial expression recognition challenge (2012) IEEE Trans. Syst. Man Cybern. Part B Cybern, 42, pp. 966-979. , 22736651; Vincent, J., (2019) AI “Emotion Recognition” Can’t Be Trusted. The Verge, , https://www.theverge.com/2019/7/25/8929793/emotion-recognition-analysis-ai-machine-learning-facial-expression-review, (Accessed November 29, 2019), Available online at; Yik, M., Russell, J.A., Steiger, J.H., A 12-point circumplex structure of core affect (2011) Emotion, 11, pp. 705-731. , 21707162; Yitzhak, N., Giladi, N., Gurevich, T., Messinger, D.S., Prince, E.B., Martin, K., Gently does it: Humans outperform a software classifier in recognizing subtle, nonstereotypical facial expressions (2017) Emotion, 17, pp. 1187-1198. , 28406679; Yu, C.-Y., Ko, C.-H., Applying facereader to recognize consumer emotions in graphic styles (2017) Proc. CIRP, 60, pp. 104-109; Zeng, Z., Pantic, M., Roisman, G.I., Huang, T.S., A survey of affect recognition methods: audio. visual, and spontaneous expressions (2009) IEEE Trans. Pattern Anal. Mach. Intell, 31, pp. 39-58. , 19029545; Zickfeld, J.H., Schubert, T.W., Seibt, B., Fiske, A.P., Moving through the literature: what is the emotion often denoted being moved? (2019) Emot. Rev, 11, pp. 123-139; Zijderveld, G., (2017) The World’s Largest Emotion Database: 5.3 Million Faces and Counting, , https://blog.affectiva.com/the-worlds-largest-emotion-database-5.3-million-faces-and-counting, (Accessed December 4, 2019), Available online at},
correspondence_address1={Küster, D.; Cognitive Systems Lab, Department of Mathematics and Computer Science, University of BremenGermany; email: kuester@uni-bremen.de},
publisher={Frontiers Media S.A.},
issn={16624548},
language={English},
abbrev_source_title={Front. Neurosci.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Putze2020,
author={Putze, F. and Ihrig, T. and Schultz, T. and Stuerzlinger, W.},
title={Platform for Studying Self-Repairing Auto-Corrections in Mobile Text Entry based on Brain Activity, Gaze, and Context},
journal={Conference on Human Factors in Computing Systems - Proceedings},
year={2020},
doi={10.1145/3313831.3376815},
art_number={3376815},
note={cited By 0; Conference of 2020 ACM CHI Conference on Human Factors in Computing Systems, CHI 2020 ; Conference Date: 25 April 2020 Through 30 April 2020;  Conference Code:160500},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091304340&doi=10.1145%2f3313831.3376815&partnerID=40&md5=d61caa4ceb21ac08f9a5b7345d872dbf},
affiliation={University of Bremen, Bremen, Germany; Simon Fraser University, Vancouver, BC, Canada},
abstract={Auto-correction is a standard feature of mobile text entry. While the performance of state-of-the-art auto-correct methods is usually relatively high, any errors that occur are cumbersome to repair, interrupt the flow of text entry, and challenge the user's agency over the process. In this paper, we describe a system that aims to automatically identify and repair auto-correction errors. This system comprises a multi-modal classifier for detecting auto-correction errors from brain activity, eye gaze, and context information, as well as a strategy to repair such errors by replacing the erroneous correction or suggesting alternatives. We integrated both parts in a generic Android component and thus present a research platform for studying self-repairing end-to-end systems. To demonstrate its feasibility, we performed a user study to evaluate the classification performance and usability of our approach. © 2020 ACM.},
author_keywords={auto-correction;  EEG;  eye gaze;  self-repair;  text entry},
keywords={Classification (of information);  Errors;  Human engineering;  Neurophysiology, Autocorrection;  Classification performance;  Context information;  End-to-end systems;  Mobile text entry;  Research platforms;  Self repairing;  State of the art, Brain},
funding_details={Natural Sciences and Engineering Research Council of CanadaNatural Sciences and Engineering Research Council of Canada, NSERC},
funding_details={Deutsche ForschungsgemeinschaftDeutsche Forschungsgemeinschaft, DFG, 316930318},
funding_details={Deutsche ForschungsgemeinschaftDeutsche Forschungsgemeinschaft, DFG},
funding_text 1={This work was done within the project DINCO “Detection of Interaction Competencies and Obstacles”. We thank the German Research Foundation (DFG) for funding this DINCO project under the reference number 316930318 and the Canadian NSERC Discovery program.},
references={Alharbi, O., Sabbir Arif, A., Stuerzlinger, W., Dunlop, M.D., Komninos, A., Wisetype: A tablet keyboard with color-coded visualization and various editing options for error correction (2019) Graphics Interface, 2019. , (2019); Sabbir Arif, A., Kim, S., Stuerzlinger, W., Lee, G., Mazalek, A., Evaluation of a smart-restorable backspace technique to facilitate text entry error correction (2016) Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16), pp. 5151-5162. , http://dx.doi.org/10.1145/2858036.2858407, ACM, New York, NY, USA; Sabbir Arif, A., Stuerzlinger, W., Analysis of text entry performance metrics (2009) Science and Technology for Humanity (TIC-STH), 2009 IEEE Toronto International Conference, pp. 100-105; Sabbir Arif, A., Stuerzlinger, W., Predicting the cost of error correction in character-based text entry technologies (2010) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '10), pp. 5-14. , ACM, New York, NY, USA; Bâce, M., Staal, S., Bulling, A., (2019) Accurate and Robust Eye Contact Detection during Everyday Mobile Device Interactions, , (2019); Banovic, N., Rao, V., Saravanan, A., Dey, A.K., Mankoff, J., Quantifying aversion to costly typing errors in expert mobile text entry (2017) Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI '17), pp. 4229-4241. , http://dx.doi.org/10.1145/3025453.3025695, ACM, New York, NY, USA; Bi, X., Ouyang, T., Zhai, S., Both complete and correct: Multi-objective optimization of touchscreen keyboard (2014) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '14), pp. 2297-2306. , ACM, New York, NY, USA; Buschek, D., Bisinger, B., Alt, F., Researchime: A mobile keyboard application for studying free typing behaviour in the wild (2018) Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18), pp. 2551-25514. , http://dx.doi.org/10.1145/3173574.3173829, ACM, New York, NY, USA; Castellucci, S.J., Scott Mackenzie, I., Gathering text entry metrics on android devices (2011) CHI '11 Extended Abstracts on Human Factors in Computing Systems (CHI EA '11), pp. 1507-1512. , ACM, New York, NY, USA; Chen, T., Guestrin, C., Xgboost: A scalable tree boosting system (2016) Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '16), pp. 785-794. , http://dx.doi.org/10.1145/2939672.2939785, ACM, New York, NY, USA; Clawson, J., Lyons, K., Rudnick, A., Iannucci, R.A., Jr., Starner, T., Automatic whiteout++: Correcting mini-qwerty typing errors using keypress timing (2008) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '08), pp. 573-582. , ACM, New York, NY, USA; Dalmaijer, E.S., Mathôt, S., Van Der Stigchel, S., Pygaze: An open-source, cross-platform toolbox for minimal-effort programming of eyetracking experiments (2014) Behavior Research Methods, 46 (4), pp. 913-921. , (2014); Förster, K., Biasiucci, A., Chavarriaga, R., Del Jose, M.R., Roggen, D., Tröster, G., On the use of brain decoded signals for online user adaptive gesture recognition systems (2010) Pervasive Computing. Number 6030 in Lecture Notes in Computer Science, pp. 427-444. , Springer Berlin Heidelberg; Fowler, A., Partridge, K., Chelba, C., Bi, X., Ouyang, T., Zhai, S., Effects of language modeling and its personalization on touchscreen typing performance (2015) Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI '15), pp. 649-658. , ACM, New York, NY, USA; Goel, M., Findlater, L., Wobbrock, J., Walktype: Using accelerometer data to accomodate situational impairments in mobile touch screen text entry (2012) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '12), pp. 2687-2696. , ACM, New York, NY, USA; Goel, M., Jansen, A., Mandel, T., Patel, S.N., Wobbrock, J.O., Contexttype: Using hand posture information to improve mobile touch screen text entry (2013) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '13), pp. 2795-2798. , ACM, New York, NY, USA; Goldhahn, D., Eckart, T., Quasthoff, U., Building large monolingual dictionaries at the leipzig corpora collection: From 100 to 200 languages (2012) LREC, 29, pp. 31-43; Goodman, J., Venolia, G., Steury, K., Parker, C., Language modeling for soft keyboards (2002) Proceedings of the 7th International Conference on Intelligent User Interfaces (IUI '02), pp. 194-195. , ACM, New York, NY, USA; Hirshfield, L.M., Bobko, P., Barelka, A., Hirshfield, S.H., Farrington, M.T., Gulbronson, S., Paverman, D., Using noninvasive brain measurement to explore the psychological effects of computer malfunctions on users during human-computer interactions (2014) Adv. In Hum.-Comp. Int. 2014, p. 22. , http://dx.doi.org/10.1155/2014/101038, (Jan. 2014); Anthony Hoff, K., Bashir, M., Trust in automation: Integrating empirical evidence on factors that influence trust (2015) Human Factors, 57 (3), pp. 407-434. , (2015); Horstmann, G., Herwig, A., Surprise attracts the eyes and binds the gaze (2015) Psychonomic Bulletin & Review, 22 (3), pp. 743-749. , http://dx.doi.org/10.3758/s13423-014-0723-1, (June 2015); Kalaganis, F.P., Chatzilari, E., Nikolopoulos, S., Kompatsiaris, I., Laskaris, N.A., An error-aware gaze-based keyboard by means of a hybrid bci system (2018) Scientific Reports, 8 (1), p. 13176. , http://dx.doi.org/10.1038/s41598-018-31425-2, (Sept. 2018); Komninos, A., Dunlop, M., Katsaris, K., Garofalakis, J., A glimpse of mobile text entry errors and corrective behaviour in the wild (2018) Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct (MobileHCI '18), pp. 221-228. , http://dx.doi.org/10.1145/3236112.3236143, ACM, New York, NY, USA; Kristensson, P., Zhai, S., Relaxing stylus typing precision by geometric pattern matching (2005) Proceedings of the 10th International Conference on Intelligent User Interfaces (IUI '05), pp. 151-158. , ACM, New York, NY, USA; Ledoit, O., Wolf, M., A well-conditioned estimator for large-dimensional covariance matrices (2004) Journal of Multivariate Analysis, 88 (2), pp. 365-411. , (2004); Margaux, P., Emmanuel, M., Sébastien, D., Olivier, B., Jérémie, M., Objective and subjective evaluation of online error correction during p300-based spelling (2012) Adv. In Hum.-Comp. Int., 2012. , (2012); Mueller, S., Inferring target locations from gaze data: A smartphone study (2019) Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications, pp. 1-4; Joon Park, S., MacDonald, C.M., Khoo, M., Do you care if a computer says sorry: User experience design through affective messages (2012) Proceedings of the Designing Interactive Systems Conference (DIS '12), pp. 731-740. , http://dx.doi.org/10.1145/2317956.2318067, ACM, New York, NY, USA; Putze, F., Amma, C., Schultz, T., Design and evaluation of a self-correcting gesture interface based on error potentials from EEG (2015) Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI '15), pp. 3375-3384. , ACM, New York, NY, USA; Putze, F., Heger, D., Schultz, T., Reliable subject-adapted recognition of EEG error potentials using limited calibration data (2013) 6th International Conference on Neural Engineering, , San Diego, USA; Putze, F., Schünemann, M., Schultz, T., Stuerzlinger, W., Automatic classification of auto-correction errors in predictive text entry based on EEG and context information (2017) Proceedings of the 19th ACM International Conference on Multimodal Interaction. ACM, pp. 137-145; Quinn, P., Zhai, S., A cost-benefit study of text entry suggestion interaction (2016) Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16), pp. 83-88. , http://dx.doi.org/10.1145/2858036.2858305, ACM, New York, NY, USA; Sanchis-Trilles, G., Leiva, L.A., A systematic comparison of 3 phrase sampling methods for text entry experiments in 10 languages (2014) Proceedings of the International Conference on Human-computer Interaction with Mobile Devices and Services (MobileHCI); Schalk, G., Wolpaw, J.R., McFarland, D.J., Pfurtscheller, G., EEG-based communication: Presence of an error potential (2000) Clinical Neurophysiology, 111 (12), pp. 2138-2144. , (2000); Spüler, M., Bensch, M., Kleih, S., Rosenstiel, W., Bogdan, M., Kübler, A., Online use of error-related potentials in healthy users and people with severe motor impairment increases performance of a p300-bci (2012) Clinical Neurophysiology: Official Journal of the International Federation of Clinical Neurophysiology, 123 (7), pp. 1328-1337. , (2012); Stolcke, A., Srilm-an extensible language modeling toolkit (2002) Seventh International Conference on Spoken Language Processing; Suhm, B., Myers, B., Waibel, A., Multimodal error correction for speech user interfaces (2001) ACM Trans. Comput.-Hum. Interact., 8 (1), pp. 60-98. , (2001); Tinwala, H., Scott Mackenzie, I., Eyes-free text entry with error correction on touchscreen mobile devices (2010) Proceedings of the 6th Nordic Conference on Human-Computer Interaction: Extending Boundaries (NordiCHI '10), pp. 511-520. , ACM, New York, NY, USA; Vi, C., Subramanian, S., Detecting error-related negativity for interaction design (2012) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '12), , New York, USA; Voelker, M., Berberich, S., Andreev, E., Fiederer, L.D.J., Burgard, W., Ball, T., Between-subject transfer learning for classification of error-related signals in high-density EEG (2017) The First Biannual Neuroadaptive Technology Conference, 81, p. 47; Weir, D., Pohl, H., Rogers, S., Vertanen, K., Ola Kristensson, P., Uncertain text entry on mobile devices (2014) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '14), pp. 2307-2316. , ACM, New York, NY, USA},
sponsors={ACM SIGCHI},
publisher={Association for Computing Machinery},
isbn={9781450367080},
language={English},
abbrev_source_title={Conf Hum Fact Comput Syst Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hartmann2020135,
author={Hartmann, Y. and Liu, H. and Schultz, T.},
title={Feature space reduction for multimodal human activity recognition},
journal={BIOSIGNALS 2020 - 13th International Conference on Bio-Inspired Systems and Signal Processing, Proceedings; Part of 13th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2020},
year={2020},
pages={135-140},
note={cited By 0; Conference of 13th International Conference on Bio-Inspired Systems and Signal Processing, BIOSIGNALS 2020 - Part of 13th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2020 ; Conference Date: 24 February 2020 Through 26 February 2020;  Conference Code:158884},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083548627&partnerID=40&md5=3a52be9a6e451269ee0e78ad25adc8f9},
affiliation={Cognitive Systems Lab, University of Bremen, Germany},
abstract={This work describes the implementation, optimization, and evaluation of a Human Activity Recognition (HAR) system using 21-channel biosignals. These biosignals capture multiple modalities, such as motion and muscle activity based on two 3D-inertial sensors, one 2D-goniometer, and four electromyographic sensors. We start with an early fusion, HMM-based recognition system which discriminates 18 human activities at 91% recognition accuracy. We then optimize preprocessing with a feature space reduction and feature vector stacking. For this purpose, a Linear Discriminant Analysis (LDA) was performed based on HMM state alignments. Our experimental results show that LDA feature space reduction improves recognition accuracy by four percentage points while stacking feature vectors currently does not show any positive effects. To the best of our knowledge, this is the first work on feature space reduction in a HAR system using various biosensors integrated into a knee bandage recognizing a diverse set of activities. Copyright © 2020 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.},
author_keywords={Biosensors;  Feature Space Reduction;  Human Activity Recognition;  Multi-channel Signal Processing;  Stacking},
keywords={Biomedical engineering;  Biomimetics;  Discriminant analysis;  Joints (anatomy);  Pattern recognition;  Vector spaces, Electromyographic;  HMM based recognition systems;  Human activity recognition;  Linear discriminant analysis;  Multiple modalities;  Muscle activities;  Percentage points;  Recognition accuracy, Signal processing},
references={Amma, C., Gehrig, D., Schultz, T., Airwriting recognition using wearable motion sensors (2010) First Augmented Human International Conference, p. 10; Fleischer, C., Reinicke, C., Predicting the intended motion with emg signals for an exoskeleton orthosis controller (2005) 2005 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2005), pp. 2029-2034; Hellmers, S., Kromke, T., Dasenbrock, L., Heinks, A., Bauer, J., Hein, A., Fudickar, S., (2017) Stair Climb Power Measurements Via Inertial Measurement Units, pp. 1-9; Hu, H., Zahorian, S.A., Dimensionality reduction methods for HMM phonetic recognition (2010) 2010 IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 4854-4857; Liu, H., Schultz, T., ASK: A framework for data acquisition and activity recognition (2018) 11th International Conference on Bio-Inspired Systems and Signal Processing, pp. 262-268. , Madeira, Portugal; Liu, H., Schultz, T., A Wearable Real-time Human Activity Recognition System using Biosensors Integrated into a Knee Bandage (2019) Proceedings of the 12th International Joint Conference on Biomedical Engineering Systems and Technologies, pp. 47-55. , SCITEPRESS Science and Technology Publications; Liu, X., Zhou, Z., Mai, J., Wang, Q., Multiclass SVM based real-time recognition of sit-to-stand and stand-to-sit transitions for a bionic knee exoskeleton in transparent mode (2017) The Semantic Web - ISWC 2015, pp. 262-272. , Springer International Publishing, Cham; Lukowicz, P., Ward, J.A., Junker, H., Stäger, M., Tröster, G., Atrash, A., Starner, T., Recognizing workshop activity using body worn microphones and accelerometers (2004) Pervasive Computing, pp. 18-32. , Springer, Berlin, Heidelberg, Berlin, Heidelberg; Mathie, M., Coster, A., Lovell, N., Celler, B., Detection of daily physical activities using a triaxial accelerometer (2003) Medical and Biological Engineering and Computing, 41 (3), pp. 296-301; Mezghani, N., Fuentes, A., Gaudreault, N., Mitiche, A., Aissaoui, R., Hagmeister, N., de Guise, J.A., Identification of knee frontal plane kinematic patterns in normal gait by principal component analysis (2013) Journal of Mechanics in Medicine and Biology, 13 (3); Mezghani, N., Ouakrim, Y., Fuentes, A., Mitiche, A., Hagemeister, N., Vendittoli, P.-A., de Guise, J.A., Mechanical biomarkers of medial compartment knee osteoarthritis diagnosis and severity grading: Discovery phase (2017) Journal of Biomechanics, 52, pp. 106-112; Rebelo, D., Amma, C., Gamboa, H., Schultz, T., Activity recognition for an intelligent knee orthosis (2013) 6th International Conference on Bio-Inspired Systems and Signal Processing, pp. 368-371. , BIOSIGNALS 2013; Teague, C.N., Hersek, S., Toreyin, H., Millard-Stafford, M.L., Jones, M.L., Kogler, G.F., Sawka, M.N., Inan, O.T., Novel methods for sensing acoustical emissions from the knee for wearable joint health assessment (2016) IEEE Transactions on Biomedical Engineering, 63 (8), pp. 1581-1590; Whittle, M.W., Clinical gait analysis: A review (1996) Human Movement Science, 15 (3), pp. 369-387; Whittle, M.W., (2014) Gait Analysis, , Normal Gait. Butterworth-Heinemann},
editor={Vilda P.G., Fred A., Gamboa H.},
sponsors={Institute for Systems and Technologies of Information, Control and Communication (INSTICC)},
publisher={SciTePress},
isbn={9789897583988},
language={English},
abbrev_source_title={BIOSIGNALS - Int. Conf. Bio-Inspired Syst. Signal Process., Proc.; Part Int. Jt. Conf. Biomed. Eng. Syst. Technol., BIOSTEC},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Barandas2020,
author={Barandas, M. and Folgado, D. and Fernandes, L. and Santos, S. and Abreu, M. and Bota, P. and Liu, H. and Schultz, T. and Gamboa, H.},
title={TSFEL: Time Series Feature Extraction Library},
journal={SoftwareX},
year={2020},
volume={11},
doi={10.1016/j.softx.2020.100456},
art_number={100456},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081936815&doi=10.1016%2fj.softx.2020.100456&partnerID=40&md5=575d14a61224c3a726ef4dad4d626c33},
affiliation={Associação Fraunhofer Portugal Research, Rua Alfredo Allen 455/461, Porto, Portugal; Cognitive Systems Lab, University of Bremen, Bremen, Germany; Laboratório de Instrumentação, Engenharia Biomédica e Física da Radiação (LIBPhys-UNL), Departamento de Física, Faculdade de Ciê,ncias e Tecnologia da Universidade Nova de Lisboa, Monte da Caparica, Caparica, 2892-516, Portugal},
abstract={Time series feature extraction is one of the preliminary steps of conventional machine learning pipelines. Quite often, this process ends being a time consuming and complex task as data scientists must consider a combination between a multitude of domain knowledge factors and coding implementation. We present in this paper a Python package entitled Time Series Feature Extraction Library (TSFEL), which computes over 60 different features extracted across temporal, statistical and spectral domains. User customisation is achieved using either an online interface or a conventional Python package for more flexibility and integration into real deployment scenarios. TSFEL is designed to support the process of fast exploratory data analysis and feature extraction on time series with computational cost evaluation. © 2020},
author_keywords={Feature extraction;  Machine learning;  Python;  Time series},
keywords={Extraction;  Feature extraction;  High level languages;  Learning systems;  Machine learning;  Time series, Computational costs;  Conventional machines;  Deployment scenarios;  Domain knowledge;  Exploratory data analysis;  Online interface;  Python;  Time series features, Time series analysis},
funding_details={Programa Operacional TemÃ¡tico Factores de CompetitividadePrograma Operacional TemÃ¡tico Factores de Competitividade, POFC},
funding_details={European CommissionEuropean Commission, EC, POCI-01-0247-FEDER-038436},
funding_details={European Regional Development FundEuropean Regional Development Fund, FEDER},
funding_text 1={We would like to acknowledge the financial support obtained from the project Total Integrated and Predictive Manufacturing System Platform for Industry 4.0, co-funded by Portugal 2020, framed under the COMPETE 2020 (Operational Programme Competitiveness and Internationalisation) and European Regional Development Fund (ERDF) from European Union (EU), with operation code POCI-01-0247-FEDER-038436 .},
references={Domingos, P., A few useful things to know about machine learning (2012) Commun ACM, 55 (10), p. 78. , http://dl.acm.org/citation.cfm?doid=2347736.2347755, URL:; Nun, I., Protopapas, P., Sim, B., Zhu, M., Dave, R., Castro, N., Pichara, K., FATS: Feature analysis for time series (2015), [astro-ph]; Naul, B., van der Walt, S., Crellin-Quick, A., Bloom, J.S., Pérez, F., Cesium: Open-source platform for time-series inference (2016), [cs]; Christ, M., Braun, N., Neuffer, J., Kempa-Liehr, A.W., Time series feature extraction on basis of scalable hypothesis tests (tsfresh – a python package) (2018) Neurocomputing, 307, pp. 72-77. , https://linkinghub.elsevier.com/retrieve/pii/S0925231218304843, URL; Fulcher, B., Jones, N., Hctsa: A computational framework for automated time-series phenotyping using massive feature extraction (2017) Cell Syst, 5, pp. 527-531; Machado, I.P., Luísa Gomes, A., Gamboa, H., Paixão, V., Costa, R.M., Human activity data discovery from triaxial accelerometer sensor: Non-supervised learning sensitivity to feature extraction parametrization (2015) Inf Process Manage, 51 (2), pp. 204-214. , https://linkinghub.elsevier.com/retrieve/pii/S0306457314000685, URL; Figueira, C., Matias, R., Gamboa, H., Body location independent activity monitoring (2016) Proceedings of the 9th international joint conference on biomedical engineering systems and technologies, pp. 190-197. , http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0005699601900197, SCITEPRESS - Science and and Technology Publications Rome, Italy URL; Abreu., M., Barandas., M., Leonardo., R., Gamboa., H., Detailed human activity recognition based on multiple HMM (2019) Proceedings of the 12th international joint conference on biomedical engineering systems and technologies - Volume 4: BIOSIGNALS, pp. 171-178. , SciTePress; Bota, P., Silva, J., Folgado, D., Gamboa, H., A semi-automatic annotation approach for human activity recognition (2019) Sensors, 19 (3), p. 501. , http://www.mdpi.com/1424-8220/19/3/501, URL; Liu, H., Schultz, T., Ask: A framework for data acquisition and activity recognition (2018) Proceedings of the 11th international joint conference on biomedical engineering systems and technologies - Volume 4 BIOSIGNALS: BIOSIGNALS, pp. 262-268. , SciTePress; Pereira, A., Folgado, D., Cotrim, R., Sousa, I., Physiotherapy exercises evaluation using a combined approach based on semg and wearable inertial sensors: (2019) Proceedings of the 12th international joint conference on biomedical engineering systems and technologies, pp. 73-82. , http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0007391300730082, SCITEPRESS - Science and Technology Publications Prague, Czech Republic URL; Liu, H., Schultz, T., A wearable real-time human activity recognition system using biosensors integrated into a knee bandage (2019) Proceedings of the 12th international joint conference on biomedical engineering systems and technologies - Volume 3: BIODEVICES, pp. 47-55. , SciTePress; Varandas, R., Folgado, D., Gamboa, H., Evaluation of spatial-temporal anomalies in the analysis of human movement: (2019) Proceedings of the 12th international joint conference on biomedical engineering systems and technologies, pp. 163-170. , http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0007386701630170, SCITEPRESS - Science and Technology Publications Prague, Czech Republic URL; Zwillinger, D., Kokoska, S., CRC standard probability and statistics tables and formulae (2000), Chapman & Hall New York; Oliphant, T.E., A guide to NumPy, Vol. 1 (2006), Trelgol Publishing USA; Raschka, S., Mlxtend: Providing machine learning and data science utilities and extensions to python's scientific computing stack (2018) J Open Source Softw, 3 (24). , http://joss.theoj.org/papers/10.21105/joss.00638, URL; Welch, P.D., The use of fast fourier transform for the estimation of power spectra: a method based on time averaging over short, modified periodograms (1967) IEEE Trans Audio Electroacoust, 15 (2), pp. 70-73; Peeters, G., Giordano, B., Susini, P., Misdariis, N., Mcadams, S., The timbre toolbox: Extracting audio descriptors from musical signals (2011) J Acoust Soc Am, 130, pp. 2902-2916; Fernandes, L.M.S., Learning human behaviour patterns by trajectory and activity recognition (2019), (Master's thesis) FCT NOVA; Das, O., Speaker Recognition: Tech. rep. (2016), https://ccrma.stanford.edu/ orchi/Documents/speaker_recognition_report.pdf, URL; Henriksson, U., Power Spectrum and Bandwidth: Tech. rep. (2003), https://www.commsys.isy.liu.se/TSDT45/Material/UlfsSpectrum2003.pdf, Linköping University URL; Pan, Y., Chen, J., Li, X., Spectral entropy: A complementary index for rolling element bearing performance degradation assessment (2009) Proceedings Inst Mech Eng C, 223, pp. 1223-1231; Yan, B., Miyamoto, A., Brühwiler, E., Wavelet transform-based modal parameter identification considering uncertainty (2006) J Sound Vib, 291 (1), pp. 285-301. , http://www.sciencedirect.com/science/article/pii/S0022460X05003913, URL; Kocaman, Ç., Özdemir, M., Comparison of statistical methods and wavelet energy coefficients for determining two common PQ disturbances: Sag and swell (2009) IEEE International Conference on Electrical and Electronics Engineering, pp. I-80I84},
correspondence_address1={Barandas, M.; Associação Fraunhofer Portugal Research, Rua Alfredo Allen 455/461, Portugal; email: marilia.barandas@fraunhofer.pt},
publisher={Elsevier B.V.},
issn={23527110},
language={English},
abbrev_source_title={SoftwareX},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Diener2019682,
author={Diener, L. and Umesh, T. and Schultz, T.},
title={Improving Fundamental Frequency Generation in EMG-To-Speech Conversion Using a Quantization Approach},
journal={2019 IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2019 - Proceedings},
year={2019},
pages={682-689},
doi={10.1109/ASRU46091.2019.9003804},
art_number={9003804},
note={cited By 0; Conference of 2019 IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2019 ; Conference Date: 15 December 2019 Through 18 December 2019;  Conference Code:157953},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081538736&doi=10.1109%2fASRU46091.2019.9003804&partnerID=40&md5=26ef507eadb81cf1358aa03c92d4c323},
affiliation={Cognitive Systems Lab, University of Bremen, Germany},
abstract={We present a novel approach to generating fundamental frequency (intonation and voicing) trajectories in an EMG-To-Speech conversion Silent Speech Interface, based on quantizing the EMG-To-F0 mappings target values and thus turning a regression problem into a recognition problem. We present this method and evaluate its performance with regard to the accuracy of the voicing information obtained as well as the performance in generating plausible intonation trajectories within voiced sections of the signal. To this end, we also present a new measure for overall F0 trajectory plausibility, the trajectory-label accuracy (TLAcc), and compare it with human evaluations. Our new F0 generation method achieves a significantly better performance than a baseline approach in terms of voicing accuracy, correlation of voiced sections, trajectory-label accuracy and, most importantly, human evaluations. © 2019 IEEE.},
author_keywords={electromyography;  EMG-To-Speech;  F0;  intonation;  Silent Speech Interfaces},
keywords={Electromyography;  Natural frequencies;  Speech;  Trajectories, Fundamental frequencies;  Generation method;  Human evaluation;  intonation;  Quantization approach;  Regression problem;  Silent speech interfaces;  Speech conversion, Speech recognition},
references={Gŕosz, T., Gosztolya, G., Tóth, L., Ǵabor Csaṕo, T., Marḱo, A., F0 estimation for dnn-based ultrasound silent speech interfaces (2018) 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, pp. 291-295; Fabre, D., Hueber, T., Girin, L., Alameda-Pineda, X., Badin, P., Automatic animation of an articulatory tongue model from ultrasound images of the vocal tract Speech Communication, 93 (2017), pp. 63-75; Gonzalez, J.A., Cheah, L.A., Gilbert, J.M., Bai, J., Ell, S.R., Green, P.D., Moore, R.K., A silent speech system based on permanent magnet articulography and direct synthesis (2016) Computer Speech & Language, 39, pp. 67-87; Birkholz, P., Stone, S., Wolf, K., Plettemeier, D., Wolf, K., Plettemeier, D., Non-invasive silent phoneme recognition using microwave signals (2018) IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), 26 (12), pp. 2404-2411; Diener, L., Schultz, T., Investigating objective intelligibility in real-Time emg-To-speech conversion (2018) INTERSPEECH 2018-19th Annual Conference of the International Speech Communication Association; Kapur, A., Kapur, S., Maes, P., Alterego: A personalized wearable silent speech interface (2018) 23rd International Conference on Intelligent User Interfaces ACM, pp. 43-53; Toda, T., Shikano, K., Nam-To-speech conversion with gaussian mixture models (2005) Proceedings of the Annual Conference of the International Speech Communication Association, pp. 1957-1960; Herff, C., Johnson, G., Diener, L., Shih, J., Krusienski, D., Schultz, T., Towards direct speech synthesis from ecog: A pilot study (2016) Engineering in Medicine and Biology Society (EMBC) 2016 38th Annual International Conference of the IEEE; Van Den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Kavukcuoglu, K., (2016) Wavenet: A Generative Model for Raw Audio, , arXiv preprint arXiv 1609.03499; Janke, M., Diener, L., Emg-To-speech: Direct generation of speech from facial electromyographic signals IEEE/ACM Transactions on Audio, Speech and Language Processing, 25 (12), pp. 2375-2385. , nov 2017; Stan, S., Tanja Schultz, J., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proceedings of the 9th ISCA International Conference on Spoken Language Processing; Imai, S., Cepstral analysis synthesis on the mel frequency scale (1983) IEEE International Conference on Acoustics, Speech and Signal Processing, 8, pp. 93-96; De Cheveigńe, A., Kawahara, H., Yin, a fundamental frequency estimator for speech and music (2002) The Journal of the Acoustical Society of America, 111 (4), pp. 1917-1930; Heckbert, P., (1982) Color Image Quantization for Frame Buffer Display, 16. , ACM; Erro, D., Sainz, I., Navas, E., Hernaez, I., Harmonics plus noise model based vocoder for statistical parametric speech synthesis (2014) IEEE Journal of Selected Topics in Signal Processing, 8 (2), pp. 184-194; Diener, L., Janke, M., Schultz, T., Direct conversion from facial myoelectric signals to speech using deep neural networks (2015) International Joint Conference on Neural Networks, pp. 1-7; Matthew, D., Zeiler, (2012) Adadelta: An Adaptive Learning Rate Method, , arXiv preprint arXiv 1212.5701; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-Audible speech recognition using surface electromyography (2005) Proceedings of the Automatic Speech Recognition and Understanding Workshop; Wand, M., Schultz, T., Session-independent emg-based speech recognition (2011) International Conference on Bio-inspired Systems and Signal Processing; Wand, M., Janke, M., Schultz, T., The emg-uka corpus for electromyographic speech processing (2014) The 15th Annual Conference of the International Speech Communication Association, Singapore, , Interspeech 2014; Cavanagh, P.R., Komi, P.V., Electromechanical delay in human skeletal muscle under concentric and eccentric contractions (1979) European Journal of Applied Physiology and Occupational Physiology, 42 (3), pp. 159-163; Kominek, J., Black, A.W., The cmu arctic speech databases (2004) Fifth ISCA Workshop on Speech Syn, , thesis; Garofolo, J.S., Lamel, L.F., Fisher, W.M., Fiscus, J.G., Pallett, D.S., Darpa timit acoustic-phonetic continous speech corpus cd-rom. Nist speech disc 1-1.1 (1993) NASA STI/Recon Technical Report N, 93; Radiocommunication Bureau, I., Method for the subjective assessment of intermediate quality level of coding systems (2001) Recommendation ITU-R BS, 1534; Kraft, S., Zölzer, U., Beaqlejs: Html5 and javascript based framework for the subjective evaluation of audio quality (2014) Linux Audio Conference, Karlsruhe, de},
sponsors={IEEE Signal Processing Society; The Institute of Electrical and Electronics Engineers (IEEE)},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781728103068},
language={English},
abbrev_source_title={IEEE Autom. Speech Recognit. Underst. Workshop, ASRU - Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Weiner2019674,
author={Weiner, J. and Frankenberg, C. and Schroder, J. and Schultz, T.},
title={Speech reveals future risk of developing dementia: Predictive dementia screening from biographic interviews},
journal={2019 IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2019 - Proceedings},
year={2019},
pages={674-681},
doi={10.1109/ASRU46091.2019.9003908},
art_number={9003908},
note={cited By 1; Conference of 2019 IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2019 ; Conference Date: 15 December 2019 Through 18 December 2019;  Conference Code:157953},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081579939&doi=10.1109%2fASRU46091.2019.9003908&partnerID=40&md5=5d79b66d1b94441bf57638525786abea},
affiliation={Cognitive Systems Lab, University of Bremen, Germany; Section of Geriatric Psychiatry, University of Heidelberg, Germany},
abstract={Alzheimer's disease is a progressive incurable condition for which the success of any symptomatic therapy depends crucially on the starting time. Ideally it starts before the disease has caused any cognitive impairments. Our work aims at developing speech-based dementia screening methods that detect dementia as early as possible. Here, we aim to predict the outbreak even before clinical screening tests can diagnose the disease. Using the longitudinal ILSE study, we automatically extract features from biographic interviews and predict the development of dementia 5 and 12 years into the future. Our prediction system achieves results of 73.3% and 75.7% unweighted average recall (UAR), respectively, which clearly outperform a prediction based on prior diagnoses or disease prevalence. Thus, the automated analysis of spoken interviews offers a highly effective prediction procedure that allows for easy-To-use, cost-effective casual testing. © 2019 IEEE.},
author_keywords={dementia screening;  ILSE;  predictive screening},
keywords={Cost effectiveness;  Diagnosis;  Disease control;  Forecasting;  Neurodegenerative diseases, Alzheimer's disease;  Automated analysis;  Cognitive impairment;  Dementia screenings;  ILSE;  Prediction systems;  Prediction-based;  Screening tests, Speech recognition},
funding_details={Deutsche ForschungsgemeinschaftDeutsche Forschungsgemeinschaft, DFG, DFG SCHR 471/5-1, SCHU 2452/11-1},
funding_text 1={The present study was supported by the German research foundation (DFG SCHR 471/5-1 and SCHU 2452/11-1).},
references={(2012) Rld Health Organization and Alzheimers Disease International, , Dementia: A public health priority, World Health Organization; (2016) Demenzen, , Deutsche Gesellschaft fur Psychiatrie und Psychotherapie, Psychosomatik und Nervenheilkunde (DGPPN) and Deutsche Gesellschaft fur Neurologie (DGN S3-Leitlinie; Prince, M., Wimo, A., Guerchet, M., Ali, G., Wu, Y., Prina, M., (2015) World Alzheimer Report 2015 the Global Impact of Dementia: An Analysis of Prevalence, Incidence, Cost and Trends, Alzheimers Disease International, London; Appell, J., Kertesz, A., Fisman, M., A study of language functioning in Alzheimer patients (1982) Brain and Language, 17 (1), pp. 73-91; Bucks, R.S., Singh, S., Cuerden, J.M., Kwilcock, G., Analysis of spontaneous, conversational speech in dementia of Alzheimer type: Evaluation of an objective technique for analysing lexical performance (2000) Aphasiology, 14 (1), pp. 71-91; Espinoza-Cuadros, F., Garcia-Zamora, M.A., Torres-Boza, D., Ferrer-Riesgo, C.A., Montero-Benavides, A., Gonzalez-Moreira, E., Hernandez-Gomez, L.A., A spoken language database for research on moderate cognitive impairment: Design and preliminary analysis (2014) Advances in Speech and Language Technologies for Iberian Languages, pp. 219-228. , Juan Luis Navarro Mesa, Alfonso Ortega, Antonio Teixeira, Eduardo Hernandez Perez, Pedro Quintana Morales, Antonio Ravelo Garcia, Ivan Guerra Moreno, and Doroteo T. Toledano, Eds., Cham Springer International Publishing; Khodabakhsh, A., Yesil, F., Guner, E., Demiroglu, C., Evaluation of linguistic and prosodic features for detection of Alzheimers disease in Turkish conversational speech (2015) EURASIP Journal on Audio, Speech, and Music Processing, 2015 (1), pp. 1-15; Sadeghian, R., David Schaffer, J., Zahorian, S.A., Speech Processing Approach for Diagnosing Dementia in an Early Stage (2017) INTERSPEECH 2017-18th Annual Conference of the International Speech Communication Association, pp. 2705-2709; Weiner, J., Schultz, T., Selecting Features for Automatic Screening for Dementia Based on Speech (2018) Speech and Computer, pp. 747-756. , Springer International Publishing Alexey Karpov, Oliver Jokisch, and Rodmonga Potapova, Eds., Cham; Toth, L., Gosztolya, G., Vincze, V., Hoffmann, I., Szatloczki, G., Automatic detection of mild cognitive impairment from spontaneous speech using ASR (2015) INTERSPEECH 2015-16th Annual Conference of the International Speech Communication Association, pp. 2694-2698; Tucker Prudhommeaux, E., Roark, B., Extraction of narrative recall patterns for neuropsychological assessment (2011) INTERSPEECH 2011-12th Annual Conference of the International Speech Communication Association, pp. 3021-3024; Zhou, L., Fraser, K.C., Rudzicz, F., Speech Recognition in Alzheimers Disease and in its Assessment (2016) INTERSPEECH 2016-17th Annual Conference of the International Speech Communication Association, pp. 1948-1952; Wankerl, S., Noth, E., Evert, S., An Analysis of Perplexity to Reveal the Effects of Alzheimers Disease on Language (2016) 12th ITG Conference on Speech Communication, pp. 254-258; Mirheidari, B., Blackburn, D., Walker, T., Reuber, M., Christensen, H., Dementia detection using automatic analysis of conversations (2019) Computer Speech & Language, 53, pp. 65-79; Gosztolya, G., Vincze, V., Toth, L., Pakaski, M., Kalman, J., Hoffmann, I., Identifying Mild Cognitive Impairment and mild Alzheimers disease based on spontaneous speech using ASR and linguistic features (2019) Computer Speech & Language, 53, pp. 181-197; Weiner, J., Schultz, T., Automatic Screening for Transition into Dementia using Speech (2018) 13th ITG Conference on Speech Communication; Sattler, C., Wahl, H., Schroder, J., Kruse, A., Schonknecht, P., Kunzmann, U., Braun, T., Zenthofer, A., (2017) Interdisciplinary Longitudinal Study on Adult Development and Aging (ILSE, pp. 1213-1222. , Springer, Singapore; Weiner, J., Frankenberg, C., Telaar, D., Wendelstein, B., Schroder, J., Schultz, T., Towards Automatic Transcription of ILSE-An Interdisciplinary Longitudinal Study of Adult Development and Aging (2016) Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC16, pp. 718-725; Martin, P., Martin, M., Design und Methodik der Interdisziplinaren Langsschnittstudie des Erwachsenenalters (2000) Aspekte der Entwicklung im Mittleren und Hoheren Lebensalter: Ergebnisse der Interdisziplinaren Langsschnittstudie des Erwachsenenalters (ILSE), pp. 17-27. , Peter Martin, Klaus Udo Ettrich, Ursula Lehr, Dorothea Roether, Mike Martin, and Antje Fischer-Cyrulies, Eds Steinkopff; Weiner, J., Engelbart, M., Schultz, T., Manual and Automatic Transcription in Dementia Detection from Speech (2017) INTERSPEECH 2017-18th Annual Conference of the International Speech Communication Association, pp. 3117-3121; Schuller, B., Steidl, S., Batliner, A., Hirschberg, J., Burgoon, J.K., Baird, A., Elkins, A., Evanini, K., The INTERSPEECH 2016 Computational Paralinguistics Challenge: Deception, Sincerity & Native Language (2016) INTERSPEECH 2016-17th Annual Conference of the International Speech Communication Association, pp. 2001-2005; Weiner, J., Herff, C., Schultz, T., Speech-Based Detection of Alzheimers Disease in Conversational German (2016) INTERSPEECH 2016-17th Annual Conference of the International Speech Communication Association; Dehak, N., Kenny, P.J., Dehak, R., Dumouchel, P., Ouellet, P., Front-end factor analysis for speaker verification (2011) IEEE Transactions on Audio, Speech, and Language Processing, 19 (4), pp. 788-798; Shum, S.H., Dehak, N., Dehak, R., Glass, J.R., Unsupervised methods for speaker diarization: An integrated and iterative approach (2013) IEEE Transactions on Audio, Speech, and Language Processing, 21 (10), pp. 2015-2028; Brunet, E., (1978) Le Vocabulaire de Jean Giraudoux, , Structure et evolution, Slatkine, Geneva; Tweedie, F.J., Harald Baayen, R., How Variable May a Constant be? Measures of Lexical Richness in Perspective (1998) Computers and the Humanities, 32 (5), pp. 323-352; Honore, A., Some Simple Measures of Richness of Vocabulary (1979) Association for Literary and Linguistic Computing Bulletin, 7 (2), pp. 172-177; Schmid, H., Improvements in Part-of-Speech Tagging with an Application to German (1995) Proceedings of the ACL SIGDAT-Workshop, pp. 47-50; Schiller, A., Teufel, S., Stockert, C., (1999) Guidelines fur das Tagging Deutscher Textcorpora Mit STTS (Kleines und Groes Tagset); Westpfahl, S., Schmidt, T., FOLK-Gold A GOLD standard for Part-of-Speech Tagging of Spoken German (2016) Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC16, pp. 1493-1499; Pennebaker, J.W., Francis, M.E., Booth, R.J., Linguistic inquiry and word count: LIWC 2001 (2001) Erlbaum; Wolf, M., Horn, A.B., Mehl, M.R., Haug, S., Pennebaker, J.W., Kordy, H., Computergesttzte quantitative Textanalyse (2008) Diagnostica, 54 (2), pp. 85-98; Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Duchesnay, E., Scikit-learn: Machine Learning in Python (2011) Journal of Machine Learning Research, 12, pp. 2825-2830},
sponsors={IEEE Signal Processing Society; The Institute of Electrical and Electronics Engineers (IEEE)},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781728103068},
language={English},
abbrev_source_title={IEEE Autom. Speech Recognit. Underst. Workshop, ASRU - Proc.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Herff2019,
author={Herff, C. and Diener, L. and Angrick, M. and Mugler, E. and Tate, M.C. and Goldrick, M.A. and Krusienski, D.J. and Slutzky, M.W. and Schultz, T.},
title={Generating Natural, Intelligible Speech From Brain Activity in Motor, Premotor, and Inferior Frontal Cortices},
journal={Frontiers in Neuroscience},
year={2019},
volume={13},
doi={10.3389/fnins.2019.01267},
art_number={1267},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076675517&doi=10.3389%2ffnins.2019.01267&partnerID=40&md5=f2e0f04a0a9d45c1b072c813a41b1bc5},
affiliation={School of Mental Health Neuroscience, Maastricht University, Maastricht, Netherlands; Cognitive Systems Lab, University of Bremen, Bremen, Germany; Department of Neurology, Northwestern University, Chicago, IL, United States; Department of Neurosurgery, Northwestern University, Chicago, IL, United States; Department of Linguistics, Northwestern University, Chicago, IL, United States; Biomedical Engineering Department, Virginia Commonwealth University, Richmond, VA, United States; Department of Physiology, Northwestern University, Chicago, IL, United States; Department of Physical Medicine Rehabilitation, Northwestern University, Chicago, IL, United States},
abstract={Neural interfaces that directly produce intelligible speech from brain activity would allow people with severe impairment from neurological disorders to communicate more naturally. Here, we record neural population activity in motor, premotor and inferior frontal cortices during speech production using electrocorticography (ECoG) and show that ECoG signals alone can be used to generate intelligible speech output that can preserve conversational cues. To produce speech directly from neural data, we adapted a method from the field of speech synthesis called unit selection, in which units of speech are concatenated to form audible output. In our approach, which we call Brain-To-Speech, we chose subsequent units of speech based on the measured ECoG activity to generate audio waveforms directly from the neural recordings. Brain-To-Speech employed the user's own voice to generate speech that sounded very natural and included features such as prosody and accentuation. By investigating the brain areas involved in speech production separately, we found that speech motor cortex provided more information for the reconstruction process than the other cortical areas. © Copyright © 2019 Herff, Diener, Angrick, Mugler, Tate, Goldrick, Krusienski, Slutzky and Schultz.},
author_keywords={BCI;  brain-computer interface;  brain-to-speech;  ECoG;  speech;  synthesis},
keywords={adult;  Article;  audio recording;  brain computer interface;  brain mapping;  brain region;  clinical article;  controlled study;  deep learning;  electrocorticography;  electroencephalogram;  female;  frontal cortex;  human;  male;  middle aged;  motor cortex;  premotor cortex;  signal processing;  speech;  speech intelligibility;  waveform},
funding_details={Doris Duke Charitable FoundationDoris Duke Charitable Foundation, DDCF, 2011039},
funding_details={National Institutes of HealthNational Institutes of Health, NIH, 1321015, F32DC015708, R01NS094748},
funding_details={Bundesministerium fÃ¼r Bildung und FrauenBundesministerium fÃ¼r Bildung und Frauen, BMBF, 01GQ1602},
funding_details={National Center for Advancing Translational SciencesNational Center for Advancing Translational Sciences, NCATS, UL1TR000150, UL1TR001422},
funding_details={National Institute on Deafness and Other Communication DisordersNational Institute on Deafness and Other Communication Disorders, NIDCD, 1 F32 DC015708-01},
funding_details={National Science FoundationNational Science Foundation, NSF, 1608140},
funding_text 1={CH, DK, and TS acknowledge funding by BMBF (01GQ1602) and NSF (1608140) as part of the NSF/NIH/BMBF Collaborative Research in Computational Neuroscience Program. MS acknowledges funding by the Doris Duke Charitable Foundation (Clinical Scientist Development Award, grant 2011039), a Northwestern Memorial Foundation Dixon Translational Research Award (including partial funding from NIH National Center for Advancing Translational Sciences, UL1TR000150 and UL1TR001422), NIH grants F32DC015708 and R01NS094748, and NSF grant 1321015. EM acknowledges funding by the NIDCD (grant 1 F32 DC015708-01).},
references={Akbari, H., Khalighinejad, B., Herrero, J.L., Mehta, A.D., Mesgarani, N., Towards reconstructing intelligible speech from the human auditory cortex (2019) Sci. Rep, 9, p. 874. , 30696881; Angrick, M., Herff, C., Mugler, E., Tate, M.C., Slutzky, M.W., Krusienski, D.J., Speech synthesis from ecog using densely connected 3d convolutional neural networks (2019) J. Neural Eng, 16, p. 036019. , 30831567; Anumanchipalli, G.K., Chartier, J., Chang, E.F., Speech synthesis from neural decoding of spoken sentences (2019) Nature, 568, pp. 493-498. , 31019317; Black, A.W., Taylor, P.A., Automatically clustering similar units for unit selection in speech synthesis (1997) EUROSPEECH, pp. 601-604. , Rhodes; Bouchard, K.E., Mesgarani, N., Johnson, K., Chang, E.F., Functional organization of human sensorimotor cortex for speech articulation (2013) Nature, 495, pp. 327-332. , 23426266; Brumberg, J., Krusienski, D., Chakrabarti, S., Gunduz, A., Brunner, P., Ritaccio, A., Spatio-temporal progression of cortical activity related to continuous overt and covert speech production in a reading task (2016) PLoS ONE, 11, p. e0166872. , 27875590; Chartier, J., Anumanchipalli, G.K., Johnson, K., Chang, E.F., Encoding of articulatory kinematic trajectories in human speech sensorimotor cortex (2018) Neuron, 98, pp. 1042-1054. , 29779940; Crone, N.E., Boatman, D., Gordon, B., Hao, L., Induced electrocorticographic gamma activity during auditory perception (2001) Clin. Neurophysiol, 112, pp. 565-582. , 11275528; Dichter, B.K., Breshears, J.D., Leonard, M.K., Chang, E.F., The control of vocal pitch in human laryngeal motor cortex (2018) Cell, 174, pp. 21-31. , 29958109.e9; Glanz, O., Derix, J., Kaur, R., Schulze-Bonhage, A., Auer, P., Aertsen, A., Real-life speech production and perception have a shared premotor-cortical substrate (2018) Sci. Rep, 8, p. 8898; Guenther, F.H., Brumberg, J.S., Wright, E.J., Nieto-Castanon, A., Tourville, J.A., Panko, M., A wireless brain-machine interface for real-time speech synthesis (2009) PLoS ONE, 4, p. e8218. , 20011034; Herff, C., Heger, D., de Pesters, A., Telaar, D., Brunner, P., Schalk, G., Brain-to-text: decoding spoken phrases from phone representations in the brain (2015) Front. Neurosci, 9, p. 217. , 26124702; Herff, C., Schultz, T., Automatic speech recognition from neural signals: a focused review (2016) Front. Neurosci, 10, p. 429. , 27729844; Hermes, D., Miller, K.J., Noordmans, H.J., Vansteensel, M.J., Ramsey, N.F., Automated electrocorticographic electrode localization on individually rendered brain surfaces (2010) J. Neurosci. Methods, 185, pp. 293-298. , 19836416; Hickok, G., Computational neuroanatomy of speech production (2012) Nat. Rev. Neurosci, 13, pp. 135-145. , 22218206; Hochberg, L.R., Serruya, M.D., Friehs, G.M., Mukand, J.A., Saleh, M., Caplan, A.H., Neuronal ensemble control of prosthetic devices by a human with tetraplegia (2006) Nature, 442, pp. 164-171. , 16838014; House, A.S., Williams, C., Hecker, M.H., Kryter, K.D., Psychoacoustic speech tests: a modified rhyme test (1963) J. Acoust. Soc. Am, 35, p. 1899; Hunt, A.J., Black, A.W., Unit selection in a concatenative speech synthesis system using a large speech database (1996) Acoustics, Speech, and Signal Processing, 1996. ICASSP-96. Conference Proceedings., 1996 IEEE International Conference on, 1, pp. 373-376. , Atlanta, GA, IEEE; Kellis, S., Miller, K., Thomson, K., Brown, R., House, P., Greger, B., Decoding spoken words using local field potentials recorded from the cortical surface (2010) J. Neural Eng, 7, p. 056007. , 20811093; Kraft, S., Zölzer, U., Beaqlejs: Html5 and javascript based framework for the subjective evaluation of audio quality (2014) Linux Audio Conference, Karlsruhe, DE, , Karlsruhe, in; Kubanek, J., Brunner, P., Gunduz, A., Poeppel, D., Schalk, G., The tracking of speech envelope in the human cortex (2013) PLoS ONE, 8, p. e53398. , 23408924; Leuthardt, E., Pei, X.-M., Breshears, J., Gaona, C., Sharma, M., Freudenburg, Z., Temporal evolution of gamma activity in human cortex during an overt and covert word repetition task (2012) Front. Hum. Neurosci, 6, p. 99. , 22563311; Leuthardt, E.C., Gaona, C., Sharma, M., Szrama, N., Roland, J., Freudenberg, Z., Using the electrocorticographic speech network to control a brain–computer interface in humans (2011) J. Neural Eng, 8, p. 036004. , 21471638; Lotte, F., Brumberg, J.S., Brunner, P., Gunduz, A., Ritaccio, A.L., Guan, C., Electrocorticographic representations of segmental features in continuous speech (2015) Front. Hum. Neurosci, 9, p. 97. , 25759647; Lou, H.-L., Implementing the viterbi algorithm (1995) IEEE Signal Process. Magaz, 12, pp. 42-52; Martin, S., Brunner, P., Holdgraf, C., Heinze, H.-J., Crone, N., Rieger, J., Decoding spectrotemporal features of overt and covert speech from the human cortex (2014) Front. Neuroeng, 7, p. 14. , 24904404; Mesgarani, N., Cheung, C., Johnson, K., Chang, E.F., Phonetic feature encoding in human superior temporal gyrus (2014) Science, 343, pp. 1006-1010. , 24482117; Miller, K.J., Leuthardt, E.C., Schalk, G., Rao, R.P., Anderson, N.R., Moran, D.W., Spectral changes in cortical surface potentials during motor movement (2007) J. Neurosci, 27, pp. 2424-2432. , 17329441; Milsap, G., Collard, M., Coogan, C., Rabbani, Q., Wang, Y., Crone, N.E., Keyword spotting using human electrocorticographic recordings (2019) Front. Neurosci, 13, p. 60. , 30837823; Mines, M.A., Hanson, B.F., Shoup, J.E., Frequency of occurrence of phonemes in conversational english (1978) Lang. Speech, 21, pp. 221-241. , 732398; Moses, D.A., Leonard, M.K., Chang, E.F., Real-time classification of auditory sentences using evoked cortical activity in humans (2018) J. Neural Eng, 15, p. 036005. , 29378977; Moses, D.A., Mesgarani, N., Leonard, M.K., Chang, E.F., Neural speech recognition: continuous phoneme decoding using spatiotemporal representations of human cortical activity (2016) J. Neural Eng, 13, p. 056004. , 27484713; Moulines, E., Charpentier, F., Pitch-synchronous waveform processing techniques for text-to-speech synthesis using diphones (1990) Speech Commun, 9, pp. 453-467; Mugler, E.M., Patton, J.L., Flint, R.D., Wright, Z.A., Schuele, S.U., Rosenow, J., Direct classification of all american english phonemes using signals from functional speech motor cortex (2014) J. Neural Eng, 11, p. 035015. , 24836588; Mugler, E.M., Tate, M.C., Livescu, K., Templer, J.W., Goldrick, M.A., Slutzky, M.W., Differential representation of articulatory gestures and phonemes in precentral and inferior frontal gyri (2018) J. Neurosci, 38, pp. 9803-9813. , 30257858; Nguyen, H.V., Bai, L., Cosine similarity metric learning for face verification (2010) Asian Conference on Computer Vision, pp. 709-720. , Queenstown, Springer; Nuyujukian, P., Albites Sanabria, J., Saab, J., Pandarinath, C., Jarosiewicz, B., Blabe, C.H., Cortical control of a tablet computer by people with paralysis (2018) PLoS One, 13, p. e0204566. , 30462658; Okada, K., Matchin, W., Hickok, G., Phonological feature repetition suppression in the left inferior frontal gyrus (2018) J. Cogn. Neurosci, 30, pp. 1549-1557. , 29877763; Pandarinath, C., Nuyujukian, P., Blabe, C.H., Sorice, B.L., Saab, J., Willett, F.R., High performance communication by people with paralysis using an intracortical brain-computer interface (2017) Elife, 6, p. e18554. , 28220753; Pasley, B.N., David, S.V., Mesgarani, N., Flinker, A., Shamma, S.A., Crone, N.E., Reconstructing speech from human auditory cortex (2012) PLoS Biol, 10, p. e1001251. , 22303281; Ramsey, N., Salari, E., Aarnoutse, E., Vansteensel, M., Bleichner, M., Freudenburg, Z., Decoding spoken phonemes from sensorimotor cortex with high-density ecog grids (2017) Neuroimage, 180, pp. 301-311. , 28993231; Ray, S., Crone, N.E., Niebur, E., Franaszczuk, P.J., Hsiao, S.S., Neural correlates of high-gamma oscillations (60–200 hz) in macaque local field potentials and their potential implications in electrocorticography (2008) J. Neurosci, 28, pp. 11526-11536. , 18987189; Sahin, N.T., Pinker, S., Cash, S.S., Schomer, D., Halgren, E., Sequential processing of lexical, grammatical, and phonological information within brocas area (2009) Science, 326, pp. 445-449; Santoro, R., Moerel, M., De Martino, F., Valente, G., Ugurbil, K., Yacoub, E., Reconstructing the spectrotemporal modulations of real-life sounds from fmri response patterns (2017) Proc. Natl. Acad. Sci. U.S.A, 114, pp. 4799-4804. , 28420788; Schalk, G., McFarland, D.J., Hinterberger, T., Birbaumer, N., Wolpaw, J.R., Bci2000: a general-purpose brain-computer interface (bci) system (2004) IEEE Trans. Biomed. Eng, 51, pp. 1034-1043. , 15188875; Schultz, T., Wand, M., Hueber, T., Krusienski, D.J., Herff, C., Brumberg, J.S., Biosignal-based spoken communication: A survey (2017) IEEE ACM Trans. Audio Speech Lang. Process, 25, pp. 2257-2271; Stavisky, S.D., Rezaii, P., Willett, F.R., Hochberg, L.R., Shenoy, K.V., Henderson, J.M., Decoding speech from intracortical multielectrode arrays in dorsal arm/hand areas of human motor cortex (2018) 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), pp. 93-97. , a, Honululu, HI, IEEE; Stavisky, S.D., Willett, F.R., Murphy, B.A., Rezaii, P., Memberg, W.D., Miller, J.P., Neural ensemble dynamics in dorsal motor cortex during speech in people with paralysis (2018) bioRxiv, , b, 505487; Steinbach, M., Karypis, G., Kumar, V., A comparison of document clustering techniques (2000) KDD Workshop on Text Mining, 400, pp. 525-526. , Boston, MA; Stevens, S.S., Volkmann, J., Newman, E.B., A scale for the measurement of the psychological magnitude pitch (1937) J. Acoust. Soc. Am, 8, pp. 185-190; Stuart, A., Kalinowski, J., Rastatter, M.P., Lynch, K., Effect of delayed auditory feedback on normal speakers at two speech rates (2002) J. Acoust. Soc. Am, 111, pp. 2237-2241. , 12051443; Sundermann, D., Hoge, H., Bonafonte, A., Ney, H., Black, A., Narayanan, S., Text-independent voice conversion based on unit selection (2006) Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 2006 IEEE International Conference on, 1. , Toulouse, IEEE; Taal, C.H., Hendriks, R.C., Heusdens, R., Jensen, J., An algorithm for intelligibility prediction of time–frequency weighted noisy speech (2011) IEEE Trans. Audio Speech Lang Process, 19, pp. 2125-2136; Tian, X., Poeppel, D., Mental imagery of speech and movement implicates the dynamics of internal forward models (2010) Front. Psychol, 1, p. 166. , 21897822; Tourville, J.A., Guenther, F.H., The diva model: a neural theory of speech acquisition and production (2011) Lang. Cogn. Process, 26, pp. 952-981. , 23667281; Wang, W., Arora, R., Livescu, K., Bilmes, J.A., Unsupervised learning of acoustic features via deep canonical correlation analysis (2015) Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, pp. 4590-4594. , Brisbane, QLD, IEEE; Willett, F.R., Deo, D.R., Avansino, D.T., Rezaii, P., Hochberg, L., Henderson, J., Hand knob area of motor cortex in people with tetraplegia represents the whole body in a modular way (2019) bioRxiv, , 659839; Wolpaw, J., Birbaumer, N., McFarland, D., Pfurtscheller, G., Vaughan, T., Brain–computer interfaces for communication and control (2002) Clin. Neurophysiol, 113, pp. 767-791. , 12048038; Wu, Z., Virtanen, T., Kinnunen, T., Chng, E., Li, H., Exemplar-based unit selection for voice conversion utilizing temporal information (2013) INTERSPEECH, pp. 3057-3061. , Lyon; Zahner, M., Janke, M., Wand, M., Schultz, T., Conversion from facial myoelectric signals to speech: a unit selection approach (2014) Fifteenth Annual Conference of the International Speech Communication Association, , Singapore, in},
correspondence_address1={Herff, C.; School of Mental Health Neuroscience, Maastricht UniversityNetherlands; email: c.herff@maastrichtuniversity.nl},
publisher={Frontiers Media S.A.},
issn={16624548},
language={English},
abbrev_source_title={Front. Neurosci.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Prorokovic2019,
author={Prorokovic, K. and Wand, M. and Schultz, T. and Schmidhuber, J.},
title={Adaptation of an EMG-based speech recognizer via meta-learning},
journal={GlobalSIP 2019 - 7th IEEE Global Conference on Signal and Information Processing, Proceedings},
year={2019},
doi={10.1109/GlobalSIP45357.2019.8969231},
art_number={8969231},
note={cited By 0; Conference of 7th IEEE Global Conference on Signal and Information Processing, GlobalSIP 2019 ; Conference Date: 11 November 2019 Through 14 November 2019;  Conference Code:157155},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079287460&doi=10.1109%2fGlobalSIP45357.2019.8969231&partnerID=40&md5=976064d92301e2f3245feb401ca209b6},
affiliation={USI SUPSI, Istituto Dalle Molle di Studi sull'Intelligenza Artificiale (IDSIA), Manno-Lugano, Switzerland; University of Bremen, Cognitive Systems Lab, Bremen, Germany},
abstract={In nonacoustic speech recognition based on electromyography, i.e. on electrical muscle activity captured by noninvasive surface electrodes, differences between recording sessions are known to cause deteriorating system accuracy. Efficient adaptation of an existing system to an unseen recording session is therefore imperative for practical usage scenarios. We report on a meta-learning approach to pretrain a deep neural network frontend for a myoelectric speech recognizer in a way that it can be easily adapted to a new session. Fine-tuning this specially pretrained network yields lower Word Error Rates and higher frame accuracies than fine-tuning a conventionally pretrained network, without creating an increased computational burden on a possibly mobile device. © 2019 IEEE.},
author_keywords={Adaptation;  Electromyography;  Silent speech interfaces},
keywords={Audio recordings;  Deep learning;  Deep neural networks;  Electromyography;  Tuning, Adaptation;  Computational burden;  Deteriorating system;  Meta-learning approach;  Muscle activities;  Silent speech interfaces;  Speech recognizer;  Surface electrode, Speech recognition},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme, H2020, 687795},
funding_text 1={This research was partially supported by the H2020 project INPUT (grant #687795). This work used computational resources from the Swiss National-Supercomputing Centre (CSCS) under project ID d96.},
references={Schultz, T., Wand, M., Hueber, T., Krusienski, D.J., Herff, C., Brumberg, J.S., Biosignal-based spoken communication: A survey (2017) IEEE/ACM Transactions on Audio, Speech, and Language Processing, 25 (12), pp. 2257-2271; Afouras, T., Chung, J.S., Zisserman, A., Deep Lip Reading: A comparison of models and an online application (2018) Proc. Interspeech, pp. 3514-3518; Hueber, T., Bailly, G., Statistical conversion of silent articulation into audible speech using full-covariance HMM (2016) Computer Speech and Language, 36, pp. 274-293; Gonzalez, J.A., Cheah, L.A., Gomez, A.M., Green, P.D., Gilbert, J.M., Ell, S.R., Moore, R.K., Holdsworth, E., Direct speech reconstruction from articulatory sensor data by machine learning (2017) IEEE/ACM Transactions on Audio, Speech, and Language Processing, 25 (12), pp. 2362-2374; Meltzner, G.S., Heaton, J.T., Deng, Y., Luca, G.D., Roy, S.H., Kline, J.C., Development of semg sensors and algorithms for silent speech recognition (2018) Journal of Neural Engineering, 15 (4); Wand, M., (2014) Advancing Electromyographic Continuous Speech Recognition: Signal Preprocessing and Modeling, , Dissertation, Karlsruhe Institute of Technology; Diener, L., Herff, C., Janke, M., Schultz, T., An initial investigation into the real-time conversion of facial surface emg signals to audible speech (2016) Proc. EMBC; Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Silent speech interfaces (2010) Speech Communication, 52 (4), pp. 270-287; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proc. Interspeech, pp. 573-576; Wand, M., Schultz, T., Session-independent emg-based speech recognition (2011) Proc. Biosignals, pp. 295-300; Wand, M., Schultz, T., Towards real-life application of emg-based speech recognition by using unsupervised adaptation (2014) Proc. Interspeech, pp. 1189-1193; Wand, M., Schulte, C., Janke, M., Schultz, T., Compensation of recording position shifts for a myoelectric silent speech recognizer (2014) Proc. ICASSP, pp. 2113-2117; Wand, M., Schultz, T., Schmidhuber, J., Domain-adversarial training for session independent emg-based speech recognition (2018) Proc. Interspeech, pp. 3167-3171; Finn, C., Abbeel, P., Levine, S., Model-agnostic meta-learning for fast adaptation of deep networks (2017) Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1126-1135. , JMLR. org; Freitas, J., Teixeira, A., Días, M.S., Silva, S., (2017) An Introduction to Silent Speech Interfaces, , SpringerBriefs in Speech technology; Morse, M.S., Day, S.H., Trull, B., Morse, H., Use of myoelectric signals to recognize speech (1989) Proc. 11th Annual Conference of the IEEE Engineering in Medicine and Biology Society, pp. 1793-1794; Schultz, T., Wand, M., Modeling coarticulation in large vocabulary emg-based speech recognition (2010) Speech Communication, 52 (4), pp. 341-353; Toth, A., Wand, M., Schultz, T., Synthesizing Speech from Electromyography using Voice Transformation Techniques (2009) Proc. Interspeech, pp. 652-655; Lee, K.-S., Prediction of acoustic feature parameters using myoelectric signals (2010) IEEE Transactions on Biomedical Engineering, 57, pp. 1587-1595; Wand, M., Janke, M., Schultz, T., The emg-uka corpus for electromyographic speech processing (2014) Proc. Interspeech, pp. 1593-1597; Schünke, M., Schulte, E., Schumacher, U., (2006) Prometheus-Lernatlas der Anatomie, 3. , Stuttgart, New York: Thieme Verlag: Kopf und Neuroanatomie; Wand, M., Schmidhuber, J., Deep neural network frontend for continuous emg-based speech recognition (2016) Proc. Interspeech, pp. 3032-3036; Bourlard, H., Morgan, N., (1994) Connectionist Speech Recognition. A Hybrid Approach, , Kluwer Academic Publishers; Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A., Jaitly, N., Senior, A., Kingsbury, B., Deep neural networks for acoustic modeling in speech recognition (2012) IEEE Signal Processing Magazine, 29 (6), pp. 82-97; Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., Darrell, T., Deep domain confusion: Maximizing for domain invariance (2014) CoRR; Ustinova, E., Lempitsky, V., Learning deep embeddings with histogram loss (2016) Proc. NIPS; Ganin, Y., Lempitsky, V., Unsupervised domain adaptation by backpropagation (2015) Proc. ICML, pp. 1180-1189; French, R.M., Catastrophic forgetting in connectionist networks: Causes, consequences and solutions (1999) Trends in Cognitive Sciences, 3 (4), pp. 128-135; Schmidhuber, J., Evolutionary principles in self-referential learning, or on learning how to learn: The meta-meta hook (1987) Master's Thesis, Institut für Informatik, Technische Universität München; Schmidhuber, J., (1994) On Learning How to Learn Learning Strategies, , Fakultät für Informatik, Technische Universität München, Tech. Rep; Schmidhuber, J., Learning to control fast-weight memories: An alternative to recurrent nets (1992) Neural Computation, 4 (1), pp. 131-139; Schmidhuber, J., A self-referential weight matrix (1993) Proc. ICANN, pp. 446-451; Schmidhuber, J., Reducing the ratio between learning complexity and number of time varying variables in fully recurrent nets (1993) Proc. ICANN, pp. 460-463; Younger, S.A., Hochreiter, S., Conwell, P.R., Meta-learning with backpropagation (2001) Proc. IJCNN, 3. , IEEE; Andrychowicz, M., Denil, M., Gomez, S., Hoffman, M.W., Pfau, D., Schaul, T., Shillingford, B., Freitas, N.D., Learning to learn by gradient descent by gradient descent (2016) Advances in Neural Information Processing Systems, pp. 3981-3989; Ravi, S., Larochelle, H., Optimization as a Model for Few-Shot Learning (2017) Proc. ICLR; Nichol, A., Achiam, J., Schulman, J., (2018) On First-Order Meta-Learning Algorithms; Klejch, O., Fainberg, J., Bell, P., Learning to adapt: A meta-learning approach for speaker adaptation (2018) Proc. Interspeech, pp. 867-871; Bahl, L.R., De Souza, P.V., Gopalakrishnan, P.S., Nahamoo, D., Picheny, M.A., Decision trees for phonological rules in continuous speech (1991) Proc. ICASSP, pp. 185-188; Kingma, D.P., Ba, J., Adam: A method for stochastic optimization (2015) Proc. ICLR; Telaar, D., Wand, M., Gehrig, D., Putze, F., Amma, C., Heger, D., Vu, N.T., Schultz, T., Biokit-real-time decoder for biosignal processing (2014) Proc. Interspeech; Soltau, H., George Saon, G., Dynamic network decoding revisited (2009) Proc. ASRU, pp. 276-281; Yu, H., Waibel, A., Streamlining the front end of a speech recognizer (2000) Proc. ICSLP; Wand, M., Janke, M., Schultz, T., Tackling speaking mode varieties in emg-based speech recognition (2014) IEEE Transaction on Biomedical Engineering, 61 (10), pp. 2515-2526; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) Proc. ASRU, pp. 331-336; Hudgins, B., Parker, P., Scott, R., A new strategy for multifunction myoelectric control (1993) IEEE Transactions on Biomedical Engineering, 40, pp. 82-94; Shire, M.L., Relating frame accuracy with word error in hybrid ann-hmm asr (2001) Proc. Eurospeech},
sponsors={IEEE; IEEE Signal Processing Society},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781728127231},
language={English},
abbrev_source_title={GlobalSIP - IEEE Glob. Conf. Signal Inf. Process., Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Putze20192812,
author={Putze, F. and Weib, D. and Vortmann, L.-M. and Schultz, T.},
title={Augmented reality interface for smart home control using SSVEP-BCI and eye gaze},
journal={Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
year={2019},
volume={2019-October},
pages={2812-2817},
doi={10.1109/SMC.2019.8914390},
art_number={8914390},
note={cited By 4; Conference of 2019 IEEE International Conference on Systems, Man and Cybernetics, SMC 2019 ; Conference Date: 6 October 2019 Through 9 October 2019;  Conference Code:155660},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076787442&doi=10.1109%2fSMC.2019.8914390&partnerID=40&md5=a37948a74d8424537d54d71be99d7bb3},
affiliation={University of Bremen, Cognitive Systems Lab, Bremen, 28359, Germany},
abstract={In this paper, we investigate the integration of eye-tracking and a Brain-Computer Interface into an Augmented Reality system to control a smart home environment. Through a head-mounted display, we present context-dependent control elements which the user selects by directing attention towards them. We show that the combination of both modalities leads to the most robust detection of selections and an interface which is accepted by its users. © 2019 IEEE.},
keywords={Augmented reality;  Automation;  Computer control systems;  Eye tracking;  Helmet mounted displays, Augmented reality systems;  Context dependent;  Control elements;  Eye-gaze;  Head mounted displays;  Robust detection;  Smart homes, Brain computer interface},
references={Zerafa, R., Camilleri, T., Falzon, O., Camilleri, K.P., A real-time ssvep-based brain-computer interface music player application (2016) XIV Mediterranean Conference on Medical and Biological Engineering and Computing 2016, Ser. IFMBE Proceedings, pp. 173-178. , E. Kyriacou, S. Christofides, and C. S. Pattichis, Eds. Springer International Publishing; Güneysu, A., Akin, H.L., An ssvep based bci to control a humanoid robot by using portable EEG device (2013) 2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), pp. 6905-6908. , July; Chen, X., Wang, Y., Gao, S., Jung, T.-P., Gao, X., Filter bank canonical correlation analysis for implementing a high-speed ssvep-based brain-computer interface (2015) Journal of Neural Engineering, 12 (4), p. 046008. , June; Lalor, E.C., Kelly, S.P., Finucane, C., Burke, R., Smith, R., Reilly, R.B., McDarby, G., Steady-state vep-based brain-computer interface control in an immersive 3d gaming environment (2005) EURASIP Journal on Advances in Signal Processing, 2005 (19), p. 706906. , Nov; Si-Mohammed, H., Argelaguet, F., Casiez, G., Roussel, N., Lécuyer, A., Brain-computer interfaces and augmented reality: A state of the art (2017) Proceedings of the 7th International BCI Conference; Wang, M., Li, R., Zhang, R., Li, G., Zhang, D., A wearable ssvepbased bci system for quadcopter control using head-mounted device (2018) IEEE Access, 6, pp. 26789-26798; Escobedo, L., Tentori, M., Quintana, E., Favela, J., Garcia-Rosas, D., Using augmented reality to help children with autism stay focused (2014) IEEE Pervasive Computing, 13 (1), pp. 38-46; Si-Mohammed, H., Petit, J., Jeunet, C., Argelaguet, F., Spindler, F., Évain, A., Roussel, N., Lécuyer, A., Towards bci-based interfaces for augmented reality: Feasibility, design and evaluation (2018) IEEE Transactions on Visualization and Computer Graphics, p. 1; Takano, K., Hata, N., Kansaku, K., Towards intelligent environments: An augmented reality-brain-machine interface operated with a see-through head-mount display (2011) Frontiers in Neuroscience, 5; Faller, J., Allison, B.Z., Brunner, C., Scherer, R., Schmalstieg, D., Pfurtscheller, G., Neuper, C., (2017) A Feasibility Study on SSVEP-based Interaction with Motivating and Immersive Virtual and Augmented Reality; Saboor, A., Rezeika, A., Stawicki, P., Gembler, F., Benda, M., Grunenberg, T., Volosyak, I., Ssvep-based bci in a smart home scenario (2017) Advances in Computational Intelligence, Ser. Lecture Notes in Computer Science, pp. 474-485. , I. Rojas, G. Joya, and A. Catala, Eds. Springer International Publishing; Angrisani, L., Arpaia, P., Moccaldi, N., Esposito, A., Wearable augmented reality and brain computer interface to improve human-robot interactions in smart industry: A feasibility study for ssvep signals (2018) 2018 IEEE 4th International Forum on Research and Technology for Society and Industry (RTSI), pp. 1-5. , Sept; Coogan, C.G., He, B., Brain-computer interface control in a virtual reality environment and applications for the internet of things (2018) IEEE Access, 6, pp. 10840-10849; Evain, A., Argelaguet, F., Roussel, N., Casiez, G., Lécuyer, A., Can i think of something else when using a bci: Cognitive demand of an ssvep-based bci (2017) Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, Ser. CHI '17, pp. 5120-5125. , New York, NY, USA: ACM; Kishore, S., González-Franco, M., Hintemüller, C., Kapeller, C., Guger, C., Slater, M., Blom, K.J., Comparison of ssvep bci and eye tracking for controlling a humanoid robot in a social environment (2014) Presence: Teleoperators and Virtual Environments, 23 (3), pp. 242-252. , Oct; Ma, X., Yao, Z., Wang, Y., Pei, W., Chen, H., Combining brain-computer interface and eye tracking for high-speed text entry in virtual reality (2018) 23rd International Conference on Intelligent User Interfaces, Ser. IUI '18, pp. 263-267. , New York, NY, USA: ACM event-place: Tokyo, Japan; Putze, F., Methods and tools for using bci with augmented and virtual reality (2019) Brain Art: Brain-Computer Interfaces for Artistic Expression, pp. 433-446. , A. Nijholt, Ed. Cham: Springer International Publishing; Cecotti, H., A self-paced and calibration-less ssvep-based brain-computer interface speller (2010) IEEE Transactions on Neural Systems and Rehabilitation Engineering, 18 (2), pp. 127-133. , Apr; Bangor, A., Kortum, P., Miller, J., Determining what individual sus scores mean: Adding an adjective rating scale (2009) J. Usability Studies, 4 (3), pp. 114-123. , May; Putze, F., Scherer, M., Schultz, T., Starring into the void: Classifying internal vs. external attention from EEG (2016) Proceedings of the 9th Nordic Conference on Human-Computer Interaction. ACM, p. 47},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={1062922X},
isbn={9781728145693},
coden={PICYE},
language={English},
abbrev_source_title={Conf. Proc. IEEE Int. Conf. Syst. Man Cybern.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Salous20191664,
author={Salous, M. and Putze, F. and Ihrig, M. and Schultz, T.},
title={Visual and memory-based HCI obstacles: Behaviour-based detection and user interface adaptations analysis},
journal={Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
year={2019},
volume={2019-October},
pages={1664-1671},
doi={10.1109/SMC.2019.8914233},
art_number={8914233},
note={cited By 0; Conference of 2019 IEEE International Conference on Systems, Man and Cybernetics, SMC 2019 ; Conference Date: 6 October 2019 Through 9 October 2019;  Conference Code:155660},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076781189&doi=10.1109%2fSMC.2019.8914233&partnerID=40&md5=6dd2284b557ed247275ac18794f55031},
affiliation={University of Bremen, Cognitive Systems Lab (CSL), Germany},
abstract={Human Computer Interaction (HCI) performance can be impaired by several HCI obstacles. Cognitive adaptive systems should dynamically detect such obstacles and compensate them with suitable User Interface (UI) adaptation. In this paper, we discuss the detection of two main HCI obstacles: Memory-based and visual obstacles. A sequential model based on Long-Short Term Memory (LSTM) is suggested for such a detection of HCI obstacles. UI adaptations for both types of obstacles are discussed and analyzed. We investigate the classification performance on data from a user study with 17 participants. Furthermore, we also investigate the influence of different adaptation mechanisms on performance and subjective assessment. Results show advantages of the proposed sequential LSTM model: On the one hand, the LSTM outperforms the baseline random guess and also a baseline static model LDA in the detection of visual obstacles with 70.6% as an average accuracy. On the other hand, the evaluation of HCI sessions impeded by obstacles but supported with different UI adaptations shows that LSTM results well match the subjective assessment as a plausible detector of behaviour changes. © 2019 IEEE.},
keywords={Cognitive systems;  Human computer interaction;  Long short-term memory;  Palmprint recognition;  Petroleum reservoir evaluation, Adaptation mechanism;  Behaviour changes;  Classification performance;  Human computer interaction (HCI);  Interface adaptation;  Sequential model;  Static model;  Subjective assessments, User interfaces},
funding_details={Deutsche ForschungsgemeinschaftDeutsche Forschungsgemeinschaft, DFG, PU 613/1-1},
funding_details={German-Israeli Foundation for Scientific Research and DevelopmentGerman-Israeli Foundation for Scientific Research and Development, GIF},
funding_text 1={*This work has been done within the project DINCO ”Detection of Interaction Competencies and Obstacles”. We thank the German Research Foundation (DFG) for funding DINCO project under the reference number PU 613/1-1.},
references={Berka, C., Levendowski, D.J., Lumicao, M.N., Yau, A., Dvisobs Voicadapt, G., Zivkovic, V.T., Olmstead, R.E., Craven, P.L., EEG correlates of task engagement and mental workload in vigilance, learning, and memory tasks (2007) Aviat Space Environ Med; Baldwin, C.L., Penaranda, B.N., Adaptive training using an artificial neural network and EEG metrics for within-and cross-task workload classification (2012) NeuroImage, 59 (1), pp. 48-56; Herff, C., Fortmann, O., Tse, C.Y., Cheng, X., Putze, F., Heger, D., Schultz, T., Hybrid fnirs-EEG based discrimination of 5 levels of memory load (2015) 2015 7th International IEEE/EMBS Conference on Neural Engineering (NER), pp. 5-8. , Apr 22. IEEE; Putze, F., Salous, M., Schultz, T., Detecting memorybased interaction obstacles with a recurrent neural model of user behavior (2018) 23rd International Conference on Intelligent User Interfaces. ACM; Sguerra, B., Jouvelot, P., An unscented hound forworking memory and the cognitive adaptation of user interfaces (2019) Communication Pour la Conference UMAP 19, , Larnaca, Juin 2019; Moran, P.A.P., Random processes in genetics (1958) Mathematical Proceedings Ofthe Cambridge Philosophical Society, 54 (1), p. 6071; Jefferson, L., Harvey, R., An interface to support color blind computer users (2007) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM; Zohran, Q.A., Taha Khan, M., (2017) Adaptive Interface for Accommodating Color-Blind Users by Using Ishihara Test; https://nei.nih.gov/health/color_blindness/facts_about, National Eye Institute, 28 03 2019; Hochreiter, S., Schmidhuber, J., Long short-term memory (1997) Neural Computation, 9 (8), pp. 1735-1780; Proepper, R., Putze, F., Schultz, T., Jam: Java-based associative memory (2011) Proceedings of the Paralinguistic Information and Its Integration in Spoken Dialogue Systems Workshop, , Springer, New York, NY; Anderson, J.R., Bothell, D., Byrne, M.D., Douglass, S., Lebiere, C., Qin, Y., An integrated theory of the mind (2004) Psychological Review, 111 (4), p. 1036. , Oct; Putze, F., Schultz, T., Ehret, S., Miller-Teynor, H., Kruse, A., Modelbased evaluation of playing strategies in a memo game for elderly users (2015) 2015 IEEE International Conference on Systems, Man, and Cybernetics, pp. 929-934. , Oct 9. IEEE; Agresti, A., Caffo, B., Simple and effective confidence intervals for proportions and differences of proportions result from addingtwo successes and two failures (2000) The American Statistician, 54 (4), pp. 280-288; Müller-Putz, G., Scherer, R., Brunner, C., Leeb, R., Pfurtscheller, G., Better than random: A closer look on bci results (2008) International Journal of Bioelectromagnetism, 10, pp. 52-55. , (ARTICLE); Putze, F., Schultz, T., Adaptive cognitive technical systems (2014) Journal of Neuroscience Methods, 234, pp. 108-115. , Aug 30; Costa Vieira, D., Cosme, P., (2016) T-Test with Likert Scale Variables},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={1062922X},
isbn={9781728145693},
coden={PICYE},
language={English},
abbrev_source_title={Conf. Proc. IEEE Int. Conf. Syst. Man Cybern.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Putze20193103,
author={Putze, F. and Herff, C. and Tremmel, C. and Schultz, T. and Krusienski, D.J.},
title={Decoding Mental Workload in Virtual Environments: A fNIRS Study using an Immersive n-back Task},
journal={Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
year={2019},
pages={3103-3106},
doi={10.1109/EMBC.2019.8856386},
art_number={8856386},
note={cited By 4; Conference of 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2019 ; Conference Date: 23 July 2019 Through 27 July 2019;  Conference Code:152547},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076345833&doi=10.1109%2fEMBC.2019.8856386&partnerID=40&md5=29042823da7ff74cf5608042bce1d03a},
affiliation={Cognitive Systems Lab, University of Bremen, Germany; School for Mental Health and Neuroscience, Maastricht University, Netherlands; Biomedical Engineerng Program, Old Dominion University (ODU), Norfolk, VA, United States; ASPEN Lab, Virginia Commonwealth University (VCU), Richmond, VA, United States},
abstract={Virtual Reality (VR) has emerged as a novel paradigm for immersive applications in training, entertainment, rehabilitation, and other domains. In this paper, we investigate the automatic classification of mental workload from brain activity measured through functional near-infrared spectroscopy (fNIRS) in VR. We present results from a study which implements the established n-back task in an immersive visual scene, including physical interaction. Our results show that user workload can be detected from fNIRS signals in immersive VR tasks both person-dependently and -adaptively. © 2019 IEEE.},
keywords={adult;  article;  brain function;  female;  functional near-infrared spectroscopy;  human;  human experiment;  male;  n-back test;  virtual reality;  workload},
references={Makransky, G., Terkildsen, T.S., Mayer, R.E., Adding immersive virtual reality to a science lab simulation causes more presence but less learning (2017) Learning and Instruction; Girouard, A., Solovey, E.T., Hirshfield, L.M., Peck, E.M., Chauncey, K., Sassaroli, A., Fantini, S., Jacob, R.J.K., From brain signals to adaptive interfaces: Using fNIRS in HCI (2010) Brain-Computer Interfaces: Applying Our Minds to Human-Computer Interaction, Ser. Human-Computer Interaction Series, pp. 221-237. , D. S. Tan and A. Nijholt, Eds. London: Springer London; Heger, D., Putze, F., Schultz, T., An EEG adaptive information system for an empathic robot (2011) International Journal of Social Robotics, 3 (4), pp. 415-425; Afergan, D., Peck, E.M., Solovey, E.T., Jenkins, A., Hincks, S.W., Brown, E.T., Chang, R., Jacob, R.J., Dynamic difficulty using brain metrics of workload (2014) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, pp. 3797-3806; Yuksel, B.F., Oleson, K.B., Harrison, L., Peck, E.M., Afergan, D., Chang, R., Jacob, R.J., Learn piano with bach: An adaptive learning interface that adjusts task difficulty based on brain state (2016) Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, pp. 5372-5384. , New York, NY, USA: ACM; Strait, M., Scheutz, M., What we can and cannot (yet) do with functional near infrared spectroscopy (2014) Frontiers in Neuroscience, 8; Solovey, E.T., Girouard, A., Chauncey, K., Hirshfield, L.M., Sassaroli, A., Zheng, F., Fantini, S., Jacob, R.J., Using fnirs brain sensing in realistic hci settings: Experiments and guidelines (2009) Proceedings of the 22nd Annual ACM Symposium on User Interface Software and Technology. ACM, pp. 157-166; Heger, D., Herff, C., Putze, F., Mutter, R., Schultz, T., Continuous affective states recognition using functional near infrared spectroscopy (2014) Brain-Computer Interfaces, 1 (2), pp. 113-125; Khan, M.J., Hong, K.-S., Passive BCI based on drowsiness detection: An fNIRS study (2015) Biomedical Optics Express, 6 (10), pp. 4063-4078; Herff, C., Fortmann, O., Tse, C.-Y., Cheng, X., Putze, F., Heger, D., Schultz, T., Hybrid fnirs-EEG based discrimination of 5 levels of memory load (2015) Neural Engineering (NER), 2015 7th International IEEE/EMBS Conference On. IEEE, pp. 5-8; Aghajani, H., Garbey, M., Omurtag, A., Measuring mental workload with EEG+fNIRS (2017) Frontiers in Human Neuroscience, 11; Putze, F., Hesslinger, S., Tse, C.-Y., Huang, Y., Herff, C., Guan, C., Schultz, T., Hybrid fnirs-EEG based classification of auditory and visual perception processes (2014) Frontiers in Neuroscience, 8, p. 373; Gateau, T., Durantin, G., Lancelot, F., Scannella, S., Dehais, F., Real-time state estimation in a flight simulator Using fNIRS (2015) PLOS ONE, 10 (3), p. e0121279; Unni, A., Ihme, K., Surm, H., Weber, L., Ldtke, A., Nicklas, D., Jipp, M., Rieger, J.W., Brain activity measured with fNIRS for the prediction of cognitive workload (2015) 2015 6th IEEE International Conference on Cognitive Infocommunications, pp. 349-354; McKendrick, R., Parasuraman, R., Murtza, R., Formwalt, A., Baccus, W., Paczynski, M., Ayaz, H., Into the wild: Neuroergonomic differentiation of hand-held and augmented reality wearable displays during outdoor navigation with functional near infrared spectroscopy (2016) Frontiers in Human Neuroscience, 10; Holper, L., Muehlemann, T., Scholkmann, F., Eng, K., Kiper, D., Wolf, M., Testing the potential of a virtual reality neurorehabilitation system during performance of observation, imagery and imitation of motor actions recorded by wireless functional near-infrared spectroscopy (fNIRS) (2010) Journal of NeuroEngineering and Rehabilitation, 7 (1), p. 57; Blume, F., Hudak, J., Dresler, T., Ehlis, A.-C., Khnhausen, J., Renner, T.J., Gawrilow, C., NIRS-based neurofeedback training in a virtual reality classroom for children with attention-deficit/hyperactivity disorder: Study protocol for a randomized controlled trial (2017) Trials, 18 (1), p. 41; Hudak, J., Blume, F., Dresler, T., Haeussinger, F.B., Renner, T.J., Fallgatter, A.J., Gawrilow, C., Ehlis, A.-C., Near-infrared spectroscopy-based frontal lobe neurofeedback integrated in virtual reality modulates brain and behavior in highly impulsive adults (2017) Frontiers in Human Neuroscience, 11; McMillan, K.M., Laird, A.R., Witt, S.T., Meyerand, M.E., Selfpaced working memory: Validation of verbal variations of the n-back paradigm (2007) Brain Research, 1139, pp. 133-142; Putze, F., Jarvis, J.-P., Schultz, T., Multimodal recognition of cognitive workload for multitasking in the car (2010) 20th International Conference on Pattern Recognition. IEEE, pp. 3748-3751; Herff, C., Heger, D., Fortmann, O., Hennrich, J., Putze, F., Schultz, T., Mental workload during n-back taskquantified in the prefrontal cortex using fnirs (2014) Frontiers in Human Neuroscience, 7, p. 935; Tremmel, C., Herff, C., Krusienski, D., EEG movement artifact suppression in interactive virtual reality (2019) Proceedings of the 41st Engineering Medicine and Biology Conference, , Berlin, Germany; Cooper, R., Selb, J., Gagnon, L., Phillip, D., Schytz, H.W., Iversen, H.K., Ashina, M., Boas, D.A., A systematic comparison of motion artifact correction techniques for functional near-infrared spectroscopy (2012) Frontiers in Neuroscience, 6, p. 147},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={1557170X},
isbn={9781538613115},
pubmed_id={31946544},
language={English},
abbrev_source_title={Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schultz20193111,
author={Schultz, T. and Angrick, M. and Diener, L. and Kuster, D. and Meier, M. and Krusienski, D.J. and Herff, C. and Brumberg, J.S.},
title={Towards Restoration of Articulatory Movements: Functional Electrical Stimulation of Orofacial Muscles},
journal={Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
year={2019},
pages={3111-3114},
doi={10.1109/EMBC.2019.8857670},
art_number={8857670},
note={cited By 0; Conference of 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2019 ; Conference Date: 23 July 2019 Through 27 July 2019;  Conference Code:152547},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077875656&doi=10.1109%2fEMBC.2019.8857670&partnerID=40&md5=10f7457d87be02beb1b0625dadcc0dde},
affiliation={Cognitive Systems Lab, University of Bremen, Bremen, Germany; ASPEN Lab, Virginia Commonwealth University, Richmond, VA, United States; School for Mental Health and Neuroscience, Maastricht University, Netherlands; Speech and Applied Neurosciene Lab, University of Kansas, Lawrence, KS, United States},
abstract={Millions of individuals suffer from impairments that significantly disrupt or completely eliminate their ability to speak. An ideal intervention would restore one's natural ability to physically produce speech. Recent progress has been made in decoding speech-related brain activity to generate synthesized speech. Our vision is to extend these recent advances toward the goal of restoring physical speech production using decoded speech-related brain activity to modulate the electrical stimulation of the orofacial musculature involved in speech. In this pilot study we take a step toward this vision by investigating the feasibility of stimulating orofacial muscles during vocalization in order to alter acoustic production. The results of our study provide necessary foundation for eventual orofacial stimulation controlled directly from decoded speech-related brain activity. © 2019 IEEE.},
keywords={article;  brain function;  controlled study;  feasibility study;  functional electrical stimulation;  human;  human experiment;  muscle;  pilot study;  vision;  vocalization},
references={Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Brumberg, J.S., Silent speech interfaces (2010) Speech Communication, 52 (4), pp. 270-287; Schultz, T., Wand, M., Hueber, T., Krusienski, D.J., Herff, C., Brumberg, J.S., Biosignal-based spoken communication: A survey (2017) IEEE/ACM Transactions on Audio, Speech, and Language Processing, 25 (12), pp. 2257-2271; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 331-336. , San Juan, Puerto Rico; Diener, L., Herff, C., Janke, M., Schultz, T., An initial investigation into the real-time conversion of facial surface EMG signals to audible speech (2016) IEEE EMBC, pp. 888-891; Hueber, T., Benaroya, E.-L., Chollet, G., Denby, B., Dreyfus, G., Stone, M., Development of a silent speech interface driven by ultrasound and optical images of the tongue and lips (2010) Speech Communication, 52 (4), pp. 288-300; Gonzalez, J.A., Cheah, L.A., Gilbert, J.M., Bai, J., Ell, S.R., Green, P.D., Moore, R.K., Direct speech generation for a silent speech interface based on permanent magnet articulography (2016) Proceedings of the International Joint Conference on Biomedical Engineering Systems and Technologies, pp. 96-105; Herff, C., Schultz, T., Automatic speech recognition from neural signals: A focused review (2016) Frontiers in Neuroscience, 10, p. 429; Chakrabarti, S., Sandberg, H.M., Brumberg, J.S., Krusienski, D.J., Progress in speech decoding from the electrocorticogram (2015) Biomedical Engineering Letters, 5 (1), pp. 10-21; Angrick, M., Herff, C., Mugler, E., Tate, M.C., Slutzky, M.W., Krusienski, D.J., Schultz, T., Speech synthesis from ecog using densely connected 3d convolutional neural networks (2018) BioRxiv, p. 478644; Anumanchipalli, G.K., Chartier, J., Chang, E.F., Intelligible speech synthesis from neural decoding of spoken sentences (2018) BioRxiv, p. 481267; Brumberg, J.S., Pitt, K.M., Burnison, J.D., A non-invasive brain-computer interface for real-time speech synthesis: The importance of multimodal feedback (2018) IEEE Transactions on Neural Systems and Rehabilitation Engineering, 26 (4), pp. 874-881; Ajiboye, A.B., Willett, F.R., Young, D.R., Memberg, W.D., Murphy, B.A., Miller, J.P., Walter, B.L., Sweet, J.A., Restoration of reaching and grasping movements through brain-controlled muscle stimulation in a person with tetraplegia: A proof-of-concept demonstration (2017) The Lancet, 389, pp. 1821-1830. , 10081; Wagner, F.B., Mignardot, J.-B., Le Goff-Mignardot, C.G., Demesmaeker, R., Komi, S., Capogrosso, M., Rowald, A., Caban, M., Targeted neurotechnology restores walking in humans with spinal cord injury (2018) Nature, 563 (7729), p. 65; Fridlund, A.J., Cacioppo, J.T., Guidelines for human electromyographic research (1986) Psychophysiology, 23 (5), pp. 567-589},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={1557170X},
isbn={9781538613115},
pubmed_id={31946546},
language={English},
abbrev_source_title={Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Angrick2019145,
author={Angrick, M. and Herff, C. and Johnson, G. and Shih, J. and Krusienski, D. and Schultz, T.},
title={Interpretation of convolutional neural networks for speech spectrogram regression from intracranial recordings},
journal={Neurocomputing},
year={2019},
volume={342},
pages={145-151},
doi={10.1016/j.neucom.2018.10.080},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061427260&doi=10.1016%2fj.neucom.2018.10.080&partnerID=40&md5=da80f48a950488083566da339ea6e1d4},
affiliation={Cognitive Systems Lab, University of Bremen, Enrique-Schmidt-Straße 5, Bremen, 28359, Germany; Biomedical Engineering, Old Dominion University, 5115 Hampton Blvd, Norfolk, VA  23529, United States; Epilepsy Center, UC San Diego Health, 200 West Arbor Drive, San Diego, CA  92103, United States; ASPEN Lab, Virginia Commonwealth University, 737 N 5th St, Richmond, VA  23219, United States},
abstract={The direct synthesis of continuously spoken speech from neural activity could provide a fast and natural way of communication for users suffering from speech disorders. Mapping the complex dynamics of neural activity to spectral representations of speech is a demanding task for regression models. Convolutional neural networks have recently shown promise for finding patterns in neural signals and might be a good candidate for this particular regression task. However, the intrinsic agency of the resulting networks is challenging to interpret and thus provides little opportunity to gain insights on neural processes underlying speech. While activation maximization can be used to get a glimpse into what a network has learned for a classification task, it usually does not benefit regression problems. Here, we show that convolutional neural networks can be used to reconstruct an audible waveform from invasively-measured brain activity. By adapting activation maximization, we present a method that can provide insights from neural networks targeting regression problems. Based on experimental data, we achieve statistically significant correlations between spectrograms of synthesized and original speech. Our interpretation approach shows that trained models reveal that electrodes placed in cortical regions associated with speech production tasks have a large impact on the reconstruction of speech segments. © 2019 Elsevier B.V.},
author_keywords={Activation maximization;  Convolutional neural networks;  Electrocorticography;  Regression;  Speech synthesis},
keywords={Brain;  Chemical activation;  Convolution;  Neural networks;  Neurons;  Photomapping;  Regression analysis;  Spectrographs;  Speech synthesis, Classification tasks;  Convolutional neural network;  Electrocorticography;  Regression;  Regression problem;  Spectral representations;  Speech production;  Speech spectrogram, Speech communication, activation maximization approach;  analytic method;  Article;  artificial neural network;  audio recording;  clinical article;  controlled study;  convolutional neural network;  electroencephalogram;  female;  human;  perception;  priority journal;  speech;  speech planning;  speech segment;  speech spectrogram regression;  superior temporal gyrus},
funding_details={National Science FoundationNational Science Foundation, NSF, 1608140},
funding_details={Bundesministerium fÃ¼r Bildung und ForschungBundesministerium fÃ¼r Bildung und Forschung, BMBF, 01GQ1602},
funding_text 1={CH, JS, DK and TS acknowledge funding by BMBF ( 01GQ1602 ) and NSF ( 1608140 ) as part of the NSF/NIH/BMBF Collaborative Research in Computational Neuroscience Program.},
references={Wolpaw, J., Birbaumer, N., McFarland, D., Pfurtscheller, G., Vaughan, T., Brain–computer interfaces for communication and control (2002) Clin. Neurophysiol., 113 (6), pp. 767-791; Schultz, T., Wand, M., Hueber, T., Krusienski, D.J., Herff, C., Brumberg, J.S., Biosignal-based spoken communication: a survey (2017) IEEE/ACM Trans. Audio Speech Lang. Process., 25 (12), pp. 2257-2271; Chen, X., Wang, Y., Nakanishi, M., Gao, X., Jung, T.-P., Gao, S., High-speed spelling with a noninvasive brain–computer interface (2015) Proc. Natl. Acad. Sci., 112 (44), pp. E6058-E6067; Herff, C., Schultz, T., Automatic speech recognition from neural signals: a focused review (2016) Front. Neurosci., 10; Ramsey, N., Salari, E., Aarnoutse, E., Vansteensel, M., Bleichner, M., Freudenburg, Z., Decoding spoken phonemes from sensorimotor cortex with high-density ECoG grids (2017) NeuroImage; Mugler, E., Patton, J., Flint, R., Wright, Z., Schuele, S., Rosenow, J., Shih, J., Slutzky, M., Direct classification of all American English phonemes using signals from functional speech motor cortex (2014) J. Neural Eng., 11 (3), p. 035015; Mugler, E.M., Tate, M.C., Livescu, K., Templer, J.W., Goldrick, M.A., Slutzky, M.W., Differential Representation of Articulatory Gestures and Phonemes in Precentral and Inferior Frontal Gyri (2018) J. Neurosci, 38 (46), pp. 9803-9813; Lotte, F., Brumberg, J.S., Brunner, P., Gunduz, A., Ritaccio, A.L., Guan, C., Schalk, G., Electrocorticographic representations of segmental features in continuous speech (2015) Front. Human Neurosci., 9, p. 97; Dichter, B.K., Breshears, J.D., Leonard, M.K., Chang, E.F., The control of vocal pitch in human laryngeal motor cortex (2018) Cell, 174 (1). , 21–31.e9 doi:; Herff, C., Heger, D., de Pesters, A., Telaar, D., Brunner, P., Schalk, G., Schultz, T., Brain-to-text: decoding spoken phrases from phone representations in the brain (2015) Front. Neurosci., 9; Moses, D.A., Mesgarani, N., Leonard, M.K., Chang, E.F., Neural speech recognition: continuous phoneme decoding using spatiotemporal representations of human cortical activity (2016) J. Neural Eng., 13 (5), p. 056004; Moses, D.A., Leonard, M.K., Chang, E.F., Real-time classification of auditory sentences using evoked cortical activity in humans (2018) J. Neural Eng., 15 (3), p. 036005; Pei, X., Barbour, D., Leuthardt, E., Schalk, G., Decoding vowels and consonants in spoken and imagined words using electrocorticographic signals in humans (2011) J. Neural Eng., 8 (4), p. 046028; Martin, S., Brunner, P., Holdgraf, C., Heinze, H.-J., Crone, N., Rieger, J., Schalk, G., Pasley, B., Decoding spectrotemporal features of overt and covert speech from the human cortex (2014) Front. Neuroeng., 7 (14); Martin, S., Brunner, P., Iturrate, I., Millán, J., Schalk, G., Knight, R., Pasley, B., Word pair classification during imagined speech using direct brain recordings (2016) Sci. Rep., 6, p. 25803; Pasley, B.N., David, S.V., Mesgarani, N., Flinker, A., Shamma, S.A., Crone, N.E., Knight, R.T., Chang, E.F., Reconstructing speech from human auditory cortex (2012) PLoS Biol., 10 (1), p. e1001251; Akbari, H., Khalighinejad, B., Herrero, J., Mehta, A., Mesgarani, N., Towards reconstructing intelligible speech from the human auditory cortex (2019) Sci. Rep., 9, p. 874; Santoro, R., Moerel, M., De Martino, F., Valente, G., Ugurbil, K., Yacoub, E., Formisano, E., Reconstructing the spectrotemporal modulations of real-life sounds from fmri response patterns (2017) Proc. Natl. Acad. Sci., p. 201617622; Bartels, J., Andreasen, D., Ehirim, P., Mao, H., Seibert, S., Wright, E.J., Kennedy, P., Neurotrophic electrode: method of assembly and implantation into human motor speech cortex (2008) J. Neurosci. Methods, 174 (2), pp. 168-176; Guenther, F.H., Brumberg, J.S., Wright, E.J., Nieto-Castanon, A., Tourville, J.A., Panko, M., Law, R., Andreasen, D.S., A wireless brain-machine interface for real-time speech synthesis (2009) PloS one, 4 (12), p. e8218; Brumberg, J.S., Wright, E.J., Andreasen, D.S., Guenther, F.H., Kennedy, P.R., Classification of intended phoneme production from chronic intracortical microelectrode recordings in speech motor cortex (2011) Front. Neurosci., 5, p. 65; Martin, S., Millán, J.D.R., Knight, R.T., Pasley, B.N., The use of intracranial recordings to decode human language: challenges and opportunities (2016) Brain Lang., , https://www.sciencedirect.com/science/article/pii/S0093934X15301243?via%3Dihub; Sturm, I., Lapuschkin, S., Samek, W., Müller, K.-R., Interpretable deep neural networks for single-trial EEG classification (2016) J. Neurosci. Methods, 274, pp. 141-145; Livezey, J.A., Bouchard, K.E., Chang, E.F., (2018), Deep learning as a tool for neural data analysis: speech classification and cross-frequency coupling in human sensorimotor cortex, arXiv:; Krizhevsky, A., Sutskever, I., Hinton, G.E., Imagenet classification with deep convolutional neural networks (2012) Proceedings of the Advances in Neural Information Processing Systems, pp. 1097-1105; Lawrence, S., Giles, C.L., Tsoi, A.C., Back, A.D., Face recognition: a convolutional neural-network approach (1997) IEEE Trans. Neural Netw., 8 (1), pp. 98-113; Schirrmeister, R., Springenberg, J., Fiederer, L., Glasstetter, M., Eggensperger, K., Tangermann, M., Hutter, F., Ball, T., Deep learning with convolutional neural networks for EEG decoding and visualization (2017) Hum. Brain Map.; Erhan, D., Bengio, Y., Courville, A., Vincent, P., Visualizing higher-layer features of a deep network (2009) Univ. Montreal, 1341, p. 3; Montavon, G., Samek, W., Müller, K.-R., Methods for interpreting and understanding deep neural networks (2018) Digit. Signal Process., 73, pp. 1-15; Schalk, G., McFarland, D.J., Hinterberger, T., Birbaumer, N., Wolpaw, J.R., Bci2000: a general-purpose brain–computer interface (BCI) system (2004) IEEE Trans. Biomed. Eng., 51 (6), pp. 1034-1043; Rothauser, E., IEEE recommended practice for speech quality measurements (1969) IEEE Trans. Audio Electroacoust., 17, pp. 225-246; Crone, N.E., Boatman, D., Gordon, B., Hao, L., Induced electrocorticographic gamma activity during auditory perception (2001) Clin. Neurophysiol., 112 (4), pp. 565-582; Ray, S., Crone, N.E., Niebur, E., Franaszczuk, P.J., Hsiao, S.S., Neural correlates of high-gamma oscillations (60–200 Hz) in macaque local field potentials and their potential implications in electrocorticography (2008) J. Neurosci., 28 (45), pp. 11526-11536; Leuthardt, E., Pei, X.-M., Breshears, J., Gaona, C., Sharma, M., Freudenburg, Z., Barbour, D., Schalk, G., Temporal evolution of gamma activity in human cortex during an overt and covert word repetition task (2012) Front. Hum. Neurosci., 6, p. 99; Crone, N., Hao, L., Hart, J., Boatman, D., Lesser, R., Irizarry, R., Gordon, B., Electrocorticographic gamma activity during word production in spoken and sign language (2001) Neurology, 57 (11), pp. 2045-2053; Miller, K.J., Leuthardt, E.C., Schalk, G., Rao, R.P., Anderson, N.R., Moran, D.W., Miller, J.W., Ojemann, J.G., Spectral changes in cortical surface potentials during motor movement (2007) J. Neurosci., 27 (9), pp. 2424-2432; Imai, S., Cepstral analysis synthesis on the MEL frequency scale (1983) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP'83., 8, pp. 93-96. , IEEE; Ioffe, S., Szegedy, C., Batch normalization: Accelerating deep network training by reducing internal covariate shift (2015) Proceedings of the 32nd International Conference on Machine Learning, ICML, 37, pp. 448-456. , PMLR; Clevert, D.-A., Unterthiner, T., Hochreiter, S., Fast and accurate deep network learning by exponential linear units (ELUS) (2016) International Conference on Learning Representations, , ICLR; Kingma, D., Ba, J., Adam: A method for stochastic optimization (2015) International Conference on Learning Representations, , ICLR; Herff, C., Johnson, G., Diener, L., Shih, J., Krusienski, D., Schultz, T., Towards direct speech synthesis from ECoG: a pilot study (2016) Proceedings of the 2016 IEEE Thirty-eighth Annual International Conference of the Engineering in Medicine and Biology Society (EMBC), pp. 1540-1543. , IEEE; Griffin, D., Lim, J., Signal estimation from modified short-time Fourier transform (1984) IEEE Trans. Acoust. Speech Signal Process., 32 (2), pp. 236-243; Tourville, J.A., Guenther, F.H., The diva model: a neural theory of speech acquisition and production (2011) Lang. Cognit. Process., 26 (7), pp. 952-981; Hickok, G., Poeppel, D., The cortical organization of speech processing (2007) Nat. Rev. Neurosci., 8 (5), pp. 393-402; Hickok, G., Computational neuroanatomy of speech production (2012) Nat. Rev. Neurosci., 13 (2), p. 135; Brumberg, J., Krusienski, D., Chakrabarti, S., Gunduz, A., Brunner, P., Ritaccio, A., Schalk, G., Spatio-temporal progression of cortical activity related to continuous overt and covert speech production in a reading task (2016) PloS One, 11 (11), p. e0166872},
correspondence_address1={Angrick, M.; Cognitive Systems Lab, University of Bremen, Enrique-Schmidt-Straße 5, Germany; email: miguel.angrick@uni-bremen.de},
publisher={Elsevier B.V.},
issn={09252312},
coden={NRCGE},
language={English},
abbrev_source_title={Neurocomputing},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lesaja2019522,
author={Lesaja, S. and Herff, C. and Johnson, G.D. and Shih, J.J. and Schultz, T. and Krusienski, D.J.},
title={Decoding Lip Movements during Continuous Speech using Electrocorticography},
journal={International IEEE/EMBS Conference on Neural Engineering, NER},
year={2019},
volume={2019-March},
pages={522-525},
doi={10.1109/NER.2019.8716914},
art_number={8716914},
note={cited By 0; Conference of 9th International IEEE EMBS Conference on Neural Engineering, NER 2019 ; Conference Date: 20 March 2019 Through 23 March 2019;  Conference Code:148145},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066744906&doi=10.1109%2fNER.2019.8716914&partnerID=40&md5=c6279600025c9061aa0e010395daf937},
affiliation={Biomedical Engineering Program, Old Dominion University, Norfolk, VA, United States; School for Mental Health and Neuroscience, Maastricht University, Netherlands; Neurology Department, UCSD Health, San Diego, CA, United States; Cognitive Systems Laboratory, University of Bremen, Germany; Department of Biomedical Engineering, Virginia Commonwealth University, Richmond, VA, United States},
abstract={Recent work has shown that it is possible to decode aspects of continuously-spoken speech from electrocorticographic (ECoG) signals recorded on the cortical surface. The ultimate objective is to develop a speech neuroprosthetic that can provide seamless, real-time synthesis of continuous speech directly from brain activity. Instead of decoding acoustic properties or classes of speech, such a neuroprosthetic might be realized by decoding articulator movements associated with speech production, as recent work highlights a representation of articulator movement in ECoG signals. The aim of this work is to investigate the neural correlates of speech-related lip movements from video recordings. We present how characteristics of lip movement can be decoded and lip-landmark positions can be predicted. © 2019 IEEE.},
keywords={Acoustic properties;  Brain;  Decoding;  Electroencephalography;  Electrophysiology;  Neural prostheses;  Video recording, Brain activity;  Continuous speech;  Cortical surfaces;  Electrocorticography;  Neural correlate;  Neuroprosthetic;  Real-time synthesis;  Speech production, Speech},
funding_details={Bundesministerium fÃ¼r Bildung und FrauenBundesministerium fÃ¼r Bildung und Frauen, BMBF, 1608140},
funding_text 1={This work was supported 01GQ1602 (BMBF) and 1608140 (NSF). S Lesaja and GD Johnson are with the Biomedical Engineering Program, Old Dominion University, Norfolk, VA, USA slesa001@odu.edu; gjohn037@odu.edu C Herff is with the School for Mental Health and Neuroscience, Maastricht University, The Netherlands c.herff@maastrichtuniversity.nl JJ Shih is with the Neurology Department, UCSD Health, San Diego, CA, USA jerryshih@ucsd.edu T Schultz is with the Cognitive Systems Laboratory, University of Bremen, Germany tanja.schultz@uni-bremen.de DJ Krusienski is with the Department of Biomedical Engineering, Virginia Commonwealth University, Richmond, VA, USA djkrusienski@vcu.edu},
references={Schultz, T., Wand, M., Hueber, T., Krusienski, D.J., Herff, C., Brumberg, J.S., Biosignal-based spoken communication: A survey (2017) IEEE/ACM Transactions on Audio, Speech, and Language Processing, 25 (12), pp. 2257-2271; Schalk, G., Leuthardt, E.C., Brain-computer interfaces using electrocorticographic signals (2011) IEEE Reviews in Biomedical Engineering, 4, pp. 140-154; Herff, C., Schultz, T., Automatic speech recognition from neural signals: A focused review (2016) Frontiers in Neuroscience, 10, p. 429; Nurse, E.S., John, S.E., Freestone, D.R., Oxley, T.J., Ung, H., Berkovic, S.F., O'Brien, T.J., Grayden, D.B., Consistency of long-term subdural electrocorticography in humans (2017) IEEE Trans Biomed Eng; Mugler, E.M., Patton, J.L., Flint, R.D., Wright, Z.A., Schuele, S.U., Rosenow, J., Shih, J.J., Slutzky, M.W., Direct classification of all American english phonemes using signals from functional speech motor cortex (2014) Journal of Neural Engineering, 11 (3), p. 035015; Kellis, S., Miller, K., Thomson, K., Brown, R., House, P., Greger, B., Decoding spoken words using local field potentials recorded from the cortical surface (2010) Journal of Neural Engineering, 7 (5), p. 056007; Herff, C., Heger, D., De Pesters, A., Telaar, D., Brunner, P., Schalk, G., Schultz, T., Brain-to-text: Decoding spoken phrases from phone representations in the brain (2015) Frontiers in Neuroscience, 9, p. 217; Pasley, B.N., David, S.V., Mesgarani, N., Flinker, A., Shamma, S.A., Crone, N.E., Knight, R.T., Chang, E.F., Reconstructing speech from human auditory cortex (2012) PLoS Biology, 10 (1), p. e1001251; Martin, S., Brunner, P., Holdgraf, C., Heinze, H.-J., Crone, N.E., Rieger, J., Schalk, G., Pasley, B.N., Decoding spectrotemporal features of overt and covert speech from the human cortex (2014) Frontiers in Neuroengineering, 7, p. 14; Herff, C., Johnson, G., Diener, L., Shih, J., Krusienski, D., Schultz, T., Towards direct speech synthesis from ecog: A pilot study (2016) Engineering in Medicine and Biology Society (EMBC), 2016 IEEE 38th Annual International Conference of The. IEEE, pp. 1540-1543; Bouchard, K.E., Mesgarani, N., Johnson, K., Chang, E.F., Functional organization of human sensorimotor cortex for speech articulation (2013) Nature, 495 (7441), p. 327; Conant, D.F., Bouchard, K.E., Leonard, M.K., Chang, E.F., Human sensorimotor cortex control of directly-measured vocal tract movements during vowel production (2018) Journal of Neuroscience, pp. 2382-2417; Bouchard, K.E., Conant, D.F., Anumanchipalli, G.K., Dichter, B., Chaisanguanthum, K.S., Johnson, K., Chang, E.F., High-resolution, non-invasive imaging of upper vocal tract articulators compatible with human brain recordings (2016) PLoS One, 11 (3), p. e0151327; Mugler, E.M., Tate, M.C., Livescu, K., Templer, J.W., Goldrick, M.A., Slutzky, M.W., Differential representation of articulatory gestures and phonemes in precentral and inferior frontal gyri (2018) Journal of Neuroscience, , http://www.jneurosci.org/content/early/2018/09/26/JNEUROSCI.1206-18.2018; Chartier, J., Anumanchipalli, G.K., Johnson, K., Chang, E.F., Encoding of articulatory kinematic trajectories in human speech sensorimotor cortex (2018) Neuron, 98 (5), pp. 1042-1054; Rothauser, E., Ieee recommended practice for speech quality measurements (1969) IEEE Trans. on Audio and Electroacoustics, 17, pp. 225-246; Schalk, G., McFarland, D.J., Hinterberger, T., Birbaumer, N., Wolpaw, J.R., Bci2000: A general-purpose brain-computer interface (bci) system (2004) IEEE Transactions on Biomedical Engineering, 51 (6), pp. 1034-1043; King, D.E., Dlib-ml: A machine learning toolkit (2009) Journal of Machine Learning Research, 10, pp. 1755-1758. , Jul},
sponsors={EMB; IEEE},
publisher={IEEE Computer Society},
issn={19483546},
isbn={9781538679210},
language={English},
abbrev_source_title={Int. IEEE/EMBS Conf. Neural Eng., NER},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu201947,
author={Liu, H. and Schultz, T.},
title={A wearable real-time human activity recognition system using biosensors integrated into a knee bandage},
journal={BIODEVICES 2019 - 12th International Conference on Biomedical Electronics and Devices, Proceedings; Part of 12th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2019},
year={2019},
pages={47-55},
doi={10.5220/0007398800470055},
note={cited By 2; Conference of 12th International Conference on Biomedical Electronics and Devices, BIODEVICES 2019 - Part of 12th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2019 ; Conference Date: 22 February 2019 Through 24 February 2019;  Conference Code:146926},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064634159&doi=10.5220%2f0007398800470055&partnerID=40&md5=e832c94641006f6275eb293e66a45a51},
affiliation={Cognitive Systems Lab, University of Bremen, Germany},
abstract={This work introduces an innovative wearable real-time Human Activity Recognition (HAR) system. The system processes and decodes various biosignals that are captured from biosensors integrated into a knee bandage. The presented work includes (1) the selection of an appropriate equipment in terms of devices and sensors to capture human activity-related biosignals in real time, (2) the experimental tuning of system parameters which balances recognition accuracy with real-time performance, (3) the intuitive visualization of biosignals as well as n-best recognition results in the graphical user interfaces, and (4) the on-the-air extensions for rapid prototyping of applications. The presented system recognizes seven daily activities: sit, stand, stand up, sit down, walk, turn left and turn right. The amount of activity classes to be recognized can be easily extended by the”plug-and-play” function. To the best of our knowledge, this is the first work which demonstrates a real-time HAR system using biosensors integrated into a knee bandage. © 2019 by SCITEPRESS - Science and Technology Publications, Lda.},
author_keywords={Biodevices;  Biosensors;  Human Activity Recognition;  Rehabilitation Technology;  Wearable Devices},
keywords={Biomedical engineering;  Biosensors;  Electronic medical equipment;  Graphical user interfaces;  Joints (anatomy);  Pattern recognition, Amount of activities;  Bio-devices;  Human activity recognition;  Human activity recognition systems;  Real time performance;  Recognition accuracy;  Rehabilitation technology;  Wearable devices, Wearable technology},
references={Amma, C., Gehrig, D., Schultz, T., Airwriting recognition using wearable motion sensors (2010) First Augmented Human International Conference, p. 10; Bao, L., Intille, S.S., Activity recognition from user-annotated acceleration data (2004) Pervasive Computing, pp. 1-17. , Springer; De Leonardis, G., Rosati, S., Balestra, G., Agostini, V., Panero, E., Gastaldi, L., Knaflitz, M., Human activity recognition by wearable sensors: Comparison of different classifiers for real-time applications (2018) 2018 IEEE International Symposium on Medical Measurements and Applications (MeMeA), pp. 1-6; Fleischer, C., Reinicke, C., Predicting the intended motion with emg signals for an exoskeleton orthosis controller (2005) 2005 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2005), pp. 2029-2034; Kwapisz, J.R., Weiss, G.M., Moore, S.A., Activity recognition using cell phone accelerometers (2010) Proceedings of the Fourth International Workshop on Knowledge Discovery from Sensor Data, pp. 10-18; Liu, H., Schultz, T., ASK: A framework for data acquisition and activity recognition (2018) 11th International Conference on Bio-Inspired Systems and Signal Processing, pp. 262-268. , Madeira, Portugal; Lukowicz, P., Ward, J.A., Junker, H., Stäger, M., Tröster, G., Atrash, A., Starner, T., Recognizing workshop activity using body worn microphones and accelerometers (2004) Pervasive Computing, pp. 18-32. , In; Mathie, M., Coster, A., Lovell, N., Celler, B., Detection of daily physical activities using a triaxial accelerometer (2003) Medical and Biological Engineering and Computing, 41 (3), pp. 296-301; Naeemabadi, M.R., Dinesen, B., Andersen, O.K., Najafi, S., Hansen, J., Evaluating accuracy and usability of microsoft kinect sensors and wearable sensor for tele knee rehabilitation after knee operation (2018) 11th International Conference on Biomedical Electronics and Devices, Biodevices, , 2018; Rebelo, D., Amma, C., Gamboa, H., Schultz, T., Activity recognition for an intelligent knee orthosis (2013) 6th International Conference on Bio-Inspired Systems and Signal Processing, pp. 368-371. , BIOSIG-NALS 2013; Rowe, P., Myles, C., Walker, C., Nutton, R., Knee joint kinematics in gait and other functional activities measured using exible electrogoniometry: How much knee motion is sucient for normal daily life? (2000) Gait & Posture, 12 (2), pp. 143-155; Sutherland, D.H., The evolution of clinical gait analysis: Part II kinematics (2002) Gait & Posture, 16 (2), pp. 159-179; Yepes, J.C., Saldarriaga, A., Vélez, J.M., Pérez, V.Z., Betancur, M.J., A hardware-in-the-loop simulation study of a mechatronic system for anterior cruciate ligament injuries rehabilitation (2017) BIODEVICES, pp. 69-80},
editor={Roque A., Fred A., Gamboa H.},
sponsors={Institute for Systems and Technologies of Information, Control and Communication (INSTICC)},
publisher={SciTePress},
isbn={9789897583537},
language={English},
abbrev_source_title={BIODEVICES - Int. Conf. Biomed. Electron. Devices, Proc.; Part Int. Jt. Conf. Biomed. Eng. Syst. Technol., BIOSTEC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Meier2019559,
author={Meier, M. and Mason, C. and Putze, F. and Schultz, T.},
title={Comparative analysis of think-aloud methods for everyday activities in the context of cognitive robotics},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2019},
volume={2019-September},
pages={559-563},
doi={10.21437/Interspeech.2019-3072},
note={cited By 0; Conference of 20th Annual Conference of the International Speech Communication Association: Crossroads of Speech and Language, INTERSPEECH 2019 ; Conference Date: 15 September 2019 Through 19 September 2019;  Conference Code:153091},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074708879&doi=10.21437%2fInterspeech.2019-3072&partnerID=40&md5=a7a5fdf9117dd2653a3b9a7da83b7a24},
affiliation={Cognitive Systems Lab, University of Bremen, Germany},
abstract={We describe our efforts to compare data collection methods using two think-aloud protocols in preparation to be used as a basis for automatic structuring and labeling of a large database of high-dimensional human activities data into a valuable resource for research in cognitive robotics. The envisioned dataset, currently in development, will contain synchronously recorded multimodal data, including audio, video, and biosignals (eye-tracking, motion-tracking, muscle and brain activity) from about 100 participants performing everyday activities while describing their task through use of think-aloud protocols. This paper provides details of our pilot recordings in the well-established and scalable “table setting scenario,” describes the concurrent and retrospective think-aloud protocols used, the methods used to analyze them, and compares their potential impact on the data collected as well as the automatic data segmentation and structuring process. Copyright © 2019 ISCA},
author_keywords={Activities of daily living;  Biosignals;  Cognitive robotics;  Multimodal;  Think-aloud},
funding_text 1={The research reported in this paper has been supported by the German Research Foundation DFG, as part of Collaborative Research Center (Sonderforschungsbereich) 1320 EASE - Everyday Activity Science and Engineering, University of Bremen (http://www.ease-crc.org/). The research was conducted in subproject H03 Descriptive models of human everyday activity.},
references={Beetz, M., Beßler, D., Haidu, A., Pomarlan, M., Bozcuoglu, A.K., Bartels, G., Knowrob 2.0 - A 2nd generation knowledge processing framework for cognition-enabled robotic agents (2018) International Conference on Robotics and Automation (ICRA), , Brisbane, Australia; Wittenburg, P., Brugman, H., Russel, A., Klassmann, A., Sloetjes, H., ELAN: A professional framework for multimodality research (2006) 5th International Conference on Language Resources and Evaluation (LREC 2006), pp. 1556-1559. , https://tla.mpi.nl/tools/tla-tools/elan/, Max Planck Institute for Psycholinguistics, The Language Archive, Nijmegen, The Netherlands, Online; Meier, M., Mason, C., Porzel, R., Putze, F., Schultz, T., Synchronized multimodal recording of a table setting dataset (2018) IROS 2018: Workshop on Latest Advances in Big Activity Data Sources for Robotics & New Challenges, , Madrid, Spain; Mason, C., Meier, M., Ahrens, F., Fehr, T., Herrmann, M., Putze, F., Schultz, T., Human activities data collection and labeling using a think-aloud protocol in a table setting scenario (2018) IROS 2018: Workshop on Latest Advances in Big Activity Data Sources for Robotics & New Challenges, , Madrid, Spain; Gehrig, D., Krauthausen, P., Rybok, L., Kuehne, H., Hanebeck, U.D., Schultz, T., Stiefelhagen, R., Combined intention, activity, and motion recognition for a humanoid household robot (2011) 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 4819-4825. , Sept; Tenorth, M., Bandouch, J., Beetz, M., The TUM kitchen data set of everyday manipulation activities for motion tracking and action recognition (2009) IEEE International Workshop on Tracking Humans for the Evaluation of Their Motion in Image Sequences (THEMIS), in Conjunction with ICCV2009; Stein, S., McKenna, S.J., Combining embedded accelerometers with computer vision for recognizing food preparation activities (2013) Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing, pp. 729-738; Damen, D., Doughty, H., Farinella, G.M., Fidler, S., Furnari, A., Kazakos, E., Moltisanti, D., Wray, M., Scaling egocentric vision: The epic-kitchens dataset (2018) European Conference on Computer Vision (ECCV); Ericsson, K.A., Herbert, A.S., (1980) Verbal Reports as Data, 87 (3), pp. 215-251; Boren, T., Ramey, J., Thinking aloud: Reconciling theory and practice (2000) IEEE Transactions on Professional Communication, 43 (3), pp. 261-278. , Sept; Hertzum, M., Hansen, K.D., Andersen, H.H., Scrutinising usability evaluation: Does thinking aloud affect behaviour and mental workload? (2009) Behaviour & Information Technology, 28 (2), pp. 165-181. , https://doi.org/10.1080/01449290701773842; Zhao, T., McDonald, S., Keep talking: An analysis of participant utterances gathered using two concurrent think-aloud methods (2010) Proceedings of the 6th Nordic Conference on Human-Computer Interaction: Extending Boundaries, Ser. NordiCHI '10, pp. 581-590. , http://doi.acm.org/10.1145/1868914.1868979, New York, NY, USA: ACM, Online; Welsh, J.C., Dewhurst, S.A., Perry, J.L., Thinking aloud: An exploration of cognitions in professional snooker (2018) Psychology of Sport and Exercise, 36, pp. 197-208. , http://www.sciencedirect.com/science/article/pii/S1469029217306763; Towne, T., Ericsson, K., Sumner, A., Uncovering mechanisms in video game research: Suggestions from the expert-performance approach (2014) Frontiers in Psychology, 5, p. 161. , https://www.frontiersin.org/article/10.3389/fpsyg.2014.00161; Tausczik, Y.R., Pennebaker, J.W., The psychological meaning of words: Liwc and computerized text analysis methods (2010) Journal of Language and Social Psychology, 29 (1), pp. 24-54},
editor={Kubin G., Hain T., Schuller B., El Zarka D., Hodl P.},
sponsors={Apple; Artificial Intelligence; ASAPP; Didi; et al.; Facebook},
publisher={International Speech Communication Association},
issn={2308457X},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Frankenberg2019,
author={Frankenberg, C. and Weiner, J. and Schultz, T. and Knebel, M. and Degen, C. and Wahl, H.-W. and Schroeder, J.},
title={Perplexity - A new predictor of cognitive changes in spoken language? - Results of the Interdisciplinary Longitudinal Study on Adult Development and Aging (ILSE)},
journal={Linguistics Vanguard},
year={2019},
volume={5},
number={s2},
doi={10.1515/lingvan-2018-0026},
art_number={20180026},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068308592&doi=10.1515%2flingvan-2018-0026&partnerID=40&md5=47f76deac671884c7522f0a9028894a1},
affiliation={Heidelberg University, Section of Geriatric Psychiatry, Heidelberg, Germany; Bremen University, Cognitive Systems Lab, Bremen, Germany; Goethe Univerisity Frankfurt, Frankfurt Forum for Interdisciplinary Ageing Research, Frankfurt, Germany; Heidelberg University, Institute of Psychology, Network Aging Research, Heidelberg, Germany},
abstract={In addition to memory loss, progressive deterioration of speech and language skills is among the main symptoms at the onset of Alzheimer's disease (AD) as well as in mild cognitive impairment (MCI). Detailed interview analyses demonstrated early symptoms years before the onset of AD/MCI. Automatic speech processing could be a promising approach to identifying underlying mechanisms in larger studies or even support diagnostics. Perplexity as a measure of predictability of text could be a sensitive indicator of cognitive deterioration. Therefore, voice recordings from the Interdisciplinary Longitudinal Study on Adult Development and Aging were analyzed with regard to neuropsychological parameters in participants that develop MCI/AD or remain cognitively healthy. Preliminary results indicate that perplexity predicts severity of cognitive deficits and information processing speed obtained 10-12 years later in participants who developed MCI/AD in contrast to those who stayed healthy. Findings support the heuristic value of research on the diagnostic potential of automatic speech processing. © 2019 Walter de Gruyter GmbH, Berlin/Boston 2019.},
author_keywords={Alzheimer's disease;  language and aging;  mild cognitive impairment;  speech processing},
funding_details={Dietmar Hopp StiftungDietmar Hopp Stiftung},
funding_details={Ministerium fÃ¼r Wissenschaft, Forschung und Kunst Baden-WÃ¼rttembergMinisterium fÃ¼r Wissenschaft, Forschung und Kunst Baden-WÃ¼rttemberg, MWK},
funding_details={Baden-WÃ¼rttemberg StiftungBaden-WÃ¼rttemberg Stiftung},
funding_details={Ministerium fÃ¼r Wissenschaft, Forschung und Kunst Baden-WÃ¼rttembergMinisterium fÃ¼r Wissenschaft, Forschung und Kunst Baden-WÃ¼rttemberg, MWK},
funding_details={Dietmar Hopp StiftungDietmar Hopp Stiftung},
funding_text 1={Acknowledgments: The Interdisciplinary Longitudinal Study on Adult Development and Aging (ILSE) was funded for the first three measurements by the Federal Ministry for Family, Senior Citizen, Women, and Youth, Germany (Bundesministerium für Familie, Senioren, Frauen und Jugend – BMBFSJ) and the Ministry of Sciences, Research, and Arts Baden-Württemberg, Germany (Ministerium für Wissenschaft, Forschung und Kunst, Baden-Württemberg – MWK). The fourth examination wave was funded by the Dietmar Hopp Foundation, Germany (Dietmar Hopp Stiftung).},
references={Chantal, A., (2002) Validierung der Neuropsychologischen Testbatterie CERAD-NP, , https://www.memoryclinic.ch/fileadmin/user-upload/Memory-Clinic/Literatur/2002/Aebi-2002.pdf, Eine Multi-Center Studie. Switzerland: University of Basel dissertation (accessed 16 March 2018); Samrah, A., Haigh, A.-M.F., De Jager, C.A., Garrard, P., Connected speech as a marker of disease progression in autopsy-proven Alzheimer's disease (2013) Brain, 136 (12), pp. 3727-3737. , https://doi.org/10.1093/brain/awt269, (accessed 15 December 2018); Sonja, B., Schönknecht, P., Pantel, J., Schröder, J., Neuropsychologische Profile in der Demenzdiagnostik: Eine Untersuchung mit der CERAD-NP-Testbatterie (2005) Fortschritte der Neurologie. Psychiatrie, 73 (10), pp. 568-576. , https://doi.org/10.1055/s-2004-830249, (accessed 15 March 2018); Gerhard, B., Dittmann, J., Haas, J.-C., Wallesch Claus, W., Spontaneous speech in senile dementia and aphasia: Implications for a neurolinguistic model of language production (1987) Cognition, 27 (3), pp. 247-274. , https://doi.org/10.1016/S0010-0277(87)80011-2, (accessed 15 December 2018); Etienne, B., (1978) Le Vocabulaire de Jean Giraudoux, , Structure et Evolution. Statistique et informatique appliquées a l'étude des textes a partir des données du Trésor de la Langue Française. Geneve: Slatkine; De Lira, J.O., Oritz, K.Z., Campanha, A.C., Ferreira, B.P.H., Minett, T.S.C., Microlinguistic aspects of the oral narrative in patients with Alzheimer's disease (2011) International Psychogeriatrics, 23 (3), pp. 404-412. , https://doi.org/10.1017/S1041610210001092, (accessed 15 December 2018); Christina, D., Toro, P., Schönknecht, P., Sattler, C., Schröder, J., Diabetes mellitus Type II and cognitive capacity in healthy aging, mild cognitive impairment and Alzheimer's disease (2016) Psychiatry Research, 30 (240), pp. 42-46. , https://doi.org/10.1016/j.psychres.2016.04.009, (accessed 18 March 2018); Santos Vasco, D., Thomann, P.A., Wüstenberg, T., Seidl, U., Essig, M., Schröder, J., Morphological cerebral correlates of CERAD test performance in mild cognitive impairment and Alzheimer's disease (2011) Journal of Alzheimer's Disease, 23 (3), pp. 411-420. , https://doi.org/10.3233/JAD-2010-100156, (accessed 15 December 2018); Folstein Marshal, F., Folstein, S.E., McHugh, P.R., Mini-mental state" A practical method for grading the cognitive state of patients for the clinician (1975) Journal of Psychiatry Research, 12 (3), pp. 189-198. , https://doi.org/10.1016/0022-3956(75)90026-6, (accessed 15 December 2018); Horn, W.C., (1983) Leistungsprüfsystem, , Göttingen: Hogrefe; Frederick, J., The development of an experimental discrete dictation recognizer (1985) Proceedings of the IEEE, 73 (11), pp. 1616-1624. , https://doi.org/10.1109/PROC.1985.13343, (accessed 20 November 2018); Edith, K., Goodglass, H., Weintraub, S., (1983) Boston Naming Test, , Philadelphia: Lea & Febiger; Maren, K., Haberstroh, J., Kümmel, A., Pantel, J., Schröder, J., CODEMamb-An observational communication behavior assessment tool for use in ambulatory dementia care (2015) Aging & Mental Health, 20 (12), pp. 1286-1296. , https://doi.org/10.1080/13607863.2015.1075959, (accessed 15 December 2018); Raymond, L., Aging-associated cognitive decline (1994) International Psychogeriatrics, 6 (1), pp. 63-68. , https://doi.org/10.1017/S1041610294001626, (accessed 15 March 2018); Katarina, L., Malloy, P., Jenkins, M., Cohen, R., The naming deficit in early Alzheimer's and vascular dementia (1998) Neuropsychology, 12 (4), pp. 565-572. , http://doi.org/10.1037/0894-4105.12.4.565, (accessed December 2018); Peter, M., Martin, M., (2000) Design und Methodik der Interdisziplinären Längsschnittstudie des Erwachsenenalters, pp. 17-27. , In Peter Martin, Klaus Udo Ettrich, Ursula Lehr, Dorothea Roether, Mike Martin & Antje Fischer-Cyrulies (eds.), Aspekte der Entwicklung im mittleren und höheren Lebensalter. Ergebnisse der Interdisziplinären Längsschnittstudie des Erwachsenenalters (ILSE) Darmstadt: Steinkopff; Guy, M., Drachman, D., Folstein, M., Katzman, R., Price, D., Emanuel, M.S., Clinical diagnosis of Alzheimer's disease. Report of the nincds-adrda work group under the auspices of department of health and human services task force on Alzheimer's disease (1984) Neurology, 34 (7), pp. 939-944. , https://doi.org/10.1212/WNL.34.7.939, (accessed 15 December 2018); Morris John, C., Heyman, A., Mohs, R.C., Hughes, J.P., Van Belle, G., Fillenbaum, G., Mellits, E.D., Clark, C., The consortium to establish a registry for Alzheimer's disease (cerad) part 1 clinical and neuropsychological assessment of Alzheimer's disease (1989) Neurology, 39 (9), pp. 1159-1165. , https://doi.org/10.1212/WNL.43.12.2457, (accessed 16 March 2018); Franz, P., Anja, C.L., (2012) Wechsler Memory Scale-Fourth Edition, German Edition, , Manual. Frankfurt: Pearson Assessment; Ralph, R.M., (1992) The Trail Making Test: Manual for Administration and Scoring, , Tucson: The Reitan Neuropsychological Laboratory; Johannes, S., Kratz, B., Pantel, J., Minnemann, E., Lehr, U., Sauer, H., (1998) Prevalence of Mild Cognitive Impairment in An Elderly Community Sample, 54, pp. 51-59. , https://doi.org/10.1007/978-3-7091-7508-8-5, In H. J. Gertz, T. Arendt (eds.), Alzheimer's Disease-From Basic Research to Clinical Applications (Journal of Neural Transmission. Supplementa(accessed 15 December 2018) Vienna: Springer; Johannes, S., Pantel, J., (2011) Die Leichte Kognitive Beeinträchtigung, , Klinik, Diagnostik, Therapie und Prävention im Vorfeld der Alzheimer-Demenz. Stuttgart: Schattauer; Peter, S., Pantel, J., Kruse, A., Schröder, J., Prevalence and natural course of agingassociated cognitive decline in a population-based sample of young-old subjects (2005) The American Journal of Psychiatry, 162 (11), pp. 2071-2077. , https://doi.org/10.1176/appi.ajp.162.11.2071, (accessed 15 December 2018); Andreas, S., SRILM-An extensible language modeling toolkit (2002) International Conference on Spoken Language Processing (ICSLP 2002), Vol II, pp. 901-904. , Denver, Colorado; Pablo, T., Degen, C., Pierer, M., Gustafson, D., Schröder, J., Schönknecht, P., Cholesterol in mild cognitive impairment and Alzheimer's disease in a birth cohort over 14 years (2014) European Archives of Psychiatry and Clinical Neuroscience, 264 (6), pp. 485-492. , https://doi.org/10.1007/s00406-013-0468-2, (accessed 15 December 2018); Jochen, W., Frankenberg, C., Telaar, D., Wendelstein, B., Schröder, J., Schultz, T., Towards automatic transcription of ILSE-an Interdisciplinary longitudinal study of adult development and aging (2016) Tenth International Conference on Language Resources and Evaluation, LREC'16. Portoroz, Slovenia, pp. 23-28. , http://www.lrec-conf.org/proceedings/lrec2016/pdf/12-Paper.pdf, (accessed 16 March 2018) May; Jochen, W., Herff, C., Schultz, T., Speech-based detection of Alzheimer's disease in conversational German (2016) Annual Conference of the International Speech Communication Association, INTERSPEECH, , https://www.isca-speech.org/archive/Interspeech-2016/pdfs/0100.PDF, (accessed 16 March 2018) San Francisco, USA, 8-12 September; Jochen, W., Engelbart, M., Schultz, T., Manual and automatic transcription in dementia detection from speech (2017) Annual Conference of the International Speech Communication Association, INTERSPEECH. Stockholm, Sweden, pp. 20-24. , https://www.isca-speech.org/archive/Interspeech-2017/pdfs/0112.PDF, (accessed 16 March 2018) August; Welsh, K.A., Butters, N., Mohs, R.C., Beekly, D., Edland, S., Fillenbaum, G., Heyman, A., The consortium to establish a registry for Alzheimer's disease (cerad). V. A normative study of the neuropsychological battery (1994) Neurology, 44 (4), pp. 609-614. , https://doi.org/10.1212/WNL.44.4.609, (accessed 16 March 2018); Britta, W., Gesprochene sprache im vorfeld der Alzheimer-demenz (2016) Linguistische Analysen im Verlauf von Präklinischen Stadien Bis Zur Leichten Demenz, , University of Heidelberg: Universitätsverlag Winter dissertation},
correspondence_address1={Frankenberg, C.; Heidelberg University, Section of Geriatric PsychiatryGermany; email: Claudia.Frankenberg@med.uni-heidelberg.de},
publisher={Walter de Gruyter GmbH},
issn={2199174X},
language={English},
abbrev_source_title={Linguist. Vanguard},
document_type={Article},
source={Scopus},
}

@ARTICLE{Angrick2019,
author={Angrick, M. and Herff, C. and Mugler, E. and Tate, M.C. and Slutzky, M.W. and Krusienski, D.J. and Schultz, T.},
title={Speech synthesis from ECoG using densely connected 3D convolutional neural networks},
journal={Journal of Neural Engineering},
year={2019},
volume={16},
number={3},
doi={10.1088/1741-2552/ab0c59},
art_number={036019},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063040748&doi=10.1088%2f1741-2552%2fab0c59&partnerID=40&md5=ef2536d49db08ad09e33ad5728ba2516},
affiliation={Cognitive Systems Lab, University of Bremen, Bremen, Germany; School of Mental Health and Neuroscience, Maastricht University, Maastricht, Netherlands; Department of Neurology, Northwestern University, Chicago, IL, United States; Department of Neurological Surgery, Northwestern University, Chicago, IL, United States; Departments of Physiology, Biomedical Engineering and Physical Medicine and Rehabilitation, Northwestern University, Chicago, IL, United States; Department of Biomedical Engineering, Virginia Commonwealth University, Richmond, VA, United States},
abstract={Objective. Direct synthesis of speech from neural signals could provide a fast and natural way of communication to people with neurological diseases. Invasively-measured brain activity (electrocorticography; ECoG) supplies the necessary temporal and spatial resolution to decode fast and complex processes such as speech production. A number of impressive advances in speech decoding using neural signals have been achieved in recent years, but the complex dynamics are still not fully understood. However, it is unlikely that simple linear models can capture the relation between neural activity and continuous spoken speech. Approach. Here we show that deep neural networks can be used to map ECoG from speech production areas onto an intermediate representation of speech (logMel spectrogram). The proposed method uses a densely connected convolutional neural network topology which is well-suited to work with the small amount of data available from each participant. Main results. In a study with six participants, we achieved correlations up to r = 0.69 between the reconstructed and original logMel spectrograms. We transfered our prediction back into an audible waveform by applying a Wavenet vocoder. The vocoder was conditioned on logMel features that harnessed a much larger, pre-existing data corpus to provide the most natural acoustic output. Significance. To the best of our knowledge, this is the first time that high-quality speech has been reconstructed from neural recordings during speech production using deep neural networks. © 2019 IOP Publishing Ltd.},
author_keywords={BCI;  Brain-computer interfaces;  Electrocorticography;  Neural networks;  Speech synthesis;  Wavenet},
keywords={Brain;  Brain computer interface;  Complex networks;  Convolution;  Decoding;  Deep neural networks;  Electroencephalography;  Electrophysiology;  Neural networks;  Neurons;  Spectrographs;  Speech synthesis;  Vocoders, Complex Processes;  Convolutional neural network;  Electrocorticography;  Intermediate representations;  Neural recordings;  Neurological disease;  Temporal and spatial;  Wavenet, Speech communication, Article;  artificial neural network;  brain mapping;  electrocorticography;  human;  human experiment;  normal human;  prediction;  priority journal;  signal processing;  speech;  statistical model;  waveform;  brain cortex;  communication aid;  electrocorticography;  photostimulation;  physiology;  procedures;  speech, Cerebral Cortex;  Communication Aids for Disabled;  Electrocorticography;  Humans;  Neural Networks, Computer;  Photic Stimulation;  Speech},
funding_details={Norsk SykepleierforbundNorsk Sykepleierforbund, NSF, 1608140},
funding_details={Doris Duke Charitable FoundationDoris Duke Charitable Foundation, DDCF, 2011039},
funding_details={National Institutes of HealthNational Institutes of Health, NIH, F32DC015708},
funding_details={National Institutes of HealthNational Institutes of Health, NIH, R01NS094748},
funding_details={National Center for Advancing Translational SciencesNational Center for Advancing Translational Sciences, NCATS, UL1TR000150},
funding_details={National Center for Advancing Translational SciencesNational Center for Advancing Translational Sciences, NCATS, UL1TR001422},
funding_text 1={MA, CH, DK and TS acknowledge funding by BMBF (01GQ1602) and NSF (1608140) as part of the NSF/NIH/ BMBF Collaborative Research in Computational Neuroscience Program. MS acknowledges funding by the Doris Duke Charitable Foundation (Clinical Scientist Development Award, grant 2011039), a Northwestern Memorial Foundation Dixon Translational Research Award (including partial funding from NIH National Center for Advancing Translational Sciences, UL1TR000150 and UL1TR001422), NIH grants F32DC015708 and R01NS094748.},
references={Pirila, S., Van-Der-Meere, J., Pentikainen, T., Ruusu-Niemi, P., Korpela, R., Kilpinen, J., Nieminen, P., Language and motor speech skills in children with cerebral palsy (2007) J. Commun. Disorders, 40, pp. 116-128; Turner, G.S., Tjaden, K., Weismer, G., The influence of speaking rate on vowel space and speech intelligibility for individuals with amyotrophic lateral sclerosis (1995) J. Speech Lang. Hear. Res., 38, pp. 1001-1013; Kent, R.D., Kent, J.F., Weismer, G., Sufit, R.L., Rosenbek, J.C., Martin, R.E., Brooks, B.R., Impairment of speech intelligibility in men with amyotrophic lateral sclerosis (1990) J. Speech Hear. Disorders, 55, pp. 721-728; Starmer, H.M., Tippett, D.C., Webster, K.T., Effects of laryngeal cancer on voice and swallowing (2008) Otolaryngol. Clin. North Am., 41, pp. 793-818; Schultz, T., Wand, M., Hueber, T., Krusienski, D.J., Herff, C., Brumberg, J.S., Biosignal-based spoken communication: A survey (2017) IEEE/ACM Trans. Audio Speech Lang. Process, 25, pp. 2257-2271; Wolpaw, J., Birbaumer, N., McFarland, D., Pfurtscheller, G., Vaughan, T., Brain-computer interfaces for communication and control (2002) Clin. Neurophysiol., 113, pp. 767-791; Bocquelet, F., Hueber, T., Girin, L., Chabardès, S., Yvert, B., Key considerations in designing a speech brain- computer interface (2016) J. Physiol., 110, pp. 392-401; Herff, C., Schultz, T., Automatic speech recognition from neural signals: A focused review (2016) Frontiers Neurosci., 10, p. 429; Slutzky, M.W., Flint, R.D., Physiological properties of brain-machine interface input signals (2017) J. Neurophysiol., 118, pp. 1329-1343; Chakrabarti, S., Sandberg, H.M., Brumberg, J.S., Krusienski, D.J., Progress in speech decoding from the electrocorticogram (2015) Biomed. Eng. Lett., 5, pp. 10-21; Rabbani, Q., Milsap, G., Crone, N.E., The potential for a speech brain-computer interface using chronic electrocorticography (2019) Neurotherapeutics, 16, pp. 1-22; Riès, S.K., Spatiotemporal dynamics of word retrieval in speech production revealed by cortical high-frequency band activity (2017) Proc. Natl Acad. Sci., 114, pp. E4530-E4538; Kellis, S., Miller, K., Thomson, K., Brown, R., House, P., Greger, B., Decoding spoken words using local field potentials recorded from the cortical surface (2010) J. Neural Eng., 7, p. 056007; Ramsey, N., Salari, E., Aarnoutse, E., Vansteensel, M., Bleichner, M., Freudenburg, Z., Decoding spoken phonemes from sensorimotor cortex with high-density ECoG grids (2018) NeuroImage, 180, pp. 301-311; Mugler, E., Patton, J., Flint, R., Wright, Z., Schuele, S., Rosenow, J., Shih, J., Slutzky, M., Direct classification of all American English phonemes using signals from functional speech motor cortex (2014) J. Neural Eng., 11, p. 035015; Lotte, F., Brumberg, J.S., Brunner, P., Gunduz, A., Ritaccio, A.L., Guan, C., Schalk, G., Electrocorticographic representations of segmental features in continuous speech (2015) Frontiers Hum. Neurosci., 9, p. 97; Mesgarani, N., Cheung, C., Johnson, K., Chang, E.F., Phonetic feature encoding in human superior temporal gyrus (2014) Science, 343, p. 1245994; Mugler, E.M., Tate, M.C., Livescu, K., Templer, J.W., Goldrick, M.A., Slutzky, M.W., Differential representation of articulatory gestures and phonemes in precentral and inferior frontal gyri (2018) J. Neurosci., pp. 1206-1218; Herff, C., Heger, D., De-Pesters, A., Telaar, D., Brunner, P., Schalk, G., Schultz, T., Brain-to-text: Decoding spoken phrases from phone representations in the brain (2015) Frontiers Neurosci., 9, p. 217; Moses, D.A., Mesgarani, N., Leonard, M.K., Chang, E.F., Neural speech recognition: Continuous phoneme decoding using spatiotemporal representations of human cortical activity (2016) J. Neural Eng., 13, p. 056004; Stuart, A., Kalinowski, J., Rastatter, M.P., Lynch, K., Effect of delayed auditory feedback on normal speakers at two speech rates (2002) J. Acoust. Soc. Am., 111, pp. 2237-2241; Stuart, A., Kalinowski, J., Effect of delayed auditory feedback, speech rate, and sex on speech production (2015) Perceptual Motor Skills, 120, pp. 747-765; Iljina, O., Derix, J., Schirrmeister, R.T., Schulze-Bonhage, A., Auer, P., Aertsen, A., Ball, T., Neurolinguistic and machine-learning perspectives on direct speech bcis for restoration of naturalistic communication (2017) Brain-Comput. Interfaces, 4, pp. 186-199; Santoro, R., Moerel, M., De-Martino, F., Valente, G., Ugurbil, K., Yacoub, E., Formisano, E., Reconstructing the spectrotemporal modulations of real-life sounds from FMRI response patterns (2017) Proc. Natl Acad. Sci., 114, p. 201617622; Kubanek, J., Brunner, P., Gunduz, A., Poeppel, D., Schalk, G., The tracking of speech envelope in the human cortex (2013) PLoS One, 8; Tang, C., Hamilton, L., Chang, E., Intonational speech prosody encoding in the human auditory cortex (2017) Science, 357, pp. 797-801; Dichter, B.K., Breshears, J.D., Leonard, M.K., Chang, E.F., The control of vocal pitch in human laryngeal motor cortex (2018) Cell, 174, pp. 21-31; Pasley, B.N., David, S.V., Mesgarani, N., Flinker, A., Shamma, S.A., Crone, N.E., Knight, R.T., Chang, E.F., Reconstructing speech from human auditory cortex (2012) PLoS Biol., 10; Martin, S., Brunner, P., Holdgraf, C., Heinze, H.-J., Crone, N., Rieger, J., Schalk, G., Pasley, B., Decoding spectrotemporal features of overt and covert speech from the human cortex (2014) Frontiers Neuroeng., 7, p. 14; Herff, C., Johnson, G., Diener, L., Shih, J., Krusienski, D., Schultz, T., Towards direct speech synthesis from ECoG: A pilot study (2016) IEEE 38th Annual Int. Conf. Eng. Med. Biol. Soc., pp. 1540-1543; Hinton, G., Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups (2012) IEEE Signal Process. Mag., 29, pp. 82-97; Ze, H., Senior, A., Schuster, M., Statistical parametric speech synthesis using deep neural networks (2013) IEEE Int. Conf. On Acoustics, Speech and Signal Processing, pp. 7962-7966; Pandarinath, C., Inferring single-trial neural population dynamics using sequential auto-encoders (2018) Nat. Methods, 15, p. 1; Seeliger, K., Fritsche, M., Güçlü, U., Schoenmakers, S., Schoffelen, J.-M., Bosch, S., Van-Gerven, M., Convolutional neural network-based encoding and decoding of visual object recognition in space and time (2018) NeuroImage, 180, pp. 253-266; Berezutskaya, J., Modeling brain responses to perceived speech with lstm networks (2017) Benelearn 2017: Proc. 26th Benelux Conf. On Machine Learning (Technische Universiteit Eindhoven, 9-10 June 2017), pp. 149-153. , ed W Duivesteijn et al; Güçlü, U., Van-Gerven, M.A., Modeling the dynamics of human brain activity with recurrent neural networks (2017) Frontiers in Comput. Neurosci., 11, p. 7; Sturm, I., Lapuschkin, S., Samek, W., Müller, K.-R., Interpretable deep neural networks for single-trial EEG classification (2016) J. Neurosci. Methods, 274, pp. 141-145; Sussillo, D., Nuyujukian, P., Fan, J.M., Kao, J.C., Stavisky, S.D., Ryu, S., Shenoy, K., A recurrent neural network for closed-loop intracortical brain-machine interface decoders (2012) J. Neural Eng., 9, p. 026027; Rezazadeh-Sereshkeh, A., Trott, R., Bricout, A., Chau, T., EEG classification of covert speech using regularized neural networks (2017) IEEE/ACM Trans. Audio Speech Lang. Process, 25, pp. 2292-2300; Schirrmeister, R., Springenberg, J., Fiederer, L., Glasstetter, M., Eggensperger, K., Tangermann, M., Hutter, F., Ball, T., Deep learning with convolutional neural networks for EEG decoding and visualization (2017) Hum. Brain Mapp., 38, pp. 5391-5420; Hennrich, J., Herff, C., Heger, D., Schultz, T., Investigating deep learning for fnirs based BCI (2015) 37th Annual Int. Conf. IEEE Eng. Med. Biol. Soc.; Angrick, M., Herff, C., Johnson, G., Shih, J., Krusienski, D., Schultz, T., Interpretation of convolutional neural networks for speech regression from electrocorticography (2018) 26th European Symp. On Artificial Neural Networks, Computational Intelligence and Machine Learning (Brugge, Belgium), pp. 7-12; Shen, J., Natural tts synthesis by conditioning wavenet on mel spectrogram predictions (2018) IEEE Int. Conf. On Acoustics, Speech and Signal Processing (ICASSP), pp. 4779-4783; Antoniades, A., Spyrou, L., Took, C.C., Sanei, S., Deep learning for epileptic intracranial EEG data (2016) IEEE 26th Int. Workshop on Machine Learning for Signal Processing, pp. 1-6; Chambon, S., Thorey, V., Arnal, P.J., Mignot, E., Gramfort, A., A deep learning architecture to detect events in EEG signals during sleep (2018) IEEE 28th Int. Workshop on Machine Learning for Signal Processing, pp. 1-6; House, A.S., Williams, C., Hecker, M.H., Kryter, K.D., Psychoacoustic speech tests: A modified rhyme test (1963) J. Acoust. Soc. Am., 35, p. 1899; Schalk, G., McFarland, D.J., Hinterberger, T., Birbaumer, N., Wolpaw, J.R., Bci2000: A general-purpose brain- computer interface (BCI) system (2004) IEEE Trans. Biomed. Eng., 51, pp. 1034-1043; Ray, S., Crone, N.E., Niebur, E., Franaszczuk, P.J., Hsiao, S.S., Neural correlates of high-gamma oscillations (60-200 Hz) in macaque local field potentials and their potential implications in electrocorticography (2008) J. Neurosci., 28, pp. 11526-11536; Livezey, J.A., Bouchard, K.E., Chang, E.F., (2018) Deep Learning As A Tool for Neural Data Analysis: Speech Classification and Cross-frequency Coupling in Human Sensorimotor Cortex; Hickok, G., Computational neuroanatomy of speech production (2012) Nat. Rev. Neurosci., 13, p. 135; Brumberg, J., Krusienski, D., Chakrabarti, S., Gunduz, A., Brunner, P., Ritaccio, A., Schalk, G., Spatio-temporal progression of cortical activity related to continuous overt and covert speech production in a reading task (2016) PLoS One, 11; Martin, S., Millán, J.D.R., Knight, R.T., Pasley, B.N., The use of intracranial recordings to decode human language: Challenges and opportunities (2016) Brain Lang., , https://doi.org/10.1016/j.bandl.2016.06.003; Stevens, S.S., Volkmann, J., Newman, E.B., A scale for the measurement of the psychological magnitude pitch (1937) J. Acoust. Soci. Am., 8, pp. 185-190; Huang, G., Liu, Z., Weinberger, K.Q., Van-Der-Maaten, L., Densely connected convolutional networks (2017) Proc. Of the IEEE Conf. Of Computer Vision and Pattern Recognition, 1, p. 3; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proc. IEEE Conf. On Computer Vision and Pattern Recognition, pp. 770-778; Kingma, D., Ba, J., Adam: A method for stochastic optimization (2015) Int. Conf. On Learning Representations (ICLR); Van-Den-Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Kavukcuoglu, K., (2016) Wavenet: A Generative Model for Raw Audio; Tamamori, A., Hayashi, T., Kobayashi, K., Takeda, K., Toda, T., Speaker-dependent wavenet vocoder (2017) Proc. Of Interspeech, pp. 1118-1122; Nagaraj-Adiga, V.T., On the use of wavenet as a statistical vocoder (2018) IEEE Int. Conf. On Acoustics, Speech and Signal Processing (ICASSP), pp. 5674-5678; Ito, K., (2017) The Lj Speech Dataset, , https://keithito.com/LJ-Speech-Dataset; Salimans, T., Karpathy, A., Chen, X., Kingma, D.P., Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications (2017) 5th Int. Conf. On Learning Representations (ICLR); Oord, A.V.D., Parallel wavenet: Fast high-fidelity speech synthesis (2018) 35th Int. Conf. On Machine Learning (ICML); Yamamoto, R., (2018) Wavenet Vocoder, , https://github.com/r9y9/wavenet-vocoder; Taal, C.H., Hendriks, R.C., Heusdens, R., Jensen, J., A short-time objective intelligibility measure for timefrequency weighted noisy speech (2010) IEEE Int. Conf. On Acoustics Speech and Signal Processing (IEEE), pp. 4214-4217; Su, H., Chen, H., (2015) Experiments on Parallel Training of Deep Neural Network Using Model Averaging; Kalchbrenner, N., Elsen, E., Simonyan, K., Noury, S., Casagrande, N., Lockhart, E., Stimberg, F., Kavukcuoglu, K., Efficient neural audio synthesis (2018) 35th Int. Conf. On Machine Learning (ICML); Hochberg, L.R., Serruya, M.D., Friehs, G.M., Mukand, J.A., Saleh, M., Caplan, A.H., Branner, A., Donoghue, J.P., Neuronal ensemble control of prosthetic devices by a human with tetraplegia (2006) Nature, 442, p. 164; Pandarinath, C., Nuyujukian, P., Blabe, C.H., Sorice, B.L., Saab, J., Willett, F.R., Hochberg, L.R., Henderson, J.M., High performance communication by people with paralysis using an intracortical brain-computer interface (2017) ELife, 6; Downey, J.E., Brane, L., Gaunt, R.A., Tyler-Kabara, E.C., Boninger, M.L., Collinger, J.L., Motor cortical activity changes during neuroprosthetic-controlled object interaction (2017) Sci. Rep., 7, p. 16947; Ajiboye, A.B., Restoration of reaching and grasping movements through brain-controlled muscle stimulation in a person with tetraplegia: A proof-of-concept demonstration (2017) Lancet, 389, pp. 1821-1830; Guenther, F.H., A wireless brain-machine interface for real-time speech synthesis (2009) PLoS One, 4; Martin, S., Brunner, P., Iturrate, I., Millán, J., Schalk, G., Knight, R., Pasley, B., Word pair classification during imagined speech using direct brain recordings (2016) Sci. Rep., 6, p. 25803},
publisher={Institute of Physics Publishing},
issn={17412560},
pubmed_id={30831567},
language={English},
abbrev_source_title={J. Neural Eng.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Salous2018,
author={Salous, M. and Putze, F. and Schultz, T. and Hild, J. and Beyerer, J.},
title={Investigating static and sequential models for intervention-free selection using multimodal data of EEG and eye tracking},
journal={Proceedings of the Workshop on Modeling Cognitive Processes from Multimodal Data, MCPMD 2018},
year={2018},
doi={10.1145/3279810.3279841},
note={cited By 1; Conference of 2018 Workshop on Modeling Cognitive Processes from Multimodal Data, MCPMD 2018 ; Conference Date: 16 October 2018;  Conference Code:142326},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058264078&doi=10.1145%2f3279810.3279841&partnerID=40&md5=4cef8dfb55edf6bdb242303d92665d53},
affiliation={University of Bremen, Bremen, Germany; Fraunhofer IOSB, Karlsruhe, Germany},
abstract={Multimodal data is increasingly used in cognitive prediction models to better analyze and predict different user cognitive processes. Classifiers based on such data, however, have different performance characteristics. We discuss in this paper an intervention-free selection task using multimodal data of EEG and eye tracking in three different models. We show that a sequential model, LSTM, is more sensitive but less precise than a static model SVM. Moreover, we introduce a confidence-based Competition-Fusion model using both SVM and LSTM. The fusion model further improves the recall compared to either SVM or LSTM alone, without decreasing precision compared to LSTM. According to the results, we recommend SVM for interactive applications which require minimal false positives (high precision), and recommend LSTM and highly recommend Competition-Fusion Model for applications which handle intervention-free selection requests in an additional post-processing step, requiring higher recall than precision. © 2018 Copyright held by the owner/author(s).},
author_keywords={Competition Model;  EEG;  Eye tracking;  Multimodal data;  Precision;  Recall},
keywords={Cognitive systems;  Data reduction;  Electroencephalography;  Eye tracking, Cognitive process;  Competition model;  Interactive applications;  Multi-modal data;  Performance characteristics;  Precision;  Prediction model;  Recall, Long short-term memory},
references={Baccino, T., Manunta, Y., Eye-fixation-related potentials: Insight into parafoveal processing (2005) Journal of Psychophysiology, 19 (3), pp. 204-215. , 2005; Choi, J.-S., Bang, J.W., Park, K.R., Whang, M., Enhanced perception of user intention by combining EEG and gaze-tracking for brain-computer interfaces (BCIs) (2013) Sensors, 13 (3), pp. 3454-3472. , 2013; Dimigen, O., Sommer, W., Hohlfeld, A., Jacobs, A.M., Kliegl, R., Coregistration of eye movements and EEG in natural reading: Analyses and review (2011) Journal of Experimental Psychology: General, 140 (4), p. 552. , 2011; Finke, A., Essig, K., Marchioro, G., Ritter, H., Toward FRP-based brain-machine interfacesâĂŤsingle-trial classification of fixation-related potentials (2016) PloS One, 11 (1), p. e0146848. , 2016; Hochreiter, S., Schmidhuber, J., Long short-term memory (1997) Neural Computation, 9 (8), pp. 1735-1780. , 1997; Hutzler, F., Braun, M., Võ, M.L.-H., Engl, V., Hofmann, M., Dambacher, M., Leder, H., Jacobs, A.M., Welcome to the real world: Validating fixation-related brain potentials for ecologically valid settings (2007) Brain Research, 1172 (2007), pp. 124-129; Kittler, J., Hatef, M., Duin, R.P.W., Matas, J., On combining classifiers (1998) IEEE Transactions on Pattern Analysis and Machine Intelligence, 20 (3), pp. 226-239. , 1998; Lotte, F., A tutorial on EEG signal-processing techniques for mental-state recognition in brain–computer interfaces (2014) Guide to Brain-Computer Music Interfacing, pp. 133-161. , Springer; Platt, J., Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods (1999) Advances in Large Margin Classifiers, 10 (3), pp. 61-74. , 1999; Putze, F., Hild, J., Kärgel, R., Herff, C., Redmann, A., Beyerer, J., Schultz, T., Locating user attention using eye tracking and EEG for spatio-temporal event selection (2013) Proceedings of The 2013 International Conference on Intelligent User Interfaces, pp. 129-136. , ACM; Putze, F., Popp, J., Hild, J., Beyerer, J., Schultz, T., Intervention-free selection using EEG and eye tracking (2016) Proceedings of The 18th ACM International Conference on Multimodal Interaction, pp. 153-160. , ACM; Putze, F., Salous, M., Schultz, T., Detecting memory-based interaction obstacles with a recurrent neural model of user behavior (2018) 23rd International Conference on Intelligent User Interfaces, pp. 205-209. , ACM; Salous, M., Putze, F., Behaviour-Based Working Memory Capacity Classification Using Recurrent Neural Networks, , n. d. n. d; Salvucci, D.D., Goldberg, J.H., Identifying fixations and saccades in eye-tracking protocols (2000) Proceedings of The 2000 Symposium on Eye Tracking Research & Applications, pp. 71-78. , ACM; Shishkin, S.L., Nuzhdin, Y.O., Svirin, E.P., Trofimov, A.G., Fedorova, A.A., Kozyrskiy, B.L., Velichkovsky, B.M., EEG negativity in fixations used for gaze-based control: Toward converting intentions into actions with an eye-brain-computer interface (2016) Frontiers in Neuroscience, 10, p. 528. , 2016},
sponsors={ACM Special Interest Group on Computer-Human Interaction (SIGCHI)},
publisher={Association for Computing Machinery, Inc},
isbn={9781450360722},
language={English},
abbrev_source_title={Proc. Workshop Model. Cogn. Process. Multimodal Data, MCPMD},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Putze2018663,
author={Putze, F. and Kasneci, E. and Hild, J. and Solovey, E. and Sano, A. and Schultz, T.},
title={Modeling cognitive processes from multimodal signals},
journal={ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction},
year={2018},
pages={663},
doi={10.1145/3242969.3265861},
note={cited By 0; Conference of 20th ACM International Conference on Multimodal Interaction, ICMI 2018 ; Conference Date: 16 October 2018 Through 20 October 2018;  Conference Code:140787},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056633858&doi=10.1145%2f3242969.3265861&partnerID=40&md5=7b348455b7ea387875c5963c0e91e855},
affiliation={University of Bremen, Bremen, Germany; University of Tübingen, Tübingen, Germany; Fraunhofer IOSB, Karlsruhe, Germany; Worcester Polytechnic Institute, Worcester, MA, United States; Rice University HoustonTX, United States},
sponsors={ACM SIGCHI},
publisher={Association for Computing Machinery, Inc},
isbn={9781450356923},
language={English},
abbrev_source_title={ICMI - Proc. Int. Conf. Multimodal Interact.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Putze2018205,
author={Putze, F. and Salous, M. and Schultz, T.},
title={Detecting memory-based interaction obstacles with a recurrent neural model of user behavior},
journal={International Conference on Intelligent User Interfaces, Proceedings IUI},
year={2018},
pages={205-209},
doi={10.1145/3172944.3173006},
note={cited By 4; Conference of 23rd ACM International Conference on Intelligent User Interfaces, IUI 2018 ; Conference Date: 7 March 2018 Through 11 March 2018;  Conference Code:135193},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045457061&doi=10.1145%2f3172944.3173006&partnerID=40&md5=483aa0734962f1bb554247aadf116836},
affiliation={University of Bremen, Bremen, Germany},
abstract={A memory-based interaction obstacle is a condition which impedes human memory during Human-Computer Interaction, for example a memory-loading secondary task. In this paper, we present an approach to detect the presence of such memory-based interaction obstacles from logged user behavior during system use. For this purpose, we use a recurrent neural network which models the resulting temporal sequences. To acquire a sufficient number of training episodes, we employ a cognitive user simulation. We evaluate the approach with data from a user test and on which we outperform a non-sequential baseline by up to 42% relative. © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
author_keywords={Classification of user behavior;  Interaction obstacles;  LSTMs;  Memory},
keywords={Data storage equipment;  Human computer interaction;  Recurrent neural networks;  User interfaces, Human memory;  Interaction obstacles;  LSTMs;  Neural modeling;  Secondary tasks;  Temporal sequences;  User behaviors;  User simulation, Behavioral research},
funding_details={Deutsche ForschungsgemeinschaftDeutsche Forschungsgemeinschaft, DFG, PU 613/1-1},
funding_text 1={*Partially funded by DFG grant PU 613/1-1: Detection of Interaction Competencies and Obstacles (DINCO).},
references={Anderson, J.R., Bothell, D., Byrne, M.D., Douglass, S., Lebiere, C., Qin, Y., An integrated theory of the mind (2004) Psychological Review, 111 (4), pp. 1036-1060. , 2004; Baldwin, C.L., Penaranda, B.N., Adaptive training using an artificial neural network and EEG metrics for within- and cross-task workload classification (2012) NeuroImage, 59 (1), pp. 48-56. , 2012; Berka, C., Levendowski, D.J., Lumicao, M.N., Yau, A., Davis, G., Zivkovic, V.T., Olmstead, R.E., Craven, P.L., EEG correlates of task engagement and mental workload in vigilance, learning, and memory tasks (2007) Aviation, Space, and Environmental Medicine, 78, pp. B231-B244. , 2007; Conway, A.R.A., Kane, M.J., Engle, R.W., Working memory capacity and its relation to general intelligence (2003) Trends in Cognitive Sciences, 7 (12), pp. 547-552. , 2003; Eck, D., Schmidhuber, J., Finding temporal structure in music: Blues improvisation with LSTM recurrent networks (2002) Neural Networks for Signal Processing, 2002. Proceedings of the 2002 12th IEEE Workshop on, pp. 747-756. , IEEE; Ehret, S., Putze, F., Miller-Teynor, H., Kruse, A., Schultz, T., Technique-based game for daycare visitors with and without dementia: Effects, heuristics and correlates (2017) Zeitschrift fur Gerontologie und Geriatrie, 50 (1), pp. 35-44. , 2017; Fogel, D.B., An introduction to simulated evolutionary optimization (1994) IEEE Transactions on Neural Networks, 5 (1), pp. 3-14. , 1994; Gers, F.A., Schmidhuber, J., Cummins, F., Learning to forget: Continual prediction with LSTM (2000) Neural Computation, 12 (10), pp. 2451-2471. , 2000; Gong, Z., Chen, H., Model-based oversampling for imbalanced sequence classification (2016) Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, pp. 1009-1018. , ACM; Graves, A., Liwicki, M., Fernndez, S., Bertolami, R., Bunke, H., Schmidhuber, J., A novel connectionist system for unconstrained handwriting recognition (2009) IEEE Transactions on Pattern Analysis and Machine Intelligence, 31 (5), pp. 855-868. , 2009; He, H., Bai, Y., Garcia, E.A., Li, S., ADASYN: Adaptive synthetic sampling approach for imbalanced learning (2008) 2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence), pp. 1322-1328; Heger, D., Putze, F., Schultz, T., An adaptive information system for an empathic robot using EEG data (2010) Social Robotics, pp. 151-160. , Springer; Herff, C., Fortmann, O., Tse, C.-Y., Cheng, X., Putze, F., Heger, D., Schultz, T., Hybrid fNIRS-EEG based discrimination of 5 levels of memory load (2015) 2015 7th International IEEE/EMBS Conference on Neural Engineering (NER), pp. 5-8; Hochreiter, S., Schmidhuber, J., Long short-term memory (1997) Neural Computation, 9 (8), pp. 1735-1780. , 1997; Katidioti, I., Borst, J.P., Bierens De Haan, D.J., Pepping, T., Van Vugt, M.K., Taatgen, N.A., Interrupted by your pupil: An interruption management system based on pupil dilation (2016) International Journal of Human-computer Interaction, 32 (10), pp. 791-801. , 2016; Ke, Y., Qi, H., He, F., Liu, S., Zhao, X., Zhou, P., Zhang, L., Ming, D., An EEG-based mental workload estimator trained on working memory task can work well under simulated multi-attribute task (2014) Frontiers in Human Neuroscience, 8. , 2014; Lewandowsky, S., Oberauer, K., Yang, L.-X., Ecker, U.K.H., A working memory test battery for MATLAB (2010) Behavior Research Methods, 42 (2), pp. 571-585. , 2010; Mühl, C., Jeunet, C., Lotte, F., EEG-based workload estimation across affective contexts (2014) Frontiers in Neuroscience, 8. , 2014; Pröpper, R., Putze, F., Schultz, T., JAM: Java-based associative memory (2011) Proceedings of the Paralinguistic Information and its Integration in Spoken Dialogue Systems Workshop, pp. 143-155. , Springer; Putze, F., Ehret, S., Miller-Teynor, H., Kruse, A., Schultz, T., Model-based evaluation of playing strategies in a memo game for elderly users (2015) Proceedings of IEEE International Conference on Systems, Man and Cybernetics, , Hongkong, China; Putze, F., Schultz, T., Adaptive cognitive technical systems (2014) Journal of Neuroscience Methods, 234, pp. 108-115. , 2014; Rozado, D., Duenser, A., Howell, B., Improving the performance of an EEG-based motor imagery brain computer interface using task evoked changes in pupil diameter (2015) PloS One, 10 (3), p. e0121262. , 2015; Schaul, T., Schmidhuber, J., Scalable neural networks for board games (2009) Artificial Neural Networks ICANN 2009, pp. 1005-1014; Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., Dropout: A simple way to prevent neural networks from overfitting (2014) Journal of Machine Learning Research, 15, pp. 1929-1958. , 2014; Wolters, M., Georgila, K., Moore, J.D., Logie, R.H., MacPherson, S.E., Watson, M., Reducing working memory load in spoken dialogue systems (2009) Interacting with Computers, 21 (4), pp. 276-287. , 2009},
sponsors={ACM SIGCHI; ACM SIGCHI},
publisher={Association for Computing Machinery},
isbn={9781450349451},
language={English},
abbrev_source_title={Int Conf Intell User Interfaces Proc IUI},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Diener20183162,
author={Diener, L. and Schultz, T.},
title={Investigating objective intelligibility in real-time EMG-to-speech conversion},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2018},
volume={2018-September},
pages={3162-3166},
doi={10.21437/Interspeech.2018-2080},
note={cited By 3; Conference of 19th Annual Conference of the International Speech Communication, INTERSPEECH 2018 ; Conference Date: 2 September 2018 Through 6 September 2018;  Conference Code:139961},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054973219&doi=10.21437%2fInterspeech.2018-2080&partnerID=40&md5=3b2d1d2fae1aa0bbbe2e092b1762a99b},
affiliation={Cognitive Systems Lab, University of Bremen, Germany},
abstract={This paper presents an analysis of the influence of various system parameters on the output quality of our neural network based real-time EMG-to-Speech conversion system. This EMG-to-Speech system allows for the direct conversion of facial surface electromyographic signals into audible speech in real time, allowing for a closed-loop setup where users get direct audio feedback. Such a setup opens new avenues for research and applications through co-adaptation approaches. In this paper, we evaluate the influence of several parameters on the output quality, such as time context, EMG-Audio delay, network-, training data- and Mel spectrogram size. The resulting output quality is evaluated based on the objective output quality measure STOI. © 2018 International Speech Communication Association. All rights reserved.},
author_keywords={Silent speech interfaces;  Speech synthesis;  Surface electromyography},
keywords={Electromyography;  Quality control;  Speech communication;  Speech synthesis, Audio feedbacks;  Direct conversion;  Electromyographic signal;  Facial surfaces;  Research and application;  Silent speech interfaces;  Speech conversion;  Surface electromyography, Speech intelligibility},
references={Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J.M., Brumberg, J.S., Silent speech interfaces (2010) Speech Communication, 52 (4), pp. 270-287; Schultz, T., Wand, M., Hueber, T., Herff, C., Brumberg, J.S., Biosignal-based spoken communication: A survey (2017) IEEE/ACM Transactions on Audio, Speech and Language Processing, 25 (12), pp. 2257-2271. , K. D. J, Nov; Gonzalez, J.A., Cheah, L.A., Gilbert, J.M., Bai, J., Ell, S.R., Green, P.D., Moore, R.K., Direct speech generation for a silent speech interface based on permanent magnet articulography (2016) Proceedings of The 9th International Joint Conference on Biomedical Engineering Systems and Technologies, 4, pp. 96-105; Hofe, R., Ell, S.R., Fagan, M.J., Gilbert, J.M., Green, P.D., Moore, R.K., Rybchenko, S.I., Evaluation of a silent speech interface based on magnetic sensing (2010) Proc. Interspeech; Kim, M., Cao, B., Mau, T., Wang, J., Speaker-independent silent speech recognition from flesh-point articulatory movements using an lstm neural network (2017) IEEE/ACM Transactions on Audio, Speech, and Language Processing, 25 (12), pp. 2323-2336; Bowden, R., Cox, S., Harvey, R., Lan, Y., Ong, E.-J., Recent developments in automated lip-reading (2013) SPIE Security+ De-Fence, , International Society for Optics and Photonics; Hueber, T., Benaroya, E.-L., Chollet, G., Denby, B., Dreyfus, G., Stone, M., Development of a silent speech interface driven by ultrasound and optical images of the tongue and lips (2010) Speech Communication, 52, pp. 288-300; Brumberg, J.S., Nieto-Castanon, A., Kennedy, P.R., Guenther, F.H., Brain-computer interfaces for speech communication (2010) Speech Communication, 52, pp. 367-379; Herff, C., Heger, D., De Pesters, A., Telaar, D., Brunner, P., Schalk, G., Schultz, T., Brain-to-text: Decoding spoken phrases from phone representations in the brain (2015) Frontiers in Neuroscience, 9, pp. 1-11; Janke, M., Diener, L., EMG-to-speech: Direct generation of speech from facial electromyographic signals (2017) IEEE/ACM Transactions on Audio, Speech and Language Processing, 25 (12), pp. 2375-2385. , Nov; Fairbanks, G., Guttman, N., Effects of delayed auditory feedback upon articulation (1958) Journal of Speech, Language, and Hearing Research, 1 (1), pp. 12-22; Janke, M., Wand, M., Schultz, T., Impact of lack of acoustic feedback in EMG-based silent speech recognition (2010) 11th Annual Conference of The International Speech Communication Association; Diener, L., Janke, M., Schultz, T., Direct conversion from facial myoelectric signals to speech using deep neural networks (2015) International Joint Conference on Neural Networks, pp. 1-7. , iJCNN 2015; Diener, L., Herff, C., Janke, M., Schultz, T., An initial investigation into the real-time conversion of facial surface EMG signals to audible speech (2016) Engineering in Medicine and Biology Society (EMBC), 2016 38th Annual International Conference of The IEEE, , Aug; Jou, S.-C.S., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proceedings of The Annual Conference of The International Speech Communication Association, pp. 573-576; Griffin, D.W., Lim, J.S., Signal estimation from modified short-time fourier transform (1984) Acoustics, Speech and Signal Processing, IEEE Transactions on, 32 (2), pp. 236-243; Taal, C.H., Hendriks, R.C., Heusdens, R., Jensen, J., A short-time objective intelligibility measure for time-frequency weighted noisy speech (2010) Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on, pp. 4214-4217. , IEEE; Cavanagh, P., Komi, P., Electromechanical delay in human skeletal muscle under concentric and eccentric contractions (1979) European Journal of Applied Physiology and Occupational Physiology, 42 (3), pp. 159-163; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) 2005 IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 331-336; Wand, M., Schulte, C., Janke, M., Schultz, T., Compensation of recording position shifts for a myoelectric silent speech recognizer (2014) The 39th International Conference on Acoustics, Speech, and Signal Processing, 2014, iCASSP},
editor={Sekhar C.C., Rao P., Ghosh P.K., Murthy H.A., Yegnanarayana B., Umesh S., Alku P., Prasanna S.R.M., Narayanan S.},
sponsors={Adobe; et al.; JD.Com; MI; Samsung; Uniphore},
publisher={International Speech Communication Association},
issn={2308457X},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Weiner2018747,
author={Weiner, J. and Schultz, T.},
title={Selecting Features for Automatic Screening for Dementia Based on Speech},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={11096 LNAI},
pages={747-756},
doi={10.1007/978-3-319-99579-3_76},
note={cited By 2; Conference of 20th International Conference on Speech and Computer, SPECOM 2018 ; Conference Date: 18 September 2018 Through 22 September 2018;  Conference Code:218179},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053807762&doi=10.1007%2f978-3-319-99579-3_76&partnerID=40&md5=a4f0304155552112d14bc1fe675fa882},
affiliation={Cognitive Systems Lab, University of Bremen, Bremen, Germany},
abstract={As the population in developed countries ages, larger numbers of people are at risk of developing dementia. In the near future large-scale time- and cost-efficient screening methods will be needed. Speech can be recorded and analyzed in this manner, and as speech and language are affected early on in the course of dementia, automatic speech processing can provide valuable support for such screening methods. We have developed acoustic and linguistic features for dementia screening and established that a combination of acoustic and linguistic features provides the best results. However, our full set of 429 fine-grained features from 15 feature types is too large to train a robust model on limited training data. We therefore need to select features to use for dementia screening. We employ forward feature selection nested in a cross-validation and identify the most commonly selected features. Both acoustic and linguistic features from seven different feature types are selected. Using sets of these features we obtain a 0.819 unweighted average recall which is a strong improvement over previous results. © 2018, Springer Nature Switzerland AG.},
author_keywords={Computational paralinguistics;  Dementia screening;  Feature selection},
keywords={Linguistics;  Neurodegenerative diseases;  Speech processing, Automatic screening;  Automatic speech processing;  Dementia screenings;  Developed countries;  Forward feature selections;  Limited training data;  Linguistic features;  Paralinguistics, Feature extraction},
references={Appell, J., Kertesz, A., Fisman, M., A study of language functioning in Alzheimer patients (1982) Brain Lang, 17 (1), pp. 73-91; Asgari, M., Kaye, J., Dodge, H., Predicting mild cognitive impairment from spontaneous spoken utterances (2017) Alzheimer’s Dement. Transl. Res. Clin. Interv., 3 (2), pp. 219-228; Brunet, E., (1978) Le Vocabulaire De Jean Giraudoux, Structure Et évolution, , Slatkine, Geneva; Bucks, R., Singh, S., Cuerden, J.M., Wilcock, G.K., Analysis of spontaneous, conversational speech in dementia of Alzheimer type: Evaluation of an objective technique for analysing lexical performance (2000) Aphasiology, 14 (1), pp. 71-91; Dehak, N., Kenny, P.J., Dehak, R., Dumouchel, P., Ouellet, P., Front-end factor analysis for speaker verification (2011) IEEE Trans. Audio Speech Lang. Process., 19 (4), pp. 788-798; (2016), https://www.dgppn.de/Resources/Persistent/ade50e44afc7eb8024e7f65ed3f44e995583c3a0/S3-LL-Demenzen-240116.pdf; Espinoza-Cuadros, F., A spoken language database for research on moderate cognitive impairment: Design and preliminary analysis (2014) Iberspeech 2014, 8854, pp. 219-228. , https://doi.org/10.1007/978-3-319-13623-323; Hakkani-Tür, D., Vergyri, D., Tür, G., Speech-based automated cognitive status assessment (2010) INTERSPEECH 2010–11th Annual Conference of the International Speech Communication Association, pp. 258-261; Hernández-Domínguez, L., García-Cano, E., Ratté, S., Sierra-Martínez, G., Detection of Alzheimer’s disease based on automatic analysis of common objects descriptions (2016) Proceedings of the 7Th Workshop on Cognitive Aspects of Computational Language Learning, pp. 10-15; Honoré, A., Some simple measures of richness of vocabulary (1979) Assoc. Literary Linguist. Comput. Bull., 7 (2), pp. 172-177; Jarrold, W., Peintner, B., Wilkins, D., Vergryi, D., Richey, C., Gorno-Tempini, M.L., Ogar, J., Aided diagnosis of dementia type through computer-based analysis of spontaneous speech (2014) Proceedings of the Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pp. 27-37; Khodabakhsh, A., Yesil, F., Guner, E., Demiroglu, C., Evaluation of linguistic and prosodic features for detection of Alzheimer’s disease in Turkish conversational speech (2015) EURASIP J. Audio Speech Music Process., 2015 (1), pp. 1-15; Lehr, M., Prud’hommeaux, E.T., Shafran, I., Roark, B.: Fully automated neuropsychological assessment for detecting mild cognitive impairment (2012) INTERSPEECH 2012–13th Annual Conference of the International Speech Communication Association, pp. 1039-1042; Martin, P., Martin, M., Design und Methodik der Interdisziplinären Längsschnittstudie des Erwachsenenalters (2000) Aspekte Der Entwicklung Im Mittleren Und höheren Lebensalter: Ergebnisse Der Interdisziplinären Längsschnittstudie Des Erwachsenenalters (ILSE), pp. 17-27; Pennebaker, J.W., Francis, M.E., Booth, R.J., (2001) Linguistic Inquiry and Word Count: LIWC 2001. Erlbaum; Pennebaker, J.W., Graybeal, A., Patterns of natural language use: Disclosure, personality, and social integration (2001) Curr. Dir. Psychol. Sci., 10 (3), pp. 90-93; Prince, M., Wimo, A., Guerchet, M., Ali, G.C., Wu, Y.T., Prina, M., (2015) World Alzheimer Report 2015. the Global Impact of Dementia: An Analysis of Prevalence, Incidence, Cost and Trends. Alzheimer’s Disease International, , London; Prud’Hommeaux, E.T., Roark, B., Extraction of narrative recall patterns for neuropsychological assessment (2011) INTERSPEECH 2011–12th Annual Conference of the International Speech Communication Association, pp. 3021-3024; Sadeghian, R., Schaffer, J.D., Zahorian, S.A., Speech processing approach for diagnosing dementia in an early stage (2017) INTERSPEECH 2017–18th Annual Conference of the International Speech Communication Association, pp. 2705-2709; Satt, A., Hoory, R., König, A., Aalten, P., Robert, P.H., Speech-based automatic and robust detection of very early dementia (2014) INTERSPEECH 2014–15th Annual Conference of the International Speech Communication Association, pp. 2538-2542; Sattler, C., Wahl, H.W., Schröder, J., Kruse, A., Schönknecht, P., Kunzmann, U., Braun, T., Zenthöfer, A., (2017) Interdisciplinary Longitudinal Study on Adult Development and Aging (ILSE), pp. 1213-1222. , Springer, Singapore; Schiller, A., Teufel, S., Stöckert, C., (1999) Guidelines für Das Tagging Deutscher Textcor-Pora Mit STTS (Kleines Und großes Tagset; Schmid, H., Improvements in part-of-speech tagging with an application to German (1995) Proceedings of the ACL Sigdat-Workshop, pp. 47-50. , https://doi.org/10.1007/978-94-017-2390-92, Armstrong, S., Church, K., Isabelle, P., Manzi, S., Tzoukermann, E., Yarowsky, D. (eds.), Springer, Dordrecht; Schuller, B., Steidl, S., Batliner, A., Hirschberg, J., Burgoon, J.K., Baird, A., Elkins, A., Evanini, K., The INTERSPEECH 2016 computational paralinguistics challenge: Deception, sincerity & native language (2016) INTERSPEECH 2016–17th Annual Conference of the International Speech Communication Association, pp. 2001-2005; Shum, S.H., Dehak, N., Dehak, R., Glass, J.R., Unsupervised methods for speaker diarization: An integrated and iterative approach (2013) IEEE Trans. Audio Speech Lang. Process., 21 (10), pp. 2015-2028; Tausczik, Y.R., Pennebaker, J.W., The psychological meaning of words: LIWC and computerized text analysis methods (2009) J. Lang. Soc. Psychol., 29 (1), pp. 24-54; Thomas, C., Ke˘Selj, V., Cercone, N., Rockwood, K., Asp, E., Automatic detection and rating of dementia of alzheimer type through lexical analysis of spontaneous speech (2005) IEEE International Conference Mechatronics and Automation, 3, pp. 1569-1574; Tóth, L., Gosztolya, G., Vincze, V., Hoffmann, I., Szatlóczki, G., Automatic detection of mild cognitive impairment from spontaneous speech using ASR (2015) INTER-SPEECH 2015–16th Annual Conference of the International Speech Communication Association, pp. 2694-2698; Tweedie, F.J., Baayen, R.H., How variable may a constant be? Measures of lexical richness in perspective (1998) Comput. Hum., 32 (5), pp. 323-352; Wankerl, S., Nöth, E., Evert, S., An analysis of perplexity to reveal the effects of Alzheimer’s disease on language (2016) 12Th ITG Conference on Speech Communication, pp. 254-258; Weiner, J., Engelbart, M., Schultz, T., Manual and automatic transcription in dementia detection from speech (2017) INTERSPEECH 2017–18th Annual Conference of the International Speech Communication Association, pp. 3117-3121; Weiner, J., Frankenberg, C., Telaar, D., Wendelstein, B., Schröder, J., Schultz, T., Towards automatic transcription of ILSE-an interdisciplinary longitudinal study of adult development and aging (2016) Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016), pp. 718-725; Weiner, J., Herff, C., Schultz, T., Speech-based detection of Alzheimer’s disease in conversational German (2016) INTERSPEECH 2016–17th Annual Conference of the International Speech Communication Association, pp. 1938-1942; Westpfahl, S., Schmidt, T., Folk-gold-a gold standard for part-of-speech tagging of spoken German (2016) Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016), pp. 1493-1499; Wolf, M., Horn, A.B., Mehl, M.R., Haug, S., Pennebaker, J.W., Kordy, H., Com-putergestützte quantitative textanalyse (2008) Diagnostica, 54 (2), pp. 85-98; Zhou, L., Fraser, K.C., Rudzicz, F., Speech recognition in Alzheimer’s disease and in its assessment (2016) INTERSPEECH 2016–17th Annual Conference of the International Speech Communication Association, pp. 1948-1952},
correspondence_address1={Weiner, J.; Cognitive Systems Lab, University of BremenGermany; email: jochen.weiner@uni-bremen.de},
editor={Potapova R., Jokisch O., Karpov A.},
sponsors={},
publisher={Springer Verlag},
issn={03029743},
isbn={9783319995786},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu2018262,
author={Liu, H. and Schultz, T.},
title={ASK: A framework for data acquisition and activity recognition},
journal={BIOSIGNALS 2018 - 11th International Conference on Bio-Inspired Systems and Signal Processing, Proceedings; Part of 11th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2018},
year={2018},
volume={4},
pages={262-268},
doi={10.5220/0006732902620268},
note={cited By 3; Conference of 11th International Conference on Bio-Inspired Systems and Signal Processing, BIOSIGNALS 2018 - Part of 11th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2018 ; Conference Date: 19 January 2018 Through 21 January 2018;  Conference Code:134854},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051721534&doi=10.5220%2f0006732902620268&partnerID=40&md5=87b9b526a2cb26ea40b7a87b313345a2},
affiliation={Cognitive Systems Lab, University of Bremen, Bremen, Germany},
abstract={This work puts forward a framework for the acquisition and processing of biosignals to indicate strain on the knee inflicted by human everyday activities. Such a framework involves the appropriate equipment in devices and sensors to capture factors that inflict strain on the knee, the long-term recording and archiving of corresponding multi-sensory biosignal data, the semi-automatic annotation and segmentation of these data, and the person-dependent or person-adaptive automatic recognition of strain. In this paper we present first steps toward our goal, i.e. person-dependent recognition of a small set of human everyday activities. The focus here is on the fully automatic end-to-end processing from signal input to recognition output. The framework was applied to collect and process a small pilot dataset from one person for a proof-of-concept validation and achieved 97% accuracy in recognizing instances of seven daily activities. Copyright © 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.},
author_keywords={Automatic Annotation and Segmentation;  Biosignals;  Human Activity Recognition;  Signal Processing},
keywords={Amplitude shift keying;  Biomedical engineering;  Biomimetics;  Joints (anatomy), Activity recognition;  Automatic recognition;  Biosignal datum;  Daily activity;  Long-term recording;  Person-dependent;  Proof of concept;  Semi-automatic annotation, Data acquisition},
references={Amma, C., Gehrig, D., Schultz, T., Airwriting recognition using wearable motion sensors (2010) First Augmented Human International Conference, p. 10. , ACM; Bao, L., Intille, S.S., Activity recognition from user-annotated acceleration data (2004) Pervasive Computing, pp. 1-17. , Springer; Fleischer, C., Reinicke, C., Predicting the intended motion with emg signals for an exoskeleton orthosis controller (2005) 2005 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2005), pp. 2029-2034; Kwapisz, J.R., Weiss, G.M., Moore, S.A., Activity recognition using cell phone accelerometers (2010) Proceedings of The Fourth International Workshop on Knowledge Discovery from Sensor Data, pp. 10-18; Lukowicz, P., Ward, J.A., Junker, H., Stger, M., Trster, G., Atrash, A., Starner, T., Recognizing workshop activity using body worn microphones and accelerometers (2004) Pervasive Computing, pp. 18-32; Mathie, M., Coster, A., Lovell, N., Celler, B., Detection of daily physical activities using a triaxial accelerometer (2003) Medical and Biological Engineering and Computing, 41 (3), pp. 296-301; Rebelo, D., Amma, C., Gamboa, H., Schultz, T., Activity recognition for an intelligent knee orthosis (2013) 6th International Conference on Bio-Inspired Systems and Signal Processing, pp. 368-371. , BIOSIGNALS 2013; Rowe, P., Myles, C., Walker, C., Nutton, R., Knee joint kinematics in gait and other functional activities measured using exible electrogoniometry: How much knee motion is sucient for normal daily life? (2000) Gait & Posture, 12 (2), pp. 143-155; Sutherland, D.H., The evolution of clinical gait analysis: Part II kinematics (2002) Gait & Posture, 16 (2), pp. 159-179},
editor={Saggio G., Gamboa H., Fred A., Bermudez i Badia S.},
sponsors={Institute for Systems and Technologies of Information, Control and Communication (INSTICC); OMICtools},
publisher={SciTePress},
isbn={9789897582790},
language={English},
abbrev_source_title={BIOSIGNALS - Int. Conf. Bio-Inspired Syst. Signal Process., Proc.; Part Int. Jt. Conf. Biomed. Eng. Syst. Technol., BIOSTEC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wand20183167,
author={Wand, M. and Schultz, T. and Schmidhuber, J.},
title={Domain-adversarial training for session independent EMG-based speech recognition},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2018},
volume={2018-September},
pages={3167-3171},
doi={10.21437/Interspeech.2018-2318},
note={cited By 5; Conference of 19th Annual Conference of the International Speech Communication, INTERSPEECH 2018 ; Conference Date: 2 September 2018 Through 6 September 2018;  Conference Code:139961},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054987262&doi=10.21437%2fInterspeech.2018-2318&partnerID=40&md5=69712afca6de42df2d8b61d0d2b65713},
affiliation={Istituto Dalle Molle di studi sull'Intelligenza Artificiale (IDSIA), USI and SUPSI, Manno-Lugano, Switzerland; University of Bremen, Bremen, Germany},
abstract={We present our research on continuous speech recognition based on Surface Electromyography (EMG), where speech information is captured by electrodes attached to the speaker's face. This method allows speech processing without requiring that an acoustic signal is present; however, reattachment of the EMG electrodes causes subtle changes in the recorded signal, which degrades the recognition accuracy and thus poses a major challenge for practical application of the system. Based on the growing body of recent work in domain-adversarial training of neural networks, we present a system which adapts the neural network frontend of our recognizer to data from a new recording session, without requiring supervised enrollment. © 2018 International Speech Communication Association. All rights reserved.},
author_keywords={Domain Adaptation;  EMG-based Speech Recognition;  Neural Networks;  Silent Speech interface},
keywords={Biomedical signal processing;  Continuous speech recognition;  Electrodes;  Electromyography;  Neural networks;  Speech processing, Acoustic signals;  Domain adaptation;  Growing bodies;  Recognition accuracy;  Recorded signals;  Silent speech interfaces;  Speech information;  Surface electromyography, Speech communication},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme, H2020, 687795},
funding_text 1={The first author was supported by the H2020 project INPUT – Intuitive Natural Prosthesis UTilization (grant #687795). This work used computational resources from the Swiss National Supercomputing Centre (CSCS) under project ID d74.},
references={Schultz, T., Wand, M., Hueber, T., Krusienski, D.J., Herff, C., Brumberg, J.S., Biosignal-based spoken communication: A survey (2017) IEEE/ACM Transactions on Audio, Speech, and Language Processing, 25 (12), pp. 2257-2271; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proc. Interspeech, pp. 573-576; Diener, L., Herff, C., Janke, M., Schultz, T., An initial investigation into the real-time conversion of facial surface EMG signals to audible speech (2016) Proc. EMBC; Wand, M., Schultz, T., Session-independent EMG-based Speech recognition (2011) Proc. Biosignals, pp. 295-300; Towards real-life application of emg-based speech recognition by using unsupervised adaptation (2014) Proc. Interspeech, pp. 1189-1193. , ''; Wand, M., Schmidhuber, J., Deep neural network frontend for continuous EMG-based speech recognition (2016) Proc. Interspeech, pp. 3032-3036; Ganin, Y., Lempitsky, V., Unsupervised domain adaptation by backpropagation (2015) Proc. ICML, pp. 1180-1189; Freitas, J., Teixeira, A., Días, M.S., Silva, S., An introduction to silent speech interfaces (2017) SpringerBriefs in Speech Technology; Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Silent speech interfaces (2010) Speech Communication, 52 (4), pp. 270-287; Gonzalez, J.A., Cheah, L.A., Gomez, A.M., Green, P.D., Gilbert, J.M., Ell, S.R., Moore, R.K., Holdsworth, E., Direct speech reconstruction from articulatory sensor data by machine learning (2017) IEEE/ACM Transactions on Audio, Speech, and Language Processing, 25 (12), pp. 2362-2374; Chung, J.S., Zisserman, A., Lip reading in the wild (2016) Proc. ACCV; Hueber, T., Bailly, G., Statistical conversion of silent articulation into audible speech using full-covariance HMM (2016) Computer Speech and Language, 36, pp. 274-293; Meltzner, G.S., Heaton, J.T., Deng, Y., Luca, G.D., Roy, S.H., Kline, J.C., Silent speech recognition as an alternative communication device for persons with laryngectomy (2017) IEEE/ACM Transactions on Audio, Speech, and Language Processing, 25 (12), pp. 2386-2398; Wand, M., (2014) Advancing Electromyographic Continuous Speech Recognition: Signal Preprocessing and Modeling, , Dissertation, Karlsruhe Institute of Technology; Bocquelet, F., Hueber, T., Girin, L., Savariaux, C., Yvert, B., Real-time control of an articulatory-based speech synthesizer for brain computer interfaces (2016) PLOS Computational Biology, 12 (11), pp. 1-28. , 11; Schultz, T., Wand, M., Modeling coarticulation in large vocabulary EMG-based speech recognition (2010) Speech Communication, 52 (4), pp. 341-353; Toth, A., Wand, M., Schultz, T., Synthesizing speech from electromyography using voice transformation techniques (2009) Proc. Interspeech, pp. 652-655; Lee, K.-S., Prediction of acoustic feature parameters using myoelectric signals (2010) IEEE Transactions on Biomedical Engineering, 57, pp. 1587-1595; Bourlard, H., Morgan, N., (1994) Connectionist Speech Recognition. A Hybrid Approach, , Kluwer Academic Publishers; Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A., Jaitly, N., Senior, A., Kingsbury, B., Deep neural networks for acoustic modeling in speech recognition (2012) IEEE Signal Processing Magazine, 29 (6), pp. 82-97; Amodei, D., Deep speech 2: End-to-end speech recognition in english and Mandarin (2016) Proc. ICML; Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., Darrell, T., Deep domain confusion: Maximizing for domain invariance (2014) CoRR; Ustinova, E., Lempitsky, V., Learning deep embeddings with histogram loss (2016) Proc. NIPS; Bousmalis, K., Silberman, N., Trigeorgis, G., Krishnan, D., Erhan, D., (2016) Domain Separation Networks; Ridgeway, K., Mozer, M.C., (2018) Learning Deep Disentangled Embeddings with The F-Statistic Loss; Schmidhuber, J., Learning factorial codes by predictability minimization (1992) Neural Computation, 4 (6), pp. 863-879; Wand, M., Janke, M., Schultz, T., The EMG-UKA corpus for electromyographic speech processing (2014) Proc. Interspeech, pp. 1593-1597; Schünke, M., Schulte, E., Schumacher, U., (2006) Prometheus - Ler-Natlas Der Anatomie, 3. , Stuttgart, New York: Thieme Verlag, Kopf und Neuroanatomie; Wand, M., Janke, M., Schultz, T., Tackling speaking mode varieties in emg-based speech recognition (2014) IEEE Transaction on Biomedical Engineering, 61 (10), pp. 2515-2526; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) Proc. ASRU, pp. 331-336; Hudgins, B., Parker, P., Scott, R., A new strategy for multifunction myoelectric control (1993) IEEE Transactions on Biomedical Engineering, 40, pp. 82-94; Telaar, D., Wand, M., Gehrig, D., Putze, F., Amma, C., Heger, D., Vu, N.T., Schultz, T., BioKIT - Real-time decoder for biosignal processing (2014) Proc. Interspeech; Abadi, M., (2015) TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems, , software tensor-flow.org; Bahl, L.R., De Souza, P.V., Gopalakrishnan, P.S., Nahamoo, D., Picheny, M.A., Decision trees for phonological rules in continuous speech (1991) Proc. ICASSP, pp. 185-188; Zhu, L., Kilgour, K., Stüker, S., Waibel, A., Gaussian free cluster tree construction using deep neural network (2015) Proc. Interspeech; Kingma, D.P., Ba, J., ADaM: A method for stochastic optimization (2015) Proc. ICLR; Yu, H., Waibel, A., Streamlining the front end of a speech recognizer (2000) Proc. ICSLP},
editor={Sekhar C.C., Rao P., Ghosh P.K., Murthy H.A., Yegnanarayana B., Umesh S., Alku P., Prasanna S.R.M., Narayanan S.},
sponsors={Adobe; et al.; JD.Com; MI; Samsung; Uniphore},
publisher={International Speech Communication Association},
issn={2308457X},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Weiner20182324,
author={Weiner, J. and Angrick, M. and Umesh, S. and Schultz, T.},
title={Investigating the effect of audio duration on dementia detection using acoustic features},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2018},
volume={2018-September},
pages={2324-2328},
doi={10.21437/Interspeech.2018-57},
note={cited By 2; Conference of 19th Annual Conference of the International Speech Communication, INTERSPEECH 2018 ; Conference Date: 2 September 2018 Through 6 September 2018;  Conference Code:139961},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054956249&doi=10.21437%2fInterspeech.2018-57&partnerID=40&md5=b1d694007b5cfac11da064c8aad2dcbe},
affiliation={Cognitive Systems Lab, University of Bremen, Germany; Department of Electrical Engineering, Indian Institute of Technology (IIT), Madras, India},
abstract={This paper presents recent progress toward our goal to enable area-wide pre-screening methods for the early detection of dementia based on automatically processing conversational speech of a representative group of more than 200 subjects. We focus on conversational speech since it is the natural form of communication that can be recorded unobtrusively, without adding stress to subjects, and without the need of controlled clinical settings. We describe our unsupervised process chain consisting of voice activity detection and speaker diarization followed by extraction of features and detection of early signs of dementia. The unsupervised system achieves up to 0.645 unweighted average recall (UAR) and compares favorably to a system that was carefully designed on manually annotated data. To further lower the burden for subjects, we investigate UAR over speech duration, and find that about 12 minutes of interview are sufficient to achieve the best UAR. © 2018 International Speech Communication Association. All rights reserved.},
author_keywords={Computational paralinguistics;  Dementia detection;  Representative dataset;  Unsupervised diarization},
keywords={Audio acoustics;  Feature extraction;  Neurodegenerative diseases;  Speech;  Speech communication, Clinical settings;  Conversational speech;  Diarization;  Paralinguistics;  Pre-screening methods;  Representative dataset;  Speaker diarization;  Voice activity detection, Speech recognition},
references={Appell, J., Kertesz, A., Fisman, M., A study of language functioning in Alzheimer patients (1982) Brain and Language, 17 (1), pp. 73-91; Bucks, R., Singh, S., Cuerden, J.M., Wilcock, G.K., Analysis of spontaneous, conversational speech in dementia of Alzheimer type: Evaluation of an objective technique for analysing lexical performance (2000) Aphasiology, 14 (1), pp. 71-91; Sattler, C., Wahl, H.-W., Schröder, J., Kruse, A., Schönknecht, P., Kunzmann, U., Braun, T., Zenthöfer, A., (2017) Interdisciplinary Longitudinal Study on Adult Development and Aging (ILSE), pp. 1213-1222. , Singapore: Springer; Weiner, J., Herff, C., Schultz, T., Speech-based detection of Alzheimer's disease in conversational German (2016) Interspeech 2016 - 17th Annual Conference of The International Speech Communication Association; Weiner, J., Engelbart, M., Schultz, T., Manual and automatic transcription in dementia detection from speech (2017) Interspeech 2017 - 18th Annual Conference of The International Speech Communication Association; Tóth, L., Gosztolya, G., Vincze, V., Hoffmann, I., Szatlóczki, G., Automatic detection of mild cognitive impairment from spontaneous speech using ASR (2015) Interspeech 2015 - 16th Annual Conference of The International Speech Communication Association, pp. 2694-2698; König, A., Satt, A., Sorin, A., Hoory, R., Toledo-Ronen, O., Derreumaux, A., Manera, V., David, R., Automatic speech analysis for the assessment of patients with predementia and Alzheimer's disease (2015) Alzheimer'S & Dementia: Diagnosis, Assessment & Disease Monitoring, 1 (1), pp. 112-124. , http://www.sciencedirect.com/science/article/pii/S2352872915000160, Online; Prud'hommeaux, E.T., Roark, B., Extraction of narrative recall patterns for neuropsychological assessment (2011) Interspeech 2011 - 12th Annual Conference of The International Speech Communication Association, pp. 3021-3024; Lehr, M., Prud'hommeaux, E.T., Shafran, I., Roark, B., Fully automated neuropsychological assessment for detecting mild cognitive impairment (2012) Interspeech 2012 - 13th Annual Conference of The International Speech Communication Association, pp. 1039-1042; Hakkani-Tür, D., Vergyri, D., Tür, G., Speech-based automated cognitive status assessment (2010) Interspeech 2010 - 11th Annual Conference of The International Speech Communication Association, pp. 258-261; Espinoza-Cuadros, F., Garcia-Zamora, M.A., Torres-Boza, D., Ferrer-Riesgo, C.A., Montero-Benavides, A., Gonzalez-Moreira, E., Hernandez-Gómez, L.A., A spoken language database for research on moderate cognitive impairment: Design and preliminary analysis (2014) Advances in Speech and Language Technologies for Iberian Languages, pp. 219-228. , Springer; Khodabakhsh, A., Yesil, F., Guner, E., Demiroglu, C., Evaluation of linguistic and prosodic features for detection of Alzheimer's disease in Turkish conversational speech (2015) Eurasip Journal on Audio, Speech, and Music Processing, 2015 (1), pp. 1-15; Hernández-Domínguez, L., García-Cano, E., Ratté, S., Sierra-Martínez, G., Detection of Alzheimer's disease based on automatic analysis of common objects descriptions (2016) Proceedings of The 7th Workshop on Cognitive Aspects of Computational Language Learning; Zhou, L., Fraser, K.C., Rudzicz, F., Speech recognition in alzheimers disease and in its assessment (2016) Interspeech 2016 - 17th Annual Conference of The International Speech Communication Association, pp. 1948-1952; Thomas, C., Keselj, V., Cercone, N., Rockwood, K., Asp, E., Automatic detection and rating of dementia of Alzheimer type through lexical analysis of spontaneous speech (2005) IEEE International Conference Mechatronics and Automation, 3, pp. 1569-1574. , 3; Jarrold, W., Peintner, B., Wilkins, D., Vergryi, D., Richey, C., Gorno-Tempini, M.L., Ogar, J., Aided diagnosis of dementia type through computer-based analysis of spontaneous speech (2014) Proceedings of The ACL Workshop on Computational Linguistics and Clinical Psychology, pp. 27-36; Weiner, J., Frankenberg, C., Telaar, D., Wendelstein, B., Schröder, J., Schultz, T., Towards automatic transcription of ILSE - An interdisciplinary longitudinal study of adult development and aging (2016) Proceedings of The Tenth International Conference on Language Resources and Evaluation (LREC'16); Martin, P., Martin, M., Design und Methodik der Interdisziplinären Längsschnittstudie des Erwachsenenalters (2000) Aspekte Der Entwicklung Im Mittleren und Höheren Lebensalter: Ergeb-Nisse Der Interdisziplinären Längsschnittstudie Des Erwachsenenalters (ILSE), pp. 17-27. , Martin, K. U. Ettrich, U. Lehr, D. Roether, M. Martin, and A. Fischer-Cyrulies, Eds. Steinkopff; Prince, M., Wimo, A., Guerchet, M., Ali, G.-C., Wu, Y.-T., Prina, M., World Alzheimer Report 2015 (2015) The Global Impact of Dementia: An Analysis of Prevalence, Incidence, Cost and Trends, , London: Alzheimer's Disease International; Telaar, D., Wand, M., Gehrig, D., Putze, F., Amma, C., Heger, D., Vu, N.T., Schultz, T., BioKIT - Real-time decoder for biosignal processing (2014) Interspeech 2014 - 15th Annual Conference of The International Speech Communication Association, pp. 2650-2654; Carletta, J., Ashby, S., Bourban, S., Flynn, M., Guillemot, M., Hain, T., Kadlec, J., Kronenthal, M., The ami meeting corpus: A pre-announcement (2005) International Workshop on Machine Learning for Multimodal Interaction, pp. 28-39. , Springer, Berlin, Heidelberg; Reynolds, D.A., Quatieri, T.F., Dunn, R.B., Speaker verification using adapted Gaussian mixture models (2000) Digital Signal Processing, 10 (1-3), pp. 19-41; Dehak, N., Kenny, P.J., Dehak, R., Dumouchel, P., Ouellet, P., Front-end factor analysis for speaker verification (2011) IEEE Transactions on Audio, Speech, and Language Processing, 19 (4), pp. 788-798; Shum, S.H., Dehak, N., Dehak, R., Glass, J.R., Unsupervised methods for speaker diarization: An integrated and iterative approach (2013) IEEE Transactions on Audio, Speech, and Language Processing, 21 (10), pp. 2015-2028; Prince, S.J., Elder, J.H., Probabilistic linear discriminant analysis for inferences about identity (2007) Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on., pp. 1-8. , IEEE; Sell, G., Garcia-Romero, D., Speaker diarization with plda i-vector scoring and unsupervised calibration (2014) Spoken Language Technology Workshop (SLT), 2014 IEEE, pp. 413-417. , IEEE; Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Duchesnay, E., Scikit-learn: Machine learning in python (2011) Journal of Machine Learning Research, 12, pp. 2825-2830; Schuller, B., Steidl, S., Batliner, A., Hirschberg, J., Burgoon, J.K., Baird, A., Elkins, A., Evanini, K., The INTERSPEECH 2016 computational paralinguistics challenge: Deception, Sincerity & Native language (2016) Interspeech 2016 - 17th Annual Conference of The International Speech Communication Association},
editor={Sekhar C.C., Rao P., Ghosh P.K., Murthy H.A., Yegnanarayana B., Umesh S., Alku P., Prasanna S.R.M., Narayanan S.},
sponsors={Adobe; et al.; JD.Com; MI; Samsung; Uniphore},
publisher={International Speech Communication Association},
issn={2308457X},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Angrick20187,
author={Angrick, M. and Herff, C. and Johnson, G. and Shih, J. and Krusienski, D. and Schultz, T.},
title={Interpretation of Convolutional Neural Networks for speech regression from electrocorticography},
journal={ESANN 2018 - Proceedings, European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
year={2018},
pages={7-12},
note={cited By 1; Conference of 26th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2018 ; Conference Date: 25 April 2018 Through 27 April 2018;  Conference Code:149253},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066307841&partnerID=40&md5=6adb2033ebb019a556007a07cd8014d8},
affiliation={University of Bremen, Cognitive Systems Lab, Enrique-Schmidt-Strafie 5, Bremen, 28359, Germany; Old Dominion University, ASPEN Lab, 5115 Hampton Blvd, Norfolk, VA  23529, United States; UC San Diego Health, Epilepsy Center, 200 West Arbor Drive, San Diego, CA  92103, United States},
abstract={The direct synthesis of continuously spoken speech from neural activity is envisioned to enable fast and intuitive Brain-Computer Interfaces. Earlier results indicate that intracranial recordings reveal very suitable signal characteristics for direct synthesis. To map the complex dynamics of neural activity to spectral representations of speech, Convolutional Neural Networks (CNNs) can be trained. However, the resulting networks are hard to interpret and thus provide little opportunity to gain insights on neural processes underlying speech. Here, we show that CNNs are useful to reconstruct speech from intracranial recordings of brain activity and propose an approach to interpret the trained CNNs. © ESANN 2018 - Proceedings, European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning.},
keywords={Brain;  Brain computer interface;  Convolution;  Machine learning;  Neural networks;  Neurons;  Speech, Complex dynamics;  Convolutional neural network;  Direct synthesis;  Electrocorticography;  Neural activity;  Neural process;  Signal characteristic;  Spectral representations, Audio recordings},
references={Wolpaw, J.R., Birbaumer, N., McFarland, D.J., Pfurtscheller, G., Vaughan, T.M., Brain-computer interfaces for communication and control (2002) Clinical Neurophysiology, 113 (6), pp. 767-791; Herff, C., Schultz, T., Automatic speech recognition from neural signals: A focused review (2016) Frontiers in Neuroscience, p. 10; Ramsey, N.F., Salari, E., Aarnoutse, E.J., Vansteensel, M.J., Bleichner, M.G., Freudenburg, Z.V., Decoding spoken phonemes from sensorimotor cortex with high-density ECoG grids (2017) Neuro Image; Mugler, E.M., Patton, J.L., Flint, R.D., Wright, Z.A., Schuele, S.U., Rosenow, J., Shih, J.J., Slutzky, M.W., Direct classification of all American english phonemes using signals from functional speech motor cortex (2014) Journal of Neural Engineering, 11 (3), p. 035015; Herff, C., Heger, D., De Pesters, A., Telaar, D., Brunner, P., Schalk, G., Schultz, T., Braintotext: Decoding spoken phrases from phone representations in the brain (2015) Frontiers in Neuroscience, p. 9; Pei, X., Barbour, D.L., Leuthardt, E.C., Schalk, G., Decoding vowels and consonants in spoken and imagined words using electrocorticographic signals in humans (2011) Journal of Neural Engineering, 8 (4), p. 046028; Martin, S., Brunner, P., Holdgraf, C., Heinze, H.-J., Crone, N.E., Rieger, J., Schalk, G., Pasley, B., Decoding spectrotemporal features of overt and covert speech from the human cortex (2014) Frontiers in Neuroengineering, 7 (14); Martin, S., Brunner, P., Iturrate, I., Millán, J.D.R., Schalk, G., Knight, R.T., Pasley, B.N., Word pair classification during imagined speech using direct brain recordings (2016) Scientific Reports, 6, p. 25803; Schirrmeister, R.T., Springenberg, J.T., Fiederer, L.D.J., Glasstetter, M., Eggensperger, K., Tangermann, M., Hutter, F., Ball, T., Deep learning with convolutional neural networks for EEG decoding and visualization (2017) Human Brain Mapping; Erhan, D., Bengio, Y., Courville, A., Vincent, P., Visualizing higher-layer features of a deep network (2009) University of Montreal, 1341, p. 3; Montavon, G., Samek, W., Müller, K.-R., (2017) Methods for Interpreting and Understanding Deep Neural Networks, , preprint; Rothauser, E.H., IEEE recommended practice for speech quality measurements (1969) IEEE Trans. on Audio and Electroacoustics, 17, pp. 225-246; Imai, S., Cepstral analysis synthesis on the mel frequency scale (1983) Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP'83., 8, pp. 93-96. , IEEE; Clevert, D.-A., Unterthiner, T., Hochreiter, S., (2015) Fast and Accurate Deep Network Learning by Exponential Linear Units (Elus), , preprint; Simonyan, K., Vedaldi, A., Zisserman, A., (2013) Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps, , preprint; Herff, C., Johnson, G., Diener, L., Shih, J., Krusienski, D., Schultz, T., Towards direct speech synthesis from ECoG: A pilot study (2016) Engineering in Medicine and Biology Society (EMBC), 2016 IEEE 38th Annual International Conference of the, pp. 1540-1543. , IEEE; Brumberg, J.S., Krusienski, D.J., Chakrabarti, S., Gunduz, A., Brunner, P., Ritaccio, A.L., Schalk, G., Spatio-temporal progression of cortical activity related to continuous overt and covert speech production in a reading task (2016) PloS One, 11 (11)},
publisher={i6doc.com publication},
isbn={9782875870476},
language={English},
abbrev_source_title={ESANN - Proc., Euro. Symp. Artif. Neural Networks, Comput. Intell. Mach. Learn.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Schultz20172254,
author={Schultz, T. and Hueber, T. and Krusienski, D.J. and Brumberg, J.S.},
title={Introduction to the Special Issue on Biosignal-Based Spoken Communication},
journal={IEEE/ACM Transactions on Audio Speech and Language Processing},
year={2017},
volume={25},
number={12},
pages={2254-2256},
doi={10.1109/TASLP.2017.2768838},
art_number={8114385},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040511413&doi=10.1109%2fTASLP.2017.2768838&partnerID=40&md5=0f5e8760f8a97a146a5838a90df50c58},
affiliation={Cognitive Systems Lab, University of Bremen, Bremen, 28359, Germany; CNRS/GIPSA-lab, Grenoble, 38400, France; ASPEN LabOld Dominion University, Norfolk, VA  23529, United States; Speech-Language-Hearing, University of Kansas, Lawrence, KS  66045, United States},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={23299290},
language={English},
abbrev_source_title={IEEE ACM Trans. Audio Speech Lang. Process.},
document_type={Editorial},
source={Scopus},
}

@ARTICLE{Schultz20172257,
author={Schultz, T. and Wand, M. and Hueber, T. and Krusienski, D.J. and Herff, C. and Brumberg, J.S.},
title={Biosignal-Based Spoken Communication: A Survey},
journal={IEEE/ACM Transactions on Audio Speech and Language Processing},
year={2017},
volume={25},
number={12},
pages={2257-2271},
doi={10.1109/TASLP.2017.2752365},
note={cited By 39},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040448558&doi=10.1109%2fTASLP.2017.2752365&partnerID=40&md5=20baabcbd72f5845e85865b125133454},
affiliation={Cognitive Systems Lab, Faculty of Computer Science and Mathematics, University of Bremen, Bremen, Germany; Swiss AI Lab, Istituto Dalle Molle di Studi sull'Intelligenza Artificiale, Manno, Switzerland; GIPSA-Lab, CNRS/Grenoble Alpes University, Grenoble, France; ASPEN Lab, Biomedical Engineering Institute, Old Dominion University, Norfolk, VA, United States; Speech and Applied Neuroscience Lab, Speech-Language-Hearing Department, University of Kansas, Lawrence, KS, United States},
abstract={Speech is a complex process involving a wide range of biosignals, including but not limited to acoustics. These biosignals - stemming from the articulators, the articulator muscle activities, the neural pathways, and the brain itself - can be used to circumvent limitations of conventional speech processing in particular, and to gain insights into the process of speech production in general. Research on biosignal-based speech processing is a wide and very active field at the intersection of various disciplines, ranging from engineering, computer science, electronics and machine learning to medicine, neuroscience, physiology, and psychology. Consequently, a variety of methods and approaches have been used to investigate the common goal of creating biosignal-based speech processing devices for communication applications in everyday situations and for speech rehabilitation, as well as gaining a deeper understanding of spoken communication. This paper gives an overview of the various modalities, research approaches, and objectives for biosignal-based spoken communication. © 2014 IEEE.},
author_keywords={Biosignals;  electrocorticography;  electroencephalography;  electromyography;  functional near-infrared spectroscopy;  multimodal technologies;  speech recognition and synthesis;  speech rehabilitation;  spoken communication;  ultrasound},
keywords={Brain;  Electroencephalography;  Electromyography;  Electrophysiology;  Engineering education;  Infrared devices;  Learning systems;  Near infrared spectroscopy;  Speech;  Speech processing;  Speech recognition;  Ultrasonics, Biosignals;  Electrocorticography;  Functional near infrared spectroscopy;  Multi-modal;  Speech rehabilitation, Speech communication},
funding_details={National Science FoundationNational Science Foundation, NSF, 1608140, 01GQ1602},
funding_details={National Institutes of HealthNational Institutes of Health, NIH, R03-DC011304},
funding_details={Bundesministerium fÃ¼r Bildung und ForschungBundesministerium fÃ¼r Bildung und Forschung, BMBF},
funding_details={687795},
funding_details={Bundesministerium fÃ¼r Bildung und ForschungBundesministerium fÃ¼r Bildung und Forschung, BMBF},
funding_text 1={Manuscript received January 29, 2017; revised July 17, 2017; accepted August 11, 2017. Date of current version November 27, 2017. The work of T. Schultz, D. J. Krusienski, and C. Herff was supported by the Federal Ministry of Education and Research (BMBF) in Germany and the National Science Foundation (NSF) in the USA for the project “RESPONSE - REvealing SPONtaneous Speech processes in Electrocorticography” under references 01GQ1602 (BMBF) and 1608140 (NSF). The work of M. Wand was supported by the EU H2020 programme (#687795). The work of J. S. Brumberg was supported by the National Institutes of Health (R03-DC011304). The guest editor coordinating the review of this manuscript and approving it for publication was Dr. Junichi Yamagishi. (Corresponding author: Tanja Schultz.) T. Schultz and C. Herff are with the Cognitive Systems Lab, Faculty of Computer Science and Mathematics, University of Bremen, Bremen 28359, Germany (e-mail: tanja.schultz@uni-bremen.de; christian.herff@uni-bremen.de).},
references={Wolpaw, J.R., Birbaumer, N., McFarland, D.J., Pfurtscheller, G., Vaughan, T.M., Brain-computer interfaces for communication and control (2002) Clin. Neurophysiol., 113 (6), pp. 767-791; Herff, C., Schultz, T., Automatic speech recognition from neural signals: A focused review (2016) Frontiers Neurosci., 10; Chakrabarti, S., Sandberg, H.M., Brumberg, J.S., Krusienski, D.J., Progress in speech decoding from the electrocorticogram (2015) Biomed. Eng. Lett., 5 (1), pp. 10-21; Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Brumberg, J.S., Silent speech interfaces (2010) Speech Commun., 52 (4), pp. 270-287; Erber, N.P., Auditory-visual perception of speech (1975) J. Speech Hear. Disorders, 40 (4), pp. 481-492; Katz, W.F., Mehta, S., Visual feedback of tongue movement for novel speech sound learning (2015) Frontiers Hum. Neurosci., 9; Berry, J.J., North, C., Meyers, B., Johnson, M.T., Speech sensorimotor learning through a virtual vocal tract (2013) J. Acoust. Soc. Amer., 133 (5), p. 3342; Kaniusas, E., (2012) Biomedical Signals and Sensors i, , New York, NY, USA: Springer; Schultz, T., Amma, C., Heger, D., Putze, F., Wand, M., Biosignalebasierte mensch-maschine-schnittstellen (2013) Automatisierungstechnik, 61 (11), pp. 760-769; Deller, J.R., Hansen, J.H., Proakis, J.G., (1993) Discrete Time Processing of Speech Signals, , Upper Saddle River, NJ, USA: Prentice-Hall; Garnier, M., Henrich, N., Dubois, D., Influence of sound immersion and communicative interaction on the Lombard effect (2010) J. Speech, Lang. Hear. Res., 53 (3), pp. 588-608; Alderson-Day, B., Fernyhough, C., Inner speech: Development, cognitive functions, phenomenology, and neurobiology (2015) Psychol. Bull., 5 (141), pp. 931-965; Vygotsky, L., Rieber, R., Carton, A., (1987) The Collected Works of L.S. Vygotsky: Volume 1: Problems of General Psychology, Including the Volume Thinking and Speech (Cognition and Language: A Series in Psycholinguistics, , New York, NY, USA: Plenum; Herff, C., Janke, M., Wand, M., Schultz, T., Impact of different feedback mechanisms in EMG-based speech recognition (2011) Proc. 12th Annu. Conf. Int. Speech Commun. Assoc, pp. 2213-2216. , Florence, Italy; Hixon, T.J., Goldman, M.D., Mead, J., Kinematics of the chest wall during speech production: Volume displacements of the rib cage, abdomen, and lung (1973) J. Speech, Lang. Hear. Res., 16 (1), pp. 78-115; Rochet-Capellan, A., Fuchs, S., Take a breath and take the turn: How breathing meets turns in spontaneous dialogue (2014) Philosoph. Trans. Roy. Soc. B, 369 (1658); Rothenberg, M., A multichannel electroglottograph (1992) J. Voice, 6, pp. 36-43; Schönle, P.W., Gräbe, K., Wenig, P., Höhne, J., Schrader, J., Conrad, B., Electromagnetic articulography: Use of alternating magnetic fields for trackingmovements ofmultiple points inside and outside the vocal tract (1987) Brain Lang., 31, pp. 26-35; Gilbert, J.M., Isolated word recognition of silent speech using magnetic implants and sensors (2010) Med. Eng. Phys., 32, pp. 1189-1197; Gonzalez, J.A., Asilent speech system based on permanent magnet articulography and direct synthesis (2016) Comput. Speech Lang., 39, pp. 67-87; Livescu, K., (2005) Feature-based Pronunciationmodeling for Automatic Speech Recognition, , Ph.D. dissertation, Dept. Elect. Eng. Comput. Sci., MIT, Cambridge, MA, USA; Chennoukh, S., Sinder, D., Richard, G., Flanagan, J., Articulatory based low bit-rate speech coding (1997) J. Acoust. Soc. Amer., 102 (5), p. 3163; Ling, Z.-H., Richmond, K., Yamagishi, J., Wang, R.-H., Integrating articulatory features into HMM-based parametric speech synthesis (2009) IEEE Audio, Speech, Lang. Process., 17 (6), pp. 1171-1185. , Aug; Birkholz, P., Neuschaefer-Rube, C., Combined optical distance sensing and electropalatography to measure articulation (2011) Proc. 12th Annu. Conf. Int. Speech Commun. Assoc., pp. 285-288; Cooke, M., Barker, J., Cunningham, S., Shao, X., An audio-visual corpus for speech perception and automatic speech recognition (2006) J. Acoust. Soc. Amer., 120 (5), pp. 2421-2424; Chung, J.S., Zisserman, A., Lip reading in the wild (2016) Proc. 13th Asian Conf. Comput. Vis., pp. 87-103; Scott, A.D., Wylezinska, M., Birch, M.J., Miquel, M.E., Speech MRI: Morphology and function (2014) Physica Medica, 30 (6), pp. 604-618; Iltis, P.W., Frahm, J., Voit, D., Joseph, A.A., Schoonderwaldt, E., Altenmüller, E., High-speed real-time magnetic resonance imaging of fast tongue movements in elite horn players (2015) Quant. Imag. Med. Surg., 5 (3), pp. 374-381; Stone, M., A guide to analysing tongue motion from ultrasound images (2005) Clin. Linguistics Phonetics, 19 (6-7), pp. 455-501; Nakajima, Y., Kashioka, H., Shikano, K., Campbell, N., Non-audible murmur recognition input interface using stethoscopic microphone attached to the skin (2003) Proc. 2003 IEEE Int. Conf. Acoust., Speech, Signal Process, pp. 127-130. , Hong Kong; Patil, S.A., Hansen, J.H., The physiological microphone (PMIC): A competitive alternative for speaker assessment in stress detection and speaker verification (2010) Speech Commun., 52 (4), pp. 327-340; Bos, J.C., Tack, D.W., (2005) Speech Input Hardware Investigation for Future Dismounted Soldier Computer Systems, , Def. Res. Develop. Canada, Toronto, ON, Canada, DRDC Toronto Rep. CR2005-064; Fridlund, A.J., Cacioppo, J.T., Guidelines for human electromyographic research (1986) Psychophysiology, 23, pp. 567-589; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) Proc. IEEE Workshop Automat. Speech Recognit. Understanding, pp. 331-336. , San Juan, Puerto Rico; Deng, Y., Heaton, J.T., Meltzner, G.S., Towards a practical silent speech recognition system (2014) Proc. 15th Annu. Conf. Int. Speech Commun. Assoc., pp. 1164-1168. , Singapore; Wand, M., Schulte, C., Janke, M., Schultz, T., Array-based electromyographic silent speech interface (2013) Proc. Biosignals; Merletti, R., Parker, P.A., (2004) Electromyography: Physiology, Engineering, and Non-Invasive Applications, Chapter 4.2, 11. , New York, NY, USA: Wiley; Netsell, R., Daniel, B., Neural and mechanical response time for speech production (1974) J. Speech Hear. Res., 17, pp. 608-618; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proc. 9th Int. Conf. Spoken Lang. Process., pp. 573-576. , Pittsburgh, PA, USA; Price, C.J., A review and synthesis of the first 20 years of PET and fMRI studies of heard speech, spoken language and reading (2012) NeuroImage, 62 (2), pp. 816-847; Jobsis, F., Noninvasive, infrared monitoring of cerebral and myocardial oxygen sufficiency and circulatory parameters (1977) Science, 198 (4323), pp. 1264-1267; Strangman, G., Boas, D., Sutton, J., Non-invasive neuroimaging using near-infrared light (2002) Biol. Psychiatry, 52 (7), pp. 679-693; Ayaz, H., Onaral, B., Izzetoglu, K., Shewokis, P., McKendrick, R., Parasuraman, R., Continuous monitoring of brain dynamics with functional near infrared spectroscopy as a tool for neuroergonomic research: Empirical examples and a technological development (2013) Frontiers Hum. Neurosci., 7; Von Lühmann, A., Herff, C., Heger, D., Schultz, T., Towards a wireless open source instrument: Functional near-infrared spectroscopy in mobile neuroergonomics and BCI applications (2015) Frontiers Hum. Neurosci., 9; Nunez, P.L., Srinivasan, R., (2006) Electric Fields of the Brain: The Neurophysics of EEG, , New York, NY, USA: Oxford Univ. Press; Goncharova, I.I., McFarland, D.J., Vaughan, T.M., Wolpaw, J.R., EMG contamination of EEG: Spectral and topographical characteristics (2003) Clin. Neurophysiol., 114 (9), pp. 1580-1593; De Vos, M., Removal of muscle artifacts from EEG recordings of spoken language production (2010) Neuroinformatics, 8 (2), pp. 135-150; Deecke, L., Engel, M., Lang, W., Kornhuber, H.H., Bereitschaftspotential preceding speech after holding breath (1986) Exp. Brain Res., 65 (1), pp. 219-223; Indefrey, P., Levelt, W.J.M., The spatial and temporal signatures of word production components (2004) Cognition, 92 (1-2), pp. 101-144; Picton, T.W., Guidelines for using human event-related potentials to study cognition: Recording standards and publication criteria (2000) Psychophysiology, 37 (2), pp. 127-152; Pfurtscheller, G., Lopes Da Silva, F., Event-related EEG/MEG synchronization and desynchronization: Basic principles (1999) Clin. Neurophysiol., 110 (11), pp. 1842-1857; Maynard, E.M., Nordhausen, C.T., Normann, R.A., The Utah intracortical electrode array: A recording structure for potential braincomputer interfaces (1997) Electroencephalogr. Clin. Neurophysiol., 102 (3), pp. 228-239; Schwartz, A.B., Cortical neural prosthetics (2004) Annu. Rev. Neurosci., 27 (1), pp. 487-507; Stark, E., Abeles, M., Predicting movement from multiunit activity (2007) J. Neurosci., 27 (31), pp. 8387-8394; Brumberg, J.S., Wright, E.J., Andreasen, D.S., Guenther, F.H., Kennedy, P.R., Classification of intended phoneme production from chronic intracortical microelectrode recordings in speech motor cortex (2011) Frontiers Neurosci., 5; Tankus, A., Fried, I., Shoham, S., Structured neuronal encoding and decoding of human speech features (2012) Nature Commun., 3; Guenther, F.H., A wireless brain-machine interface for real-time speech synthesis (2009) PLoS One, 4 (12); Brumberg, J.S., Nieto-Castanon, A., Kennedy, P.R., Guenther, F.H., Brain-computer interfaces for speech communication (2010) Speech Commun., 52 (4), pp. 367-379; Crone, N.E., Functional mapping of human sensorimotor cortex with electrocorticographic spectral analysis. I. Alpha and beta eventrelated desynchronization (1998) Brain, 121 (12), pp. 2271-2299; Gevins, A., Subdural grid recordings of distributed neocortical networks involved with somatosensory discrimination (1994) Electroencephalogr. Clin. Neurophysiol., 92 (4), pp. 282-290; Pfurtscheller, G., Cooper, R., Frequency dependence of the transmission of the EEG from cortex to scalp (1975) Electroencephalogr. Clin. Neurophysiol., 38 (1), pp. 93-96; Crone, N., Sinai, A., Korzeniewska, A., High-frequency gamma oscillations and human brain mapping with electrocorticography (2006) Progress Brain Res., 159, pp. 275-295; Zhou, Z., Zhao, G., Hong, X., Pietikäinen, M., A review of recent advances in visual speech decoding (2014) Image Vis. Comput., 32, pp. 590-605; Luettin, J., Thacker, N.A., Beet, S.W., Visual speech recognition using active shape models and hidden Markov models (1996) Proc. 1996 IEEE Int. Conf. Acoust., Speech, Signal Process., 2, pp. 817-820. , Atlanta, GA, USA; Biswas, A., Sahu, P., Chandra, M., Multiple cameras audio visual speech recognition using active appearance model visual features in car environment (2016) Int. J. Speech Technol., 19 (1), pp. 159-171; Liu G Feng, L., Beautemps, D., Automatic dynamic template tracking of inner lips based on CLNF (2017) Proc. 2017 IEEE Int. Conf. Acoust., Speech, Signal Process., pp. 5130-5134. , New Orleans, LA, USA; Li, M., Kambhamettu, C., Stone, M., Automatic contour tracking in ultrasound images (2005) Clin. Linguistics Phonetics, 19 (6-7), pp. 545-554; Roussos, A., Katsamanis, A., Maragos, P., Tongue tracking in ultrasound images with active appearance models (2009) Proc. IEEE Int. Conf. Image Process., pp. 1733-1736; Fabre, D., Hueber, T., Bocquelet, F., Badin, P., Tongue tracking in ultrasound images using Eigentongue decomposition and artificial neural networks (2015) Proc. 16th Annu. Conf. Int. Speech Commun. Assoc., pp. 2410-2414. , Dresden, Germany; Fasel, I., Berry, J., Deep belief networks for real-time extraction of tongue contours from ultrasound during speech (2010) Proc. IEEE 20th Int. Conf. Pattern Recognit., pp. 1493-1496; Heckmann, M., Kroschel, K., Savariaux, C., Berthommier, F., DCTbased video features for audio-visual speech recognition (2002) Proc. 7th Int. Conf. Spoken Lang. Process., 3, pp. 1925-1928. , Denver, CO, USA; Bregler, C., Konig, Y., Eigenlips for robust speech recognition (1994) Proc. 1994 IEEE Int. Conf. Acoust., Speech, Signal Process., 2, pp. 669-672. , Adelaide, SA, Australia; Hueber, T., Eigentongue feature extraction for an ultrasound-based silent speech interface (2007) Proc. 2007 IEEE Int. Conf. Acoust., Speech, Signal Process., pp. I1245-I1248. , Honolulu, HI, USA; LeCun, Y., Bengio, Y., (1998) The Handbook of Brain Theory and Neural Networks (Convolutional Networks for Images, Speech, and Time Series, pp. 255-258. , M. A. Arbib, Ed. Cambridge,MA, USA: MIT Press; Noda, K., Yamaguchi, Y., Nakadai, K., Okuno, H.G., Ogata, T., Lipreading using convolutional neural network (2014) Proc. 15th Annu. Conf. Int. Speech Commun. Assoc., Singapore, pp. 1149-1153; Tatulli, E., Hueber, T., Feature extraction using multimodal convolutional neural networks for visual speech recognition (2017) Proc. 2017 IEEE Int. Conf. Acoust., Speech, Signal Process., pp. 2971-2975. , New Orleans, LA, USA; Stone, S., Birkholz, P., Angle correction in optopalatographic tongue distance measurements (2017) IEEE Sensors J., 17 (2), pp. 459-468. , Jan; Sugie, N., Tsunoda, K., A speech prosthesis employing a speech synthesizer-vowel discrimination from perioral muscle activities and vowel production (1985) IEEE Trans. Biomed. Eng., BME-32 (7), pp. 485-490. , Jul; Morse, M.S., Day, S.H., Trull, B., Morse, H., Use of myoelectric signals to recognize speech (1989) Proc. 11th Annu. Conf. IEEE Eng. Med. Biol. Soc., pp. 1793-1794; Hudgins, B., Parker, P., Scott, R., A new strategy for multifunction myoelectric control (1993) IEEE Trans. Biomed. Eng., 40 (1), pp. 82-94. , Jan; Jorgensen, C., Lee, D.D., Agabon, S., Sub auditory speech recognition based on EMG/EPG signals (2003) Proc. Int. Joint Conf. Neural Netw., pp. 3128-3133. , Portland, OR, USA; Deng, Y., Disordered speech recognition using acoustic and sEMG signals (2009) Proc. 10th Annu. Conf. Int. Speech Commun. Assoc., pp. 644-647. , Brighton, U.K; Meltzner, G.S., Heaton, J.T., Deng, Y., Luca, G.D., Roy, S.H., Kline, J.C., Silent speech recognition as an alternative communication device for persons with laryngectomy (2017) IEEE/ACMTrans. Audio, Speech, Lang. Process., 25 (12), pp. xxxx-xxxx. , Dec; Herff, C., Heger, D., Fortmann, O., Hennrich, J., Putze, F., Schultz, T., Mental workload during n-back task-quantified in the prefrontal cortex using fNIRS (2014) Frontiers Hum. Neurosci., 7; Heger, D., Herff, C., Schultz, T., Combining feature extraction and classification for fNIRS BCIs by regularized least squares optimization (2014) Proc. 36th Annu. Int. Conf. IEEE Eng. Med. Biol. Soc., pp. 2012-2015. , Chicago, IL, USA; Prescott, J., Andrews, G., Early and late components of the contingent negative variation prior to manual and speech responses in stutterers and non-stutterers (1984) Int. J. Psychophysiol., 2 (2), pp. 121-130; DaSalla, C., Kambara, H., Sato, M., Koike, Y., Single-trial classification of vowel speech imagery using common spatial patterns (2009) Neural Netw., 22 (9), pp. 1334-1339; D'Zmura, M., Deng, S., Lappas, T., Thorpe, S., Srinivasan, R., Toward EEG sensing of imagined speech (2009) Human Computer Interaction, pp. 40-48. , J. A. Jacko, Ed. Berlin, Germany: Springer-Verlag; Brumberg, J.S., Spatio-temporal progression of cortical activity related to continuous overt and covert speech production in a reading task (2016) PLoS One, 11, pp. 1-21. , Nov; Rabiner, L.R., A tutorial on hidden Markov models and selected applications in speech recognition (1990) Readings in Speech Recognition, pp. 267-296. , A. Waibel and K.-F. Lee, Eds. San Francisco, CA, USA: Morgan Kaufmann; Brown, P.F., DeSouza, P.V., Mercer, R.L., Pietra, V.J.D., Lai, J.C., Class-based n-gram models of natural language (1992) Comput. Linguistics, 18 (4), pp. 467-479; Seide, F., Li, G., Yu, D., Conversational speech transcription using context-dependent deep neural networks (2011) Proc. 12th Annu. Conf. Int. Speech Commun. Assoc., pp. 437-440. , Florence, Italy; Bahdanau, D., Chorowski, J., Serdyuk, D., Brakel, P., Bengio, Y., Endto-end attention-based large vocabulary speech recognition (2016) Proc. 2016 IEEE Int. Conf. Acoust., Speech, Signal Process., pp. 4945-4949. , Shanghai, China; Chiou, G.I., Hwang, J.-N., Lipreading from color video (1997) IEEE Trans. Image Process., 6 (8), pp. 1192-1195. , Aug; Cappelletta, L., Harte, N., Viseme definitions comparison for visualonly speech recognition (2011) Proc. Eur. Signal Process. Conf., pp. 2109-2113; Schultz, T., Wand, M., Modeling coarticulation in large vocabulary EMG-based speech recognition (2010) Speech Commun., 52 (4), pp. 341-353; Lopez-Larraz, E., Mozos, O.M., Antelis, J.M., Minguez, J., Syllablebased speech recognition using EMG (2010) Proc. 32nd Annu. Int. Conf. IEEE Eng. Med. Biol. Soc., pp. 4699-4702. , Buenos Aires, Argentina; Herff, C., Brain-to-text: Decoding spoken phrases from phone representations in the brain (2015) Frontiers Neurosci., 9; Hueber, T., Benaroya, E.-L., Chollet, G., Denby, B., Dreyfus, G., Stone, M., Visuo-phonetic decoding using multi-stream and contextdependent models for an ultrasound-based silent speech interface (2009) Proc. 10th Annu. Conf. Int. Speech Commun. Assoc., pp. 640-643. , Brighton, U.K; Walliczek, M., Kraft, F., Jou, S.-C., Schultz, T., Waibel, A., Sub-word unit based non-audible speech recognition using surface electromyography (2006) Proc. 9th Int. Conf. Spoken Lang. Process., pp. 1487-1490. , Pittsburgh, PA, USA; Wand, M., Schultz, T., Analysis of phone confusion in EMG-based speech recognition (2011) Proc. 2011 IEEE Int. Conf. Acoust., Speech, Signal Process., pp. 757-760. , Prague, Czech Republic; Abdelaziz, A.H., Zeiler, S., Kolossa, D., Learning dynamic stream weights for coupled-HMM-based audio-visual speech recognition (2015) IEEE/ACMTrans. Audio, Speech, Lang. Process., 23 (5), pp. 863-876. , May; Lee, K.-S., SNR-adaptive stream weighting for audio-MES ASR (2008) IEEE Trans. Biomed. Eng., 55 (8), pp. 2001-2010. , Aug; Guenther, F.H., Ghosh, S.S., Tourville, J.A., Neural modeling and imaging of the cortical interactions underlying syllable production (2006) Brain Lang., 96, pp. 280-301; Schwartz, J.-L., Savariaux, C., No, there is no 150 ms lead of visual speech on auditory speech, but a range of audiovisual asynchronies varying from small audio lead to large audio lag (2014) PLoS Comput. Biol., 10 (7); King, S., Frankel, J., Livescu, K., McDermott, E., Richmond, K., Wester, M., Speech production knowledge in automatic speech recognition (2007) J. Acoust. Soc. Amer., 121 (2), pp. 723-742; Petajan, E.D., (1984) Automatic Lipreading to Enhance Speech Recognition (Speech Reading), , Ph.D. dissertation, Univ. Illinois Urbana-Champaign, Champaign, IL, USA; Potamianos, G., Neti, C., Luettin, J., Matthews, I., Audio-visual automatic speech recognition: An overview (2004) Issues in Visual and Audio-Visual Speech Processing, , G. Bailly, E. Vatikiotis-Bateson, and P. Perrier Eds. Cambridge, MA, USA: MIT, ch. 4; Hueber, T., Benaroya, E.-L., Chollet, G., Denby, B., Dreyfus, G., Stone, M., Development of a silent speech interface driven by ultrasound and optical images of the tongue and lips (2010) Speech Commun., 52 (4), pp. 288-300; Dupont, S., Luettin, J., Audio-visual speech modeling for continuous speech recognition (2000) IEEE Trans.Multimedia, 2 (3), pp. 141-151. , Sep; Mroueh, Y., Marcheret, E., Goel, V., Deep multimodal learning for audio-visual speech recognition (2015) Proc. 2015 IEEE Int. Conf. Acoust., Speech, Signal Process., pp. 2130-2134; Wand, M., Koutnik, J., Schmidhuber, J., Lipreading with long shortterm memory (2016) Proc. 2016 IEEE Int. Conf. Acoust., Speech, Signal Process., pp. 6115-6119. , Shanghai, China"; Gravier, G., Potamianos, G., Neti, C., Asynchrony modeling for audio-visual speech recognition (2002) Proc. Int. Conf.Hum. Lang. Technol. Res., pp. 1-6; Gowdy, J.N., Subramanya, A., Bartels, C., Bilmes, J., DBN based multi-stream models for audio-visual speech recognition (2004) Proc. 2004 IEEE Int. Conf. Acoust., Speech, Signal Process.,Montreal, QC, Canada, 1, pp. 993-996; Wrench, A., Richmond, K., Continuous speech recognition using articulatory data (2000) Proc. 6th Int. Conf. Spoken Lang. Process., pp. 145-148; Heracleous, P., Badin, P., Bailly, G., Hagita, N., A pilot study on augmented speech communication based on electro-magnetic articulography (2011) Pattern Recognit. Lett., 32 (8), pp. 1119-1125; Fagan, M., Ell, S., Gilbert, J., Sarrazin, E., Chapman, P., Development of a (silent) speech recognition system for patients following laryngectomy (2008) Med. Eng. Phys., 30 (4), pp. 419-425; Chan, A.D.C., Englehart, K.B., Hudgins, B., Lovely, D.F., Myoelectric signals to augment speech recognition (2001) Med. Biol. Eng. Comput., 39, pp. 500-506; Wand, M., Schultz, T., Towards real-life application of EMGbased speech recognition by using unsupervised adaptation (2014) Proc. 15th Annu. Conf. Int. Speech Commun. Assoc., Singapore, pp. 1189-1193; Wand, M., Schmidhuber, J., Deep neural network frontend for continuous EMG-based speech recognition (2016) Proc. 17th Annu. Conf. Int. SpeechCommun. Assoc., pp. 3032-3036. , San Francisco,CA, USA; Formisano, E., De Martino, F., Bonte, M., Goebel, R., Who" is saying what"? Brain-based decoding of human voice and speech (2008) Science, 322 (5903), pp. 970-973; Mitchell, T.M., Predicting human brain activity associated with the meanings of nouns (2008) Science, 320 (5880), pp. 1191-1195; Huth, A., De Heer, W., Griffiths, T., Theunissen, F., Gallant, J., Natural speech reveals the semantic maps that tile human cerebral cortex (2016) Nature, 532 (7600), pp. 453-458; Herff, C., Putze, F., Heger, D., Guan, C., Schultz, T., Speaking mode recognition from functional near infrared spectroscopy (2012) Proc. 34th Annu. Int. Conf. IEEE Eng. Med. Biol. Soc., pp. 1715-1718. , San Diego, CA, USA; Herff, C., Heger, D., Putze, F., Guan, C., Schultz, T., Cross-subject classification of speaking modes using fNIRS (2012) Neural Information Processing (Lecture Notes in Computer Science), 7664, pp. 417-424. , T. Huang, Z. Zeng, C. Li, and C. Leung, Eds. Berlin, Germany: Springer; Suppes, P., Lu, Z., Han, B., Brain wave recognition of words (1997) Proc. Nat. Acad. Sci. USA, 94 (26), pp. 14965-14969; Pasley, B.N., Reconstructing speech from human auditory cortex (2012) PLoS Biol., 10 (1); Yoshimura, N., Decoding of covert vowel articulation using electroencephalography cortical currents (2016) Frontiers Neurosci., 10; Porbadnigk, A., Wester, M., Callies, J.P., Schultz, T., EEG-based speech recognition-impact of temporal effects (2009) Proc. 2nd Int. Conf. Bio-Inspired Syst. Signal Process., Porto, Portugal, pp. 376-381; Mugler, E.M., Direct classification of all American english phonemes using signals from functional speech motor cortex (2014) J. Neural Eng., 11 (3); Blakely, T., Miller, K.J., Rao, R.P.N., Holmes, M.D., Ojemann, J.G., Localization and classification of phonemes using high spatial resolution electrocorticography (ECoG) grids (2008) Proc. 30nd Annu. Int. Conf. IEEE Eng. Med. Biol. Soc., pp. 4964-4967; Bouchard, K.E., Chang, E.F., Neural decoding of spoken vowels from human sensory-motor cortex with high-density electrocorticography (2014) Proc. 36th Annu. Int. Conf. IEEE Eng. Med. Biol. Soc., pp. 6782-6785. , Chicago, IL, USA; Kellis, S., Miller, K., Thomson, K., Brown, R., House, P., Greger, B., Decoding spoken words using local field potentials recorded from the cortical surface (2010) J. Neural Eng., 7 (5); Lotte, F., Electrocorticographic representations of segmental features in continuous speech (2015) Frontiers Hum. Neurosci., 9; Stylianou, Y., Cappe, O., Moulines, E., Continuous probabilistic transform for voice conversion (1998) IEEE Audio, Speech, Lang. Process., 6 (2), pp. 131-142. , Mar; Toda, T., Black, A.W., Tokuda, K., Voice conversion based on maximum-likelihood estimation of spectral parameter trajectory (2007) IEEE Audio, Speech, Lang. Process., 15 (8), pp. 2222-2235. , Nov; Toth, A., Wand, M., Schultz, T., Synthesizing speech from electromyography using voice transformation techniques (2009) Proc. 10th Annu. Conf. Int. Speech Commun. Assoc., , Brighton, U.K; Hueber, T., Bailly, G., Statistical conversion of silent articulation into audible speech using full-covariance HMM (2016) Comput. Speech Lang., 36, pp. 274-293; Kello, C.T., Plaut, D.C., A neural network model of the articulatoryacoustic forward mapping trained on recordings of articulatory parameters (2004) J. Acoust. Soc. Amer., 116 (4), pp. 2354-2364; Bocquelet, F., Hueber, T., Girin, L., Savariaux, C., Yvert, B., Real-time control of an articulatory-based speech synthesizer for brain computer interfaces (2016) PLoS Comput. Biol., 12 (11); Diener, L., Janke, M., Schultz, T., Direct conversion from facial myoelectric signals to speech using deep neural networks (2015) Proc. Int. Joint Conf. Neural Netw., Killarney, Ireland, pp. 1-7; Janke, M., Diener, L., EMG-to-speech: Direct generation of speech from facial electromyographic signals (2017) IEEE/ACM Trans. Audio, Speech, Lang. Process., 25 (12), pp. xxxx-xxxx. , Dec; Brumberg, J.S., Burnison, J.D., Pitt, K.M., Using motor imagery to control brain-computer interfaces for communication (2016) Foundations of Augmented Cognition: Neuroergonomics and Operational Neuroscience, , Basel, Switzerland: Spring Int. Publ; Herff, C., Johnson, G., Diener, L., Shih, J., Krusienski, D., Schultz, T., Towards direct speech synthesis from ECoG: A pilot study (2016) Proc. 38th Annu. Int. Conf. IEEE Eng. Med. Biol. Soc., pp. 1540-1543. , Orlando, FL, USA; Wand, M., Janke, M., Schultz, T., The emg-uka corpus for electromyographic speech processing (2014) Proc. 15th Annu. Conf. Int. Speech Commun. Assoc., pp. 1593-1597. , Singapore; Gibbon, F., Lee, A., Electropalatography for older children and adults with residual speech errors (2015) Seminars in Speech and Language, 36, pp. 271-282. , Stuttgart, Germany: Thieme Med. Publ; Janke, M., Wand, M., Schultz, T., A spectral mapping method for EMG-based recognition of silent speech (2010) Proc. B-INTERFACE, pp. 22-31; Wand, M., Janke, M., Schultz, T., Tackling speaking mode varieties in EMG-based speech recognition (2014) IEEE Trans. Biomed. Eng., 61 (10), pp. 2515-2526. , Oct; Wand, M., Schmidhuber, J., Improving speaker-independent lipreading with domain-adversarial training (2017) Proc. 18th Annu. Conf. Int. Speech Commun. Assoc., pp. 3662-3666; McGurk, H., Macdonald, J., Hearing lips and seeing voices (1976) Nature, 264, pp. 691-811; Gibbon, F., (2011) Bibliography of Electropalatographic (EPG) Studies in English (1957-2013), , http://www.articulateinstruments.com/EPGrefs.pdf, Dept. Speech Hear. Sci., Univ. College Cork, Ireland, Rep. Staeno 2013-05-21; Cavin, M., The use of ultrasound biofeedback for improving English /r/ (2015) Working Papers Linguistics Circle, 25 (1), pp. 32-41; Cleland, J., Scobbie, J.M., Wrench, A.A., Using ultrasound visual biofeedback to treat persistent primary speech sound disorders (2015) Clin. Linguistics Phonetics, 29 (8-10), pp. 575-597; Perkell, J., Speech motor control: Acoustic goals, saturation effects, auditory feedback and internal models (1997) Speech Commun., 22, pp. 227-250; Tourville, J.A., Reilly, K.J., Guenther, F.H., Neural mechanisms underlying auditory feedback control of speech (2008) NeuroImage, 32, pp. 1429-1443; Graciarena, M., Franco, H., Sonmez, K., Bratt, H., Combining standard and throatmicrophones for robust speech recognition (2003) IEEE Signal Process. Lett., 10 (3), pp. 72-74. , Mar; Betts, B.J., Binsted, K., Jorgensen, C., Small-vocabulary speech recognition using surface electromyography (2006) Interact. Comput., 18 (6), pp. 1242-1259; Jorgensen, C., Dusan, S., Speech interfaces based upon surface electromyography (2010) Speech Commun., 52 (4), pp. 354-366; Martin, S., Decoding spectrotemporal features of overt and covert speech from the human cortex (2014) Frontiers Neuroeng., 7; Martin, S., Word pair classification during imagined speech using direct brain recordings (2016) Sci. Rep., 6},
correspondence_address1={Schultz, T.; Cognitive Systems Lab, Faculty of Computer Science and Mathematics, University of BremenGermany; email: tanja.schultz@uni-bremen.de},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={23299290},
language={English},
abbrev_source_title={IEEE ACM Trans. Audio Speech Lang. Process.},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Putze2017137,
author={Putze, F. and Schünemann, M. and Schultz, T. and Stuerzlinger, W.},
title={Automatic classification of auto-correction errors in predictive text entry based on EEG and context information},
journal={ICMI 2017 - Proceedings of the 19th ACM International Conference on Multimodal Interaction},
year={2017},
volume={2017-January},
pages={137-145},
doi={10.1145/3136755.3136784},
note={cited By 6; Conference of 19th ACM International Conference on Multimodal Interaction, ICMI 2017 ; Conference Date: 13 November 2017 Through 17 November 2017;  Conference Code:131842},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046731355&doi=10.1145%2f3136755.3136784&partnerID=40&md5=470722acb2fe667a211d6fca26b522e7},
affiliation={University of Bremen, Bremen, Germany; Simon Fraser University, Surrey, BC, Canada},
abstract={State-of-The-Art auto-correction methods for predictive text entry systems work reasonably well, but can never be perfect due to the properties of human language. We present an approach for the automatic detection of erroneous auto-corrections based on brain activity and text-entry-based context features. We describe an experiment and a new system for the classification of human reactions to auto-correction errors. We show how auto-correction errors can be detected with an average accuracy of 85%. © 2017 ACM.},
author_keywords={Brain-computer interface;  Context information;  EEG;  Error potentials;  Predictive text entry},
keywords={Brain;  Brain computer interface;  Classification (of information);  Electroencephalography;  Errors;  Interactive computer systems;  Interfaces (computer);  Semantics, Automatic classification;  Automatic Detection;  Brain activity;  Context features;  Context information;  Human reaction;  Predictive text entry;  State of the art, Text processing},
references={Arif, A.S., Stuerzlinger, W., Analysis of text entry performance metrics (2009) Science and Technology for Humanity (TIC-STH), 2009 IEEE Toronto International Conference, pp. 100-105; Batuwita, R., Palade, V., Class imbalance learning methods for support vector machines (2013) Imbalanced Learning, pp. 83-99. , Haibo He and Yunqian (Eds.). John Wiley & Sons, Inc; Bi, X., Ouyang, T., Zhai, S., Both complete and correct?: Multi-objective optimization of touchscreen keyboard (2014) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '14), pp. 2297-2306. , ACM, New York, NY, USA; Castellucci, S.J., MacKenzie, I.S., Gathering text entry metrics on android devices (2011) CHI '11 Extended Abstracts on Human Factors in Computing Systems (CHI EA '11), pp. 1507-1512. , ACM, New York, NY, USA; Cavanagh, J.F., Frank, M.J., Klein, T.J., Allen, J.J.B., Frontal theta links prediction errors to behavioral adaptation in reinforcement learning (2010) Neuroimage, 49 (4), pp. 3198-3209. , 2010; Clawson, J., Lyons, K., Rudnick, A., Iannucci, R.A., Jr., Starner, T., Automatic whiteout++: Correcting mini-qwerty typing errors using keypress timing (2008) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '08), pp. 573-582. , ACM, New York, NY, USA; Combaz, A., Chumerin, N., Manyakov, N.V., Robben, A., Suykens, J.A.K., Van Hulle, M.M., Towards the detection of error-related potentials and its integration in the context of a P300 speller brain-computer interface (2012) Neurocomputing, 80, pp. 73-82. , 2012; Vos De Maarten, Gandras, K., Debener, S., Towards a Truly Mobile Auditory Brain-computer Interface: Exploring the P300 to Take Away, 91 (1), pp. 46-53. , 2014-01-01., (2014-01-01); Doppelmayr, M., Klimesch, W., Pachinger, T., Ripper, B., Individual differences in brain dynamics: Important implications for the calculation of event-related band power (1998) Biological Cybernetics, 79 (1), pp. 49-57. , 1998; Ferrez, P.W., Del Millan, J.R., Error-related EEG potentials generated during simulated brain computer interaction (2008) IEEE Transactions on Biomedical Engineering, 55 (3), pp. 923-929. , 2008; Fowler, A., Partridge, K., Chelba, C., Bi, X., Ouyang, T., Zhai, S., Effects of language modeling and its personalization on touchscreen typing performance (2015) Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI '15), pp. 649-658. , ACM, New York, NY, USA; Förster, K., Biasiucci, A., Chavarriaga, R., Millan, J.D.R., Roggen, D., Tröster, G., On the use of brain decoded signals for online user adaptive gesture recognition systems (2010) Pervasive Computing. Number 6030 in Lecture Notes in Computer Science, pp. 427-444. , Springer Berlin Heidelberg; Gerson, A.D., Parra, L.C., Sajda, P., Cortically coupled computer vision for rapid image search (2006) IEEE Transactions on Neural Systems and Rehabilitation Engineering, 14 (2), pp. 174-179. , June 2006; Goel, M., Findlater, L., Wobbrock, J., Walktype: Using accelerometer data to accomodate situational impairments in mobile touch screen text entry (2012) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '12), pp. 2687-2696. , ACM, New York, NY, USA; Goel, M., Jansen, A., Mandel, T., Patel, S.N., Wobbrock, J.O., Context type: Using hand posture information to improve mobile touch screen text entry (2013) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '13), pp. 2795-2798. , ACM, New York, NY, USA; Goodman, J., Venolia, G., Steury, K., Parker, C., Language modeling for soft keyboards (2002) Proceedings of the 7th International Conference on Intelligent User Interfaces (IUI '02), pp. 194-195. , ACM, New York, NY, USA; Gramfort, A., Luessi, M., Larson, E., Engemann, D.A., Strohmeier, D., Brodbeck, C., Goj, R., Parkkonen, L., (2013) MEG and EEG Data Analysis with MNE-Python, Frontiers in Neuroscience, 7, p. 267. , 2013; He, H., Bai, Y., Garcia, E.A., Li, S., ADASYN: Adaptive synthetic sampling approach for imbalanced learning (2008) 2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence), pp. 1322-1328; Hynes, C.J., (2016) Device, Method, and Graphical User Interface for Visible and Interactive Corrected Content, , April 2016; Iturrate, I., Chavarriaga, R., Montesano, L., Minguez, J., Del Millan, J.R., Latency correction of error potentials between different experiments reduces calibration time for single-Trial classification (2012) Proceedings of Annual International Conference of the Engineering in Medicine and Biology Society, 2012, pp. 3288-3291. , 2012; Jayaram, V., Alamgir, M., Altun, Y., Scholkopf, B., Grosse-Wentrup, M., (2016) Transfer Learning in Brain-computer Interfaces, 11 (1), pp. 20-31. , 2016; Kristensson, P.-O., Zhai, S., Relaxing stylus typing precision by geometric pattern matching (2005) Proceedings of the 10th International Conference on Intelligent User Interfaces (IUI '05), pp. 151-158. , ACM, New York, NY, USA; Krusienski, D.J., Sellers, E.W., McFarland, D.J., Vaughan, T.M., Wolpaw, J.R., Toward enhanced P300 speller performance (2008) Journal of Neuroscience Methods, 167 (1), pp. 15-21. , 2008; Ledoit, O., Wolf, M., (2003) Honey, I Shrunk the Sample Covariance Matrix, , SSRN Scholarly Paper ID 433840. Social Science Research Network, Rochester, NY; LemaÎtre, G., Nogueira, F., Aridas, C.K., Imbalanced-learn: A python toolbox to tackle the curse of imbalanced datasets in machine learning (2017) Journal of Machine Learning Research, 18 (17), pp. 1-5. , 2017; Llera, A., Van Gerven, M.A.J., Gómez, V.M., Jensen, O.K., Kappen, H.J., On the use of interaction error potentials for adaptive brain computer interfaces (2011) Neural Networks, 24 (10), pp. 1120-1127. , 2011; Margaux, P., Emmanuel, M., Sébastien, D., Olivier, B., Jérémie, M., Objective and subjective evaluation of online error correction during p300-based spelling (2012) Adv. in Hum.-Comp. Int, 2012. , 2012; Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Duchesnay, E., Scikit-learn: Machine learning in python (2011) Journal of Machine Learning Research, 12, pp. 2825-2830. , 2011; Putze, F., Amma, C., Schultz, T., Design and evaluation of a self-correcting gesture interface based on error potentials from EEG (2015) Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI '15), pp. 3375-3384. , ACM, New York, NY, USA; Putze, F., Heger, D., Schultz, T., Reliable subject-Adapted recognition of EEG error potentials using limited calibration data (2013) 6th International Conference on Neural Engineering, , San Diego, USA; Putze, F., Popp, J., Hild, J., Beyerer, J., Schultz, T., Intervention-free selection using EEG and eye tracking (2016) Proceedings of the 18th ACM International Conference on Multimodal Interaction, pp. 153-160. , ACM; Sanchis-Trilles, G., Leiva, L.A., A systematic comparison of 3 phrase sampling methods for text entry experiments in 10 languages (2014) Proceedings of the International Conference on Human-computer Interaction with Mobile Devices and Services (MobileHCI); Schmidt, N.M., Blankertz, B., Treder, M.S., Online detection of error-related potentials boosts the performance of mental typewriters (2012) BMC Neuroscience, 13, pp. 13-19. , 2012; Shenoy, P., Tan, D.S., Human-Aided computing: Utilizing implicit human processing to classify images (2008) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '08), pp. 845-854. , ACM, New York, NY, USA; Spüler, M., Bensch, M., Kleih, S., Rosenstiel, W., Bogdan, M., Kübler, A., Online use of error-related potentials in healthy users and people with severe motor impairment increases performance of a P300-BCI (2012) Clinical Neurophysiology: Official Journal of the International Federation of Clinical Neurophysiology, 123 (7), pp. 1328-1337. , 2012; Suhm, B., Myers, B., Waibel, A., Multimodal error correction for speech user interfaces (2001) ACM Trans. Comput.-Hum. Interact, 8 (1), pp. 60-98. , March 2001; Tinwala, H., MacKenzie, I.S., Eyes-free text entry with error correction on touchscreen mobile devices (2010) Proceedings of the 6th Nordic Conference on Human-Computer Interaction: Extending Boundaries (NordiCHI '10), pp. 511-520. , ACM, New York, NY, USA; Vertanen, K., Kristensson, P.O., (2014) Complementing Text Entry Evaluations with a Composition Task, 21 (2), p. 8. , 2014; Vi, C., Subramanian, S., Detecting error-related negativity for interaction design (2012) Proceedings of the Conference on Human Factors in Computing Systems, , New York, USA; Weir, D., Pohl, H., Rogers, S., Vertanen, K., Kristensson, P.O., Uncertain text entry on mobile devices (2014) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '14), pp. 2307-2316. , ACM, New York, NY, USA; Welch, P.D., The use of fast fourier transform for the estimation of power spectra: A method based on time averaging over short, modified periodograms (1967) Transactions on Audio and Electroacoustics, 15 (2), pp. 70-73. , 1967},
editor={Lank E., Hoggan E., Subramanian S., Vinciarelli A., Brewster S.A.},
sponsors={ACM SIGCHI},
publisher={Association for Computing Machinery, Inc},
isbn={9781450355438},
language={English},
abbrev_source_title={ICMI - Proc. ACM Int. Conf. Multimod. Interact.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Doneit2017735,
author={Doneit, W. and Lohse, J. and Glesing, K. and Simon, C. and Fischer, M. and Depner, A. and Kruse, A. and Franz, I. and Schultz, T. and Putze, F. and Schulze, T. and Engels, M.A. and Gaerte, P. and Bothe, D. and Ziegler, C. and Maucher, I. and Ricken, M. and Dimitrov, T. and Herzig, J. and Bernardin, K. and Gehrig, T. and Mikut, R.},
title={Data-driven analysis of interactions between people with dementia and a tablet device},
journal={Current Directions in Biomedical Engineering},
year={2017},
volume={3},
number={2},
pages={735-738},
doi={10.1515/cdbme-2017-0155},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059815466&doi=10.1515%2fcdbme-2017-0155&partnerID=40&md5=846865b5dc5f3602a8811f1dbfa7e686},
affiliation={Institut für Angewandte Informatik (IAI), Karlsruher Institut für Technologie (KIT), Hermann-von-Helmholtz-Platz 1, Eggenstein-Leopoldshafen, 76344, Germany; AWO Karlsruhe gGmbH, Rahel-Straus-Straße 2, Karlsruhe, 76137, Germany; Institut für Gerontologie, Ruprecht-Karls-Universität Heidelberg, Bergheimer Straße 20, Heidelberg, 69115, Germany; Diakonische Hausgemeinschaften Heidelberg e.V., Georg-Mechtersheimer-Straße 13, Heidelberg, 69126, Germany; Cognitive Systems Lab, Universität Bremen, Enrique-Schmidt-Str. 5, Bremen, 28359, Germany; Media4Care GmbH, Schönhauser Allee 152, Berlin, 10435, Germany; topsystem Systemhaus GmbH, Monnetstraße 24, Würselen, 52146, Germany; Deutsche Telekom Healthcare and Security Solutions GmbH, Pascalstraße 11, Berlin, 10587, Germany; Anasoft Technology AG, Querenburger Str. 38, Bochum, 44789, Germany; Videmo Intelligente Videoanalyse GmbH and Co. KG, Haid-und-Neu-Straße 7, Karlsruhe, 76131, Germany; Institut für Angewandte Informatik (IAI), Karlsruher Institut für Technologie (KIT), Hermann-von-Helmholtz-Platz 1, Eggenstein-Leopoldshafen, 76344, Germany},
abstract={In the project I-CARE a technical system for tablet devices is developed that captures the personal needs and skills of people with dementia. The system provides activation content such as music videos, biographical photographs and quizzes on various topics of interest to people with dementia, their families and professional caregivers. To adapt the system, the activation content is adjusted to the daily condition of individual users. For this purpose, emotions are automatically detected through facial expressions, motion, and voice. The daily interactions of the users with the tablet devices are documented in log files which can be merged into an event list. In this paper, we propose an advanced format for event lists and a data analysis strategy. A transformation scheme is developed in order to obtain datasets with features and time series for popular methods of data mining. The proposed methods are applied to analysing the interactions of people with dementia with the I-CARE tablet device. We show how the new format of event lists and the innovative transformation scheme can be used to compress the stored data, to identify groups of users, and to model changes of user behaviour. As the I-CARE user studies are still ongoing, simulated benchmark log files are applied to illustrate the data mining strategy. We discuss possible solutions to challenges that appear in the context of I-CARE and that are relevant to a broad range of applications. © 2017 Wolfgang Doneit et al., Published by De Gruyter.},
author_keywords={Data mining;  Dementia;  Events;  Tablet device},
funding_details={V4PID062},
funding_details={Bundesministerium fÃ¼r Bildung und ForschungBundesministerium fÃ¼r Bildung und Forschung, BMBF},
funding_details={Bundesministerium fÃ¼r Bildung und ForschungBundesministerium fÃ¼r Bildung und Forschung, BMBF},
funding_text 1={Research funding: The project I-CARE “Individuelle Aktivierung von Menschen mit Demenz” is funded by the Federal Ministry of Education and Research (BMBF) within the research programme “IKT 2020 – Forschung für Innovationen” as joint project under the reference number V4PID062. Conflict of interest: Authors state no conflict of interest. Informed consent: Informed consent has been obtained from all individuals included in this study. Ethical approval: The research related to human use complies with all the relevant national regulations, institutional policies and was performed in accordance with the tenets of the Helsinki Declaration, and has been approved by the authors' institutional review board or equivalent committee.},
references={Makanju, A.A., Clustering Event Logs Using Iterative Partitioning (2009) Proceedings of The 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining; Schultz, T., Technische Unterstützung für Menschen mit Demenz – Ein Überblick (2014) Technische Unterstützung Für Menschen Mit Demenz, , Schultz T, Putze F, Kruse A Eds. KIT Scientific Publishing; Schultz, T., I-CARE: Individual Activation of People with Dementia Proceedings of 13th Biannual Conference of The German Cognitive Science Society, , Bremen; Song, B., Surveillance Tracking System Using Passive Infrared Motion Sensors in Wireless Sensor Network (2008) International Conference on Information Networking, , IEEE; Vaarandi, R., (2005) Tools and Techniques for Event Log Analysis, , Tallinn University of Technology Press; Van der Aalst, W.M., Business Process Mining: An Industrial Application (2007) Information Systems; Van der Aalst, W.M., Weijters, A., Process Mining: A Research Agenda (2004) Computers in Industry; Yemini, S.A., High Speed and Robust Event Correlation (1996) IEEE Communications Magazine},
correspondence_address1={Doneit, W.; Institut für Angewandte Informatik (IAI), Karlsruher Institut für Technologie (KIT), Hermann-von-Helmholtz-Platz 1, Germany; email: wolfgang.doneit@kit.edu},
publisher={Walter de Gruyter GmbH},
issn={23645504},
language={English},
abbrev_source_title={Curr. Dir. Biomed. Eng.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Thürer2017,
author={Thürer, B. and Stockinger, C. and Putze, F. and Schultz, T. and Stein, T.},
title={Mechanisms within the parietal cortex correlate with the benefits of random practice in motor adaptation},
journal={Frontiers in Human Neuroscience},
year={2017},
volume={11},
doi={10.3389/fnhum.2017.00403},
art_number={403},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027831530&doi=10.3389%2ffnhum.2017.00403&partnerID=40&md5=1325b4be894c707b6b095c915d3667af},
affiliation={BioMotion Center, Institute of Sports and Sports Science, Karlsruhe Institute of Technology, Karlsruhe, Germany; Department of Sport and Health Sciences, Technical University of Munich, Munich, Germany; Cognitive Systems Lab, Faculty 3 – Computer Science, University of Bremen, Bremen, Germany},
abstract={The motor learning literature shows an increased retest or transfer performance after practicing under unstable (random) conditions. This random practice effect (also known as contextual interference effect) is frequently investigated on the behavioral level and discussed in the context of mechanisms of the dorsolateral prefrontal cortex and increased cognitive efforts during movement planning. However, there is a lack of studies examining the random practice effect in motor adaptation tasks and, in general, the underlying neural processes of the random practice effect are not fully understood. We tested 24 right-handed human subjects performing a reaching task using a robotic manipulandum. Subjects learned to adapt either to a blocked or a random schedule of different force field perturbations while subjects’ electroencephalography (EEG) was recorded. The behavioral results showed a distinct random practice effect in terms of a more stabilized retest performance of the random compared to the blocked practicing group. Further analyses showed that this effect correlates with changes in the alpha band power in electrodes over parietal areas. We conclude that the random practice effect in this study is facilitated by mechanisms within the parietal cortex during movement execution which might reflect online feedback mechanisms. © 2017 Thürer, Stockinger, Putze, Schultz and Stein.},
author_keywords={Alpha band power;  Contextual interference;  Electroencephalography (EEG);  Force field adaptation;  Sensorimotor learning;  Variable practice},
funding_details={Deutsche ForschungsgemeinschaftDeutsche Forschungsgemeinschaft, DFG},
funding_details={Karlsruhe Institute of TechnologyKarlsruhe Institute of Technology, KIT},
funding_text 1={This work was supported by the Graduate Funding from the German States. We acknowledge support by Deutsche Forschungsgemeinschaft and Open Access Publishing Fund of Karlsruhe Institute of Technology.},
references={Battig, W.F., Intratask interference as a source of facilitation in transfer and retention (1972) Topics in Learning and Performance, pp. 131-159. , eds R. F. Thompson and J. F. Voss (New York, NY: Academic Press); Battig, W.F., The flexibility of human memory (1979) Levels of Processing in Human Memory, pp. 23-44. , eds S. C. Laird and I. M. C. Fergus (Hillsdale, NJ: Lawrence Erlbaum Associates; Brady, F., Contextual interference and teaching golf skills (1997) Percept. Mot. Skills, 48, pp. 347-350; Braun, D.A., Aertsen, A., Wolpert, D.M., Mehring, C., Learning optimal adaptation strategies in unpredictable motor tasks (2009) J. Neurosci, 29, pp. 6472-6478; Canolty, R.T., Edwards, E., Dalal, S.S., Soltani, M., Nagarajan, S.S., Kirsch, H.E., High gamma power is phase-locked to theta oscillations in human neocortex (2006) Science, 313, pp. 1626-1628; Cohen, J., (1988) Statistical Power Analysis for the Behavioral Sciences, , Hillsdale, NJ: Lawrence Erlbaum Associates; Cohen, M.X., (2014) Analyzing Neural Time Series Data: Theory and Practice, , Cambridge, MA: MIT Press; Cross, E.S., Schmitt, P.J., Grafton, S.T., Neural substrates of contextual interference during motor learning support a model of active preparation (2007) J. Cogn. Neurosci, 19, pp. 1854-1871; Debarnot, U., Abichou, K., Kalenzaga, S., Sperduti, M., Piolino, P., Random motor imagery training induces sleep memory consolidation and transfer improvements (2015) Neurobiol. Learn. Mem, 119, pp. 85-92; Delorme, A., Makeig, S., EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics (2004) J. Neurosci. Methods, 134, pp. 9-21; Desmurget, M., Epstein, C.M., Turner, R.S., Prablanc, C., Alexander, G.E., Grafton, S.T., Role of the posterior parietal cortex in updating reaching movements to a visual target (1999) Nat. Neurosci, 2, pp. 563-567; Diedrichsen, J., Hashambhoy, Y., Rane, T., Shadmehr, R., Neural correlates of reach errors (2005) J. Neurosci, 25, pp. 9919-9931; Dimitriou, M., Wolpert, D.M., Franklin, D.W., The temporal evolution of feedback gains rapidly update to task demands (2013) J. Neurosci, 33, pp. 10898-10909; Edgington, E.S., Onghena, P., (2007) Randomization Tests, , London: CRC Press; Franklin, S., Wolpert, D.M., Franklin, D.W., Visuomotor feedback gains upregulate during the learning of novel dynamics (2012) J. Neurophysiol, 108, pp. 467-478; Gardner, E.P., Johnson, K.O., Sensory coding (2013) Principles of Neural Sciences, pp. 449-474. , eds E. R. Kandel, J. H. Schwartz, T. M. Jessell, S. A. Siegelbaum, and A. J. Hudspeth (New York, NY: McGraw-Hill; Gentili, R.J., Bradberry, T.J., Oh, H., Hatfield, B.D., Contreras Vidal, J.L., Cerebral cortical dynamics during visuomotor transformation: Adaptation to a cognitive-motor executive challenge (2011) Psychophysiology, 48, pp. 813-824; Hardwick, R.M., Rottschy, C., Miall, R.C., Eickhoff, S.B., A quantitative meta-analysis and review of motor learning in the human brain (2013) Neuroimage, 67, pp. 283-297; Huberdeau, D.M., Krakauer, J.W., Haith, A.M., Dual-process decomposition in human sensorimotor adaptation (2015) Curr. Opin. Neurobiol, 33, pp. 71-77; Jarus, T., Wughalter, E.H., Gianutsos, J.G., Effects of contextual interference and conditions of movement task on acquisition, retention, and transfer of motor skills by women (1997) Percept. Mot. Skills, 84, pp. 179-193; Joiner, W.M., Smith, M.A., Long-term retention explained by a model of short-term learning in the adaptive control of reaching (2008) J. Neurophysiol, 100, pp. 2948-2955; Kantak, S.S., Sullivan, K.J., Fisher, B.E., Knowlton, B.J., Winstein, C.J., Neural substrates of motor memory consolidation depend on practice structure (2010) Nat. Neurosci, 13, pp. 923-925; Kim, S., Oh, Y., Schweighofer, N., Between-trial forgetting due to interference and time in motor adaptation (2015) Plos ONE, 10; Klimesch, W., Alpha-band oscillations, attention, and controlled access to stored information (2012) Trends Cogn. Sci, 16, pp. 606-617; Lee, T.D., Magill, R.A., The locus of contextual interference in motor-skill acquisition (1983) J. Exp. Psychol. Learn. Mem. Cogn, 9, pp. 730-746; Li, Y., Wright, D.L., An assessment of the attention demands during random- and blocked-practice schedule (2000) Q. J. Exp. Psychol. A, 53, pp. 591-606; Lin, C.H.J., Chiang, M.C., Knowlton, B.J., Iacoboni, M., Udompholkul, P., Wu, A.D., Interleaved practice enhances skill learning and the functional connectivity of fronto-parietal networks (2013) Hum. Brain Mapp, 34, pp. 1542-1558; Lin, C.H.J., Knowlton, B.J., Chiang, M.C., Iacoboni, M., Udompholkul, P., Wu, A.D., Brain–behavior correlates of optimizing learning through interleaved practice (2011) Neuroimage, 56, pp. 1758-1772; Magill, R.A., Hall, K.G., A review of the contextual interference effect in motor skill acquisition (1990) Hum. Mov. Sci, 9, pp. 241-289; Makeig, S., Bell, A.J., Jung, T.P., Sejnowski, T.J., Independent component analysis of electroencephalographic data (1996) Advances in Neural Information Processing Systems, pp. 145-151. , eds D. Touretzky, M. Mozer, and M. Hasselmo (Cambridge, MA: MIT Press); Oldfield, R.C., The assessment and analysis of handedness: The Edinburgh inventory (1971) Neuropsychologia, 9, pp. 97-113; Pascual-Leone, A., Wassermann, E.M., Grafman, J., Hallett, M., The role of the dorsolateral pre-frontal cortex in implicit procedural learning (1996) Exp. Brain Res, 107, pp. 479-485; Perfetti, B., Moisello, C., Landsness, E.C., Kvint, S., Lanzafame, S., Onofrj, M., Modulation of gamma and theta spectral amplitude and phase synchronization is associated with the development of visuo-motor learning (2011) J. Neurosci, 31, pp. 14810-14819; Pfurtscheller, G., The cortical activation model (CAM (2006) Progress in Brain Research. Event-Related Dynamics of Brain Oscillations, pp. 19-27. , eds C. Neuper and W. Klimesch (Amsterdam: Elsevier; Pfurtscheller, G., Lopes Da Silva, F.H., Event-related EEG/MEG synchronization and desynchronization: Basic principles (1999) Clin. Neurophysiol, 110, pp. 1842-1857; Rathelot, J.-A., Dum, R.P., Strick, P.L., Posterior parietal cortex contains a command apparatus for hand movements (2017) Proc. Natl. Acad. Sci. U.S.A, 114, pp. 4255-4260; Richardson, J.T.E., Eta squared and partial eta squared as measures of effect size in educational research (2011) Educ. Res. Rev, 6, pp. 135-147; Robertson, E.M., The serial reaction time task: Implicit motor skill learning? (2007) J. Neurosci, 27, pp. 10073-10075; Robertson, E.M., From creation to consolidation: A novel framework for memory processing (2009) Plos Biol, 7; Robertson, E.M., Pascual-Leone, A., Miall, R.C., Current concepts in procedural consolidation (2004) Nat. Rev. Neurosci, 5, pp. 576-582; Roux, F., Uhlhaas, P.J., Working memory and neural oscillations: Alpha-gamma versus theta-gamma codes for distinct WM information? (2014) Trends Cogn. Sci, 18, pp. 16-25; Scheidt, R.A., Dingwell, J.B., Mussa-Ivaldi, F.A., Learning to move amid uncertainty (2001) J. Neurophysiol, 86, pp. 971-985; Scheidt, R.A., Reinkensmeyer, D.J., Conditt, M.A., Rymer, W.Z., Mussa-Ivaldi, F.A., Persistence of motor adaptation during constrained, multi-joint, arm movements (2000) J. Neurophysiol, 84, pp. 853-862; Schlögl, A., Keinrath, C., Zimmermann, D., Scherer, R., Leeb, R., Pfurtscheller, G., A fully automated correction method of EOG artifacts in EEG recordings (2007) Clin. Neurophysiol, 118, pp. 98-104; Schwarb, H., Schumacher, E.H., Neural evidence of a role for spatial response selection in the learning of spatial sequences (2009) Brain Res, 1247, pp. 114-125; Shadmehr, R., Brandt, J., Corkin, S., Time-dependent motor memory processes in amnesic subjects (1998) J. Neurophysiol, 80, pp. 1590-1597; Shadmehr, R., Krakauer, J.W., A computational neuroanatomy for motor control (2008) Exp. Brain Res, 185, pp. 359-381; Shadmehr, R., Mussa-Ivaldi, F.A., Adaptive representation of dynamics during learning of a motor task (1994) J. Neurosci, 14, pp. 3208-3224; Shea, J.B., Morgan, R.L., Contextual interference effects on the acquisition, retention, and transfer of a motor skill (1979) J. Exp. Psychol. Hum. Learn, 5, pp. 179-187; Shewokis, P.A., Del Rey, P., Simpson, K.J., A test of retroactive inhibition as an explanation of contextual interference (1998) Res. Q. Exerc. Sport, 69, pp. 70-74; Stockinger, C., Focke, A., Stein, T., Catch trials in force field learning influence adaptation and consolidation of human motor memory (2014) Front. Hum. Neurosci, 8, p. 231; Stockinger, C., Pöschl, M., Focke, A., Stein, T., ManipAnalysis - a software application for the analysis of force field experiments (2012) Int. J. Comput. Sci. Sport, 11, pp. 52-57; Stockinger, C., Thürer, B., Focke, A., Stein, T., Intermanual transfer characteristics of dynamic learning: Direction, coordinate frame, and consolidation of interlimb generalization (2015) J. Neurophysiol, 114, pp. 3166-3176; Tanaka, S., Honda, M., Hanakawa, T., Cohen, L.G., Differential contribution of the supplementary motor area to stabilization of a procedural motor skill acquired through different practice schedules (2010) Cereb. Cortex, 20, pp. 2114-2121; Taylor, J.A., Krakauer, J.W., Ivry, R.B., Explicit and implicit contributions to learning in a sensorimotor adaptation task (2014) J. Neurosci, 34, pp. 3023-3032; Thürer, B., Stockinger, C., Focke, A., Putze, F., Schultz, T., Stein, T., Increased gamma band power during movement planning coincides with motor memory retrieval (2016) Neuroimage, 125, pp. 172-181; Tombini, M., Zappasodi, F., Zollo, L., Pellegrino, G., Cavallo, G., Tecchio, F., Brain activity preceding a 2D manual catching task (2009) Neuroimage, 47, pp. 1735-1746; Wright, D., Verwey, W., Buchanen, J., Chen, J., Rhee, J., Immink, M., Consolidating behavioral and neurophysiologic findings to explain the influence of contextual interference during motor sequence learning (2015) Psychon. Bull. Rev, 23, pp. 1-21; Yousif, N., Diedrichsen, J., Structural learning in feedforward and feedback control (2012) J. Neurophysiol, 108, pp. 2373-2382},
correspondence_address1={Thürer, B.; BioMotion Center, Institute of Sports and Sports Science, Karlsruhe Institute of TechnologyGermany; email: benjamin.thuerer@kit.edu},
publisher={Frontiers Media S. A},
issn={16625161},
language={English},
abbrev_source_title={Front. Human Neurosci.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Weiner20173117,
author={Weiner, J. and Engelbart, M. and Schultz, T.},
title={Manual and automatic transcriptions in dementia detection from speech},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2017},
volume={2017-August},
pages={3117-3121},
doi={10.21437/Interspeech.2017-112},
note={cited By 14; Conference of 18th Annual Conference of the International Speech Communication Association, INTERSPEECH 2017 ; Conference Date: 20 August 2017 Through 24 August 2017;  Conference Code:132696},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039163279&doi=10.21437%2fInterspeech.2017-112&partnerID=40&md5=b0d8280c1e0e2c724c3a12e4d1604a09},
affiliation={Cognitive Systems Lab., Universität Bremen, Germany},
abstract={As the population in developed countries is aging, larger numbers of people are at risk of developing dementia. In the near future there will be a need for time- and cost-efficient screening methods. Speech can be recorded and analyzed in this manner, and as speech and language are affected early on in the course of dementia, automatic speech processing can provide valuable support for such screening methods. We present two pipelines of feature extraction for dementia detection: the manual pipeline uses manual transcriptions while the fully automatic pipeline uses transcriptions created by automatic speech recognition (ASR). The acoustic and linguistic features that we extract need no language specific tools other than the ASR system. Using these two different feature extraction pipelines we automatically detect dementia. Our results show that the ASR system's transcription quality is a good single feature and that the features extracted from automatic transcriptions perform similar or slightly better than the features extracted from the manual transcriptions. Copyright © 2017 ISCA.},
author_keywords={Cognitive impairment;  Computational paralinguistics;  Dementia},
keywords={Extraction;  Feature extraction;  Linguistics;  Neurodegenerative diseases;  Pipelines;  Speech;  Speech communication;  Speech processing;  Transcription, Automatic speech processing;  Automatic speech recognition;  Automatic transcription;  Cognitive impairment;  Dementia;  Developed countries;  Linguistic features;  Paralinguistics, Speech recognition},
references={(2012) Dementia: A Public Health Priority, , World Health Organization; Prince, M., Wimo, A., Guerchet, M., Ali, G.-C., Wu, Y.-T., Prina, M., (2015) World Alzheimer Report 2015. The Global Impact of Dementia: An Analysis of Prevalence, Incidence, Cost and Trends, , London: Alzheimer's Disease International; Satt, A., Hoory, R., König, A., Aalten, P., Robert, P.H., Speech-based automatic and robust detection of very early dementia (2014) INTERSPEECH 2014 - 15th Annual Conference of the International Speech Communication Association, pp. 2538-2542; Espinoza-Cuadros, F., Garcia-Zamora, M.A., Torres-Boza, D., Ferrer-Riesgo, C.A., Montero-Benavides, A., Gonzalez-Moreira, E., Hernandez-Gomez, L.A., A spoken language database for research on moderate cognitive impairment: Design and preliminary analysis (2014) Advances in Speech and Language Technologies for Iberian Languages, pp. 219-228. , Springer; Toth, L., Gosztolya, G., Vincze, V., Hoffmann, I., Szatlóczki, G., Automatic detection of mild cognitive impairment from spontaneous speech using asr (2015) INTERSPEECH 2015 - 16th Annual Conference of the International Speech Communication Association, pp. 2694-2698; Prud'hommeaux, E.T., Roark, B., Extraction of narrative recall patterns for neuropsychological assessment (2011) INTERSPEECH 2011 - 12th Annual Conference of the International Speech Communication Association, pp. 3021-3024; Lehr, M., Prud'hommeaux, E.T., Shafran, I., Roark, B., Fully automated neuropsychological assessment for detecting mild cognitive impairment (2012) INTERSPEECH 2012 - 13th Annual Conference of the International Speech Communication Association, pp. 1039-1042; Appell, J., Kertesz, A., Fisman, M., A study of language functioning in Alzheimer patients (1982) Brain and Language, 17 (1), pp. 73-91; Bucks, R., Singh, S., Cuerden, J.M., Wilcock, G.K., Analysis of spontaneous, conversational speech in dementia of Alzheimer type: Evaluation of an objective technique for analysing lexical performance (2000) Aphasiology, 14 (1), pp. 71-91; Khodabakhsh, A., Yesil, F., Guner, E., Demiroglu, C., Evaluation of linguistic and prosodic features for detection of Alzheimer's disease in Turkish conversational speech (2015) EURASIP Journal on Audio, Speech, and Music Processing, 2015 (1), pp. 1-15; Weiner, J., Herff, C., Schultz, T., Speech-based detection of Alzheimer's disease in conversational German (2016) INTERSPEECH 2016-17th Annual Conference of the International Speech Communication Association; Hakkani-Tur, D., Vergyri, D., Tür, G., Speech-based automated cognitive status assessment (2010) INTERSPEECH 2010 - 11th Annual Conference of the International Speech Communication Association, pp. 258-261; Hernandez-Domínguez, L., García-Cano, E., Ratté, S., Sierra-Martínez, G., Detection of Alzheimer's disease based on automatic analysis of common objects descriptions (2016) Proceedings of the 7th Workshop on Cognitive Aspects of Computational Language Learning; Wankerl, S., Nth, E., Evert, S., An analysis of perplexity to reveal the effects of Alzheimer's disease on language (2016) 12th ITG Conference on Speech Communication; Zhou, L., Fraser, K.C., Rudzicz, F., Speech recognition in alzheimers disease and in its assessment (2016) INTERSPEECH 2016 - 17th Annual Conference of the International Speech Communication Association, pp. 1948-1952; Thomas, C., Keselj, V., Cercone, N., Rockwood, K., Asp, E., Automatic detection and rating of dementia of Alzheimer type through lexical analysis of spontaneous speech (2005) IEEE International Conference Mechatronics and Automation, 2005, 3, pp. 1569-1574; Jarrold, W., Peintner, B., Wilkins, D., Vergryi, D., Richey, C., Gorno-Tempini, M.L., Ogar, J., Aided diagnosis of dementia type through computer-based analysis of spontaneous speech (2014) Proceedings of the ACL Workshop on Computational Linguistics and Clinical Psychology, pp. 27-36; Young, V., Mihailidis, A., Difficulties in automatic speech recognition of dysarthric speakers and implications for speech-based applications used by the elderly: A literature review (2010) Assistive Technology, 22 (2), pp. 99-112; Vipperla, R., Renals, S., Frankel, J., Longitudinal study of asr performance on ageing voices (2008) INTERSPEECH 2008 - 9th Annual Conference of the International Speech Communication Association, pp. 2550-2553; Weiner, J., Frankenberg, C., Telaar, D., Wendelstein, B., Schröder, J., Schultz, T., Towards automatic transcription of ILSE - An interdisciplinary longitudinal study of adult development and aging (2016) Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16); Martin, P., Martin, M., Design und methodik der interdisziplinären längsschnittstudie des erwachsenenalters (2000) Aspekte der Entwicklung im Mittleren und Höheren Lebensalter: Ergebnisse der Interdisziplinären Längsschnittstudie des Erwachsenenalters (ILSE), pp. 17-27. , P. Martin, K. U. Ettrich, U. Lehr, D. Roether, M. Martin, and A. Fischer-Cyrulies, Eds. Steinkopff; Telaar, D., Wand, M., Gehrig, D., Putze, F., Amma, C., Heger, D., Vu, N.T., Schultz, T., BioKIT - Real-time decoder for biosignal processing (2014) INTERSPEECH 2014 - 15th Annual Conference of the International Speech Communication Association, pp. 2650-2654; Telaar, D., Weiner, J., Schultz, T., Error signatures to identify errors in ASR in an unsupervised fashion (2015) Proceedings of the Errare Workshop (ERRARE 2015); Telaar, D., (2015) Error Correction Based on Error Signatures Applied to Automatic Speech Recognition, , Ph.D. dissertation, Karlsruhe Institute of Technology; Povey, D., Ghoshal, A., Boulianne, G., Burget, L., Glembek, O., Goel, N., Hannemann, M., Vesely, K., The kaldi speech recognition toolkit (2011) IEEE 2011 Workshop on Automatic Speech Recognition and Understanding; Brunet, E., (1978) Le Vocabulaire de Jean Giraudoux, Structure et Évolution, , Geneva: Slatkine; Tweedie, F.J., Baayen, R.H., How variable May a constant be? Measures of lexical richness in perspective (1998) Computers and the Humanities, 32 (5), pp. 323-352; Honoré, A., Some simple measures of richness of vocabulary (1979) Association for Literary and Linguistic Computing Bulletin, 7 (2), pp. 172-177; Stolcke, A., SRILM - An extensible language modeling toolkit (2002) International Confonference on Spoken Language Processing; Pedregosa, F., Varoquaux, G., Gramfort, A., Mchel, V., Thirion, B., Grisel, O., Blondel, M., Duchesnay, E., Scikit-learn: Machine learning in python (2011) Journal of Machine Learning Research, 12, pp. 2825-2830; Schuller, B., Steidl, S., Batliner, A., Hirschberg, J., Burgoon, J.K., Baird, A., Elkins, A., Evanini, K., The INTERSPEECH 2016 computational paralinguistics challenge: Deception, sincerity & native language (2016) INTERSPEECH 2016 - 17th Annual Conference of the International Speech Communication Association},
editor={Lacerda F., Strombergsson S., Wlodarczak M., Heldner M., Gustafson J., House D.},
sponsors={Amazon Alexa; Apple; DiDi; et al.; Furhat Robotics; Microsoft},
publisher={International Speech Communication Association},
issn={2308457X},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ehret201735,
author={Ehret, S. and Putze, F. and Miller-Teynor, H. and Kruse, A. and Schultz, T.},
title={Technique-based game for daycare visitors with and without dementia: Effects, heuristics and correlates [Technikbasiertes Spiel von Tagespflegebesuchern mit und ohne Demenz: Effekte, Heuristiken und Korrelate]},
journal={Zeitschrift fur Gerontologie und Geriatrie},
year={2017},
volume={50},
number={1},
pages={35-44},
doi={10.1007/s00391-016-1093-2},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976491213&doi=10.1007%2fs00391-016-1093-2&partnerID=40&md5=77952f1e2bebf7a49897678fccab2907},
affiliation={Institut für Gerontologie, Universität Heidelberg, Bergheimer Str. 20, Heidelberg, 69115, Germany; Cognitive Systems Lab, Universität Bremen, Enrique-Schmidt-Str. 5, Bremen, 28359, Germany},
abstract={Background: Playing of old people with or without dementia have not yet been substantially investigated. Objective: This study deals with the acceptance and impact of a tablet-based memory game, which was played on a weekly or semiweekly basis by visitors in two daycare units. Material and methods: Within the framework of focus groups the technical system was adapted for elderly users. The video-assisted data at the level of the game and the dynamics were investigated with respect to interaction and communication. Results: The analysis of psychological observation forms and game protocols, which were conducted over a period of 3 months, indicated different effects of the game on psychosocial and cognitive activation. The individual memory cards in particular served as an intensification of communication and a stimulation of episodic memory. Finally, with video analysis during the whole game setting three theoretical relationship patterns of the spheres playing and speech could be depicted. Conclusion: Coherence, separation and incoherence of playing and speech are different forms of interaction in which individual and collaborative competences of people with and without dementia can be visualized. Furthermore, the study provides evidence for the cultural theory of playing by Huizinga. © 2016, Springer-Verlag Berlin Heidelberg.},
author_keywords={Communication;  Culture;  Memory;  Narration;  Psychological adaptation;  User-Computer-Interface},
keywords={clinical trial;  computer assisted therapy;  computer interface;  dementia;  female;  human;  male;  Memory Disorders;  procedures;  psychology;  treatment outcome;  very elderly;  video game, Aged, 80 and over;  Dementia;  Female;  Humans;  Male;  Memory Disorders;  Therapy, Computer-Assisted;  Treatment Outcome;  User-Computer Interface;  Video Games},
references={Kruse, A., (2010) Lebensqualität bei Demenz?, , (ed), AKA, Heidelberg; Becker, S., Kaspar, R., Lindenthal, M., Zentrale theoretische Zugänge zur Lebensqualität bei Demenz (2010) Lebensqualität bei Demenz?, pp. 73-97. , Kruse A, (ed), AKA, Heidelberg; Ehret, S., Daseinsthemen und Daseinsthematische Begleitung bei Demenz (2010) Lebensqualität bei Demenz?, pp. 217-230. , Kruse A, (ed), AKA, Heidelberg; Ehret, S., Kaspar, R., Kruse, A., Daseinsthematische Begleitung zur Förderung der Individualität, Personalität und Sozialität von Menschen mit Demenz (2009) Seelische Gesundheit und Lebensqualität im Alter. Depression – Demenz – Versorgung. Schriftenreihe der Deutschen Gesellschaft für Gerontopsychiatrie und -psychotherapie (DGGPP), pp. 190-195. , Adler G, (ed), Kohlhammer, Stuttgart; Bohnsack, R., Przyborski, A., Schäffer, B., (2010) Das Gruppendiskussionsverfahren in der Forschungspraxis, , (eds), Budrich, Opladen; Huizinga, J., (1939) Homo Ludens, , Pantheon, Amsterdam; Goldstein, K., (1934) Der Aufbau des Organismus, , Nijhoff, Den Haag; Putze, F., (2015) Model-based Evaluation of Playing Strategies in a Memo Game for Elderly Users, , Proceedings of IEEE International Conference on Systems, Man and Cybernetics, IEEE, Hong Kong; Falkenberg, I., Entwicklung von Lachen und Humor in den verschiedenen Lebensphasen (2010) Z Gerontol Geriatr, 43, pp. 25-30. , COI: 1:STN:280:DC%2BC3c%2FpsVWhuw%3D%3D, PID: 20033815; Adler, A., (2007) Menschenkenntnis (1927), , Vandenhoeck und Ruprecht, Göttingen; Kueider, A.M., Computerized cognitive training with older adults: a systematic review (2012) PLoS ONE, 7 (7). , COI: 1:CAS:528:DC%2BC38XhtVKku7fE, PID: 22792378; Chan, M.Y., Training older adults to use tablet computers: does it enhance cognitive function? (2014) Gerontologist, 13, pp. 1-11; Chambon, C., Benefits of computer-based memory and attention training in healthy older adults (2014) Psychol Aging, 29, pp. 731-743. , PID: 25244490; Fua, K.C., (2013) Designing serious games for elders, pp. 291-297; Karbach, J., Game-based cognitive training for the aging brain (2014) Front Psychol, 5, p. 1100. , PID: 25324807; Peretz, C., Computer-based, personalized cognitive training versus classical computer games: a randomized double-blind prospective trial of cognitive stimulation (2011) Neuroepidemiology, 36, pp. 91-99. , PID: 21311196; Robert, P., Recommendations for the use of serious games in people with alzheimer’s disease, related disorders and frailty (2014) Front Aging Neurosci, 6, p. 54. , PID: 24715864; Oviatt, S., The impact of interface affordances on human ideation, problem solving, and inferential reasoning (2012) Acm Trans Comput Hum Interact, 19 (3), pp. 1-30; Nordheim, J., Tablet-PC und ihr Nutzen für demenzerkrankte Heimbewohner (2015) Z Gerontol Geriatr, 48, pp. 543-549. , PID: 25524141; Girtler, R., (2001) Methoden der Feldforschung, , Böhlau, Wien; Conway, M., Episodic memories (2009) Neuropsychologia, 47 (11), pp. 2305-2313. , PID: 19524094; Strouhal, E., (2011) Schach und Alter, , Springer, Berlin; Chateau, J., (1976) Das Spiel des Kindes, , Schöningh, Paderborn},
correspondence_address1={Ehret, S.; Institut für Gerontologie, Universität Heidelberg, Bergheimer Str. 20, Germany; email: sonja.ehret@gero.uni-heidelberg.de},
publisher={Dr. Dietrich Steinkopff Verlag GmbH and Co. KG},
issn={09486704},
coden={ZGGEF},
pubmed_id={27370267},
language={German},
abbrev_source_title={Z. Gerontol. Geriatr.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Weiner2017380,
author={Weiner, J. and Diener, L. and Stelter, S. and Externest, E. and Kühl, S. and Herff, C. and Putze, F. and Schulze, T. and Salous, M. and Liu, H. and Küster, D. and Schultz, T.},
title={Bremen Big Data Challenge 2017: Predicting University Cafeteria Load},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10505 LNAI},
pages={380-386},
doi={10.1007/978-3-319-67190-1_35},
note={cited By 0; Conference of 40th Annual German Conference on Artificial Intelligence, KI 2017 ; Conference Date: 25 September 2017 Through 29 September 2017;  Conference Code:199309},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030871359&doi=10.1007%2f978-3-319-67190-1_35&partnerID=40&md5=45bd1029c720108021fe3f76e58a6317},
affiliation={Cognitive Systems Lab, University of Bremen, Bremen, Germany},
abstract={Big data is a hot topic in research and industry. The availability of data has never been as high as it is now. Making good use of the data is a challenging research topic in all aspects of industry and society. The Bremen Big Data Challenge invites students to dig deep into big data. In this yearly event students are challenged to use the month of March to analyze a big dataset and use the knowledge they gained to answer a question. In this year’s Bremen Big Data Challenge students were challenged to predict the load of the university cafeteria from the load of past years. The best of 24 teams predicted the load with a root mean squared error of 8.6 receipts issued in five minutes, with a fusion system based on the top 5 entries achieving an even better result of 8.28. © 2017, Springer International Publishing AG.},
author_keywords={Big data;  Data analysis;  Data challenge},
keywords={Artificial intelligence;  Data reduction;  Industrial research;  Mean square error;  Potassium compounds;  Students, Data challenges;  Fusion systems;  Hot topics;  Research topics;  Root mean squared errors, Big data},
references={Bremen Big Data Challenge, , https://bbdc.csl.uni-bremen.de; Carpenter, J., May the best analyst win (2011) IEEE Robot. Autom. Mag.; Chen, M., Mao, S., Liu, Y., Big data: A survey (2014) Mob. Netw. Appl., 19 (2), pp. 171-209; Data Mining Cup, , http://www.data-mining-cup.de/en/dmc-wettbewerb/wettbewerb.html; Deutscher Wetterdienst - Archiv Monats- Und Tageswerte, , http://www.dwd.de/DE/leistungen/klimadatendeutschland/klarchivtagmonat.html; McKinney, W., Data structures for statistical computing in python (2010) Proceedings of the 9Th Python in Science Conference, pp. 51-56. , van der Walt, S., Millman, J. (eds.); Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Duchesnay, E., Scikit-learn: Machine learning in Python (2011) J. Mach. Learn. Res, 12, pp. 2825-2830; Bremen, S., http://www.stw-bremen.de/en},
correspondence_address1={Diener, L.; Cognitive Systems Lab, University of BremenGermany; email: lorenz.diener@uni-bremen.de},
editor={Furnkranz J., Thimm M., Kern-Isberner G.},
sponsors={},
publisher={Springer Verlag},
issn={03029743},
isbn={9783319671895},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Putze2016153,
author={Putze, F. and Popp, J. and Hild, J. and Beyerer, J. and Schultz, T.},
title={Intervention-Free selection using EEG and eye tracking},
journal={ICMI 2016 - Proceedings of the 18th ACM International Conference on Multimodal Interaction},
year={2016},
pages={153-160},
doi={10.1145/2993148.2993199},
note={cited By 9; Conference of 18th ACM International Conference on Multimodal Interaction, ICMI 2016 ; Conference Date: 12 November 2016 Through 16 November 2016;  Conference Code:124626},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016609050&doi=10.1145%2f2993148.2993199&partnerID=40&md5=810aad2da82d24fe2b63ce058cb32ab9},
affiliation={University of Bremen, Bremen, Germany; Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={In this paper, we show how recordings of gaze movements (via eye tracking) and brain activity (via electroencephalography) can be combined to provide an interface for implicit selection in a graphical user interface. This implicit selection works completely without manual intervention by the user. In our approach, we formulate implicit selection as a classification problem, describe the employed features and classification setup and introduce our experimental setup for collecting evaluation data. With a fully online-capable setup, we can achieve an F0:2-score of up to 0.74 for temporal localization and a spatial localization accuracy of more than 95%. © 2016 ACM.},
author_keywords={EEG;  Eye tracking;  Implicit selection},
keywords={Brain;  Classification (of information);  Electrophysiology;  Eye movements;  Graphical user interfaces;  Interactive computer systems;  User interfaces, Brain activity;  Eye-tracking;  Gaze movements;  Implicit selection;  Manual intervention;  Spatial localization;  Temporal localization, Electroencephalography},
references={Baccino, T., Manunta, Y., Eye-fixation-related potentials: Insight into parafoveal processing (2005) Journal of Psychophysiology, 19 (3), pp. 204-215. , Jan. 2005; Batuwita, R., Palade, V., (2013) Class Imbalance Learning Methods for Support Vector Machines, pp. 83-99. , Imbalanced Learning, Haibo He and Yunqian (Eds.). John Wiley & Sons, Inc; Beelders, T.R., Blignaut, P.J., Gaze and speech: Pointing device and text entry modality (2014) Current Trends in Eye Tracking Research, pp. 51-75. , Springer; Bigdely-Shamlo, N., Vankov, A., Ramirez, R.R., Makeig, S., Brain activity-based image classification from rapid serial visual presentation (2008) IEEE Transactions on Neural Systems and Rehabilitation Engineering, 16 (5), pp. 432-441. , Oct. 2008; Dimigen, O., Sommer, W., Hohlfeld, A., Jacobs, A.M., Kliegl, R., Coregistration of eye movements and EEG in natural reading: Analyses and review (2011) Journal of Experimental Psychology: General, 140 (4), pp. 552-572. , 2011; Donchin, E., Spencer, K.M., Wijesinghe, R., The mental prosthesis: Assessing the speed of a P300-based brain-computer interface (2000) IEEE Transactions on Rehabilitation Engineering, 8 (2), pp. 174-179. , June 2000; Finke, A., Essig, K., Marchioro, G., Ritter, H., Toward FRP-Based Brain-Machine interfaces-single-Trial classification of fixation-related potentials (2016) PLOS ONE, 11 (1), p. e0146848. , Jan. 2016; Foley, J.D., Dam, A.V., Feiner, S., Hughes, J., (1990) Com-puter Graphics: Principles and Practice, , Addison-Welsey, 1997 1990; Gerson, A.D., Parra, L.C., Sajda, P., Cortically coupled computer vision for rapid image search (2006) IEEE Transactions on Neural Systems and Rehabilitation Engineering, 14 (2), pp. 174-179. , June 2006; He, H., Bai, Y., Garcia, E.A., Li, S., ADASYN: Adaptive synthetic sampling approach for imbalanced learning (2008) 2008 IEEE International Joint Conference on Neural Networks, pp. 1322-1328. , IEEE World Congress on Computational Intelligence; Hild, J., Kühnle, C., Beyerer, J., Gaze-based moving target acquisition in real-Time full motion video (2016) Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research & Applications, pp. 241-244. , ACM; Hild, J., Müller, E., Klaus, E., Peinsipp-Byma, E., Beyerer, J., Evaluating multi-modal eye gaze interaction for moving object selection (2013) Proc. ACHI, pp. 454-459. , 2013; Hild, J., Putze, F., Kaufman, D., Kühnle, C., Schultz, T., Beyerer, J., Spatio-Temporal event selection in basic surveillance tasks using eye tracking and EEG (2014) Proceedings of the 7th Workshop on Eye Gaze in Intelligent Human Machine Interaction, pp. 3-8. , ACM; Huang, B., Anthony Lo, H.P., Shi, B.E., Integrating EEG information improves performance of gaze based cursor control (2013) 2013 6th International IEEE/EMBS Conference on Neural Engineering (NER), pp. 415-418; Hutzler, F., Braun, M., Võ, M.L.H., Engl, V., Hofmann, M., Dambacher, M., Leder, H., Jacobs, A.M., Welcome to the real world: Validating fixation-related brain potentials for ecologically valid settings (2007) Brain Research, 1172, pp. 124-129. , 2007; Jacob, R.J.K., The use of eye movements in human-computer interaction techniques: What you look at is what you get (1991) ACM Transactions on Information Systems (TOIS), 9 (2), pp. 152-169. , 1991; Just, M.A., Carpenter, P.A., A theory of reading: From eye fixations to comprehension (1980) Psychological Review, 87 (4), p. 329. , 1980; Koelstra, S., Muhl, C., Patras, I., EEG analysis for implicit tagging of video data (2009) 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops, pp. 1-6. , 2009. ACII 2009; Kumar, M., Paepcke, A., Winograd, T., Eyepoint: Practical pointing and selection using gaze and keyboard (2007) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 421-430. , ACM; Putze, F., Amma, C., Schultz, T., Design and evaluation of a self-correcting gesture interface based on error potentials from EEG (2015) Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI '15), pp. 3375-3384. , ACM, New York, NY, USA; Putze, F., Hesslinger, S., Tse, C.-Y., Huang, Y., Herff, C., Guan, C., Schultz, T., Hybrid fNIRS-EEG based classification of auditory and visual perception processes (2014) Frontiers in Neuroscience, p. 8. , 2014; Putze, F., Hild, J., Kärgel, R., Herff, C., Redmann, A., Beyerer, J., Schultz, T., Locating user attention using eye tracking and EEG for spatio-Temporal event selection (2013) Proceedings of the International Conference on Intelligent User Interfaces, , Santa Monica, USA; Putze, F., Scherer, M., Schultz, T., Starring into the void? Classifying internal vs. External attention from EEG (2016) Proceedings of 9th Nordic Conference on Human-Computer Interaction (NordiCHI), , Gothenborg, Sweden; Salvucci, D.D., Goldberg, J.H., Identifying fixations and saccades in eye-Tracking protocols (2000) Proceedings of the 2000 Symposium on Eye Tracking Research & Applications (ETRA '00), pp. 71-78. , ACM, New York, NY, USA; Schlögl, A., Keinrath, C., Zimmermann, D., Scherer, R., Leeb, R., Pfurtscheller, G., A fully automated correction method of EOG artifacts in EEG recordings (2007) Clinical Neurophysiology, 118 (1), pp. 98-104. , 2007; Shenoy, P., Tan, D.S., Human-Aided computing: Utilizing implicit human processing to classify images (2008) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '08), pp. 845-854. , ACM, New York, NY, USA; Vertegaal, R., A Fitts Law comparison of eye tracking and manual input in the selection of visual targets (2008) Proceedings of the 10th International Conference on Multimodal Interfaces, pp. 241-248. , ACM; Wang, J., Pohlmeyer, E., Hanna, B., Jiang, Y.-G., Sajda, P., Chang, S.-F., Brain state decoding for rapid image retrieval (2009) Proceedings of the 17th ACM International Conference on Multimedia (MM '09), pp. 945-954. , ACM, New York, NY, USA; Yong, X., Fatourechi, M., Ward, R.K., Birch, G.E., The design of a point-And-click system by integrating a self-paced Brain #x2013;Computer interface with an eye-Tracker (2011) IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 1 (4), pp. 590-602. , Dec. 2011; Zander, T.O., Gaertner, M., Kothe, C., Vilimek, R., Combining eye gaze input with a brain-computer interface for touchless human-computer interaction (2010) International Journal of Human-Computer Interaction, 27 (1), pp. 38-51. , Dec. 2010},
editor={Pelachaud C., Nakano Y.I., Nishida T., Busso C., Morency L.-P., Andre E.},
sponsors={ACM SIGCHI},
publisher={Association for Computing Machinery, Inc},
isbn={9781450345569},
language={English},
abbrev_source_title={ICMI - Proc. ACM Int. Conf. Multimodal Interact.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Putze2016,
author={Putze, F. and Scherer, M. and Schultz, T.},
title={Starring into the void? Classifying Internal vs. External attention from EEG},
journal={ACM International Conference Proceeding Series},
year={2016},
volume={23-27-October-2016},
doi={10.1145/2971485.2971555},
art_number={a47},
note={cited By 8; Conference of 9th Nordic Conference on Human-Computer Interaction, NordiCHI 2016 ; Conference Date: 23 October 2016 Through 27 October 2016;  Conference Code:124606},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997521967&doi=10.1145%2f2971485.2971555&partnerID=40&md5=df76ecd9e7be0591f412668c4c091330},
affiliation={University of Bremen, Bremen, Germany; Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={For any HCI system, it would be a great advantage if it was aware of the attentional state of its user: Is he or she paying attention to external stimuli provided by the user's environment or is the user focusing his or her attention on internal mental tasks? In this paper, we propose a model for the discrimination of internal and external attention by using Electroencephalography. We describe the experiment we conducted to collect data of internal and external attention, describe our setup for classification and discuss the classifications results. © 2016 ACM.},
author_keywords={Classification;  EEG;  External attention;  Internal attention},
keywords={Classification (of information);  Computer games;  Electroencephalography;  Electrophysiology, External attention;  External stimulus;  HCI system;  Internal attention;  Mental tasks, Human computer interaction},
references={Benedek, M., Schickel, R.J., Jauk, E., Fink, A., Neubauer, A.C., Alpha power increases in right parietal cortex reflects focused internal attention (2014) Neuropsychologia, 56, pp. 393-400. , 2004; Braboszcz, C., Delorme, A., Lost in thoughts: Neural markers of low alertness during mind wandering NeuroImage, 54 (4), pp. 3040-3047. , 2011-02-14; Chun, M.M., Golomb, J.D., Turk-Browne, N.B., A taxonomy of external and internal attention (2011) Annual Review of Psychology, 62 (1), pp. 73-101; Cooper, N.R., Croft, R.J., Dominey, S.J.J., Burgess, A.P., Gruzelier, J.H., Paradox lost? Exploring the role of alpha oscillations during externally vs. Internally directed attention and the implications for idling and inhibition hypotheses International Journal of Psychophysiology, 47 (1), pp. 65-74. , 2003-01; DMello, S., Kory, J., Consistent but modest: A meta-Analysis on unimodal and multimodal affect detection accuracies from 30 studies (2012) Proceedings of the 14th International Conference on Multimodal Interaction; Donchin, E., Spencer, K., Wijesinghe, R., The mental pros thesis assessing the speed of a p300-based brain-computer interface IEEE Transactions on Rehabilitation Engineering, 8 (2), pp. 174-179. , 2000-06; Haufe, S., Meinecke, F., Görgen, K., Dähne, S., Haynes, J.-D., Blankertz, B., Bießmann, F., On the interpretation of weight vectors of linear models in multivariate neuroimaging (2014) NeuroImage, 87, pp. 96-110; Hild, J., Putze, F., Kaufman, D., Kühnle, C., Schultz, T., Beyerer, J., Spatio-Temporal event selection in basic surveillance tasks using eye tracking and EEG (2014) Proceedings of the 7th Workshop on Eye Gaze in Intelligent Human Machine Interaction: Eye-Gaze & Multimodality ACM, pp. 3-8; Jhansi, N., Krishna, P.V., Meditation and attention regulation (1996) Journal of Indian Psychology, 14 (1), pp. 26-30; Klimesch, W., EEG alpha and theta oscillations reflect cognitive and memory performance: A review and analysis (1999) Brain Research Reviews, 29 (2), pp. 169-195; Mueller-Putz, G., Scherer, R., Brunner, C., Leeb, R., Pfurtscheller, G., Better than random: A closer look on BCI results (2008) International Journal of Bioelectromagnetism, 10, pp. 52-55; Ordikhani-Seyedlar, M., Sorensen, H., Kjaer, T., Siebner, H., Puthusserypady, S., SSVEP-modulation by covert and overt attention: Novel features for BCI in attention neuro-rehabilitation (2014) 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, pp. 5462-5465; Putze, F., Hesslinger, S., Tse, C.-Y., Huang, Y., Herff, C., Guan, C., Schultz, T., Hybrid fNIRS-EEG based classification of auditory and visual perception processes (2014) Frontiers in Neuroscience 8; Putze, F., Hild, J., Kärgel, R., Herff, C., Redmann, A., Beyerer, J., Schultz, T., Locating user attention using eye tracking and EEG for spatio-Temporal event selection (2013) Proceedings of the International Conference on Intelligent User Interfaces; Stiefelhagen, R., Yang, J., Waibel, A., Modeling focus of attention for meeting indexing based on multiple cues (2002) Neural Networks IEEE Transactions on, 13 (4), pp. 928-938; Welch, P.D., The use of fast fourier transform for the estimation of power spectra: A method based on time averaging over short, modified periodograms (1967) Transactions on Audio and Electroacoustics, 15 (2), pp. 70-73},
sponsors={Chalmers University of Technology; et al.; IT-University of Gothenburg; Tobii Pro; University of Gothenburg; Visage Technologies},
publisher={Association for Computing Machinery},
isbn={9781450347631},
language={English},
abbrev_source_title={ACM Int. Conf. Proc. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Diener2016888,
author={Diener, L. and Herff, C. and Janke, M. and Schultz, T.},
title={An initial investigation into the real-time conversion of facial surface EMG signals to audible speech},
journal={Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
year={2016},
volume={2016-October},
pages={888-891},
doi={10.1109/EMBC.2016.7590843},
art_number={7590843},
note={cited By 7; Conference of 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2016 ; Conference Date: 16 August 2016 Through 20 August 2016;  Conference Code:124354},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009110524&doi=10.1109%2fEMBC.2016.7590843&partnerID=40&md5=5a9a67e3599324bd7212bfcd32b0931f},
affiliation={Cognitive Systems Lab, University of Bremen, Bremen, Germany},
abstract={This paper presents early-stage results of our investigations into the direct conversion of facial surface electromyographic (EMG) signals into audible speech in a real-time setting, enabling novel avenues for research and system improvement through real-time feedback. The system uses a pipeline approach to enable online acquisition of EMG data, extraction of EMG features, mapping of EMG features to audio features, synthesis of audio waveforms from audio features and output of the audio waveforms via speakers or headphones. Our system allows for performing EMG-to-Speech conversion with low latency and on a continuous stream of EMG data, enabling near instantaneous audio output during audible as well as silent speech production. In this paper, we present an analysis of our systems components for latency incurred, as well as the tradeoffs between conversion quality, latency and training duration required. © 2016 IEEE.},
keywords={electromyography;  human;  physiology;  procedures;  signal processing;  speech, Electromyography;  Humans;  Signal Processing, Computer-Assisted;  Speech},
references={Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J.M., Brumberg, J.S., Silent speech interfaces (2010) Speech Communication, 52 (4), pp. 270-287; Hueber, T., Benaroya, E.-L., Chollet, G., Denby, B., Dreyfus, G., Stone, M., Development of a silent speech interface driven by ultrasound and optical images of the tongue and lips (2010) Speech Communication, 52 (4), pp. 288-300; Gonzalez, J.A., Cheah, L.A., Gilbert, J.M., Bai, J., Ell, S.R., Green, P.D., Moore, R.K., Direct speech generation for a silent speech interface based on permanent magnet articulography (2016) Proceedings of the 9th International Joint Conference on Biomedical Engineering Systems and Technologies, 4, pp. 96-105; Herff, C., Heger, D., De Pesters, A., Telaar, D., Brunner, P., Schalk, G., Schultz, T., Brain-to-text: Decoding spoken phrases from phone representations in the brain (2015) Frontiers in Neuroscience, (217); Toth, A.R., Wand, M., Schultz, T., Synthesizing speech from electromyography using voice transformation techniques (2009) Proceedings of the Annual Conference of the International Speech Communication Association, pp. 652-655; Janke, M., Wand, M., Schultz, T., Impact of lack of acoustic feedback in emg-based silent speech recognition (2010) 11th Annual Conference of the International Speech Communication Association; Wand, M., Schulte, C., Janke, M., Schultz, T., Array-based electromyographic silent speech interface (2013) International Conference on Bio-inspired Systems and Signal Processing, pp. 89-96; Jou, S.-C.S., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proceedings of the Annual Conference of the International Speech Communication Association, pp. 573-576; Diener, L., Janke, M., Schultz, T., Direct conversion from facial myoelectric signals to speech using deep neural networks (2015) International Joint Conference on Neural Networks; Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., Dropout: A simple way to prevent neural networks from overfitting (2014) The Journal of Machine Learning Research, 15 (1), pp. 1929-1958; Greff, K., Srivastava, R., (2015) Brainstorm, , https://github.com/IDSIA/brainstorm, IDSIA; Griffin, D.W., Lim, J.S., Signal estimation from modified shorttime fourier transform (1984) Acoustics, Speech and Signal Processing, IEEE Transactions on, 32 (2), pp. 236-243; Cavanagh, P., Komi, P., Electromechanical delay in human skeletal muscle under concentric and eccentric contractions (1979) European Journal of Applied Physiology and Occupational Physiology, 42 (3), pp. 159-163; Wand, M., Schulte, C., Janke, M., Schultz, T., Compensation of recording position shifts for a myoelectric silent speech recognizer (2014) The 39th International Conference on Acoustics, Speech, and Signal Processing, 2014, , iCASSP},
sponsors={},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={1557170X},
isbn={9781457702204},
pubmed_id={28268466},
language={English},
abbrev_source_title={Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Herff20161540,
author={Herff, C. and Johnson, G. and Diener, L. and Shih, J. and Krusienski, D. and Schultz, T.},
title={Towards direct speech synthesis from ECoG: A pilot study},
journal={Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
year={2016},
volume={2016-October},
pages={1540-1543},
doi={10.1109/EMBC.2016.7591004},
art_number={7591004},
note={cited By 13; Conference of 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2016 ; Conference Date: 16 August 2016 Through 20 August 2016;  Conference Code:124354},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009089427&doi=10.1109%2fEMBC.2016.7591004&partnerID=40&md5=55885cdbb5a069a0d024d52fcc1aea26},
affiliation={Cognitive Systems Lab, University of Bremen, Bremen, Germany; Advanced Signal Processing in Engineering and Neuroscience (ASPEN) Lab, Old Dominion UniversityVA, United States; Mayo Clinic, Jacksonville, FL, United States},
abstract={Most current Brain-Computer Interfaces (BCIs) achieve high information transfer rates using spelling paradigms based on stimulus-evoked potentials. Despite the success of this interfaces, this mode of communication can be cumbersome and unnatural. Direct synthesis of speech from neural activity represents a more natural mode of communication that would enable users to convey verbal messages in real-time. In this pilot study with one participant, we demonstrate that electrocoticography (ECoG) intracranial activity from temporal areas can be used to resynthesize speech in real-time. This is accomplished by reconstructing the audio magnitude spectrogram from neural activity and subsequently creating the audio waveform from these reconstructed spectrograms. We show that significant correlations between the original and reconstructed spectrograms and temporal waveforms can be achieved. While this pilot study uses audibly spoken speech for the models, it represents a first step towards speech synthesis from speech imagery. © 2016 IEEE.},
keywords={brain computer interface;  electroencephalography;  evoked response;  human;  pilot study;  speech, Brain-Computer Interfaces;  Electroencephalography;  Evoked Potentials;  Humans;  Pilot Projects;  Speech},
references={Donchin, E., Spencer, K.M., Wijesinghe, R., The mental prosthesis: Assessing the speed of a p300-based brain-computer interface (2000) Rehabilitation Engineering, IEEE Transactions on, 8 (2), pp. 174-179; Müller-Putz, G.R., Scherer, R., Brauneis, C., Pfurtscheller, G., Steady-state visual evoked potential (ssvep)-based communication: Impact of harmonic frequency components (2005) Journal of Neural Engineering, 2 (4), pp. 123-130; Bouchard, K.E., Mesgarani, N., Johnson, K., Chang, E.F., Functional organization of human sensorimotor cortex for speech articulation (2013) Nature, 495 (7441), pp. 327-332; Bouchard, K., Chang, E., Neural decoding of spoken vowels from human sensory-motor cortex with high-density electrocorticography (2014) Engineering in Medicine and Biology Society, 2014. EMBS 2014. 36th Annual International Conference of the IEEE. IEEE; Cheung, C., Hamiton, L.S., Johnson, K., Chang, E.F., The auditory representation of speech sounds in human motor cortex (2016) ELife, 5, p. e12577. , https://dx.doi.org/10.7554/eLife.12577, mar; Mugler, E., Goldrick, M., Slutzky, M., Cortical encoding of phonemic context during word production (2014) Engineering in Medicine and Biology Society, 2014. EMBS 2014. 36th Annual International Conference of the IEEE. IEEE; Mugler, E.M., Patton, J.L., Flint, R.D., Wright, Z.A., Schuele, S.U., Rosenow, J., Shih, J.J., Slutzky, M.W., Direct classification of all American english phonemes using signals from functional speech motor cortex (2014) Journal of Neural Engineering, 11 (3), p. 035015; Herff, C., Heger, D., De Pesters, A., Telaar, D., Brunner, P., Schalk, G., Schultz, T., Brain-to-text: Decoding spoken phrases from phone representations in the brain (2015) Frontiers in Neuroscience, 9 (217); Heger, D., Herff, C., Pesters A, D., Telaar, D., Brunner, P., Schalk, G., Schultz, T., Continuous speech recognition from ecog (2015) Sixteenth Annual Conference of the International Speech Communication Association; Pasley, B.N., David, S.V., Mesgarani, N., Flinker, A., Shamma, S.A., Crone, N.E., Knight, R.T., Chang, E.F., Reconstructing speech from human auditory cortex (2012) PLoS Biology, 10 (1), p. e1001251; Martin, S., Brunner, P., Holdgraf, C., Heinze, H.-J., Crone, N.E., Rieger, J., Schalk, G., Pasley, B., Decoding spectrotemporal features of overt and covert speech from the human cortex (2014) Frontiers in Neuroengineering, 7 (14); Schalk, G., McFarland, D.J., Hinterberger, T., Birbaumer, N., Wolpaw, J.R., Bci2000: A general-purpose brain-computer interface (bci) system (2004) Biomedical Engineering, IEEE Transactions on, 51 (6), pp. 1034-1043; Ieee recommended practice for speech quality measurements (1969) IEEE Transactions on Audio and Electroacoustics, 17, pp. 225-246. , IEEE; Leuthardt, E.C., Pei, X.-M., Breshears, J., Gaona, C., Sharma, M., Freudenberg, Z., Barbour, D., Schalk, G., Temporal evolution of gamma activity in human cortex during an overt and covert word repetition task (2011) Frontiers in Human Neuroscience, 6, p. 99; Miller, K.J., Leuthardt, E.C., Schalk, G., Rao, R.P., Anderson, N.R., Moran, D.W., Miller, J.W., Ojemann, J.G., Spectral changes in cortical surface potentials during motor movement (2007) The Journal of Neuroscience, 27 (9), pp. 2424-2432; Tibshirani, R., Regression shrinkage and selection via the lasso (1996) Journal of the Royal Statistical Society. Series B (Methodological), pp. 267-288; Diener, L., Janke, M., Schultz, T., Direct conversion from facial myoelectric signals to speech using deep neural networks (2015) Neural Networks (IJCNN), 2015 International Joint Conference On. IEEE, pp. 1-7; Griffin, D.W., Lim, J.S., Signal estimation from modified shorttime fourier transform (1984) Acoustics, Speech and Signal Processing, IEEE Transactions on, 32 (2), pp. 236-243; Haufe, S., Meinecke, F., Görgen, K., Dähne, S., Haynes, J.-D., Blankertz, B., Bießmann, F., On the interpretation of weight vectors of linear models in multivariate neuroimaging (2014) Neuroimage, 87, pp. 96-110; Kubanek, J., Schalk, G., Neuralact: A tool to visualize electrocortical (ecog) activity on a three-dimensional model of the cortex (2014) Neuroinformatics, pp. 1-8},
sponsors={},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={1557170X},
isbn={9781457702204},
pubmed_id={28268620},
language={English},
abbrev_source_title={Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Herff2016,
author={Herff, C. and Schultz, T.},
title={Automatic speech recognition from neural signals: A focused review},
journal={Frontiers in Neuroscience},
year={2016},
volume={10},
number={SEP},
doi={10.3389/fnins.2016.00429},
art_number={429},
note={cited By 26},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992123219&doi=10.3389%2ffnins.2016.00429&partnerID=40&md5=c656fa9d4241de75107617a844449b9d},
affiliation={Cognitive Systems Lab, Department for Mathematics and Computer Science, University of Bremen, Bremen, Germany},
abstract={Speech interfaces have become widely accepted and are nowadays integrated in various real-life applications and devices. They have become a part of our daily life. However, speech interfaces presume the ability to produce intelligible speech, which might be impossible due to either loud environments, bothering bystanders or incapabilities to produce speech (i.e., patients suffering from locked-in syndrome). For these reasons it would be highly desirable to not speak but to simply envision oneself to say words or sentences. Interfaces based on imagined speech would enable fast and natural communication without the need for audible speech and would give a voice to otherwise mute people. This focused review analyzes the potential of different brain imaging techniques to recognize speech from neural signals by applying Automatic Speech Recognition technology. We argue that modalities based on metabolic processes, such as functional Near Infrared Spectroscopy and functional Magnetic Resonance Imaging, are less suited for Automatic Speech Recognition from neural signals due to low temporal resolution but are very useful for the investigation of the underlying neural mechanisms involved in speech processes. In contrast, electrophysiologic activity is fast enough to capture speech processes and is therefor better suited for ASR. Our experimental results indicate the potential of these signals for speech recognition from neural data with a focus on invasively measured brain activity (electrocorticography). As a first example of Automatic Speech Recognition techniques used from neural signals, we discuss the Brain-to-text system. ï¿½ 2016 Herff and Schultz.},
author_keywords={ASR;  Automatic speech recognition;  BCI;  Brain-computer interface;  ECoG;  EEG;  FNIRS;  Speech},
keywords={automatic speech recognition;  brain metabolism;  electrocorticography;  electroencephalography;  functional magnetic resonance imaging;  functional neuroimaging;  magnetoencephalography;  microarray analysis;  near infrared spectroscopy;  nerve cell network;  Review;  signal transduction},
references={Bouchard, K., Chang, E., "Neural decoding of spoken vowels from human sensory-motor cortex with high-density electrocorticography," (2014) Engineering in Medicine and Biology Society, 2014. EMBS 2014. 36th Annual International Conference of the IEEE, , (Chicago, IL: IEEE); Brumberg, J.S., Nieto-Castanon, A., Kennedy, P.R., Guenther, F.H., Brain-computer interfaces for speech communication (2010) Speech Commun, 52, pp. 367-379; Brumberg, J.S., Wright, E.J., Andreasen, D.S., Guenther, F.H., Kennedy, P.R., Classification of intended phoneme production from chronic intracortical microelectrode recordings in speech-motor cortex (2011) Front. Neurosci, 5, p. 65; Chakrabarti, S., Sandberg, H.M., Brumberg, J.S., Krusienski, D.J., Progress in speech decoding from the electrocorticogram (2015) Biomed. Eng. Lett, 5, pp. 10-21; Chang, E.F., Rieger, J.W., Johnson, K., Berger, M.S., Barbaro, N.M., Knight, R.T., Categorical speech representation in human superior temporal gyrus (2010) Nat. Neurosci, 13, pp. 1428-1432; Cheung, C., Hamiton, L.S., Johnson, K., Chang, E.F., The auditory representation of speech sounds in human motor cortex (2016) eLife, 5; Coyle, S.M., Ward, T.E., Markham, C.M., Brain-computer interface using a simplified functional near-infrared spectroscopy system (2007) J. Neural Eng, 4, p. 219; Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J.M., Brumberg, J.S., Silent speech interfaces (2010) Speech Commun, 52, pp. 270-287; Di Liberto, G.M., O'sullivan, J.A., Lalor, E.C., Low-frequency cortical entrainment to speech reflects phoneme-level processing (2015) Curr. Biol, 25, pp. 2457-2465; Donchin, E., Spencer, K.M., Wijesinghe, R., The mental prosthesis: assessing the speed of a p300-based brain-computer interface (2000) IEEE Trans. Rehabil. Eng, 8, pp. 174-179; Eklund, A., Nichols, T.E., Knutsson, H., Cluster failure: why fMRI inferences for spatial extent have inflated false-positive rates (2016) Proc. Natl. Acad. Sci. U.S.A, 113, pp. 7900-7905; Farwell, L.A., Donchin, E., Talking off the top of your head: toward a mental prosthesis utilizing event-related brain potentials (1988) Electroencephalogr. Clin. Neurophysiol, 70, pp. 510-523; Formisano, E., De Martino, F., Bonte, M., Goebel, R., "who" is saying "what"?. brain-based decoding of human voice and speech (2008) Science, 322, pp. 970-973; Guenther, F.H., Brumberg, J.S., Wright, E.J., Nieto-Castanon, A., Tourville, J.A., Panko, M., A wireless brain-machine interface for real-time speech synthesis (2009) PLoS ONE, 4; Guimaraes, M.P., Wong, D.K., Uy, E.T., Grosenick, L., Suppes, P., Single-trial classification of meg recordings (2007) IEEE Trans. Biomed. Eng, 54, pp. 436-443; Heger, D., Herff, C., Pesters, A.D., Telaar, D., Brunner, P., Schalk, G., 'Continuous speech recognition from ECOG,' (2015) Sixteenth Annual Conference of the International Speech Communication Association, , (Dresden); Heger, D., Herff, C., Putze, F., Mutter, R., Schultz, T., Continuous affective states recognition using functional near infrared spectroscopy (2014) Brain Comput. Interf, 1, pp. 113-125; Heger, D., Mutter, R., Herff, C., Putze, F., Schultz, T., 'Continuous recognition of affective states by functional near infrared spectroscopy signals,' (2013) Affective Computing and Intelligent Interaction, pp. 832-837. , (ACII) 2013 Humaine Association Conference on (Geneva); Heinks-Maldonado, T.H., Nagarajan, S.S., Houde, J.F., Magnetoencephalographic evidence for a precise forward model in speech production (2006) Neuroreport, 17, p. 1375; Hennrich, J., Herff, C., Heger, D., Schultz, T., 'Investigating deep learning for fnirs based BCI,' (2015) Engineering in Medicine and Biology Society, , (EMBC) 2015 37th Annual International Conference of the IEEE (Milan); Herff, C., Fortmann, O., Tse, C.-Y., Cheng, X., Putze, F., Heger, D., 'Hybrid fnirs-EEG based discrimination of 5 levels of memory load,' (2015) Neural Engineering, pp. 5-8. , (NER) 2015 7th International IEEE/EMBS Conference on (Montpellier); Herff, C., Heger, D., de Pesters, A., Telaar, D., Brunner, P., Schalk, G., Brain-to-text: decoding spoken phrases from phone representations in the brain (2015) Front. Neurosci, 9, p. 217; Herff, C., Heger, D., Putze, F., Guan, C., Schultz, T., 'Cross-subject classification of speaking modes using fnirs,' (2012) Neural Information Processing, of Lecture Notes in Computer Science, 7664, pp. 417-424. , eds T. Huang, Z. Zeng, C. Li, and C. Leung (Berlin; Heidelberg: Springer); Herff, C., Heger, D., Putze, F., Guan, C., Schultz, T., 'Self-paced bci with nirs based on speech activity,' (2013) International BCI Meeting 2013, Asilomar, , (Pacific Grove, CA); Herff, C., Heger, D., Putze, F., Hennrich, J., Fortmann, O., Schultz, T., 'Classification of mental tasks in the prefrontal cortex using fnirs,' (2013) Engineering in Medicine and Biology Society, pp. 2160-2163. , (EMBC), 2013 35th Annual International Conference of the IEEE (Osaka); Herff, C., Janke, M., Wand, M., Schultz, T., "Impact of different feedback mechanisms in emg-based speech recognition," (2011) 12th Annual Conference of the International Speech Communication Association (Florence), , Interspeech 2011; Herff, C., Putze, F., Heger, D., Guan, C., Schultz, T., 'Speaking mode recognition from functional near infrared spectroscopy,' (2012) Engineering in Medicine and Biology Society, pp. 1715-1718. , (EMBC), 2012 Annual International Conference of the IEEE (San Diego, CA); Houde, J.F., Nagarajan, S.S., Sekihara, K., Merzenich, M.M., Modulation of the auditory cortex during speech: an MEG study (2002) J. Cogn. Neurosci, 14, pp. 1125-1138; Jelinek, F., (1997) Statistical Methods for Speech Recognition, , Cambridge, MA: MIT Press; Kellis, S., Miller, K., Thomson, K., Brown, R., House, P., Greger, B., Decoding spoken words using local field potentials recorded from the cortical surface (2010) J. Neural Eng, 7; Leuthardt, E.C., Pei, X.-M., Breshears, J., Gaona, C., Sharma, M., Freudenberg, Z., Temporal evolution of gamma activity in human cortex during an overt and covert word repetition task (2011) Front. Hum. Neurosci, 6, p. 99; Lotte, F., Brumberg, J.S., Brunner, P., Gunduz, A., Ritaccio, A.L., Guan, C., Schalk, G., Electrocorticographic representations of segmental features in continuous speech (2015) Front. Hum. Neurosci, 9, p. 97; Martin, S., Brunner, P., Holdgraf, C., Heinze, H.-J., Crone, N.E., Rieger, J., Decoding spectrotemporal features of overt and covert speech from the human cortex (2014) Front. Neuroeng, 7, p. 14; McFarland, D.J., Miner, L.A., Vaughan, T.M., Wolpaw, J.R., Mu and beta rhythm topographies during motor imagery and actual movements (2000) Brain Topogr, 12, pp. 177-186; Mesgarani, N., Cheung, C., Johnson, K., Chang, E.F., Phonetic feature encoding in human superior temporal gyrus (2014) Science, 343, pp. 1006-1010; Miller, K.J., Leuthardt, E.C., Schalk, G., Rao, R.P., Anderson, N.R., Moran, D.W., Spectral changes in cortical surface potentials during motor movement (2007) J. Neurosci, 27, pp. 2424-2432; Mugler, E.M., Patton, J.L., Flint, R.D., Wright, Z.A., Schuele, S.U., Rosenow, J., Direct classification of all american english phonemes using signals from functional speech motor cortex (2014) J. Neural Eng, 11; Mï¿½ller-Putz, G.R., Scherer, R., Brauneis, C., Pfurtscheller, G., Steady-state visual evoked potential (ssvep)-based communication: impact of harmonic frequency components (2005) J. Neural Eng, 2, pp. 123-130; O'sullivan, J.A., Power, A.J., Mesgarani, N., Rajaram, S., Foxe, J.J., Shinn-Cunningham, B.G., Attentional selection in a cocktail party environment can be decoded from single-trial EEG (2015) Cereb. Cortex, 25, pp. 1697-1706; Pasley, B.N., David, S.V., Mesgarani, N., Flinker, A., Shamma, S.A., Crone, N.E., Reconstructing speech from human auditory cortex (2012) PLoS Biol, 10; Pei, X., Leuthardt, E.C., Gaona, C.M., Brunner, P., Wolpaw, J.R., Schalk, G., Spatiotemporal dynamics of electrocorticographic high gamma activity during overt and covert word repetition (2011) Neuroimage, 54, pp. 2960-2972; Potes, C., Gunduz, A., Brunner, P., Schalk, G., Dynamics of electrocorticographic (ecog) activity in human temporal and frontal cortical areas during music listening (2012) NeuroImage, 61, pp. 841-848; Price, C.J., A review and synthesis of the first 20 years of pet and fmri studies of heard speech, spoken language and reading (2012) Neuroimage, 62, pp. 816-847; Schalk, G., McFarland, D.J., Hinterberger, T., Birbaumer, N., Wolpaw, J.R., Bci2000: a general-purpose brain-computer interface (bci) system (2004) IEEE Trans. Biomed. Eng, 51, pp. 1034-1043; Schultz, T., Wand, M., Modeling coarticulation in EMG-based continuous speech recognition (2010) Speech Commun, 52, pp. 341-353; Sitaram, R., Zhang, H., Guan, C., Thulasidas, M., Hoshi, Y., Ishikawa, A., Temporal classification of multichannel near-infrared spectroscopy signals of motor imagery for developing a brain-computer interface (2007) NeuroImage, 34, pp. 1416-1427; Stolcke, A., 'SriLM-an extensible extensible language modeling toolkit,' (2002) Proceedings of the 7th International Conference on Spoken Language Processing, , (ICSLP 2002) (Denver, CO); Sutter, E.E., The brain response interface: communication through visually-induced electrical brain responses (1992) J. Microcomput. Appl, 15, pp. 31-45; Talavage, T.M., Gonzalez-Castillo, J., Scott, S.K., Auditory neuroimaging with fMRI and pet (2014) Hear. Res, 307, pp. 4-15; Telaar, D., Wand, M., Gehrig, D., Putze, F., Amma, C., Heger, D., 'BioKIT-real-time decoder for biosignal processing,' (2014) The 15th Annual Conference of the International Speech Communication Association, , (Interspeech 2014) (Singapore); Tervaniemi, M.A., Kujala, A., Alho, K., Virtanen, J., Ilmoniemi, R., Nï¿½ï¿½tï¿½nen, R., Functional specialization of the human auditory cortex in processing phonetic and musical sounds: a magnetoencephalographic (MEG) study (1999) Neuroimage, 9, pp. 330-336; Vaughan, T.M., McFarland, D.J., Schalk, G., Sarnacki, W.A., Krusienski, D.J., Sellers, E.W., The wadsworth BCI research and development program: at home with BCI (2006) IEEE Trans. Neural Syst. Rehabil. Eng, 14, pp. 229-233; Wolpaw, J.R., Birbaumer, N., McFarland, D.J., Pfurtscheller, G., Vaughan, T.M., Brain-computer interfaces for communication and control (2002) Clin. Neurophysiol, 113, pp. 767-791; Yoshimura, N., Nishimoto, A., Belkacem, A.N., Shin, D., Kambara, H., Hanakawa, T., Koike, Y., Decoding of covert vowel articulation using electroencephalography cortical currents (2016) Front. Neurosci, 10, p. 175},
correspondence_address1={Herff, C.; Cognitive Systems Lab, Department for Mathematics and Computer Science, University of BremenGermany; email: christian.herff@uni-bremen.de},
publisher={Frontiers Media S.A.},
issn={16624548},
language={English},
abbrev_source_title={Front. Neurosci.},
document_type={Review},
source={Scopus},
}

@ARTICLE{Thürer2016172,
author={Thürer, B. and Stockinger, C. and Focke, A. and Putze, F. and Schultz, T. and Stein, T.},
title={Increased gamma band power during movement planning coincides with motor memory retrieval},
journal={NeuroImage},
year={2016},
volume={125},
pages={172-181},
doi={10.1016/j.neuroimage.2015.10.008},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945950195&doi=10.1016%2fj.neuroimage.2015.10.008&partnerID=40&md5=2f8b2a6571760d13e3134746590321cf},
affiliation={YIG Computational Motor Control and Learning, BioMotion Center, Institute of Sports and Sports Science, Karlsruhe Institute of Technology, Engler-Bunte-Ring 15, Karlsruhe, 76131, Germany; Cognitive Systems Lab, Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Adenauerring 4, Karlsruhe, 76131, Germany},
abstract={The retrieval of motor memory requires a previous memory encoding and subsequent consolidation of the specific motor memory. Previous work showed that motor memory seems to rely on different memory components (e.g., implicit, explicit). However, it is still unknown if explicit components contribute to the retrieval of motor memories formed by dynamic adaptation tasks and which neural correlates are linked to memory retrieval. We investigated the lower and higher gamma bands of subjects' electroencephalography during encoding and retrieval of a dynamic adaptation task. A total of 24 subjects were randomly assigned to a treatment and control group. Both groups adapted to a force field A on day 1 and were re-exposed to the same force field A on day 3 of the experiment. On day 2, treatment group learned an interfering force field B whereas control group had a day rest. Kinematic analyses showed that control group improved their initial motor performance from day 1 to day 3 but treatment group did not. This behavioral result coincided with an increased higher gamma band power in the electrodes over prefrontal areas on the initial trials of day 3 for control but not treatment group. Intriguingly, this effect vanished with the subsequent re-adaptation on day 3. We suggest that improved re-test performance in a dynamic motor adaptation task is contributed by explicit memory and that gamma bands in the electrodes over the prefrontal cortex are linked to these explicit components. Furthermore, we suggest that the contribution of explicit memory vanishes with the subsequent re-adaptation while task automaticity increases. © 2015 Elsevier Inc.},
author_keywords={Consolidation;  Electroencephalography (EEG);  Explicit memory;  Force field;  Reaching movement;  Sensorimotor learning},
keywords={adaptation;  Article;  controlled study;  electrode;  electroencephalography;  explicit memory;  gamma band power;  human;  implicit memory;  kinematics;  male;  memory;  memory consolidation;  motor performance;  movement (physiology);  neuroimaging;  prefrontal cortex;  priority journal;  randomized controlled trial;  task performance;  brain mapping;  learning;  memory;  movement (physiology);  physiology;  young adult, Brain Mapping;  Electroencephalography;  Humans;  Learning;  Male;  Memory;  Memory Consolidation;  Movement;  Prefrontal Cortex;  Young Adult},
funding_details={Karlsruher Institut fÃ¼r TechnologieKarlsruher Institut fÃ¼r Technologie, KIT},
funding_text 1={The Young Investigator Group (YIG) “Computational Motor Control and Learning” received financial support from the “Concept for the Future” of Karlsruhe Institute of Technology within the framework of the German Excellence Initiative.},
references={Ball, T., Demandt, E., Mutschler, I., Neitzel, E., Mehring, C., Vogt, K., Aertsen, A., Schulze-Bonhage, A., Movement related activity in the high gamma range of the human EEG (2008) NeuroImage, 41, pp. 302-310; Bartenbach, V., Sander, C., Pöschl, M., Wilging, K., Nelius, T., Doll, F., Burger, W., Stein, T., The BioMotionBot: a robotic device for applications in human motor learning and rehabilitation (2013) J. Neurosci. Methods, 213, pp. 282-297; Benjamini, Y., Hochberg, Y., Controlling the false discovery rate: a practical and powerful approach to multiple testing (1995) J. R. Stat. Soc. Ser. B, 57, pp. 289-300; Bonini, F., Burle, B., Liégeois-Chauvel, C., Régis, J., Chauvel, P., Vidal, F., (2014) Science, 343, pp. 888-891; Brashers-Krug, T., Shadmehr, R., Bizzi, E., Consolidation in human motor memory (1996) Nature, 382, pp. 252-255; Caithness, G., Osu, R., Bays, P., Chase, H., Klassen, J., Kawato, M., Wolpert, D.M., Flanagan, J.R., Failure to consolidate the consolidation theory of learning for sensorimotor adaptation tasks (2004) J. Neurosci., 24, pp. 8662-8671; Canolty, R.T., Edwards, E., Dalal, S.S., Soltani, M., Nagarajan, S.S., Kirsch, H.E., Berger, M.S., Knight, R.T., High gamma power is phase-locked to theta oscillations in human neocortex (2006) Science, 313, pp. 1626-1628; Cohen, J., (1988) Statistical Power Analysis for the Behavioral Sciences, , Erlbaum, Hillsdale; Cohen, M.X., (2014) Analyzing Neural Time Series Data: Theory and Practice, , MIT Press, Cambridge; Croft, R.J., Barry, R.J., Removal of ocular artifact from the EEG: a review (2000) Neurophysiol. Clin., 30, pp. 5-19; Crone, N.E., Korzeniewska, A., Franaszczuk, P.J., Cortical gamma responses: searching high and low (2011) Int. J. Psychophysiol., 79, pp. 9-15; Darvas, F., Scherer, R., Ojemann, J.G., Rao, R.P., Miller, K.J., Sorensen, L.B., High gamma mapping using EEG (2010) NeuroImage, 49, pp. 930-938; Delorme, A., Makeig, S., EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics (2004) J. Neurosci. Methods, 134, pp. 9-21; Diedrichsen, J., Hashambhoy, Y., Rane, T., Shadmehr, R., Neural correlates of reach errors (2005) J. Neurosci., 25, pp. 9919-9931; Drosopoulos, S., Harrer, D., Born, J., Sleep and awareness about presence of regularity speed the transition from implicit to explicit knowledge (2011) Biol. Psychol., 86, pp. 168-173; Edwards, E., Soltani, M., Deouell, L.Y., Berger, M.S., Knight, R.T., High gamma activity in response to deviant auditory stimuli recorded directly from human cortex (2005) J. Neurophysiol., 94, pp. 4269-4280; Fernandez-Ruiz, J., Wong, W., Armstrong, I.T., Flanagan, J.R., Relation between reaction time and reach errors during visuomotor adaptation (2011) Behav. Brain Res., 219, pp. 8-14; Fischer, S., Drosopoulos, S., Tsen, J., Born, J., Implicit learning-explicit knowing: a role for sleep in memory system interaction (2006) J. Cogn. Neurosci., 18, pp. 311-319; Floyer-Lea, A., Matthews, P.M., Changing brain networks for visuomotor control with increased movement automaticity (2004) J. Neurophysiol., 92, pp. 2405-2412; Focke, A., Stockinger, C., Diepold, C., Taubert, M., Stein, T., The influence of catch trials on the consolidation of motor memory in force field adaptation tasks (2013) Front. Psychol., 4, p. 479; Fraker, M.E., Peacor, S.D., Statistical tests for biological interactions: a comparison of permutation tests and analysis of variance (2008) Acta Oecol., 33, pp. 66-72; Gentili, R.J., Bradberry, T.J., Oh, H., Hatfield, B.D., Contreras-Vidal, J.L., Cerebral cortical dynamics during visuomotor transformation: adaptation to a cognitive-motor executive challenge (2011) Psychophysiology, 48, pp. 813-824; Gentili, R.J., Bradberry, T.J., Oh, H., Costanzo, M.E., Kerick, S.E., Contreras-Vidal, J.L., Hadfield, B.D., Evolution of cerebral cortico-cortical communication during visuomotor adaptation to a cognitive-motor executive challenge (2015) Biol. Psychol., 105, pp. 51-65; Goncharova, I.I., McFarland, D.J., Vaughan, T.M., Wolpaw, J.R., EMG contamination of EEG: spectral and topographical characteristics (2003) Clin. Neurophysiol., 114, pp. 1580-1593; Gonzalez Castro, L.N., Hadjiosif, A.M., Hemphill, M.A., Smith, M.A., Environmental consistency determines the rate of motor adaptation (2014) Curr. Biol., 24, pp. 1050-1061; Gonzalez, S.L., Grave de Peralta, R., Thut, G., Millán, J.D.R., Morier, P., Landis, T., Very high frequency oscillations (VHFO) as a predictor of movement intentions (2006) NeuroImage, 32, pp. 170-179; Haith, A.M., Huberdeau, D.M., Krakauer, J.W., The influence of movement preparation time on the expression of visuomotor learning and savings (2015) J. Neurosci., 35, pp. 5109-5117; Hallett, M., The role of the motor cortex in motor learning (2006) Motor Control and Learning, pp. 89-96. , Springer, New York, M.L. Latash, F. Lestienne (Eds.); Howard, M.W., Rizzuto, D.S., Caplan, J.B., Madsen, J.R., Lisma, J., Aschenbrenner-Scheibe, R., Schulze-Bonhage, A., Kahana, M.J., Gamma oscillations correlate with working memory load in humans (2003) Cereb. Cortex, 13, pp. 1369-1374; Huberdeau, D.M., Krakauer, J.W., Haith, A.M., Dual-process decomposition in human sensorimotor adaptation (2015) Curr. Opin. Neurobiol., 33, pp. 71-77; Izawa, J., Rane, T., Donchin, O., Shadmehr, R., Motor adaptation as a process of reoptimization (2008) J. Neurosci., 28, pp. 2883-2891; Krakauer, J.W., Mazzoni, P., Human sensorimotor learning: adaptation, skill, and beyond (2011) Curr. Opin. Neurobiol., 21, pp. 636-644; Krakauer, J.W., Ghilardi, M.F., Ghez, C., Independent learning of internal models for kinematic and dynamic control of reaching (1999) Nat. Neurosci., 2, pp. 1026-1031; Kranczioch, C., Athanassiou, S., Shen, S., Gao, G., Sterr, A., Short-term learning of a visually guided power-grip task is associated with dynamic changes in EEG oscillatory activity (2008) Clin. Neurophysiol., 119, pp. 1419-1430; Krebs, H.I., Brashers-Krug, T., Rauch, S.L., Savage, C.R., Hogan, N., Rubin, R.H., Fischman, A.J., Alpert, N.M., Robot-aided functional imaging: application to a motor learning study (1998) Hum. Brain Mapp., 6, pp. 59-72; Makeig, S., Bell, A.J., Jung, T.P., Sejnowski, T.J., Independent component analysis of electroencephalographic data (1996) Advances in Neural Information Processing Systems, pp. 145-151. , MIT Press, Cambridge, D. Touretzky, M. Mozer, M. Hasselmo (Eds.); Miller, E.K., Cohen, J.D., An integrative theory of prefrontal cortex function (2001) Annu. Rev. Neurosci., 24, pp. 167-202; Mitra, P.P., Pesaran, B., Analysis of dynamic brain imaging data (1999) Biophys. J., 76, pp. 691-708; Nezafat, R., Shadmehr, R., Holcomb, H.H., Long-term adaptation to dynamics of reaching movements: a PET study (2001) Exp. Brain Res., 140, pp. 66-76; Novakovic, V., Sanguineti, V., Adaptation to constant-magnitude assistive forces: kinematic and neural correlates (2011) Exp. Brain Res., 209, pp. 425-436; Oldfield, R.C., The assessment and analysis of handedness: the Edinburgh inventory (1971) Neuropsychologia, 9, pp. 97-113; Perfetti, B., Moisello, C., Landsness, E.C., Kvint, S., Lanzafame, S., Onofri, M., Di Rocco, A., Ghilardi, M.F., Modulation of gamma and theta spectral amplitude and phase synchronization is associated with the development of visuo-motor learning (2011) J. Neurosci., 31, pp. 14810-14819; Pfurtscheller, G., Lopes da Silva, F.H., Event-related EEG/MEG synchronization and desynchronization: basic principles (1999) Clin. Neurophysiol., 110, pp. 1842-1857; Rasch, B., Born, J., About sleep's role in memory (2013) Physiol. Rev., 93, pp. 681-766; Richardson, J.T.E., Eta squared and partial eta squared as measures of effect size in educational research (2011) Educ. Res. Rev., 6, pp. 135-147; Robertson, E.M., From creation to consolidation: a novel framework for memory-processing (2009) PLoS Biol., 7; Robertson, E.M., Pascual-Leone, A., Miall, R.C., Current concepts in procedural consolidation (2004) Nat. Rev. Neurosci., 5, pp. 576-582; Roux, F., Uhlhaas, P.J., Working memory and neural oscillations: alpha-gamma versus theta-gamma codes for distinct WM information? (2014) Trends Cogn. Sci., 18, pp. 16-25; Sauseng, P., Gerloff, C., Hummel, F.C., Two brakes are better than one: the neural bases of inhibitory control of motor memory traces (2013) NeuroImage, 65, pp. 52-58; Schlögl, A., Keinrath, C., Zimmermann, D., Scherer, R., Leeb, R., Pfurtscheller, G., A fully automated correction method of EOG artifacts in EEG recordings (2007) Clin. Neurophysiol., 118, pp. 98-104; Sederberg, P.B., Kahana, M.J., Howard, M.W., Donner, E.J., Madsen, J.R., Theta and gamma oscillations during encoding predict subsequent recall (2003) J. Neurosci., 23, pp. 10809-10814; Shadmehr, R., Holcomb, H.H., Neural correlates of motor memory consolidation (1997) Science, 277, pp. 821-825; Shadmehr, R., Holcomb, H.H., Inhibitory control of competing motor memories (1999) Exp. Brain Res., 126, pp. 235-251; Shadmehr, R., Moussavi, Z.M., Spatial generalization from learning dynamics of reaching movements (2000) J. Neurosci., 20, pp. 7807-7815; Shadmehr, R., Mussa-Ivaldi, F.A., Adaptive representation of dynamics during learning of a motor task (1994) J. Neurosci., 14, pp. 3208-3224; Shadmehr, R., Brandt, J., Corkin, S., Time-dependent motor memory processes in amnesic subjects (1998) J. Neurophysiol., 80, pp. 1590-1597; Stockinger, C., Pöschl, M., Focke, A., Stein, T., ManipAnalysis - a software application for the analysis of force field experiments (2012) Int. J. Comput. Sci. Sport, 11, pp. 52-57; Stockinger, C., Focke, A., Stein, T., Catch trials in force field learning influence adaptation and consolidation of human motor memory (2014) Front. Hum. Neurosci., 8, p. 231; Studer, B., Koeneke, S., Blum, J., Jäncke, L., The effects of practice distribution upon the regional oscillatory activity in visuomotor learning (2010) Behav. Brain Funct., 6, p. 8; Tan, H., Jenkinson, N., Brown, P., Dynamic neural correlates of motor error monitoring and adaptation during trial-to-trial learning (2014) J. Neurosci., 34, pp. 5678-5688; Taylor, J.A., Ivry, R.B., Cerebellar and prefrontal cortex contributions to adaptation, strategies, and reinforcement learning (2014) Prog. Brain Res., 2010, pp. 217-253; Tombini, M., Zappasodi, F., Zollo, L., Pellegrino, G., Cavallo, G., Tecchio, F., Guglielmelli, E., Rossini, P.M., Brain activity preceding a 2D manual catching task (2009) NeuroImage, 47, pp. 1735-1746; Wolpert, D.M., Diedrichsen, J., Flanagan, J.R., Principles of sensorimotor learning (2011) Nat. Rev. Neurosci., 12, pp. 739-751},
correspondence_address1={Thürer, B.; BioMotion Center, Institute of Sports and Sports Science, Karlsruhe Institute of Technology, Engler-Bunte-Ring 15 76131, Germany},
publisher={Academic Press Inc.},
issn={10538119},
coden={NEIME},
pubmed_id={26458517},
language={English},
abbrev_source_title={NeuroImage},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Putze2016935,
author={Putze, F. and Schultz, T. and Propper, R.},
title={Dummy Model Based Workload Modeling},
journal={Proceedings - 2015 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2015},
year={2016},
pages={935-940},
doi={10.1109/SMC.2015.171},
art_number={7379303},
note={cited By 3; Conference of IEEE International Conference on Systems, Man, and Cybernetics, SMC 2015 ; Conference Date: 9 October 2015 Through 12 October 2015;  Conference Code:119045},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964434503&doi=10.1109%2fSMC.2015.171&partnerID=40&md5=ad10c0c589a213bbb122d431ea7779b4},
affiliation={Karlsruhe Institute of Technology, Cognitive Systems Lab, Karlsruhe, Germany; Technical University of Berlin, Neural Information Processing Group, Berlin, Germany},
abstract={In this paper, we show how a model of human cognition based on ACT-R can be improved to accurately predict cognitive performance under different workload levels. For this purpose, we propose a novel approach which uses an EEG-based workload model to (de-)activate a dummy model which runs in parallel to the actual task model. The dummy model consumes cognitive resources to reflect the effect of workload on behavior and performance. We evaluate the approach in two user studies with different tasks and show a significant reduction of prediction error. © 2015 IEEE.},
author_keywords={Adaptation;  Cognitive model;  EEG;  Workload},
keywords={Cybernetics;  Electroencephalography, Adaptation;  Cognitive model;  Cognitive performance;  Cognitive resources;  Human cognition;  Prediction errors;  Work-load models;  Workload, Cognitive systems},
references={Anderson, J.R., Bothell, D., Byrne, M.D., Douglass, S., Lebiere, C., Qin, Y., An integrated theory of the mind (2004) Psychological Review, 111 (4), pp. 1036-1060; Amant, R.S., Horton, T.E., Ritter, F.E., Model-based evaluation of expert cell phone menu interaction (2007) Transactions on Computer-Human Interaction, 14 (1); Chikhaoui, B., Pigot, H., Towards analytical evaluation of human machine interfaces developed in the context of smart homes (2010) Interacting with Computers, 22 (6), pp. 449-464; Salvucci, D.D., Zuber, M., Beregovaia, E., Markley, D., Distract-R: Rapid prototyping and evaluation of in-vehicle interfaces (2005) Proceedings of the Conference on Human Factors in Computing Systems, pp. 581-589. , New York, USA; Salvucci, D.D., Modeling driver behavior in a cognitive architecture (2006) Human Factors: The Journal of the Human Factors and Ergonomics Society, 48 (2), pp. 362-380; Fu, W.-T., Bothell, D., Douglass, S., Haimson, C., Sohn, M.-H., Anderson, J., Toward a real-Time model-based training system (2006) Interacting with Computers, 18 (6), pp. 1215-1241. , http://iwc.oxfordjournals.org/content/18/6/1215, Dec; Cochran, R.E., Lee, F.J., Chown, E., Modeling emotion: Arousals impact on memory (2006) Proceedings of the 28th Annual Conference of the Cognitive Science Society; Ritter, F.E., Reifers, A.L., Schoelles, M.J., Lessons from defining theories of stress (2006) Integrated Models of Cognitive Systems; Gunzelmann, G., Richard, L., Gluck, K.A., Dinges, D.F., Fatigue in sustained attention: Generalizing mechanisms for time awake to time on task (2011) Cognitive Fatigue: Multidisciplinary Perspectives on Current Research and Future Applications, Ser. Decade of Behavior/Science Conference. Washington, DC, USA, pp. 83-101. , American Psychological Association; Evertsz, R., Ritter, F.E., Busetta, P., Pedrotti, M., Bittner, J.L., CoJACKAchieving principled behaviour variation in a moderated cognitive architecture (2008) Proceedings of the 17th Conference on Behavior Representation in Modeling and Simulation, pp. 80-89; Dancy, C.L., ACT-R: A cognitive architecture with physiology and affect (2013) Biologically Inspired Cognitive Architectures, 6, pp. 40-45; Borst, J.P., Anderson, J.R., Using the ACT-R cognitive architecture in combination with fMRI data (2015) An Introduction to Model-Based Cognitive Neuroscience, pp. 339-352. , Springer; Putze, F., Holt, D.V., Schultz, T., Funke, J., Model-based identification of EEG markers for learning opportunities in an associative learning task with delayed feedback (2014) Proc. of 24th International Conference on Artificial Neural Networks, Hamburg, Germany; Salvucci, D.D., Taatgen, N.A., Threaded cognition: An integrated theory of concurrent multitasking (2008) Psychological Review, 115 (1), pp. 101-130; Bttner, P., Hello Java: Linking ACT-R 6 with a Java Simulation (2010) Proceedings of the 10th International Conference on Cognitive Modeling, Philadelphia, USA, pp. 289-290; Jarvis, J., Putze, F., Heger, D., Schultz, T., Multimodal person independent recognition of workload related Biosignal Patterns (2011) Proceedings of the 13th International Conference on Multimodal Interfaces, Ser. ICMI 11, Alicante, Spain, pp. 205-208; Jasper, H.H., The ten twenty electrode system of the international federation (1958) Electroencephalography and Clinical Neurophysiology, 10, pp. 371-375; Sternberg, S., High-speed scanning in human memory (1966) Science, 153 (3736), pp. 652-654; Mattes, S., The lane-change-Task as a tool for driver distraction evaluation (2003) Quality of Work and Products in Enterprises of the Future, pp. 57-60},
sponsors={IEEE SMC Society; K C Wong Foundation},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781479986965},
language={English},
abbrev_source_title={Proc. - IEEE Int. Conf. Syst., Man, Cybern., SMC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Putze2016929,
author={Putze, F. and Schultz, T. and Ehret, S. and Miller-Teynor, H. and Kruse, A.},
title={Model-Based Evaluation of Playing Strategies in a Memo Game for Elderly Users},
journal={Proceedings - 2015 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2015},
year={2016},
pages={929-934},
doi={10.1109/SMC.2015.170},
art_number={7379302},
note={cited By 3; Conference of IEEE International Conference on Systems, Man, and Cybernetics, SMC 2015 ; Conference Date: 9 October 2015 Through 12 October 2015;  Conference Code:119045},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964545608&doi=10.1109%2fSMC.2015.170&partnerID=40&md5=585d0a9394e391b9633400518534f62a},
affiliation={Karlsruhe Institute of Technology, Cognitive Systems Lab, Karlsruhe, Germany; University of Heidelberg, Institute for Gerontology, Heidelberg, Germany},
abstract={In this paper, we analyze game protocols for a Memo game for elderly users. We show how we can use generative statistical models to automatically reveal different playing strategies. We present a quantitative and qualitative evaluation of the approach on simulated and real data. We show that we can reliably detect different strategies and that we can use those strategy profiles to uncover relevant information on the players beyond pure performance measures. © 2015 IEEE.},
author_keywords={cognitive and social activation;  dementia;  elderly users;  memory;  mental model},
keywords={Data storage equipment, dementia;  Elderly users;  Game protocol;  Mental model;  Model-based evaluation;  Performance measure;  Playing Strategies;  Qualitative evaluations, Cybernetics},
references={Organization, W.H., (2012) Dementia: A Public Health Priority, , and others World Health Organization; Vernooij-Dassen, M., Vasse, E., Zuidema, S., Cohen-Mansfield, J., Moyle, W., Psychosocial interventions for dementia patients in longterm care (2010) International Psychogeriatrics, 22 (7), pp. 1121-1128; Brodaty, H., Arasaratnam, C., Meta-Analysis of nonpharmacological interventions for neuropsychiatric symptoms of dementia (2012) American Journal of Psychiatry, 169 (9), pp. 946-953; Smith, K.L., Crete-Nishihata, M., Damianakis, T., Baecker, R.M., Marziali, E., Multimedia biographies: A reminiscence and social stimulus tool for persons with cognitive impairment (2009) Journal of Technology in Human Services, 27 (4), pp. 287-306. , Nov; Webster, G., Fels, D.I., Gowans, G., Hanson, V.L., Portraits of individuals with dementia: Views of care managers (2011) Proceedings of the 25th BCS Conference on Human-Computer Interaction, Ser. BCS-HCI 11, pp. 331-340. , Swinton, UK: British Computer Society; Gowans, G., Campbell, J., Alm, N., Dye, R., Astell, A., Ellis, M., Designing a multimedia conversation aid for reminiscence therapy in dementia care environments (2004) CHI 04 Extended Abstracts on Human Factors in Computing Systems, Ser. CHI EA 04, pp. 825-836. , New York, NY, USA ACM; Botella, C., Etchemendy, E., Castilla, D., Baos, R.M., Garca-Palacios, A., Quero, S., Alcaiz, M., Lozano, J.A., An e-health system for the elderly (butler project): A pilot study on Acceptance and Satisfaction (2009) CyberPsychology & Behavior, 12 (3), pp. 255-262. , Jun; Alm, N., Dye, R., Gowans, G., Campbell, J., Astell, A., Ellis, M., A communication support system for older people with dementia (2007) Computer, 40 (5), pp. 35-41. , May; Buschkuehl, M., Jaeggi, S.M., Hutchison, S., Perrig-Chiello, P., Dpp, C., Mller, M., Breil, F., Perrig, W.J., Impact of working memory training on memory performance in old-old adults (2008) Psychology and Aging, 23 (4), pp. 743-753; Basak, C., Boot, W.R., Voss, M.W., Kramer, A.F., Can training in a real-Time strategy video game attenuate cognitive decline in older adults? (2008) Psychology and Aging, 23 (4), pp. 765-777; Woods, B., Spector, A.E., Jones, C.A., Orrell, M., Davies, S.P., Reminiscence therapy for dementia (1996) Cochrane Database of Systematic Reviews, , John Wiley & Sons, Ltd; Kobayashi, M., Hiyama, A., Miura, T., Asakawa, C., Hirose, M., Ifukube, T., Elderly user evaluation of mobile touchscreen interactions (2011) Human-Computer Interaction INTERACT 2011, Ser. Lecture Notes in Computer Science, (6946), pp. 83-99. , P. Campos, N. Graham, J. Jorge, N. Nunes, P. Palanque, and M. Winckler, Eds Springer Berlin Heidelberg, Jan; Neuroscience, N., (2006) Ratgeber Demenz Fr Betreuer und Angehrige, , Plejaden Communication GmbH & Co KG; Dubois, B., Slachevsky, A., Litvan, I., Pillon, B., The FAB A frontal assessment battery at bedside (2000) Neurology, 55 (11), pp. 1621-1626; Goldstein, K., (1934) Der Aufbau des Organismus, , M. Nijhoff; Anderson, J.R., Bothell, D., Byrne, M.D., Douglass, S., Lebiere, C., Qin, Y., An integrated theory of the mind (2004) Psychological Review, 111 (4), pp. 1036-1060},
sponsors={IEEE SMC Society; K C Wong Foundation},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781479986965},
language={English},
abbrev_source_title={Proc. - IEEE Int. Conf. Syst., Man, Cybern., SMC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Weiner2016240,
author={Weiner, J. and Schultz, T.},
title={Detection of intra-personal development of cognitive impairment from conversational speech},
journal={Speech Communication - 12. ITG-Fachtagung Sprachkommunikation},
year={2016},
pages={240-244},
note={cited By 2; Conference of 12. ITG-Fachtagung Sprachkommunikation - 12th  ITG Conference on Speech Communication ; Conference Date: 5 October 2016 Through 7 October 2016;  Conference Code:151912},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039154609&partnerID=40&md5=9b390b7aed2f8659f33a36126609a680},
affiliation={Cognitive Systems Lab, Universität Bremen, Bremen, 28359, Germany},
abstract={As the population in developed countries is aging, cognitive impairment such as Alzheimer's disease becomes an urging challenge for these societies. In order to mitigate the consequences, diagnosing cognitive impairment early is crucial. We present automatic detection of an intra-personal development of cognitive impairment from speech. Using conversational speech data from the ILSE corpus we detect subjects which were considered cognitively healthy at one examination and were diagnosed with a cognitive impairment at a later examination. From the speech recordings we extract 14 speech-based features using voice activity detection and transcriptions. With these features we train a linear discriminant analysis classifier that distinguishes subjects who developed a cognitive impairment from subjects who did not. The classifier achieves an accuracy of 80.4%, classifying half the cognitively impaired subjects correctly and assigning that label to hardly any cognitively health subjects. This shows our approach is well suited for longitudinal cognitive status monitoring. © 2016 VDE VERLAG GMBH.},
keywords={Diagnosis;  Discriminant analysis;  Neurodegenerative diseases;  Speech;  Speech communication, Alzheimer's disease;  Cognitive impairment;  Cognitively impaired;  Conversational speech;  Developed countries;  Linear discriminant analysis;  Personal development;  Voice activity detection, Speech recognition},
references={Bevölkerungs- und Haushaltsentwicklung im Bund und in den Ländern (2011) Demografischer Wandel in Deutschland, 1; Prince, M., Wimo, A., Guerchet, M., Ali, G.-C., Wu, Y.-T., Prina, M., (2015) World Alzheimer Report 2015. The Global Impact of Dementia: An Analysis of Prevalence, Incidence, Cost and Trends, , London: Alzheimer's Disease International; (2012) Dementia: A Public Health Priority, , World Health Organization; Folstein, M.F., Folstein, S.E., McHugh, P.R., "Minimental state": A practical method for grading the cognitive state of patients for the clinician (1975) Journal of Psychiatric Research, 12 (3), pp. 189-198; Appell, J., Kertesz, A., Fisman, M., A study of language functioning in Alzheimer patients (1982) Brain and Language, 17 (1), pp. 73-91; Reilly, J., Rodriguez, A.D., Lamy, M., Neils-Strunjas, J., Cognition, language, and clinical pathological features of non-Alzheimer's dementias: An overview (2010) Journal of Communication Disorders, 43 (5), pp. 438-452; Bucks, R., Singh, S., Cuerden, J.M., Wilcock, G.K., Analysis of spontaneous, conversational speech in dementia of Alzheimer type: Evaluation of an objective technique for analysing lexical performance (2000) Aphasiology, 14 (1), pp. 71-91; Satt, A., Sorin, A., Toledo-Ronen, O., Barkan, O., Kompatsiaris, I., Kokonozi, A., Tsolaki, M., Evaluation of speech-based protocol for detection of early-stage dementia (2013) INTERSPEECH 2013 - 14th Annual Conference of the International Speech Communication Association, pp. 1692-1696; Satt, A., Hoory, R., König, A., Aalten, P., Robert, P.H., Speech-based automatic and robust detection of very early dementia (2014) INTERSPEECH 2014 - 15th Annual Conference of the International Speech Communication Association, pp. 2538-2542; Espinoza-Cuadros, F., Garcia-Zamora, M.A., Torres-Boza, D., Ferrer-Riesgo, C.A., Montero-Benavides, A., Gonzalez-Moreira, E., Hernandez-Gómez, L.A., A spoken language database for research on moderate cognitive impairment: Design and preliminary analysis (2014) Advances in Speech and Language Technologies for Iberian Languages, pp. 219-228. , Springer; Tóth, L., Gosztolya, G., Vincze, V., Hoffmann, I., Szatlóczki, G., Automatic detection of mild cognitive impairment from spontaneous speech using asr (2015) INTERSPEECH 2015 - 16th Annual Conference of the International Speech Communication Association, pp. 2694-2698; Hakkani-Tür, D., Vergyri, D., Tür, G., Speech-based automated cognitive status assessment (2010) INTERSPEECH 2010 - 11th Annual Conference of the International Speech Communication Association, pp. 258-261; Prud'hommeaux, E.T., Roark, B., Extraction of narrative recall patterns for neuropsychological assessment (2011) INTERSPEECH 2011 - 12th Annual Conference of the International Speech Communication Association, pp. 3021-3024; Lehr, M., Prud'hommeaux, E.T., Shafran, I., Roark, B., Fully automated neuropsychological assessment for detecting mild cognitive impairment (2012) INTERSPEECH 2012 - 13th Annual Conference of the International Speech Communication Association, pp. 1039-1042; Thomas, C., Kešelj, V., Cercone, N., Rockwood, K., Asp, E., Automatic detection and rating of dementia of Alzheimer type through lexical analysis of spontaneous speech (2005) 2005 IEEE International Conference on Mechatronics and Automation, 3, pp. 1569-1574; Khodabakhsh, A., Yesil, F., Guner, E., Demiroglu, C., Evaluation of linguistic and prosodic features for detection of Alzheimer's disease in Turkish conversational speech (2015) EURASIP Journal on Audio, Speech, and Music Processing, 2015 (1), pp. 1-15; Yu, B., Quatieri, T.F., Williamson, J.R., Mundt, J.C., Prediction of cognitive performance in an animal fluency task based on rate and articulatory markers (2014) INTERSPEECH 2014 - 15th Annual Conference of the International Speech Communication Association, pp. 1038-1042; Yu, B., Quatieri, T.F., Williamson, J.R., Mundt, J.C., Cognitive impairment prediction in the elderly based on vocal biomarkers (2015) INTERSPEECH 2015 - 16th Annual Conference of the International Speech Communication Association, pp. 3734-3738; Weiner, J., Frankenberg, C., Telaar, D., Wendelstein, B., Schröder, J., Schultz, T., Towards automatic transcription of ILSE - An interdisciplinary longitudinal study of adult development and aging (2016) Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16); Telaar, D., Wand, M., Gehrig, D., Putze, F., Amma, C., Heger, D., Vu, N.T., Schultz, T., BioKIT - Real-time decoder for biosignal processing (2014) INTERSPEECH 2014 - 15th Annual Conference of the International Speech Communication Association, pp. 2650-2654; Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Duchesnay, E., Scikit-learn: Machine learning in python (2011) Journal of Machine Learning Research, 12, pp. 2825-2830},
correspondence_address1={Weiner, J.; Cognitive Systems Lab, Universität BremenGermany; email: jochen.weiner@uni-bremen.de},
publisher={VDE Verlag GmbH},
isbn={9783800742752},
language={English},
abbrev_source_title={Speech Commun. - 12. ITG-Fachtag. Sprachkommun.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Weiner20161938,
author={Weiner, J. and Herff, C. and Schultz, T.},
title={Speech-based detection of Alzheimer's disease in conversational German},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2016},
volume={08-12-September-2016},
pages={1938-1942},
doi={10.21437/Interspeech.2016-100},
note={cited By 21; Conference of 17th Annual Conference of the International Speech Communication Association, INTERSPEECH 2016 ; Conference Date: 8 September 2016 Through 16 September 2016;  Conference Code:124342},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994226480&doi=10.21437%2fInterspeech.2016-100&partnerID=40&md5=295bc7ee2278b9267e13c01ac3ce772d},
affiliation={Cognitive Systems Lab, University of Bremen, Germany},
abstract={The worldwide population is aging. With a larger population of elderly people, the numbers of people affected by cognitive impairment such as Alzheimer's disease are growing. Unfortunately, there is no known cure for Alzheimer's disease. The only way to alleviate it's serious effects is to start therapy very early before the disease has wrought too much irreversible damage. Current diagnostic procedures are neither cost nor time efficient and therefore do not meet the demands for frequent mass screening required to mitigate the consequences of cognitive impairments on the global scale. We present an experiment to detect Alzheimer's disease using spontaneous conversational speech. The speech data was recorded during biographic interviews in the Interdisciplinary Longitudinal Study on Adult Development and Aging (ILSE), a large data resource on healthy and satisfying aging in middle adulthood and later life in Germany. From these recordings we extract ten speech-based features using voice activity detection and transcriptions. In an experimental setup with 98 data samples we train a linear discriminant analysis classifier to distinguish subjects with Alzheimer's disease from the control group. This setup results in an F-score of 0.8 for the detection of Alzheimer's disease, clearly showing our approach detects dementia well. Copyright © 2016 ISCA.},
author_keywords={Alzheimer;  Computational paralinguistics;  Dementia;  ILSE},
keywords={Diagnosis;  Discriminant analysis;  Neurodegenerative diseases;  Speech;  Speech communication;  Speech processing;  Speech recognition, Alzheimer;  Cognitive impairment;  Conversational speech;  Dementia;  ILSE;  Linear discriminant analysis;  Paralinguistics;  Voice activity detection, Disease control},
references={United Nations, (2013) Department of Economic and Social Affairs, Population Division, World Population Ageing 2013, , United Nations; Prince, M., Wimo, A., Guerchet, M., Ali, G.-C., Wu, Y.-T., Prina, M., (2015) World Alzheimer Report 2015, , The Global Impact of Dementia: an Analysis of Prevalence, Incidence, Cost and Trends. London: Alzheimer's Disease International; World Health Organization and Alzheimers Disease International, (2012) Dementia: A Public Health Priority, , World Health Organization; Folstein, M.F., Folstein, S.E., McHugh, P.R., "Mini-mental state": A practical method for grading the cognitive state of patients for the clinician (1975) Journal of Psychiatric Research, 12 (3), pp. 189-198; Appell, J., Kertesz, A., Fisman, M., A study of language functioning in Alzheimer patients (1982) Brain and Language, 17 (1), pp. 73-91; Reilly, J., Rodriguez, A.D., Lamy, M., Neils-Strunjas, J., Cognition, language, and clinical pathological features of nonalzheimer's dementias: An overview (2010) Journal of Communication Disorders, 43 (5), pp. 438-452; Bucks, R., Singh, S., Cuerden, J.M., Wilcock, G.K., Analysis of spontaneous, conversational speech in dementia of Alzheimer type: Evaluation of an objective technique for analysing lexical performance (2000) Aphasiology, 14 (1), pp. 71-91; Satt, A., Sorin, A., Toledo-Ronen, O., Barkan, O., Kompatsiaris, I., Kokonozi, A., Tsolaki, M., Evaluation of speech-based protocol for detection of early-stage dementia (2013) INTERSPEECH 2013-14th Annual Conference of the International Speech Communication Association, pp. 1692-1696; Satt, A., Hoory, R., König, A., Aalten, P., Robert, P.H., Speechbased automatic and robust detection of very early dementia (2014) INTERSPEECH 2014-15th Annual Conference of the International Speech Communication Association, pp. 2538-2542; Espinoza-Cuadros, F., Garcia-Zamora, M.A., Torres-Boza, D., Ferrer-Riesgo, C.A., Montero-Benavides, A., Gonzalez-Moreira, E., Hernandez-Gómez, L.A., A spoken language database for research on moderate cognitive impairment: Design and preliminary analysis (2014) Advances in Speech and Language Technologies for Iberian Languages, pp. 219-228. , Springer; Tóth, L., Gosztolya, G., Vincze, V., Hoffmann, I., Szatlóczki, G., Automatic detection of mild cognitive impairment from spontaneous speech using asr (2015) INTERSPEECH 2015-16th Annual Conference of the International Speech Communication Association, pp. 2694-2698; Hakkani-Tür, D., Vergyri, D., Tür, G., Speech-based automated cognitive status assessment (2010) INTERSPEECH 2010-11th Annual Conference of the International Speech Communication Association, pp. 258-261; Prud'Hommeaux, E.T., Roark, B., Extraction of narrative recall patterns for neuropsychological assessment (2011) INTERSPEECH 2011-12th Annual Conference of the International Speech Communication Association, pp. 3021-3024; Lehr, M., Prud'Hommeaux, E.T., Shafran, I., Roark, B., Fully automated neuropsychological assessment for detecting mild cognitive impairment (2012) INTERSPEECH 2012-13th Annual Conference of the International Speech Communication Association, pp. 1039-1042; Thomas, C., Kešelj, V., Cercone, N., Rockwood, K., Asp, E., Automatic detection and rating of dementia of Alzheimer type through lexical analysis of spontaneous speech (2005) 2005 IEEE International Conference on Mechatronics and Automation, 3, pp. 1569-1574. , IEEE; Khodabakhsh, A., Yesil, F., Guner, E., Demiroglu, C., Evaluation of linguistic and prosodic features for detection of Alzheimer's disease in Turkish conversational speech (2015) EURASIP Journal on Audio, Speech, and Music Processing, 2015 (1), pp. 1-15; Yu, B., Quatieri, T.F., Williamson, J.R., Mundt, J.C., Prediction of cognitive performance in an animal fluency task based on rate and articulatory markers (2014) INTERSPEECH 2014-15th Annual Conference of the International Speech Communication Association, pp. 1038-1042; Cognitive impairment prediction in the elderly based on vocal biomarkers (2015) INTERSPEECH 2015-16th Annual Conference of the International Speech Communication Association, pp. 3734-3738; Weiner, J., Frankenberg, C., Telaar, D., Wendelstein, B., Schröder, J., Schultz, T., Towards automatic transcription of ilse-an interdisciplinary longitudinal study of adult development and aging (2016) Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16); Wendelstein, B., (2016) Gesprochene Sprache im Vorfeld der Alzheimer-Demenz. Linguistische Analysen im Verlauf von Prklinischen Stadien Bis Zur Leichten Demenz, , Heidelberg: Winter; Telaar, D., Wand, M., Gehrig, D., Putze, F., Amma, C., Heger, D., Vu, N.T., Schultz, T., BioKIT-Real-time decoder for biosignal processing (2014) INTERSPEECH 2014-15th Annual Conference of the International Speech Communication Association, pp. 2650-2654; Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Duchesnay, E., Scikit-learn: Machine Learning in Python (2011) Journal of Machine Learning Research, 12, pp. 2825-2830},
editor={Morgan N., Georgiou P., Morgan N., Narayanan S., Metze F.},
sponsors={Amazon Alexa; Apple; eBay; et al.; Google; Microsoft},
publisher={International Speech and Communication Association},
issn={2308457X},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Stahlberg2016234,
author={Stahlberg, F. and Schlippe, T. and Vogel, S. and Schultz, T.},
title={Word segmentation and pronunciation extraction from phoneme sequences through cross-lingual word-to-phoneme alignment},
journal={Computer Speech and Language},
year={2016},
volume={35},
pages={234-261},
doi={10.1016/j.csl.2014.10.001},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942532197&doi=10.1016%2fj.csl.2014.10.001&partnerID=40&md5=69e83323668cc9ecc7f1f2a0313a3bc9},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology (KIT), Germany; Qatar Computing Research Institute, Qatar Foundation, Qatar},
abstract={In this paper, we study methods to discover words and extract their pronunciations from audio data for non-written and under-resourced languages. We examine the potential and the challenges of pronunciation extraction from phoneme sequences through cross-lingual word-to-phoneme alignment. In our scenario a human translator produces utterances in the (non-written) target language from prompts in a resource-rich source language. We add the resource-rich source language prompts to help the word discovery and pronunciation extraction process. By aligning the source language words to the target language phonemes, we segment the phoneme sequences into word-like chunks. The resulting chunks are interpreted as putative word pronunciations but are very prone to alignment and phoneme recognition errors. Thus we suggest our alignment model Model 3P that is particularly designed for cross-lingual word-to-phoneme alignment. We present two different methods (source word dependent and independent clustering) that extract word pronunciations from word-to-phoneme alignments and compare them. We show that both methods compensate for phoneme recognition and alignment errors. We also extract a parallel corpus consisting of 15 different translations in 10 languages from the Christian Bible to evaluate our alignment model and error recovery methods. For example, based on noisy target language phoneme sequences with 45.1% errors, we build a dictionary for an English Bible with a Spanish Bible translation with 4.5% OOV rate, where 64% of the extracted pronunciations contain no more than one wrong phoneme. Finally, we use the extracted pronunciations in an automatic speech recognition system for the target language and report promising word error rates - given that pronunciation dictionary and language model are learned completely unsupervised and no written form for the target language is required for our approach. © 2014 Elsevier Ltd. All rights reserved.},
author_keywords={Lexical language discovery;  Non-written languages;  Pronunciation dictionary;  Speech-to-speech translation;  Under-resourced languages;  Word segmentation},
keywords={Alignment;  Errors;  Extraction;  Natural resources;  Speech recognition;  Translation (languages), Lexical language discovery;  Pronunciation dictionaries;  Speech-to-speech translation;  Under-resourced languages;  Word segmentation, Computational linguistics},
references={Achtert, E., Goldhofer, S., Kriegel, H.P., Schubert, E., Zimek, A., Evaluation of clusterings - Metrics and visual support (2012) ICDE; Besacier, L., Barnard, E., Karpov, A., Schultz, T., Automatic speech recognition for under-resourced languages: A survey (2014) Speech Commun., 56, pp. 85-100; Besacier, L., Zhou, B., Gao, Y., Towards speech translation of non written languages (2006) SLT; Bisani, M., Ney, H., Joint-sequence models for grapheme-to-phoneme conversion (2008) Speech Commun., 50, pp. 434-451; Borland, J.A., (2003) The English Standard Version - A Review Article, p. 162. , Faculty Publications and Presentations; Brown, P.F., Pietra, V.J.D., Pietra, S.A.D., Mercer, R.L., The mathematics of statistical machine translation: Parameter estimation (1993) Comput. Linguist., 19, pp. 263-311; Can, D., Cooper, E., Ghoshal, A., Jansche, A., Khudanpur, S., Ramabhadran, B., Riley, M., White, S., Web derived pronunciations for spoken term detection (2009) SIGIR; Chaudhuri, S., Harvilla, M., Raj, B., Unsupervised learning of acoustic unit descriptors for audio content representation and classification (2011) Interspeech; Chiang, D., Diab, M., Habash, N., Rambow, O., Shareef, S., Parsing arabic dialects (2005) JHU Summer Workshop; Crossway, (2001) The Holy Bible: English Standard Version; Ester, M., Kriegel, H.P., Sander, J., Xu, X., A density-based algorithm for discovering clusters in large spatial databases with noise (1996) KDD; Fiscus, J., (2007) Speech Recognition Scoring Toolkit Ver. 2.3 (Sctk); Gales, M.J.F., Knill, K.M., Ragni, A., Rath, P.R., Speech recognition and keyword spotting for low resource languages: Babel project research at CUED (2014) SLTU; Ghoshal, A., Jansche, M., Khudanpur, S., Riley, M., Ulinski, M., Web-derived pronunciations (2009) ICASSP; Goldsmith, J., An algorithm for the unsupervised learning of morphology (2006) Nat. Lang. Eng., 12, pp. 353-371; Goldsmith, J.A., (2010) Segmentation and Morphology, pp. 364-393. , Wiley-Blackwell; Gordon, R.G., Grimes, B.F., (2014) Ethnologue: Languages of the World, , 17th ed. SIL International; Harper, M.P., (2014) IARPA Babel Program, , http://www.iarpa.gov/index.php/research-programs/babel, (accessed 05.10.14); Jansen, A., Dupoux, E., Goldwater, S., Johnson, M., Khudanpur, S., Church, K., Feldman, N., Rose, R., A summary of the 2012 JHU CLSP workshop on zero resource speech technologies and models of early language acquisition (2013) ICASSP; Johnson, M., Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure (2008) ACL-HLT; Johnson, M., Goldwater, S., Improving non-parametric Bayesian inference: Experiments on unsupervised word segmentation with adaptor grammars (2009) HLT-NAACL; Kikui, G., Sumita, E., Takezawa, T., Yamamoto, S., Creating corpora for speech-to-speech translation (2003) Eurospeech; Kit, C., (2000) Unsupervised Lexical Learning As Inductive Inference. Technical Report, , City University of HK Press; Knight, K., A statistical MT tutorial workbook (1999) JHU Summer Workshop; Koehn, P., (2010) Statistical Machine Translation, , Cambridge University Press; Lee, C., Glass, J., A nonparametric bayesian approach to acoustic model discovery (2012) ACL-HLT; Lockman, (1986) La Biblia de Las Américas, , http://www.lockman.org/lblainfo/, (accessed 05.10.14); MacKay, D.J.C., (2003) Information Theory, Inference and Learning Algorithms, , Cambridge University Press; Manning, C.D., Schütze, H., (1999) Foundations of Statistical Natural Language Processing, , MIT Press; Martirosian, O., Davel, M., Error analysis of a public domain pronunciation dictionary (2007) PRASA; Nettle, D., Romaine, S., (2000) Vanishing Voices: The Extinction of the World's Languages, , Oxford University Press; Novak, J., Minematsu, N., Hirose, K., WFST-based grapheme-to-phoneme conversion: Open source tools for alignment, model-building and decoding (2012) FSMNLP; Och, F.J., An Efficient Method for Determining Bilingual Word Classes (1999) ACL; Och, F.J., Ney, H., Improved statistical alignment models (2000) ACL; Och, F.J., Ney, H., A systematic comparison of various statistical alignment models (2003) Comput. Linguist., 29, pp. 19-51; Okrand, M., (2008) The Star Trek: The Klingon Dictionary, , Simon and Schuster; Paul, D.B., Baker, J.M., The design for the wall street journal-based CSR corpus (1992) Speech and Natural Language, ACL; Paulik, M., Waibel, A., Training speech translation from audio recordings of interpreter-mediated communication (2013) Comput. Speech Lang., 27, pp. 455-474; Rabiner, L., A tutorial on hidden markov models and selected applications in speech recognition (1989) Proceedings of the IEEE, pp. 257-286; Rodgers, J.L., Nicewander, W.A., Thirteen ways to look at the correlation coefficient (1988) Am. Stat., 42, pp. 59-66; Schlippe, T., Ochs, S., Schultz, T., Wiktionary as a source for automatic pronunciation extraction (2010) Interspeech; Schlippe, T., Ochs, S., Schultz, T., Grapheme-to-phoneme model generation for indo-european languages (2012) ICASSP; Schlippe, T., Ochs, S., Schultz, T., Web-based tools and methods for rapid pronunciation dictionary creation (2014) Speech Commun., 56, pp. 101-118; Schultz, T., Kirchhoff, K., (2006) Multilingual Speech Processing., , Academic Press Amsterdam; Schultz, T., Vu, N.T., Schlippe, T., GlobalPhone: A multilingual text and speech database in 20 languages (2013) ICASSP; Sitaram, S., Palkar, S., Chen, Y.N., Parlikar, A., Black, A.W., Bootstrapping text-to-speech for speech processing in languages without an orthography (2013) ICASSP; Stahlberg, F., Schlippe, T., Vogel, S., Schultz, T., Word segmentation through cross-lingual word-to-phoneme alignment (2012) SLT; Stahlberg, F., Schlippe, T., Vogel, S., Schultz, T., Pronunciation extraction from phoneme sequences through cross-lingual word-to-phoneme alignment (2013) SLSP; Stahlberg, F., Schlippe, T., Vogel, S., Schultz, T., Towards automatic speech recognition without pronunciation dictionary, transcribed speech and text resources in the target language using cross-lingual word-to-phoneme alignment (2014) SLTU; Stolcke, A., Konig, Y., Weintraub, M., Explicit word error minimization in N-best list rescoring (1997) Eurospeech; Stolcke, A., SRILM - An extensible language modeling toolkit (2002) ICSLP; Stüker, S., Besacier, L., Waibel, A., Human translations guided language discovery for ASR systems (2009) Interspeech; Stüker, S., Waibel, A., Towards human translations guided language discovery for ASR systems (2008) SLTU; Thomas, R.L., Bible translations: The link between exegesis and expository preaching (1990) Masters Semin. J., 1, pp. 53-74; Ure, J., Lexical density and register differentiation (1971) Appl. Linguist., pp. 443-452; Varadarajan, B., Khudanpur, S., Dupoux, E., Unsupervised learning of acoustic sub-word units (2008) ACL-HLT; Vim, (2004) International Vocabulary of Basic and General Terms in Metrology. International Organization, 09-14; Vogel, S., Ney, H., Tillmann, C., HMM-based word alignment in statistical translation (1996) COLING; Vu, N.T., Kraus, F., Schultz, T., Rapid building of an ASR system for under-resourced languages based on multilingual unsupervised training (2011) Interspeech; Vu, N.T., Schlippe, T., Kraus, F., Schultz, T., Rapid bootstrapping of five eastern european languages using the rapid language adaptation toolkit (2010) Interspeech; Weide, R., (2005) The Carnegie Mellon Pronouncing Dictionary 0.6; Zhu, S., Wang, D., Li, T., Data clustering with size constraints (2010) Knowl. Based Syst., 23, pp. 883-889},
correspondence_address1={Stahlberg, F.; Cognitive Systems Lab, Karlsruhe Institute of Technology (KIT)Germany},
publisher={Academic Press},
issn={08852308},
coden={CSPLE},
language={English},
abbrev_source_title={Comput Speech Lang},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Weiner2016718,
author={Weiner, J. and Frankenberg, C. and Telaar, D. and Wendelstein, B. and Schröder, J. and Schultz, T.},
title={Towards automatic transcription of ILSE - An interdisciplinary longitudinal study of adult development and aging},
journal={Proceedings of the 10th International Conference on Language Resources and Evaluation, LREC 2016},
year={2016},
pages={718-725},
note={cited By 10; Conference of 10th International Conference on Language Resources and Evaluation, LREC 2016 ; Conference Date: 23 May 2016 Through 28 May 2016;  Conference Code:131727},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994215974&partnerID=40&md5=19e237d7f083a27a5a2a27df381f408e},
affiliation={Cognitive Systems Lab, University of Bremen, Karlsruhe Institute of Technology, Germany; Section of Geriatric Psychiatry, University Hospital Heidelberg, Germany; Institute of Gerontology, Universität Heidelberg, Germany},
abstract={The Interdisciplinary Longitudinal Study on Adult Development and Aging (ILSE) was created to facilitate the study of challenges posed by rapidly aging societies in developed countries such as Germany. ILSE contains over 8,000 hours of biographic interviews recorded from more than 1,000 participants over the course of 20 years. Investigations on various aspects of aging, such as cognitive decline, often rely on the analysis of linguistic features which can be derived from spoken content like these interviews. However, transcribing speech is a time and cost consuming manual process and so far only 380 hours of ILSE interviews have been transcribed. Thus, it is the aim of our work to establish technical systems to fully automatically transcribe the ILSE interview data. The joint occurrence of poor recording quality, long audio segments, erroneous transcriptions, varying speaking styles & crosstalk, and emotional & dialectal speech in these interviews presents challenges for automatic speech recognition (ASR). We describe our ongoing work towards the fully automatic transcription of all ILSE interviews and the steps we implemented in preparing the transcriptions to meet the interviews' challenges. Using a recursive long audio alignment procedure 96 hours of the transcribed data have been made accessible for ASR training.},
author_keywords={ASR corpus;  Dementia;  ILSE;  Long audio alignment},
keywords={Linguistics;  Speech recognition, ASR corpus;  Audio alignments;  Automatic speech recognition;  Automatic transcription;  Dementia;  Developed countries;  ILSE;  Linguistic features, Transcription},
funding_details={301-1720-295/2},
funding_text 1={The Interdisciplinary Longitudinal Study of Adult Development and Aging (ILSE) was supported by the Research Program of the State of Baden-Württemberg, the Federal Ministry of Family, Senior Citizens, Women and Youth (AZ: 301-1720-295/2), and the Dietmar-Hopp-Stiftung.},
references={(1944) Manual of Directions and Scoring, , Army Individual Test Battery, War Department, Adjutant General's Office, Washington, DC; Banse, R., Scherer, K.R., Acoustic profiles in vocal emotion expression (1996) Journal of Personality and Social Psychology, 70 (3), pp. 614-636; Brickenkamp, R., (1994) Der Aufmerksamkeits-Belastungs-Test (d2-Test), , Handanweisung. Hogrefe, Göttingen, 8. edition; Byrne, W., Doermann, D., Franz, M., Gustman, S., Hajic, J., Oard, D., Picheny, M., Zhu, W.-J., Automatic recognition of spontaneous speech for access to multilingual oral history archives (2004) IEEE Transactions on Speech and Audio Processing, 12 (4), pp. 420-435; Dartigues, J.-F., Gagnon, M., Barberger-Gateau, P., Leten-Neur, L., Commenges, D., Sauvel, C., Michel, P., Salamon, R., The Paquid epidemiological program on brain ageing (1992) Neuroepidemiology, 11, pp. 14-18; Dixon, R.A., De Frias, C.M., The Victoria longitudinal study: From characterizing cognitive aging to illustrating changes in memory compensation (2004) Aging, Neuropsychology, and Cognition, 11 (2-3), pp. 346-376; Edwards, J.A., (1993) Talking Data: Transcription and Coding in Discourse Research, , Lawrence Erlbaum, Hillsdale; Fast, K., Fujiwara, E., Markowitsch, H., (2006) Biele-felder Autobiographisches Gedächtnis Inventar (BAGI), , Swets & Zeitlinger, Lisse; Fast, K., Fujiwara, E., Schröder, J., Markowitsch, H., (2007) Erweitertes Autobiographisches Gedächtnis In-ventar (E-AGI), , Harcourt, Frankfurt a. Main; Folstein, M.F., Folstein, S.E., McHugh, P.R., "Mini-mental state": A practical method for grading the cognitive state of patients for the clinician (1975) Journal of Psychiatric Research, 12 (3), pp. 189-198; Hansen, J.H., Deller, J., Seadle, M., Engineering challenges in the creation of a National Gallery of the Spoken Word: Transcript-free search of audio archives (2001) Proc. IEEE ACM Joint Conf. Digital Libraries, pp. 235-236; Härting, C., Markowitsch, H.J., Neufeld, H., Calabrese, P., Deisinger, K., Kessler, J., (2000) Wech-sler Gedächtnis Test - Revidierte Fassung, , Huber, Göttingen; Hazen, T.J., Automatic alignment and error correction of human generated transcripts for long speech recordings (2006) INTERSPEECH 2006-7th Annual Conference of the International Speech Communication Association; Horn, W., (1983) Leistungsprüfsystem (LPS): Handan-weisung, , Hogrefe, Göttingen, 2. revised and improved edition; Juster, F.T., Suzman, R., An overview of the health and retirement study (1995) The Journal of Human Resources, 30, pp. 7-56; Kim, C., Stern, R.M., Robust signal-to-noise ratio estimation based on waveform amplitude distribution analysis (2008) INTERSPEECH 2008-9th Annual Conference of the International Speech Communication Association, pp. 2598-2601; Lehr, U., Thomae, H., Schmitt, M., Minnemann, E., Interdisziplinäre Längsschnittstudie des Erwachsenenalters: Geschichte, theoretische Begründung und ausgewählte Ergebnisse des 1. Messzeitpunktes (2000) Aspekte der Entwicklung im Mittleren und Höheren Lebensalter: Ergebnisse der Interdisziplinären Längsschnittstudie des Erwachsenenalters (ILSE), pp. 1-16. , Peter Martin, et al., editors, Steinkopff; Levy, R., Aging-associated cognitive decline (1994) International Psychogeriatrics, 6 (1), pp. 63-68; Martin, P., Martin, M., Design und Methodik der Interdisziplinären Längsschnittstudie des Erwachsenenalters (2000) Aspekte der Entwicklung im Mittleren und Höheren Lebensalter: Ergebnisse der Interdisziplinären Längsschnittstudie des Erwachsenenalters (ILSE), pp. 17-27. , Peter Martin, et al., editors, Steinkopff; Martin, P., Grünendahl, M., Schmitt, M., Persönlichkeit, kognitive Leistungsfähigkeit und Gesundheit in Ost und West: Ergebnisse der Inter-disziplinären Längsschnittstudie des Erwachsenenalters (ILSE) (2000) Zeitschrift für Gerontologie und Geriatrie, 33 (2), pp. 111-123; McKhann, G., Drachman, D., Folstein, M., Katzman, R., Price, D., Stadlan, E.M., Clinical diagnosis of Alzheimer's disease: Report of the NINCDS-ADRDA Work Group under the auspices of Department of Health and Human Services Task Force on Alzheimer's Disease (1984) Neurology, 34 (7), p. 939; Mikolov, T., Kombrink, S., Deoras, A., Burget, L., Cer-Nocky, J.H., RNNLM - Recurrent neural network language modeling toolkit (2011) IEEE Automatic Speech Recognition and Understanding Workshop; Moreno, P.J., Joerg, C.F., Van Thong, J.-M., Glick-Man, O., A recursive algorithm for the forced alignment of very long audio segments (1998) International Conference on Spoken Language Processing, ICSLP, pp. 2711-2714; Morris, J.C., Heyman, A., Mohs, R.C., Hughes, J.P., Van Belle, G., Fillenbaum, G., The consortium to establish a registry for Alzheimer's disease (CERAD) Part I: Clinical and neuropsychological assessment of Alzheimer's disease (1989) Neurology, 39, pp. 1159-1165; Nouza, J., Cerva, P., Zdansky, J., Blavka, K., Bohac, M., Silovsky, J., Chaloupka, J., Rott, M., Speech-to-text technology to transcribe and disclose 100, 000+ hours of bilingual documents from historical czech and czechoslovak radio archive (2014) INTERSPEECH 2014-15th Annual Conference of the International Speech Communication Association, pp. 964-968; Oswald, W., Fleischmann, U., (1993) Das Nürnberger Alters-Inventar NAI, , Kurzbeschreibung, Testanweisung, Normwerte, Testmaterial. Hogrefe, Göttingen; Povey, D., Ghoshal, A., Boulianne, G., Burget, L., Glem-Bek, O., Goel, N., Hannemann, M., Vesely, K., The kaldi speech recognition toolkit (2011) IEEE 2011 Workshop on Automatic Speech Recognition and Understanding; Prince, M., Wimo, A., Guerchet, M., Ali, G.-C., Wu, Y.-T., Prina, M., (2015) World Alzheimer Report 2015, , The Global Impact of Dementia: an Analysis of Prevalence, Incidence, Cost and Trends. Alzheimer's Disease International, London; Román, G.C., Tatemichi, T.K., Erkinjuntti, T., Cummings, J., Masdeu, J., Garcia, J.A., Amaducci, L., Hofman, A., Vascular dementia diagnostic criteria for research studies: Report of the NINDS-AIREN International Workshop (1993) Neurology, 43 (2), p. 250; Schlosberg, H., Three dimensions of emotion (1954) Psychological Review, 61 (2), pp. 81-88; Schönknecht, P., Pantel, J., Kruse, A., Schröder, J., Prevalence and natural course of aging-associated cognitive decline in a population-based sample of young-old subjects (2005) American Journal of Psychiatry, 162 (11), pp. 2071-2077; Schultz, T., Vu, N.T., Schlippe, T., Global-phone: A multilingual text & speech database in 20 languages (2013) IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP; Selting, M., Auer, P., Barth-Weingarten, D., Bergmann, J.R., Bergmann, P., Birkner, K., Couper-Kuhlen, E., Uhmann, S., Gesprächsanalytisches Transkriptionssystem 2 (GAT 2) (2009) Gesprächsforschung - Online-Zeitschrift zur Verbalen Interaktion, 10, pp. 353-402; Bevölkerungs- und Haushaltsentwicklung im Bund und in den Ländern (2011) Demografischer Wandel in Deutschland, p. 1. , Statistische Ämter des Bundes und der Länder; Stolcke, A., SRILM - An extensible language modeling toolkit (2002) International Confonference on Spoken Language Processing; Telaar, D., Wand, M., Gehrig, D., Putze, F., Amma, C., Heger, D., Vu, N.T., Schultz, T., BioKIT - Real-time decoder for biosignal processing (2014) INTER-SPEECH 2014-15th Annual Conference of the International Speech Communication Association, pp. 2650-2654; Tewes, U., (1991) Hamburg-Wechsler-Intelligenztest für Erwachsene: Handbuch und Testanweisung, , Huber, Göttingen; Veselý, K., Ghoshal, A., Burget, L., Povey, D., Sequence-discriminative training of deep neural networks (2013) INTERSPEECH 2013-14th Annual Conference of the International Speech Communication Association, pp. 2345-2349; Wendelstein, B., Sattler, C., Das ILSE-Korpus. Eine korpuslinguistische Perspektive psychologisch-psychiatrischer Forschung am Beispiel der Alzheimer-Demenz (2011) Korpuspragmatik. Thematische Korpora Als Basis Diskurslin-guistischer Analysen, pp. 488-511. , Ekkehard Felder, et al., editors, De Gruyter, Berlin/Boston; Wendelstein, B., (2016) Gesprochene Sprache im Vorfeld der Alzheimer-Demenz. Linguistische Analysen im Ver-lauf von Präklinischen Stadien bis zur Leichten Demenz, , Winter, Heidelberg; (2012) Dementia: A Public Health Priority, , World Health Organization and Alzheimer's Disease International, World Health Organization; Zipf, G.K., (1949) Human Behavior and the Principle of Least Effort, , Addison-Wesley Press; Schultz, T., (2014) GlobalPhone German, , distributed by ELRA, GlobalPhone, ISLRN 937-733-002-847-8},
editor={Calzolari N., Choukri K., Mazo H., Moreno A., Declerck T., Goggi S., Grobelnik M., Odijk J., Piperidis S., Maegaard B., Mariani J.},
sponsors={European Media Laboratory GmbH (EML); Intel},
publisher={European Language Resources Association (ELRA)},
isbn={9782951740891},
language={English},
abbrev_source_title={Int. Conf. Lang. Resourc. and Eval. - LREC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Navarro20154568,
author={Navarro, S.E. and Heger, F. and Putze, F. and Beyl, T. and Schultz, T. and Hein, B.},
title={Telemanipulation with force-based display of proximity fields},
journal={IEEE International Conference on Intelligent Robots and Systems},
year={2015},
volume={2015-December},
pages={4568-4574},
doi={10.1109/IROS.2015.7354027},
art_number={7354027},
note={cited By 8; Conference of IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2015 ; Conference Date: 28 September 2015 Through 2 October 2015;  Conference Code:117884},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958183973&doi=10.1109%2fIROS.2015.7354027&partnerID=40&md5=8bebb4672968a224ddeeb4ba46aae9e6},
affiliation={Institute for Anthropomatics and Robotics (IAR)-Intelligent Process Control and Robotics Lab (IPR), Karlsruhe Institute of Technology (KIT), Germany; Institute for Anthropomatics and Robotics (IAR)-Cognitive Systems Lab (CSL), Karlsruhe Institute of Technology (KIT), Germany},
abstract={In this paper we show and evaluate the design of a novel telemanipulation system that maps proximity values, acquired inside of a gripper, to forces a user can feel through a haptic input device. The command console is complemented by input-devices that give the user an intuitive control over parameters relevant to the system. Furthermore, proximity sensors enable the autonomous alignment/centering of the gripper to objects in user-selected DoFs with the potential of aiding the user and lowering the workload. We evaluate our approach in a user study that shows that the telemanipulation system benefits from the supplementary proximity information and that the workload can indeed be reduced when the system operates with partial autonomy. © 2015 IEEE.},
keywords={Grippers;  Knobs;  Manipulators, Force-based;  Input devices;  Intuitive controls;  Telemanipulation;  Telemanipulation systems;  User study, Intelligent robots},
references={Escaida Navarro, S., Schonert, M., Hein, B., Wörn, H., 6D proximity servoing for preshaping and haptic exploration using capacitive tactile proximity sensors (2014) Intelligent Robots and Systems (IROS), 2014 IEEE/RSJ International Conference on, pp. 7-14; Li, M., Kapoor, A., Taylor, R., Telerobotic control by virtual fixtures for surgical applications (2007) Advances in Telerobotics, Ser. Springer Tracts in Advanced Robotics, 31, pp. 381-401. , M. Ferre, M. Buss, R. Aracil, C. Melchiorri, and C. Balaguer, Eds. Springer Berlin Heidelberg; Bowyer, S., Davies, B., Rodriguezy Baena, F., Active constraints/ virtual fixtures: A survey (2014) Robotics, IEEE Transactions on, 30 (1), pp. 138-157. , Feb; Davies, B., Jakopec, M., Harris, S., Rodriguezy Baena, F., Barrett, A., Evangelidis, A., Gomes, P., Cobb, J., Active-constraint robotics for surgery (2006) Proceedings of the IEEE, 94 (9), pp. 1696-1704. , Sept; Jakopec, M., Harris, S.J., Rodriguezy Baena, F., Gomes, P., Cobb, J., Davies, B.L., The first clinical application of a hands-on robotic knee surgery system (2001) Comput. Aided Surg., 6 (6), pp. 329-339; Ramos, A., Prattichizzo, D., Vibrotactile stimuli for distinction of virtual constraints and environment feedback (2014) Haptics: Neuroscience, Devices, Modeling, and Applications, pp. 141-149. , Springer; Beyl, T., Nicolai, P., Mönnich, H., Raczkowksy, J., Wörn, H., Haptic feedback in op: Sense-augmented reality in telemanipulated robotic surgery (2012) MMVR, pp. 58-63; Wildenbeest, J., Abbink, D., Heemskerk, C., Helm Der Van, F., Boessenkool, H., The impact of haptic feedback quality on the performance of teleoperated assembly tasks (2013) Haptics, IEEE Transactions on, 6 (2), pp. 242-252. , April; Lumelsky, V.J., Cheung, E., Real-time collision avoidance in teleoperated whole-sensitive robot arm manipulators (1993) Systems, Man and Cybernetics, IEEE Transactions on, 23 (1), pp. 194-203. , Jan; Notheis, S., Hein, B., Wörn, H., Evaluation of a Method for Intuitive Telemanipulation based on View-dependent Mapping and Inhibition of Movements (2014) Proc. of IEEE Int. Conf. on Robotics and Automation (ICRA); Mayton, B., LeGrand, L., Smith, J., An electric field pretouch system for grasping and co-manipulation (2010) Robotics and Automation (ICRA), 2010 IEEE International Conference on, pp. 831-838. , may; Wistort, R., Smith, J.R., Electric field servoing for robotic manipulation (2008) Intelligent Robots and Systems, 2008. IROS 2008. IEEE/RSJ International Conference on, pp. 494-499. , sept; Hasegawa, H., Mizoguchi, Y., Tadakuma, K., Ming, A., Ishikawa, M., Shimojo, M., Development of intelligent robot hand using proximity, contact and slip sensing (2010) Robotics and Automation (ICRA), 2010 IEEE International Conference on, pp. 777-784. , may; Koyama, K., Hasegawa, H., Suzuki, Y., Ming, A., Shimojo, M., Pre-shaping for various objects by the robot hand equipped with resistor network structure proximity sensors (2013) Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International Conference on, pp. 4027-4033. , Nov; Göger, D., Blankertz, M., Wörn, H., A tactile proximity sensor (2010) IEEE Sensors 2010, pp. 589-594; Göger, D., Alagi, H., Wörn, H., Tactile proximity sensors for robotic applications (2013) International Conference on Industrial Technology (ICIT); Escaida Navarro, S., Marufo, M., Ding, Y., Puls, S., Göger, D., Hein, B., Wörn, H., Methods for safe human-robot-interaction using capacitive tactile proximity sensors (2013) Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International Conference on, pp. 1149-1154; Hart, S.G., Staveland, L.E., Development of nasa-tlx (task load index): Results of empirical and theoretical research (1988) Advances in Psychology, 52, pp. 139-183; Lu, S., Zhao, H., Ju, K., Shin, K., Lee, M., Shelley, K., Chon, K.H., Can photoplethysmography variability serve as an alternative approach to obtain heart rate variability information (2008) Journal of Clinical Monitoring and Computing, 22 (1), pp. 23-29},
sponsors={Bosch; dji; et al.; KUKA; rethink robotics; SIAT},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21530858},
isbn={9781479999941},
coden={85RBA},
language={English},
abbrev_source_title={IEEE Int Conf Intell Rob Syst},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{vonLühmann2015,
author={von Lühmann, A. and Herff, C. and Heger, D. and Schultz, T.},
title={Toward a wireless open source instrument: Functional near-infrared spectroscopy in mobile neuroergonomics and BCI applications},
journal={Frontiers in Human Neuroscience},
year={2015},
volume={9},
number={NOVEMBER},
doi={10.3389/fnhum.2015.00617},
art_number={617},
note={cited By 33},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947597818&doi=10.3389%2ffnhum.2015.00617&partnerID=40&md5=dbfe11853db6f3dc84eadf98f188debf},
affiliation={Machine Learning Department, Computer Science, Technische Universität Berlin, Berlin, Germany; Institute of Biomedical Engineering, Karlsruhe Institute of Technology, Karlsruhe, Germany; Cognitive Systems Lab, Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={Brain-Computer Interfaces (BCIs) and neuroergonomics research have high requirements regarding robustness and mobility. Additionally, fast applicability and customization are desired. Functional Near-Infrared Spectroscopy (fNIRS) is an increasingly established technology with a potential to satisfy these conditions. EEG acquisition technology, currently one of the main modalities used for mobile brain activity assessment, is widely spread and open for access and thus easily customizable. fNIRS technology on the other hand has either to be bought as a predefined commercial solution or developed from scratch using published literature. To help reducing time and effort of future custom designs for research purposes, we present our approach toward an open source multichannel stand-alone fNIRS instrument for mobile NIRS-based neuroimaging, neuroergonomics and BCI/BMI applications. The instrument is low-cost, miniaturized, wireless and modular and openly documented on www.opennirs.org. It provides features such as scalable channel number, configurable regulated light intensities, programmable gain and lock-in amplification. In this paper, the system concept, hardware, software and mechanical implementation of the lightweight stand-alone instrument are presented and the evaluation and verification results of the instrument’s hardware and physiological fNIRS functionality are described. Its capability to measure brain activity is demonstrated by qualitative signal assessments and a quantitative mental arithmetic based BCI study with 12 subjects. © 2015 von Lühmann, Herff, Heger and Schultz.},
author_keywords={Brain computer interface (BCI);  Functional near-infrared spectroscopy (fNIRS);  Modularity;  Neuroergonomics;  Open source;  Wearable devices},
keywords={Article;  brain computer interface;  brain function;  computer program;  computer system;  controlled study;  ergonomics;  human;  human experiment;  light intensity;  man machine interaction;  mental performance;  multichannel recorder;  near infrared spectroscopy;  neuroergonomics;  neuroimaging;  normal human;  portable equipment;  power supply;  signal noise ratio;  signal processing;  wireless communication},
references={Ang, K., Guan, C., Lee, K., Lee, J., Nioka, S., Chance, B., A brain-computer interface for mental arithmetic task from single-trial near-infrared spectroscopy brain signals (2010) International Conference on Pattern Recognition (Istanbul), pp. 3764-3767; Atsumori, H., Kiguchi, M., Obata, A., Sato, H., Katura, T., Utsugi, K., Development of a multi-channel, portable optical topography system (2007) Engineering in Medicine and Biology Society, pp. 3362-3364. , 2007. EMBS 2007. 29th Annual International Conference of the IEEE (Lyon); Ayaz, H., Onaral, B., Izzetoglu, K., Shewokis, P.A., McKendrick, R., Parasuraman, R., Continuous monitoring of brain dynamics with functional near infrared spectroscopy as a tool for neuroergonomic research: Empirical examples and a technological development (2013) Front. Hum. Neurosci, 7, 871p; Ayaz, H., Shewokis, P.A., Bunce, S., Izzetoglu, K., Willems, B., Onaral, B., Optical brain monitoring for operator training and mental workload assessment (2012) Neuroimage, 59, pp. 36-47; Bauernfeind, G., Steyrl, D., Brunner, C., Muller-Putz, G.R., Single trial classification of fnirs-based brain-computer interface mental arithmetic data: A comparison between different classifiers (2014) Engineering in Medicine and Biology Society (EMBC), 2014 36Th Annual International Conference of the IEEE, pp. 2004-2007. , Chicago, IL: IEEE; Biessmann, F., Plis, S., Meinecke, F., Eichele, T., Muller, K., Analysis of multimodal neuroimaging data. Biomed (2011) Eng. IEEE Rev, 4, pp. 26-58; Blankertz, B., Tomioka, R., Lemm, S., Kawanabe, M., Muller, K.-R., Optimizing spatial filters for robust eeg single-trial analysis (2008) Signal Process. Mag. IEEE, 25, pp. 41-56; Boas, D., Brooks, D., Miller, E., Dimarzio, C., Kilmer, M., Gaudette, R., Imaging the body with diffuse optical tomography (2001) Signal Process. Mag. IEEE, 18, pp. 57-75; Boas, D.A., Dale, A.M., Franceschini, M.A., Diffuse optical imaging of brain activation: Approaches to optimizing image sensitivity, resolution, and accuracy (2004) Neuroimage, 23, pp. S275-S288; Bozkurt, A., Onaral, B., Safety assessment of near infrared light emitting diodes for diffuse optical measurements (2004) Biomed. Eng. Online, 3, 9p; Bozkurt, A., Rosen, A., Rosen, H., Onaral, B., A portable near infrared spectroscopy system for bedside monitoring of newborn brain (2005) Biomed. Eng. Online, 4, 29p; Bunce, S., Izzetoglu, M., Izzetoglu, K., Onaral, B., Pourrezaei, K., Functional near-infrared spectroscopy (2006) Eng. Med. Biol. Mag. IEEE, 25, pp. 54-62; Calhoun, V., Adali, T., Pearlson, G., Pekar, J., Spatial and temporal independent component analysis of functional mri data containing a pair of task-related waveforms. Hum (2001) Brain Mapp, 13, pp. 43-53; Chenier, F., Sawan, M., A new brain imaging device based on fnirs (2007) Biomedical Circuits and Systems Conference, 2007. BIOCAS 2007, pp. 1-4. , Montreal, QC: IEEE; Cope, M., (1991) The Application of near Infrared Spectroscopy to Non Invasive Monitoring of Cerebral Oxygenation in the Newborn Infant, , Ph.D. thesis, Department of Medical Physics and Bioengineering, University College London; Cope, M., Delpy, D., System for long-term measurement of cerebral blood and tissue oxygenation on newborn infants by near infra-red transillumination (1988) Med. Biol. Eng. Comput, 26, pp. 289-294; Coyle, S., Ward, T., Markham, C., Brain-computer interface using a simplified functional near-infrared spectroscopy system (2007) J. Neural Eng, 4, pp. 219-226; Coyle, S., Ward, T., Markham, C., McDarby, G., On the suitability of near-infrared systems for next generation brain computer interfaces (2004) Physiol. Meas, 25, pp. 815-822; Fairclough, S.H., Fundamentals of physiological computing. Interact (2009) Comput, 21, pp. 133-145; Fazli, S., Mehnert, J., Steinbrink, J., Curio, G., Villringer, A., Müller, K.-R., Enhanced performance by a hybrid nirs-eeg brain computer interface (2012) Neuroimage, 59, pp. 519-529; Fekete, T., Rubin, D., Carlson, J.M., Mujica-Parodi, L.R., The nirs analysis package: Noise reduction and statistical inference (2011) Plos ONE, 6; Heger, D., Herff, C., Schultz, T., Combining feature extraction and classification for fnirs bcis by regularized least squares optimization (2014) Engineering in Medicine and Biology Society (EMBC), 2014 36Th Annual International Conference of the IEEE (IEEE), pp. 2012-2015; Herff, C., Heger, D., Fortmann, O., Hennrich, J., Putze, F., Schultz, T., Mental workload during n-back task-quantified in the prefrontal cortex using fnirsFront. Hum (2014) Neurosci, 7, p. 935; Herff, C., Heger, D., Putze, F., Hennrich, J., Fortmann, O., Schultz, T., Classification of mental tasks in the prefrontal cortex using fnirs (2013) Engineering in Medicine and Biology Society (EMBC), pp. 2160-2163. , 2013 35th Annual International Conference of the IEEE (Osaka); Jöbsis, F., Noninvasive, infrared monitoring of cerebral and myocardial oxygen sufficiency and circulatory parameters (1977) Science, 198, pp. 1264-1267; Kanoh, S., Murayama, Y.-M., Miyamoto, K.-I., Yoshinobu, T., Kawashima, R., A nirs-based brain-computer interface system during motor imagery: System development and online feedback training (2009) Engineering in Medicine and Biology Society, 2009. EMBC 2009. Annual International Conference of the IEEE, pp. 594-597. , Minneapolis, MN; Kiguchi, M., Atsumori, H., Fukasaku, I., Kumagai, Y., Funane, T., Maki, A., Note: Wearable near-infrared spectroscopy imager for haired region (2012) Rev. Sci. Instrum, 83; Lareau, E., Simard, G., Lesage, F., Nguyen, D., Sawan, M., Near infrared spectrometer combined with multichannel eeg for functional brain imaging (2011) Medical Information Communication Technology (ISMICT), pp. 122-126. , 2011 5th International Symposium on (Montreux); Lemm, S., Blankertz, B., Dickhaus, T., Müller, K.-R., Introduction to machine learning for brain imaging (2011) Neuroimage, 56, pp. 387-399; Matthews, F., Pearlmutter, B.A., Ward, T., Soraghan, C., Markham, C., Hemodynamics for brain-computer interfaces (2008) Signal Process. Mag. IEEE, 25, pp. 87-94; McKendrick, R., Parasuraman, R., Ayaz, H., Wearable functional near infrared spectroscopy (Fnirs) and transcranial direct current stimulation (tdcs): Expanding vistas for neurocognitive augmentation (2015) Front. Syst. Neurosci, 9, 27p; Meade, M.L., Advances in lock-in amplifiers (1982) J. Phys. E, 15, 395p; Meade, M.L., (1983) Lock-In Amplifiers: Principles and Applications, , London, UK: Peter Peregrinus Ltd; Naseer, N., Hong, K.-S., Classification of functional near-infrared spectroscopy signals corresponding to the right-and left-wrist motor imagery for development of a brain-computer interface (2013) Neurosci. Lett, 553, pp. 84-89; Obrig, H., Villringer, A., Beyond the visible - Imaging the human brain with light (2003) J. Cereb. Blood Flow Metab, 12, pp. 1-18; Parasuraman, R., Neuroergonomics: Research and practice (2003) Theor. Issues Ergon. Sci, 4, pp. 5-20; Parasuraman, R., Neuroergonomics: Brain, cognition, and performance at work (2011) Curr. Dir. Psychol. Sci, 20, pp. 181-186; Pfurtscheller, G., Bauernfeind, G., Wriessnegger, S.C., Neuper, C., Focal frontal (De)oxyhemoglobin responses during simple arithmetic (2010) Int. J. Psychophysiol, 76, pp. 186-192; Piper, S.K., Krueger, A., Koch, S.P., Mehnert, J., Habermehl, C., Steinbrink, J., A wearable multi-channel fNIRS system for brain imaging in freely moving subjects (2014) Neuroimage, 85, pp. 64-71; Putze, F., Hesslinger, S., Tse, C.-Y., Huang, Y., Herff, C., Guan, C., Hybrid fnirs-eeg based classification of auditory and visual perception processes (2014) Front. Neurosci, 8, 373p; Rajkumar, E.R., Safaie, J., Gupta, R., Pattnaik, D., Abrishamimoghaddam, H., Grebe, R., Development of an autonomic portable singleboard computer based high resolution nirs device for microcirculation analysis (2012) Engineering in Medicine and Biology Society (EMBC), 2012 Annual International Conference of the IEEE, pp. 3235-3238. , San Diego, CA; Rolfe, P., In vivo near infra-red spectrophotometry (2000) Annu. Rev. Biomed. Eng, 2, pp. 315-354; Safaie, J., Grebe, R., Moghaddam, H.A., Wallois, F., Toward a fully integrated wireless wearable eeg-nirs bimodal acquisition system (2013) J. Neural Eng, 10; Sassaroli, A., Fantini, S., Comment on the modified beer-lambert law for scattering media (2004) Phys. Med. Biol, 49, pp. N225-N257; Schmidt, F., Fry, M., Hillman, E., Hebden, J., Delpy, D., A 32-channel time-resolved instrument for medical optical tomography (2000) Rev. Sci. Instrum, 71, pp. 256-265; Scholkmann, F., Kleiser, S., Metz, A.J., Zimmermann, R., Pavia, J.M., Wolf, U., A review on continuous wave functional near-infrared spectroscopy and imaging instrumentation and methodology (2014) Neuroimage, 85, pp. 6-27; Schudlo, L.C., Chau, T., Single-trial classification of near-infrared spectroscopy signals arising from multiple cortical regions (2015) Behav. Brain Res, 290, pp. 131-142; Son, I.-Y., Yazici, B., Near infrared imaging and spectroscopy for brain activity monitoring (2006) Advances in Sensing with Security Applications, pp. 341-372. , http://www.springer.com/de/book/9781402042843?wt_mc=ThirdParty.SpringerLink.3.EPR653.About_eBook, J. Byrnes (NATO Security Through Science Series-A: Chemistry and Biology, Springer); Strangman, G., Culver, G., Thompson, J., Boas, D., A quantitative comparison of simultaneous bold fmri and nirs recordings during functional brain activation (2002) Neuroimage, 17, pp. 719-731; Vaithianathan, T., Tullis, I.D.C., Everdell, N., Leung, T., Gibson, A., Design of a portable near infrared system for topographic imaging of the brain in babies (2004) Rev. Sci. Instrum, 75, pp. 3276-3283; Yanagisawa, K., Asaka, K., Sawai, H., Tsunashima, H., Nagaoka, T., Tsujii, T., Brain-computer interface using near-infrared spectroscopy for rehabilitation (2010) Control Automation and Systems (ICCAS), pp. 2248-2253. , 2010 International Conference on (Gyeonggi-do); Zander, T.O., Kothe, C., Towards passive brain-computer interfaces: Applying brain-computer interface technology to human-machine systems in general (2011) J. Neural Eng, 8},
correspondence_address1={von Lühmann, A.; Machine Learning Department Computer Science, Technische Universität BerlinGermany; email: a.vonluehmann@campus.tu-berlin.de},
publisher={Frontiers Media S. A},
issn={16625161},
language={English},
abbrev_source_title={Front. Human Neurosci.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Hennrich20152844,
author={Hennrich, J. and Herff, C. and Heger, D. and Schultz, T.},
title={Investigating deep learning for fNIRS based BCI},
journal={Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
year={2015},
volume={2015-November},
pages={2844-2847},
doi={10.1109/EMBC.2015.7318984},
art_number={7318984},
note={cited By 26; Conference of 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2015 ; Conference Date: 25 August 2015 Through 29 August 2015;  Conference Code:116805},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953310802&doi=10.1109%2fEMBC.2015.7318984&partnerID=40&md5=b4ffaca9bc3fc138247579304801b651},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={Functional Near infrared Spectroscopy (fNIRS) is a relatively young modality for measuring brain activity which has recently shown promising results for building Brain Computer Interfaces (BCI). Due to its infancy, there are still no standard approaches for meaningful features and classifiers for single trial analysis of fNIRS. Most studies are limited to established classifiers from EEG-based BCIs and very simple features. The feasibility of more complex and powerful classification approaches like Deep Neural Networks has, to the best of our knowledge, not been investigated for fNIRS based BCI. These networks have recently become increasingly popular, as they outperformed conventional machine learning methods for a variety of tasks, due in part to advances in training methods for neural networks. In this paper, we show how Deep Neural Networks can be used to classify brain activation patterns measured by fNIRS and compare them with previously used methods. © 2015 IEEE.},
keywords={artificial neural network;  brain;  brain computer interface;  machine learning;  near infrared spectroscopy, Brain;  Brain-Computer Interfaces;  Machine Learning;  Neural Networks (Computer);  Spectroscopy, Near-Infrared},
references={Heger, D., Mutter, R., Herff, C., Putze, F., Schultz, T., Continuous recognition of affective states by functional near infrared spectroscopy signals (2013) 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction, pp. 832-837. , Sept; Bauernfeind, G., Steyrl, D., Brunner, C., Muller-Putz, G.R., Single trial classification of fnirs-based brain-computer interface mental arithmetic data: A comparison between different classifiers (2014) Engineering in Medicine and Biology Society (EMBC), 2014 36th Annual International Conference of the IEEE. IEEE; Coyle, S.M., Ward, T.E., Markham, C.M., Brain-computer interface using a simplified functional near-infrared spectroscopy system (2007) Journal of Neural Engineering, 4 (3), p. 219; Sitaram, R., Zhang, H., Guan, C., Thulasidas, M., Hoshi, Y., Ishikawa, A., Shimizu, K., Birbaumer, N., Temporal classification of multichannel near-infrared spectroscopy signals of motor imagery for developing a brain-computer interface (2007) NeuroImage, 34 (4); Herff, C., Heger, D., Fortmann, O., Hennrich, J., Putze, F., Schultz, T., Mental workload during n-back task-quantified in the prefrontal cortex using fNIRS (2014) Frontiers in Human Neuroscience, 7, p. 935. , January, Jan; Batula, A.M., Ayaz, H., Kim, Y.E., Evaluating a four-class motorimagery-based optical brain-computer interface (2014) Engineering in Medicine and Biology Society (EMBC), 2014 36th Annual International Conference of the IEEE. IEEE, pp. 2000-2003; Power, S.D., Kushki, A., Chau, T., Intersession consistency of single-trial classification of the prefrontal response to mental arithmetic and the no-control state by nirs (2012) PloS One, 7 (7), p. e37791; Herff, C., Heger, D., Putze, F., Hennrich, J., Fortmann, O., Schultz, T., Classification of mental tasks in the prefrontal cortex using fNIRS (2013) Engineering in Medicine and Biology Society (EMBC), 2013 35th Annual International Conference of the IEEE, pp. 2160-2163; Hinton, G.E., Osindero, S., Teh, Y.-W., A fast learning algorithm for deep belief nets (2006) Neural Computation, 18 (7), pp. 1527-1554. , July; Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H., (2007) Greedy Layer-Wise Training of Deep Networks, (1); Bengio, Y., Learning deep architectures for AI (2009) Foundations and Trends in Machine Learning, 2 (1), pp. 1-127; Mohamed, A.-R., Dahl, G., Hinton, G., (2009) Deep Belief Networks for Phone Recognition, pp. 1-9; Mohamed, A.-R., Dahl, G.E., Hinton, G., (2010) Acoustic Modeling Using Deep Belief Networks, (C), pp. 1-10; Hinton, G.E., Salakhutdinov, R.R., Reducing the dimensionality of data with neural networks (2006) Science, 313 (5786), pp. 504-507. , July (New York, N. Y. ); Cirean, D., Meier, U., Schmidhuber, J., (2012) Multi-column Deep Neural Networks for Image Classification, p. 20. , February, Feb; Seide, F., Li, G., Chen, X., Yu, D., Feature engineering in context-dependent deep neural networks for conversational speech transcription (2011) 2011 IEEE Workshop on Automatic Speech Recognition & Understanding, pp. 24-29. , Dec; Bengio, Y., Georgios, N.Y., (2013) Learning Deep Physiological Models of Affect, pp. 20-33. , April; Blue, J.L., Grother, P.J., Training feed-forward neural networks using conjugate gradients (1992) SPIE/IS&T 1992 Symposium on Electronic Imaging: Science and Technology. International Society for Optics and Photonics, pp. 179-190; Martens, J., Deep learning via hessian-free optimization (2010) Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 735-742; Hinton, G., (2010) A Practical Guide to Training Restricted Boltzmann Machines; Sarle, W.S., Stopped training and other remedies for overfitting (1995) Proc. of the 27th Symposium on the Interface of Computing Science and Statistics, pp. 352-360; Blankertz, B., Lemm, S., Treder, M., Haufe, S., Müller, K.-R., Single-trial analysis and classification of ERP components-a tutorial (2011) NeuroImage, 56 (2), pp. 814-825. , May; Ledoit, O., Wolf, M., A well-conditioned estimator for largedimensional covariance matrices (2004) Journal of Multivariate Analysis, 88 (2), pp. 365-411. , Feb},
sponsors={},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={1557170X},
isbn={9781424492718},
pubmed_id={26736884},
language={English},
abbrev_source_title={Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Diener2015,
author={Diener, L. and Janke, M. and Schultz, T.},
title={Direct conversion from facial myoelectric signals to speech using Deep Neural Networks},
journal={Proceedings of the International Joint Conference on Neural Networks},
year={2015},
volume={2015-September},
doi={10.1109/IJCNN.2015.7280404},
art_number={7280404},
note={cited By 18; Conference of International Joint Conference on Neural Networks, IJCNN 2015 ; Conference Date: 12 July 2015 Through 17 July 2015;  Conference Code:116724},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951114437&doi=10.1109%2fIJCNN.2015.7280404&partnerID=40&md5=c50d7928fc3224b407ae92f60008c76d},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany},
abstract={This paper presents our first results using Deep Neural Networks for surface electromyographic (EMG) speech synthesis. The proposed approach enables a direct mapping from EMG signals captured from the articulatory muscle movements to the acoustic speech signal. Features are processed from multiple EMG channels and are fed into a feed forward neural network to achieve a mapping to the target acoustic speech output. We show that this approach is feasible to generate speech output from the input EMG signal and compare the results to a prior mapping technique based on Gaussian mixture models. The comparison is conducted via objective Mel-Cepstral distortion scores and subjective listening test evaluations. It shows that the proposed Deep Neural Network approach gives substantial improvements for both evaluation criteria. © 2015 IEEE.},
author_keywords={Electromyography},
keywords={Electromyography;  Mapping;  Speech;  Speech analysis;  Speech synthesis, Deep neural networks;  Direct conversion;  Electromyographic;  Evaluation criteria;  Gaussian Mixture Model;  Mapping techniques;  Myoelectric signals;  Subjective listening test, Speech communication},
references={Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Silent speech interfaces (2010) Speech Communication, 52 (4), pp. 270-287; Chan, A.D.C., Englehart, K., Hudgins, B., Lovely, D.F., Hidden markov model classification of myolectric signals in speech (2002) IEEE Engineering in Medicine and Biology Society, 21 (5), pp. 143-146; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proceedings of the Annual Conference of the International Speech Communication Association (Lnterspeech), pp. 573-576; Toth, A.R., Wand, M., Schultz, T., Synthesizing speech from electromyography using voice transformation techniques (2009) Proceedings of Interspeech 2009, pp. 652-655; Toda, T., Shikano, K., NAM-to-speech conversion with Gaussian mixture models (2005) Proceedings of Interspeech 2005, pp. 1957-1960; Nakamura, K., Janke, M., Wand, M., Schultz, T., Estimation of fundamental frequency from surface electromyographic data: EMGto-FO (2011) Proceedings of International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 573-576; Zahner, M., Janke, M., Wand, M., Schultz, T., Conversion from facial myoelectric signals to speech: A unit selection approach (2014) Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), pp. 1184-1188; Tsuji, T., Bu, N., Arita, J., Ohga, M., A speech synthesizer using facial EMG signals (2008) International Journal of Computational Intelligence and Applications, pp. 1-15; Bocquelet, F., Hueber, T., Girin, L., Badin, P., Vert, B.Y., Robust articulatory speech synthesis using deep neural networks for bci applications (2014) Proceedings of the Annual Conference of the International Speech Communication Association (Lnterspeech), pp. 2288-2292; Janke, M., Wand, M., Nakamura, K., Schultz, T., Further investigations on emg-to-speech conversion (2012) IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 365-368; Imai, S., Cepstral analysis synthesis on the mel frequency scale (1983) IEEE International Conference on Acoustics, Speech, and Signal Processing, 8, pp. 93-96; Wand, M., Schulte, C., Janke, M., Schultz, T., Array-based electromyographic silent speech interface (2013) International Conference on Bio-inspired Systems and Signal Processing (BIOSIGNALS), pp. 89-96; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 331-336; Schultz, T., Wand, M., Modeling coarticulation in large vocabulary emg-based speech recognition (2010) Speech Communication, 52 (4), pp. 341-353; Kominek, J., Black, A.W., The CMU Arctic speech databases (2004) Fifth ISCA Workshop on Speech Synthesis, pp. 223-224; Garofolo, J., Lamel, L., Fisher, W., Fiscus, L., Pallett, D., Dahlgren, N., DARPA TIMlT: Acoustic-phonetic continuous speech corpus (1993) National Institute of Standards and Technology; Yu, D., Eversole, A., Seltzer, M., Yao, K., Huang, Z., Guenter, B., Kuchaiev, O., Slaney, M., An introduction to computational networks and the computational network toolkit (2014) Microsoft Technical Report MSR-TR-2014-112; Janke, M., Wand, M., Heistermann, T., Schultz, T., Prahallad, K., Fundamental frequency generation for whisper-to-audible speech conversion (2014) IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2598-2602; Fukada, T., Tokuda, K., Kobayashi, T., Lmai, S., An adaptive algorithm for mel-cepstral analysis of speech (1992) IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 137-140; Jou, S.-C., Maier-Hein, L., Schultz, T., Waibel, A., Articulatory feature classification using surface electromyography (2006) IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 605-608; Scheme, E.J., Hudgins, B., Parker, P.A., Myoelectric signal classification for phoneme-based speech recognition (2007) IEEE Transactions on Biomedical Engineering, 54 (4), pp. 694-699; Kubichek, R.F., Mel-cepstral distance measure for objective speech quality assessment (1993) IEEE Pacific Rim Conference on Communications, Computers and Signal Processing, pp. 125-128; Kraft, S., Zlzer, U., BeaqleJS: HTML5 and javascript based framework for the subjective evaluation of audio quality (2014) Linux Audio Conference},
sponsors={},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781479919604; 9781479919604; 9781479919604; 9781479919604},
coden={85OFA},
language={English},
abbrev_source_title={Proc Int Jt Conf Neural Networks},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Putze20153375,
author={Putze, F. and Amma, C. and Schultz, T.},
title={Design and evaluation of a self-correcting gesture interface based on error potentials from EEG},
journal={Conference on Human Factors in Computing Systems - Proceedings},
year={2015},
volume={2015-April},
pages={3375-3384},
doi={10.1145/2702123.2702184},
note={cited By 5; Conference of 33rd Annual CHI Conference on Human Factors in Computing Systems, CHI 2015 ; Conference Date: 18 April 2015 Through 23 April 2015;  Conference Code:116824},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951122511&doi=10.1145%2f2702123.2702184&partnerID=40&md5=9bf95300fa711cb42b1c1b431113b14a},
affiliation={Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={Any user interface which automatically interprets the user's input using natural modalities like gestures makes mistakes. System behavior depending on such mistakes will confuse the user and lead to an erroneous interaction flow. The automatic detection of error potentials in electroencephalographic data recorded from a user allows the system to detect such states of confusion and automatically bring the interaction back on track. In this work, we describe the design of such a selfcorrecting gesture interface, implement different strategies to deal with detected errors, use a simulation approach to analyze performance and costs of those strategies and execute a user study to evaluate user satisfaction. We show that selfcorrection significantly improves gesture recognition accuracy at lower costs and with higher acceptance than manual correction. © Copyright 2015 ACM.},
author_keywords={Adaptive interface;  Error-potentials;  Gesture recognition;  Self-correction;  Simulation;  User study},
keywords={Electroencephalography;  Errors;  Human computer interaction;  Human engineering;  User interfaces, Adaptive interface;  Automatic Detection;  Design and evaluations;  Recognition accuracy;  Self-correction;  Simulation;  Simulation approach;  User study, Gesture recognition},
references={Bohus, D., Rudnicky, A.I., (2008) Sorry, I Didn't Catch That! in Recent Trends in Discourse and Dialogue, No. 39 in Text, Speech and Language Technology, pp. 123-154. , Springer Netherlands; Combaz, A., Chumerin, N., Manyakov, N.V., Robben, A., Suykens, J.A.K., Van Hulle, M.M., Towards the detection of error-related potentials and its integration in the context of a p300 speller brain-computer interface (2012) Neurocomputing, 80, pp. 73-82; Ferrez, P.W., Millan, J.D.R., Error-related EEG potentials generated during simulated brain computer interaction (2008) IEEE Transactions on Biomedical Engineering, 55 (3), pp. 923-929; Förster, K., Biasiucci, A., Chavarriaga, R., Millan, J.D.R., Roggen, D., Tröster, G., On the use of brain decoded signals for online user adaptive gesture recognition systems (2010) Pervasive Computing, No. 6030 in Lecture Notes in Computer Science, pp. 427-444. , Springer Berlin Heidelberg; Große, P.W., Holzapfel, H., Waibel, A., Confidence based multimodal fusion for person identification (2008) Proceedings of the 16th International Conference on Multimedia, , New York, USA; Iturrate, I., Chavarriaga, R., Montesano, L., Minguez, J., Millan, J.D.R., Latency correction of error potentials between different experiments reduces calibration time for single-trial classification (2012) Proceedings of Annual International Conference of the Engineering in Medicine and Biology Society. 2012, pp. 3288-3291; Jameson, A.D., Understanding and dealing with usability side effects of intelligent processing (2009) AI Magazine, 30 (4), pp. 23-40; Krusienski, D.J., Sellers, E.W., McFarland, D.J., Vaughan, T.M., Wolpaw, J.R., Toward enhanced p300 speller performance (2008) Journal of Neuroscience Methods, 167 (1), pp. 15-21; Llera, A., Van Gerven, M.A.J., Gómez, V.M., Jensen, O.K., Kappen, H.J., On the use of interaction error potentials for adaptive brain computer interfaces (2011) Neural Networks, 24 (10), pp. 1120-1127; Marathe, A.R., Ries, A.J., McDowell, K., A novel method for single-trial classification in the face of temporal variability (2013) Foundations of Augmented Cognition, pp. 345-352. , no. 8027 in Lecture Notes in Computer Science. Springer Berlin Heidelberg; Margaux, P., Emmanuel, M., ͆ebastien, D., Olivier, B., Jéŕemie, M., Objective and subjective evaluation of online error correction during p300-based spelling (2012) Adv. in Hum.-Comp. Int. 2012; Putze, F., Heger, D., Schultz, T., Reliable subject-adapted recognition of EEG error potentials using limited calibration data (2013) 6th International Conference on Neural Engineering, , San Diego, USA; Schmidt, N.M., Blankertz, B., Treder, M.S., Online detection of error-related potentials boosts the performance of mental typewriters (2012) BMC Neuroscience, 13, pp. 13-19; Scholl, P.M., Van Laerhoven, K., Gordon, D., Scholz, M., Berning, M., JNode: A sensor network platform that supports distributed inertial kinematic monitoring (2012) Proceedings of the Ninth International Conference on Networked Sensing Systems; Spüler, M., Bensch, M., Kleih, S., Rosenstiel, W., Bogdan, M., Kübler, A., Online use of error-related potentials in healthy users and people with severe motor impairment increases performance of a p300-BCI (2012) Clinical Neurophysiology: Official Journal of the International Federation of Clinical Neurophysiology, 123 (7), pp. 1328-1337; Stoyanchev, S., Salletmayr, P., Yang, J., Hirschberg, J., Localized detection of speech recognition errors (2012) Proceedings of the Spoken Language Technology Workshop, pp. 25-30; Vi, C., Subramanian, S., Detecting error-related negativity for interaction design (2012) Proceedings of the Conference on Human Factors in Computing Systems, , New York, USA; Vu, N.T., Kraus, F., Schultz, T., Multilingual a-stabil: A new confidence score for multilingual unsupervised training (2010) Proceedings of the Spoken Language Technology Workshop, pp. 183-188},
sponsors={ACM SIGCHI},
publisher={Association for Computing Machinery},
isbn={9781450331456},
language={English},
abbrev_source_title={Conf Hum Fact Comput Syst Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Amma2015929,
author={Amma, C. and Krings, T. and Böer, J. and Schultz, T.},
title={Advancing muscle-computer interfaces with high-density electromyography},
journal={Conference on Human Factors in Computing Systems - Proceedings},
year={2015},
volume={2015-April},
pages={929-938},
doi={10.1145/2702123.2702501},
note={cited By 64; Conference of 33rd Annual CHI Conference on Human Factors in Computing Systems, CHI 2015 ; Conference Date: 18 April 2015 Through 23 April 2015;  Conference Code:116824},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951109185&doi=10.1145%2f2702123.2702501&partnerID=40&md5=d540309166b0b9ccf9207a9267e2159b},
affiliation={Karlsruhe Institute of Technology (KIT), Germany},
abstract={In this paper we present our results on using electromyographic (EMG) sensor arrays for finger gesture recognition. Sensing muscle activity allows to capture finger motion without placing sensors directly at the hand or fingers and thus may be used to build unobtrusive body-worn interfaces. We use an electrode array with 192 electrodes to record a highdensity EMG of the upper forearm muscles. We present in detail a baseline system for gesture recognition on our dataset, using a naive Bayes classifier to discriminate the 27 gestures. We recorded 25 sessions from 5 subjects. We report an average accuracy of 90% for the within-session scenario, showing the feasibility of the EMG approach to discriminate a large number of subtle gestures. We analyze the effect of the number of used electrodes on the recognition performance and show the benefit of using high numbers of electrodes. Crosssession recognition typically suffers from electrode position changes from session to session. We present two methods to estimate the electrode shift between sessions based on a small amount of calibration data and compare it to a baseline system with no shift compensation. The presented methods raise the accuracy from 59% baseline accuracy to 75% accuracy after shift compensation. The dataset is publicly available. © Copyright 2015 ACM.},
author_keywords={Electrode arrays;  EMG;  Gesture recognition;  Muscle-computer-interfaces},
keywords={Classification (of information);  Classifiers;  Electrodes;  Electromyography;  Human computer interaction;  Human engineering;  Muscle;  Wearable sensors, Baseline accuracy;  Calibration data;  Electrode arrays;  Electromyographic;  EMG;  Muscle activities;  Naive Bayes classifiers;  Shift compensation, Gesture recognition},
references={Ang, K.K., Chin, Z.Y., Zhang, H., Guan, C., Mutual information-based selection of optimal spatial-temporal patterns for single-trial EEG-based bcis (2012) Pattern Recognition, 45 (6), pp. 2137-2144; Assad, C., Wolf, M., Theodoridis, T., Glette, K., Stoica, A., Biosleeve: A natural emg-based interface for hri (2013) Proc. HRI 2013, pp. 69-70; Atzori, M., Gijsberts, A., Kuzborskij, I., Heynen, S., Mittaz Hager, A., Deriaz, O., Castellini, C., Caputo, B., Characterization of a benchmark database for myoelectric movement classification (2014) Neural Systems and Rehabilitation Engineering, IEEE Transactions on PP, 99, pp. 1-1; Castellini, C., Fiorilla, A.E., Sandini, G., Multi-subject/daily-life activity EMG-based control of mechanical hands (2009) Journal of Neuroengineering and Rehabilitation, 6, p. 41. , Jan; Cormen, T.H., Leiserson, C.E., Rivest, R.L., Stein, C., (2001) Introduction to Algorithms, 2. , MIT press Cambridge; De Luca, C.J., The use of surface electromyography in biomechanics (1997) Journal of Applied Biomechanics, 13, pp. 135-163; Heger, D., Putze, F., Amma, C., Wand, M., Plotkin, I., Wielatt, T., Schultz, T., Biosignalsstudio: A flexible framework for biosignal capturing and processing (2010) Proc. KI 2010, pp. 33-39; Hermens, H.J., Freriks, B., Merletti, R., Stegeman, D., Blok, J., Rau, G., Disselhorst-Klug, C., Hägg, G., (1999) European Recommendations for Surface Electromyography, , Roessingh Research and Development The Netherlands; Kim, D., Hilliges, O., Izadi, S., Butler, A.D., Chen, J., Oikonomidis, I., Olivier, P., Digits: Freehand 3d interactions anywhere using a wrist-worn gloveless sensor (2012) Proc. UIST 2012, pp. 167-176; Kim, J., Mastnik, S., Andŕe, E., Emg-based hand gesture recognition for realtime biosignal interfacing (2008) Proc. IUI 2008, pp. 30-39; Merletti, R., Holobar, A., Farina, D., Analysis of motor units with high-density surface electromyography (2008) Journal of Electromyography and Kinesiology, 18 (6), pp. 879-890; Mistry, P., Maes, P., Chang, L., Wuw-wear ur world: A wearable gestural interface (2009) CHI EA, (2009), pp. 4111-4116. , ACM; Muceli, S., Farina, D., Simultaneous and proportional estimation of hand kinematics from emg during mirrored movements at multiple degrees-of-freedom. Neural Systems and Rehabilitation Engineering (2012) IEEE Transactions on, 20 (3), pp. 371-378; Rekimoto, J., Gesturewrist and gesturepad: Unobtrusive wearable interaction devices (2001) Proc. ISWC, (2001), pp. 21-27; Rojas-Martínez, M., Mãnanas, M.A., Alonso, J.F., High-density surface emg maps from upper-arm and forearm muscles (2012) J. Neuroeng. Rehabil, 9, p. 1186. , 85.10; Samadani, A.-A., Kulíc, D., Hand gesture recognition based on surface electromyography (2014) Proc. EMBC 2014, IEEE, pp. 4196-4199; Saponas, T.S., Tan, D.S., Morris, D., Balakrishnan, R., Demonstrating the feasibility of using forearm electromyography for muscle-computer interfaces (2008) Proc. SIGCHI 2008, pp. 515-524; Saponas, T.S., Tan, D.S., Morris, D., Turner, J., Landay, J.A., Making muscle-computer interfaces more practical (2010) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 851-854. , ACM; Starner, T., Auxier, J., Ashbrook, D., Gandy, M., The gesture pendant: A self-illuminating, wearable, infrared computer vision system for home automation control and medical monitoring (2000) Wearable Computers, the Fourth International Symposium On, IEEE, pp. 87-94; Vincent, L., Soille, P., Watersheds in digital spaces: An efficient algorithm based on immersion simulations (1991) IEEE Transactions on Pattern Analysis and Machine Intelligence, 13 (6), pp. 583-598; Wand, M., Schulte, C., Janke, M., Schultz, T., Compensation of recording position shifts for a myoelectric silent speech recognizer (2014) Proc. ICASSP 2014, pp. 2094-2098; Wheeler, K.R., Chang, M.H., Knuth, K.H., Gesture-based control and emg decomposition. Systems, Man, and Cybernetics, Part C: Applications and Reviews (2006) IEEE Transactions on, 36 (4), pp. 503-514; Zhang, H., (2004) The Optimality of Naive Bayes, 1 (2), p. 3. , AA},
sponsors={ACM SIGCHI},
publisher={Association for Computing Machinery},
isbn={9781450331456},
language={English},
abbrev_source_title={Conf Hum Fact Comput Syst Proc},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Adel2015431,
author={Adel, H. and Vu, N.T. and Kirchhoff, K. and Telaar, D. and Schultz, T.},
title={Syntactic and semantic features for Code-Switching factored language models},
journal={IEEE Transactions on Audio, Speech and Language Processing},
year={2015},
volume={23},
number={3},
pages={431-440},
doi={10.1109/TASLP.2015.2389622},
art_number={7005440},
note={cited By 27},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923916293&doi=10.1109%2fTASLP.2015.2389622&partnerID=40&md5=2a8886aa96b2077b3f4c423d42ebf244},
affiliation={Center for Information and Language Processing (CIS), University of Munich, Munich, 80538, Germany; Department of Electrical Engineering, University of Washington, Seattle, WA  98195, United States; Cognitive Systems Lab., Karlsruhe Institute of Technology (KIT), Karlsruhe, 76131, Germany},
abstract={This paper presents our latest investigations on different features for factored language models for Code-Switching speech and their effect on automatic speech recognition (ASR) performance. We focus on syntactic and semantic features which can be extracted from Code-Switching text data and integrate them into factored language models. Different possible factors, such as words, part-of-speech tags, Brown word clusters, open class words and clusters of open class word embeddings are explored. The experimental results reveal that Brown word clusters, part-of-speech tags and open-class words are the most effective at reducing the perplexity of factored language models on the Mandarin-English Code-Switching corpus SEAME. In ASR experiments, the model containing Brown word clusters and part-of-speech tags and the model also including clusters of open class word embeddings yield the best mixed error rate results. In summary, the best language model can significantly reduce the perplexity on the SEAME evaluation set by up to 10.8% relative and the mixed error rate by up to 3.4% relative. © 2014 IEEE.},
author_keywords={Automatic speech recognition (ASR);  natural language processing;  recurrent neural networks},
keywords={Codes (symbols);  Computational linguistics;  Natural language processing systems;  Recurrent neural networks;  Semantics;  Speech;  Switching;  Syntactics, Automatic speech recognition;  Code-switching;  Language model;  Mixed errors;  NAtural language processing;  Part-of-speech tags;  Semantic features;  Word-clusters, Speech recognition},
references={Poplack, S., (1978) Syntactic Structure and Social Function of Code-Switching., , New York, NY, USA: City Univ. of New York, Centro de Estudios Puertorriqueños; Bokamba, E., Are there syntactic constraints on code-mixing? (1989) World Englishes, 8 (3), pp. 277-292; Muysken, P., (2000) Bilingual Speech: A Typology of Code-Mixing., 11. , Cambridge, U.K.: Cambridge Univ. Press; Auer, P., From codeswitching via language mixing to fused lects toward a dynamic typology of bilingual speech (1999) Int. J. Bilingualism, 3 (4), pp. 309-332; Vu, N.T., Adel, H., Schultz, T., An investigation of code-switching attitude dependent language modeling Proc. SLSP, 2013; Poplack, S., Sometimes i'II start a sentence in Spanish y termino en Español: Toward a typology of code-switching (1980) Linguistics, 18 (7-8), pp. 581-618; Solorio, T., Liu, Y., Learning to predict code-switching points Proc. EMNLP, 2008, , ACL; Chan, J., Ching, P., Lee, T., Cao, H., Automatic Speech Recognition of Cantonese-English Code-mixing utterances Proc.Interspeech, 2006; Li, Y., Fung, P., Code-switch language model with inversion constraints for mixed language speech recognition Proc. COLING, 2012; Adel, H., Vu, N.T., Kraus, F., Schlippe, T., Li, H., Schultz, T., Recurrent neural network language modeling for code switching conversational speech Proc. ICASSP, 2013, pp. 8411-8415; Duh, K., Kirchhoff, K., Automatic learning of language model structure Proc. COLING, 2011; El-Desoky, A., Schlüter, R., Ney, H., A hybrid morphologically decomposed factored language models for Arabic LVCSR Proc. NAACL, 2010; Adel, H., Vu, N.T., Schultz, T., Combination of recurrent neural networks and factored language models for code-switching language modeling Proc. ACL, 2013; Adel, H., Kirchhoff, K., Telaar, D., Vu, N.T., Schlippe, T., Schultz, T., Features for factored language models for code-switching speech Proc. SLTU, 2014; Mikolov, T., Yih, W.-T., Zweig, G., Linguistic regularities in continuous space word representations Proc. NAACL, 2013; Lyu, D., Tan, T., Chng, E., Li, H., An analysis of a Mandarin-English code-switching speech corpus: SEAME (2010) Age, 21, pp. 25-28; Toutanova, K., Klein, D., Manning, C., Singer, Y., Feature-rich part-of-speech tagging with a cyclic dependency network Proc. NAACL, 2003; Schultz, T., Fung, P., Burgmer, C., (2009) Detecting Code-Switch Events Based on Textual Features, , Diploma thesis, Karlsruhe Inst. of Technol., Karlsruhe, Germany; Scotton, C.M., (1997) Duelling Languages: Grammatical Structure in Codeswitching., , Oxford, U.K.: Oxford Univ. Press; Toutanova, K., Manning, C., Enriching the knowledge sources used in a maximum entropy part-of-speech tagger Proc. EMNLP/VLC, 2000; Xue, N., Xia, F., Chiou, F., Palmer, M., The Penn Chinese treebank: Phrase structure annotation of a large corpus (2005) Natural Lang. Eng., 11 (2), p. 207; Marcus, M., Marcinkiewicz, M., Santorini, B., Building a large annotated corpus of English: The Penn treebank (1993) Comput. Linguist., 19 (2), pp. 313-330; Brown, P.F., Desouza, P.V., Mercer, R.L., Pietra, V.J.D., Lai, J.C., Class-based n-gram models of natural language (1992) Comput. Linguist., 18 (4), pp. 467-479; Stolcke, A., SRILM - an extensible language modeling toolkit Proc. SLP, 2002, 2; Haffari, G., Razavi, M., Sarkar, A., An ensemble model that combines syntactic and semantic clustering for discriminative dependency parsing Proc. ACL: HLT, 2011, pp. 710-714; Fromkin, V., (2013) An Introduction to Language, , Boston, MA, USA: Cengage Learning; (2014) English Functional Words, , http://www2.fs.u-bunkyo.ac.jp/~gilner/wordlists.html#functionwords, [Online]. Available; (2014) Mandarin Functional Words, , http://chinesenotes.com/topic.php?english=Function+Words, [Online]. Available; MacQueen, J., Some methods for classification and analysis of multivariate observations Proc. Berkeley Symp. Math. Statist. Probab., 1967, pp. 281-297; Dhillon, I.S., Guan, Y., Kulis, B., Weighted graph cuts without eigenvectors: A multilevel approach (2007) IEEE Trans. Pattern Anal. Mach. Intell., 29 (11), pp. 1944-1957. , Nov; Mikolov, T., Karafiát, M., Burget, L., Cernocky, J., Khudanpur, S., Recurrent neural network based language model Proc. Interspeech, 2010; Bodén, M., (2002) A Guide to Recurrent Neural Networks and backpropagation,"The Dallas Project, , SICS Tech. Rep; Mikolov, T., Kombrink, S., Burget, L., Cernocky, J., Khudanpur, S., Extensions of recurrent neural network language model Proc. ICASSP, 2011, pp. 5528-5531; Mikolov, T., Kombrink, S., Deoras, A., Burget, L., Cernocký, J., RNNLM-recurrent neural network language modeling toolkit Proc. ASRU, 2011; Rosenfeld, R., Two decades of statistical language modeling: Where do we go from here? (2000) Proc. IEEE, 88 (8), pp. 1270-1278. , Aug; Kirchhoff, K., Bilmes, J., Duh, K., (2007) Factored Language Models Tutorial Dept of Elect. Eng., Univ. Of Washington, , Seattle, WA, Tech. Rep; Bilmes, J., Kirchhoff, K., Factored language models and generalized parallel backoff Proc. NAACL, 2003; Ray, S., Turi, R., Determination of number of clusters in k-means clustering and application in colour image segmentation Proc. ICAPRDT, 1999; Vu, N.T., Lyu, D., Weiner, J., Telaar, D., Schlippe, T., Blaicher, F., Chng, E., Li, H., A first speech recognition system for Mandarin-English code-switch conversational speech Proc. ICASSP, 2012; Telaar, D., Wand, M., Gehrig, D., Putze, F., Amma, C., Heger, D., Vu, N., Schultz, T., Automatic speech recognition of Cantonese-English code-mixing utterances Proc.Interspeech, 2014},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15587916},
language={English},
abbrev_source_title={IEEE Trans. Audio Speech Lang. Process.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Diener20152420,
author={Diener, L. and Janke, M. and Schultz, T.},
title={Codebook clustering for unit selection based EMG-to-speech conversion},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2015},
volume={2015-January},
pages={2420-2424},
note={cited By 3; Conference of 16th Annual Conference of the International Speech Communication Association, INTERSPEECH 2015 ; Conference Date: 6 September 2015 Through 10 September 2015;  Conference Code:118697},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959162314&partnerID=40&md5=136e27fe0bffe80e5a18b000240f5643},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany},
abstract={This paper reports on our recent advances in using Unit Selection to directly synthesize speech from facial surface electromyographic (EMG) signals generated by movement of the articulatory muscles during speech production. We achieve a robust Unit Selection mapping by using a more sophisticated unit codebook. This codebook is generated from a set of base units using a two stage unit clustering process. The units are first clustered based on the audio-, and afterwards on the EMG feature vectors they cover, and a new codebook is generated using these cluster assignments. We evaluate different cluster counts for both stages and revisit our evaluation of unit sizes in light of this clustering approach. Our final system achieves a significantly better Mel-Cepstral distortion score than the Unit Selection based EMG-to-Speech conversion system from our previous work while, due to the reduced codebook size, taking less time to perform the conversion. Copyright © 2015 ISCA.},
author_keywords={Electromyography;  Silent speech interface;  Unit selection},
keywords={Clustering algorithms;  Electromyography;  Speech, Cluster assignment;  Clustering approach;  Clustering process;  Electromyographic signal;  Silent speech interfaces;  Speech conversion;  Speech production;  Unit selection, Speech communication},
references={Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J.M., Brumberg, J.S., Silent speech interfaces (2010) Speech Communication, 52 (4), pp. 270-287; Chan, A., Englehart, K., Hudgins, B., Lovely, D., Hidden markov model classification of myoelectric signals in speech (2002) Engineering in Medicine and Biology Magazine, 21 (5), pp. 143-146; Toda, T., Shikano, K., Nam-to-speech conversion with Gaussian mixture models (2005) Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), pp. 1957-1960; Fagan, M., Ell, S., Gilbert, J., Sarrazin, E., Chapman, P., Development of a (silent) speech recognition system for patients following laryngectomy (2008) Medical Engineering and Physics, 30 (4), pp. 419-425; Toth, A.R., Wand, M., Schultz, T., Synthesizing speech from electromyography using voice transformation techniques (2009) Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), pp. 652-655; Jou, S.-C., Maier-Hein, L., Schultz, T., Waibel, A., Articulatory feature classification using surface electromyography (2006) IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1, pp. 605-608; Zahner, M., Janke, M., Wand, M., Schultz, T., Conversion from facial myoelectric signals to speech: A unit selection approach (2014) Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech); Hunt, A.J., Ai, W.B., Unit selection in a concatenative speech synthesis system using a large speech database (1996) IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 373-376; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) 2005 IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 331-336; Wand, M., Schulte, C., Janke, M., Schultz, T., Array-based electromyographic silent speech interface (2013) International Conference on Bio-inspired Systems and Signal Processing (BIOSIGNALS), pp. 89-96; Schultz, T., Wand, M., Modeling coarticulation in emg-based continuous speech recognition (2010) Speech Communication, 52 (4), pp. 341-353; Fukada, T., Tokuda, K., Kobayashi, T., Imai, S., An adaptive algorithm for mel-cepstral analysis of speech (1992) IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1, pp. 137-140; Imai, S., Cepstral analysis synthesis on the mel frequency scale (1983) IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 8, pp. 93-96; Jou, S.-C.S., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), pp. 573-576; Kubichek, R.F., Mel-cepstral distance measure for objective speech quality assessment (1993) IEEE Pacific Rim Conference on Communications, Computers and Signal Processing, pp. 125-128; Kraft, S., Zoelzer, U., Beaqlejs: Html5 and javascript based framework for the subjective evaluation of audio quality (2014) Linux Audio Conference},
editor={Noth E., Steidl S., Moller S., Ney H., Mobius B.},
sponsors={Alibaba Group; Amazon; et al.; Facebook; Google; Telekom Innovation Laboratories},
publisher={International Speech and Communication Association},
issn={2308457X},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Georgi2015308,
author={Georgi, M. and Amma, C. and Schultz, T.},
title={Fusion and comparison of IMU and EMG signals for wearable gesture recognition},
journal={Communications in Computer and Information Science},
year={2015},
volume={574},
pages={308-323},
doi={10.1007/978-3-319-27707-3_19},
note={cited By 3; Conference of 8th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2015 ; Conference Date: 12 January 2015 Through 15 January 2015;  Conference Code:160529},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955251103&doi=10.1007%2f978-3-319-27707-3_19&partnerID=40&md5=919f78417a36cccc7d7c423cd02c58ac},
affiliation={Cognitive Systems Lab, Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={We evaluate the performance of a wearable gesture recognition system for arm, hand, and finger motions, using the signals of an Inertial Measurement Unit (IMU) worn at the wrist, and the Electromyogram (EMG) of muscles in the forearm. A set of 12 gestures was defined, similar to manipulatory movements and to gestures known from the interaction with mobile devices. We recorded performances of our gesture set by five subjects in multiple sessions. The resulting data corpus is made publicly available to build a common ground for future evaluations and benchmarks. Hidden Markov Models (HMMs) are used as classifiers to discriminate between the defined gesture classes. We achieve a recognition rate of 97.8% in session-independent, and of 74.3% in person-independent recognition. We give a detailed analysis of error characteristics and of the influence of each modality to the results to underline the benefits of using both modalities together. © Springer International Publishing Switzerland 2015.},
author_keywords={Electromyography;  Gesture recognition;  Inertial Measurement Unit;  Wearable computing},
keywords={Biomedical engineering;  Biomedical signal processing;  Electromyography;  Hidden Markov models;  Markov processes;  Mobile devices;  Units of measurement;  Wearable sensors;  Wearable technology, Electromyogram;  Error characteristics;  Gesture recognition system;  Hidden markov models (HMMs);  Inertial measurement unit;  Multiple sessions;  Person-independent;  Wearable computing, Gesture recognition},
funding_details={GoogleGoogle},
funding_text 1={This research was partly funded by Google through a Google Faculty Research Award.},
references={Alibali, M.W., Gesture in spatial cognition: Expressing, communicating, and thinking about spatial information (2005) Spat. Cogn. Comput, 5 (4), pp. 307-331; Amma, C., Georgi, M., Schultz, T., Airwriting: A wearable handwriting recognition system (2014) Pers. Ubiquit. Comput, 18 (1), pp. 191-203. , http://link.springer.com/10.1007/s00779-013-0637-3; Benbasat, A.Y., Paradiso, J.A., An inertial measurement framework for gesture recognition and applications (2002) GW 2001. LNCS (LNAI), 2298, pp. 9-20. , Wachsmuth, I., Sowa, T, Springer, Heidelberg; Chen, X., Zhang, X., Zhao, Z., Yang, J., Lantz, V., Wang, K., Hand gesture recognition research based on surface emg sensors and 2d-accelerometers (2007) 2007 11Th IEEE International Symposium on Wearable Computers, pp. 11-14. , http://dblp.org/db/conf/iswc/iswc2007.html#ChenZZYLW07; Cho, S.J., Oh, J.K., Bang, W.C., Chang, W., Choi, E., Jing, Y., Cho, J., Kim, D.Y., Magic wand: A hand-drawn gesture input device in 3-d space with inertial sensors (2004) Ninth International Workshop on Frontiers in Handwriting Recognition, 2004 IWFHR-9 2004, pp. 106-111. , October; Hartmann, B., Link, N., Gesture recognition with inertial sensors and optimized dtw prototypes (2010) Proceedings of the IEEE International Conference on Systems, pp. 2102-2109. , http://dblp.org/db/conf/smc/smc2010.html#HartmannL10, Man and Cybernetics, Istanbul, Turkey, 10-13 October 2010, Proceedings of the IEEE International Conference on Systems, Man and Cybernetics, Istanbul, Turkey, 10-13 October, IEEE (Oct 2010); Hauptmann, A.G., McAvinney, P., Gestures with speech for graphic manipulation (1993) Int. J. Man Mach. Stud, 38 (2), pp. 231-249. , http://linkinghub.elsevier.com/retrieve/doi/10.1006/imms.1993.1011; Kim, D., Hilliges, O., Izadi, S., Butler, A.D., Chen, J., Oikonomidis, I., Olivier, P., Digits: Freehand 3d interactions anywhere using a wrist-worn gloveless sensor (2012) Proceedings of the 25Th Annual ACM Symposium on User Interface Software and Technology, pp. 167-176. , http://doi.acm.org/10.1145/2380116.2380139, UIST 2012, ACM, New York, NY, USA; Kim, J., Mastnik, S., Ré, E., Emg-based hand gesture recognition for realtime biosignal interfacing (2008) Proceedings of the 13Th International Conference on Intelligent User Interfaces. IUI 2008, 39, pp. 30-39. , http://portal.acm.org/citation.cfm?doid=1378773.1378778, ACM Press, New York, NY, USA; Li, Y., Chen, X., Tian, J., Zhang, X., Wang, K., Yang, J., Automatic recognition of sign language subwords based on portable accelerometer and emg sensors (2010) International Conference on Multimodal Interfaces and the Workshop on Machine Learning for Multimodal Interaction, pp. 17-21. , http://portal.acm.org/citation.cfm?id=1891926, ICMI-MLMI 2010, ACM, New York, NY, USA; Mistry, P., Maes, P., Chang, L., WUW-wear Ur world: A wearable gestural interface (2009) Proceedings of CHI, pp. 4111-4116. , http://dl.acm.org/citation.cfm?id=1520626/npapers://c80d98e4-9a96-4487-8d06-8e1acc780d86/Paper/p10196, (2009); Oviatt, S., Ten myths of multimodal interaction (1999) Commun. ACM, 42 (11), pp. 74-81; Rabiner, L., A tutorial on hidden markov models and selected applications in speech recognition (1989) Proc. IEEE, 77 (2), pp. 257-286; Rekimoto, J., GestureWrist and GesturePad: Unobtrusive wearable interaction devices (2001) Proceedings Fifth International Symposium on Wearable Computers; Samadani, A.A., Kulic, D., Hand gesture recognition based on surface electromyography (2014) 2014 36Th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC); Saponas, T.S., Tan, D.S., Morris, D., Balakrishnan, R., Demonstrating the feasibility of using forearm electromyography for muscle-computer interfaces (2008) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 515-524. , http://portal.acm.org/citation.cfm?doid=1357054.1357138, CHI 2008, ACM Press, New York, New York, USA; Wolf, M.T., Assad, C., Stoica, A., You, K., Jethani, H., Vernacchia, M.T., Fromm, J., Iwashita, Y., Decoding static and dynamic arm and hand gestures from the jpl biosleeve (2013) 2013 IEEE Aerospace Conference, pp. 1-9. , http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6497171&isnumber=6496810; Zhang, X., Chen, X., Li, Y., Lantz, V., Wang, K., Yang, J., A framework for hand gesture recognition based on accelerometer and emg sensors (2011) IEEE Trans. Syst., Man Cybern., Part A: Syst. Humans, 41 (6), pp. 1064-1076},
correspondence_address1={Georgi, M.; Cognitive Systems Lab, Institute for Anthropomatics and Robotics, Karlsruhe Institute of TechnologyGermany; email: marcus.georgi@kit.edu},
editor={Elias D., Fred A., Gamboa H.},
sponsors={Control and Communication (INSTICC); Institute for Systems and Technologies of Information},
publisher={Springer Verlag},
issn={18650929},
isbn={9783319277066},
language={English},
abbrev_source_title={Commun. Comput. Info. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Georgi201599,
author={Georgi, M. and Amma, C. and Schultz, T.},
title={Recognizing hand and finger gestures with IMU based motion and EMG based muscle activity sensing},
journal={BIOSIGNALS 2015 - 8th International Conference on Bio-Inspired Systems and Signal Processing, Proceedings; Part of 8th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2015},
year={2015},
pages={99-108},
doi={10.5220/0005276900990108},
note={cited By 58; Conference of 8th International Conference on Bio-Inspired Systems and Signal Processing, BIOSIGNALS 2015 ; Conference Date: 12 January 2015 Through 15 January 2015;  Conference Code:112655},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938872035&doi=10.5220%2f0005276900990108&partnerID=40&md5=546e9f7651388a3d611b5f3c2d6bad73},
affiliation={Cognitive Systems Lab, Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={Session-and person-independent recognition of hand and finger gestures is of utmost importance for the practicality of gesture based interfaces. In this paper we evaluate the performance of a wearable gesture recognition system that captures arm, hand, and finger motions by measuring movements of, and muscle activity at the forearm. We fuse the signals of an Inertial Measurement Unit (IMU) worn at the wrist, and the Electromyogram (EMG) of muscles in the forearm to infer hand and finger movements. A set of 12 gestures was defined, motivated by their similarity to actual physical manipulations and to gestures known from the interaction with mobile devices. We recorded performances of our gesture set by five subjects in multiple sessions. The resulting datacorpus will be made publicly available to build a common ground for future evaluations and benchmarks. Hidden Markov Models (HMMs) are used as classifiers to discriminate between the defined gesture classes. We achieve a recognition rate of 97.8% in session-independent, and of 74.3% in person-independent recognition. Additionally, we give a detailed analysis of error characteristics and of the influence of each modality to the results to underline the benefits of using both modalities together.},
author_keywords={Electromyography;  Gesture Recognition;  Inertial Measurement Unit;  Wearable Computing},
keywords={Biomedical engineering;  Biomimetics;  Electromyography;  Hidden Markov models;  Muscle;  Palmprint recognition;  Signal processing;  Wearable technology, Error characteristics;  Gesture recognition system;  Gesture-based interface;  Hidden markov models (HMMs);  Inertial measurement unit;  Multiple sessions;  Person-independent;  Wearable computing, Gesture recognition},
references={Alibali, M.W., Gesture in spatial cognition: Expressing, communicating, and thinking about spatial information (2005) Spatial Cognition Computation, 5 (4), pp. 307-331; Amma, C., Georgi, M., Schultz, T., Airwriting: A wearable handwriting recognition system (2014) Personal and Ubiquitous Computing, 18 (1), pp. 191-203; Benbasat, A.Y., Paradiso, J.A., An inertial measurement framework for gesture recognition and applications (2001) Revised Papers from the International Gesture Workshop on Gesture and Sign Languages in Human-Computer Interaction, Volume LNCS 2298 of Lecture Notes in Computer Science, pp. 9-20. , Springer-Verlag. InWachsmuth, I. and Sowa, T., editors; Chen, X., Zhang, X., Zhao, Z., Yang, J., Lantz, V., Wang, K., Hand gesture recognition research based on surface emg sensors and 2d-accelerometers (2007) Wearable Computers, 2007 11th IEEE International Symposium on, pp. 11-14; Cho, S.-J., Oh, J.-K., Bang, W.-C., Chang, W., Choi, E., Jing, Y., Cho, J., Kim, D.-Y., Magic wand: A hand-drawn gesture input device in 3-d space with inertial sensors (2004) Frontiers in Handwriting Recognition 2004 IWFHR-9 2004 Ninth International Workshop on, pp. 106-111; Hartmann, B., Link, N., Gesture recognition with inertial sensors and optimized dtw prototypes (2010) Proceedings of the IEEE International Conference on Systems, Man and Cybernetics, pp. 2102-2109. , Istanbul, Turkey, 10-13 October 2010, Proceedings of the IEEE International Conference on Systems, Man and Cybernetics, Istanbul, Turkey, 10-13 October 2010. IEEE; Hauptmann, A.G., McAvinney, P., Gestures with speech for graphic manipulation (1993) International Journal of ManMachine Studies, 38 (2), pp. 231-249; Kim, D., Hilliges, O., Izadi, S., Butler, A.D., Chen, J., Oikonomidis, I., Olivier, P., Digits: Freehand 3d interactions anywhere using a wrist-worn gloveless sensor (2012) Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology, UIST '12, pp. 167-176. , New York, NY, USA. ACM; Kim, J., Mastnik, S., André, E., Emg-based hand gesture recognition for realtime biosignal interfacing (2008) Proceedings of the 13th International Conference on Intelligent User Interfaces, Volume 39 of IUI '08, pp. 30-39. , New York, NY, USA. ACM Press; Li, Y., Chen, X., Tian, J., Zhang, X., Wang, K., Yang, J., Automatic recognition of sign language subwords based on portable accelerometer and emg sensors (2010) International Conference on Multimodal Interfaces and the Workshop on Machine Learning for Multimodal Interaction, ICMI-MLMI '10, pp. 17-21. , New York, NY, USA. ACM; Mistry, P., Maes, P., Chang, L., WUW-wear Ur world: A wearable gestural interface (2009) Proceedings of CHI 2009, pp. 4111-4116; Oviatt, S., Ten myths of multimodal interaction (1999) Communications of the ACM, 42 (11), pp. 74-81; Rabiner, L., A tutorial on hidden markov models and selected applications in speech recognition (1989) Proceedings of the IEEE, 77 (2), pp. 257-286; Rekimoto, J., GestureWrist and GesturePad: Unobtrusive wearable interaction devices (2001) Proceedings Fifth International Symposium on Wearable Computers; Samadani, A.-A., Kulic, D., Hand gesture recognition based on surface electromyography (2014) Engineering in Medicine and Biology Society (EMBC), 2014 36th Annual International Conference of the IEEE; Saponas, T.S., Tan, D.S., Morris, D., Balakrishnan, R., Demonstrating the feasibility of using forearm electromyography for muscle-computer interfaces (2008) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI '08, pp. 515-524. , New York, New York, USA. ACM Press; Wolf, M.T., Assad, C., Stoica, A., You, K., Jethani, H., Vernacchia, M.T., Fromm, J., Iwashita, Y., Decoding static and dynamic arm and hand gestures from the jpl biosleeve (2013) Aerospace Conference, 2013 IEEE, pp. 1-9; Zhang, X., Chen, X., Li, Y., Lantz, V., Wang, K., Yang, J., A framework for hand gesture recognition based on accelerometer and emg sensors (2011) Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on, 41 (6), pp. 1064-1076},
editor={Loose H., Fred A., Gamboa H., Elias D.},
sponsors={Institute for Systems and Technologies of Information, Control and Communication (INSTICC)},
publisher={SciTePress},
isbn={9789897580697},
language={English},
abbrev_source_title={BIOSIGNALS - Int. Conf. Bio-Inspired Syst. Signal Process., Proc.; Part Int. Jt. Conf. Biomed. Eng. Syst. Technol., BIOSTEC},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Plantier2015,
author={Plantier, G. and Schultz, T. and Fred, A. and Gamboa, H.},
title={Biomedical Engineering Systems and Technologies: 7th International Joint Conference, BIOSTEC 2014 Angers, France, March 3–6, 2014 Revised Selected Papers},
journal={Communications in Computer and Information Science},
year={2015},
volume={511},
note={cited By 0; Conference of 7th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2014 ; Conference Date: 3 March 2014 Through 6 March 2014;  Conference Code:160609},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955312954&partnerID=40&md5=f338410dea8544dc10aa5e3269675182},
affiliation={ESEO, Angers Cedex 02, France; Cognitive Systems Lab., Karlsruhe Institute of Technology, Karlsruhe, Baden-Württemberg, Germany; Technical University of Lisbon, Lisbon, Portugal; New University of Lisbon, Lisboa, Portugal},
editor={Plantier G., Schultz T., Fred A., Gamboa H.},
sponsors={Institute for Systems and Technologies of Information, Control and Communication (INSTICC)},
publisher={Springer Verlag},
issn={18650929},
isbn={9783319261287},
language={English},
abbrev_source_title={Commun. Comput. Info. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Stahlberg20155823,
author={Stahlberg, F. and Schlippe, T. and Vogel, S. and Schultz, T.},
title={Cross-lingual lexical language discovery from audio data using multiple translations},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2015},
volume={2015-August},
pages={5823-5827},
doi={10.1109/ICASSP.2015.7179088},
art_number={7179088},
note={cited By 0; Conference of 40th IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2015 ; Conference Date: 19 April 2014 Through 24 April 2014;  Conference Code:116006},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946057599&doi=10.1109%2fICASSP.2015.7179088&partnerID=40&md5=e94bb039b66ad1bc5e0f9d962dfe5e7b},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology, Germany; Qatar Computing Research Institute, Qatar Foundation, Doha, Qatar},
abstract={Zero-resource Automatic Speech Recognition (ZR ASR) addresses target languages without given pronunciation dictionary, transcribed speech, and language model. Lexical discovery for ZR ASR aims to extract word-like chunks from speech. Lexical discovery benefits from the availability of written translations in another source language [1, 2, 3]. In this paper, we improve lexical discovery even more by combining multiple source languages. We present a novel method for combining noisy word segmentations resulting in up to 11.2% relative F-score gain. When we extract word pronunciations from the combined segmentations to bootstrap an ASR system, we improve accuracy by 9.1% relative compared to the best system with only one translation, and by 50.1% compared to monolingual lexical discovery. © 2015 IEEE.},
author_keywords={Lexical language discovery;  non-written languages;  word-to-phoneme alignment;  zero-resource automatic speech recognition},
references={Stüker, S., Waibel, A., Towards human translations guided language discovery for ASR systems (2008) SLTU; Stahlberg, F., Schlippe, T., Vogel, S., Schultz, T., Word segmentation through cross-lingual word-to-phoneme alignment (2012) SLT; Stahlberg, F., Schlippe, T., Vogel, S., Schultz, T., Pronunciation extraction from phoneme sequences through cross-lingual word-to-phoneme alignment (2013) SLSP; Besacier, L., Barnard, E., Karpov, A., Schultz, T., Automatic speech recognition for under-resourced languages: A survey (2014) Speech Communication; Schlippe, T., Ochs, S., Schultz, T., Web-based tools and methods for rapid pronunciation dictionary creation (2014) Speech Communication; Gelas, H., Abate, S.T., Besacier, L., Pellegrino, F., Quality assessment of crowdsourcing transcriptions for african languages (2011) Interspeech; Kanthak, S., Ney, H., Context-dependent acoustic modeling using graphemes for large vocabulary speech recognition (2002) ICASSP; Schultz, T., Waibel, A., Language-independent and language-adaptive acoustic modeling for speech recognition (2001) Speech Communication; Jansen, A., Dupoux, E., Goldwater, S., Johnson, M., Khudanpur, S., Church, K., Feldman, N., A summary of the 2012 JHU CLSP workshop on zero resource speech technologies and models of early language acquisition (2013) ICASSP; Johnson, M., Goldwater, S., Improving Non-parameteric Bayesian inference: Experiments on unsupervised word segmentation with adaptor grammars (2009) HLT-NAACL; Heymann, J., Walter, O., Haeb-Umbach, R., Raj, B., Unsupervised word segmentation from noisy input (2013) ASRU. IEEE; Mochihashi, D., Yamada, T., Ueda, N., Bayesian unsupervised word segmentation with nested pitman-yor language modeling (2009) ACL; Stahlberg, F., Schlippe, T., Vogel, S., Schultz, T., Word segmentation and pronunciation extraction from phoneme sequences through cross-lingual word-to-phoneme alignment (2014) Computer Speech & Language; Stahlberg, F., Schlippe, T., Vogel, S., Schultz, T., Towards automatic speech recognition without pronunciation dictionary, transcribed speech and text resources in the target language using cross-lingual word-to-phoneme alignment (2014) SLTU; Cysouw, M., Wälchli, B., Parallel texts: Using translational equivalents in linguistic typology (2007) STUF-Sprachtypologie und Universalienforschung; Mayer, T., Cysouw, M., Creating a massively parallel bible corpus (2013) Oceania; Creson, B., (2014) Wycliffe Bible Translators, , http//wycliffe.org/, Accessed on 4th October 2014; Abdelali, A., Guzman, F., Sajjad, H., Vogel, S., The amara corpus: Building parallel language resources for the educational domain (2014) LREC; Koehn, P., Europarl: A parallel corpus for statistical machine translation (2005) MT Summit; Wälchli, B., (2005) Co-Compounds and Natural Coordination, , Oxford University Press; Mayer, T., Cysouw, M., Language comparison through sparse multilingual word alignment (2012) EACL; Ostling, R., Bayesian word alignment for massively parallel texts (2014) EACL; Och, F.J., Ney, H., A systematic comparison of various statistical alignment models (2003) Computational Linguistics; (2001) The Holy Bible: English Standard Version, , Crossway; Weide, R., (2005) The Carnegie Mellon Pronouncing Dictionary 0. 6; Lee, C., Glass, J., A nonparametric Bayesian approach to acoustic model discovery (2012) ACL-HLT; Gales, M.J.F., Knill, K.M., Ragni, A., Rath, P.R., Speech recognition and keyword spotting for low resource languages: Babel project research at cued (2014) SLTU; Varadarajan, B., Khudanpur, S., Dupoux, E., Unsupervised learning of acoustic sub-word units (2008) ACL-HLT},
sponsors={The Institute of Electrical and Electronics Engineers Signal Processing Society},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15206149},
isbn={9781467369978},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Heger201582,
author={Heger, D. and Herff, C. and Putze, F. and Schultz, T.},
title={Joint optimization for discriminative, compact and robust Brain-Computer Interfacing},
journal={International IEEE/EMBS Conference on Neural Engineering, NER},
year={2015},
volume={2015-July},
pages={82-85},
doi={10.1109/NER.2015.7146565},
art_number={7146565},
note={cited By 1; Conference of 7th International IEEE/EMBS Conference on Neural Engineering, NER 2015 ; Conference Date: 22 April 2015 Through 24 April 2015;  Conference Code:113593},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940392010&doi=10.1109%2fNER.2015.7146565&partnerID=40&md5=bb92479e5524e7898aa5e26692978340},
affiliation={Cognitive Systems Lab, Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Adenauerring 4, Karlsruhe, 76131, Germany},
abstract={We present a new pattern recognition framework for Brain-Computer Interfacing that learns discriminative brain activity patterns, compact modeling, and robustness against signal variabilities by a single joint optimization. We present an algorithm based on the Alternating Direction Method of Multipliers, which finds an optimal solution for this approach extremely efficiently. A first evaluation using a publicly available EEG motor imagery data corpus with 105 subjects shows that our framework outperformed state-of-the-art methods and successfully performed subject transfer. © 2015 IEEE.},
keywords={Brain;  Pattern recognition, Alternating direction method of multipliers;  Brain activity patterns;  Brain-computer interfacing;  Joint optimization;  Motor imagery;  Optimal solutions;  Signal variability;  State-of-the-art methods, Brain computer interface},
references={Wolpaw, J., Wolpaw, E.W., (2011) Brain-computer Interfaces: Principles and Practice, , Oxford University Press; Müller, K.-R., Krauledat, M., Dornhege, G., Curio, G., Blankertz, B., (2004) Machine Learning Techniques for Brain-computer Interfaces; Vapnik, V., (2000) The Nature of Statistical Learning Theory, , springer; Tomioka, R., Müller, K.-R., A regularized discriminative framework for EEG analysis with application to brain-computer interface (2010) Neuroimage, 49 (1), pp. 415-432; Tomioka, R., Sugiyama, M., Dual-augmented lagrangian method for efficient sparse reconstruction (2009) Signal Processing Letters, IEEE, 16 (12), pp. 1067-1070; Kothe, C.A., Makeig, S., Bcilab: A platform for brain-computer interface development (2013) Journal of Neural Engineering, 10 (5), p. 056014; Samek, W., Kawanabe, M., Müller, K.-R., Divergence-based framework for common spatial patterns algorithms (2014) IEEE Reviews in Biomedical Engineering, (99), pp. 1-1; Fazli, S., Popescu, F., Danóczy, M., Blankertz, B., Müller, K., Grozea, C., Subject-independent mental state classification in single trials (2009) Neural Networks, 22 (9), pp. 1305-1312; Tu, W., Sun, S., A subject transfer framework for EEG classification (2011) Neurocomputing; Kang, H., Nam, Y., Choi, S., Composite common spatial pattern for subject-to-subject transfer (2009) Signal Processing Letters, IEEE, 16 (8), pp. 683-686; Kang, H., Choi, S., Bayesian multi-task learning for common spatial patterns (2011) Pattern Recognition in NeuroImaging (PRNI), 2011 International Workshop On. IEEE, pp. 61-64; Reuderink, B., Farquhar, J., Poel, M., Nijholt, A., A subjectindependent brain-computer interface based on smoothed, secondorder baselining (2011) IEEE Engineering in Medicine and Biology Society, 2011. IEEE, pp. 4600-4604; Arvaneh, M., Robertson, I., Ward, T.E., Subject-to-subject adaptation to reduce calibration time in motor imagery-based brain-computer interface (2014) Engineering in Medicine and Biology Society (EMBC), 2014 36th Annual International Conference of the IEEE. IEEE, pp. 6501-6504; Boyd, S., Parikh, N., Chu, E., Peleato, B., Eckstein, J., Distributed optimization and statistical learning via the alternating direction method of multipliers (2011) Foundations and Trends in Machine Learning, 3 (1), pp. 1-122; Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P., Mark, R., Mietus, J., Stanley, H., Physiobank, physiotoolkit, and physionet: Components of a new research resource for complex physiologic signals (2000) Circulation, 101 (23), pp. e215-e220; Schalk, G., McFarland, D., Hinterberger, T., Birbaumer, N., Wolpaw, J., Bci2000: A general-purpose brain-computer interface (bci) system (2004) Biomedical Engineering, IEEE Transactions on, 51 (6), pp. 1034-1043; Ledoit, O., Wolf, M., A well-conditioned estimator for largedimensional covariance matrices (2004) Journal of Multivariate Analysis, 88 (2), pp. 365-411; Duda, R.O., Hart, P.E., Stork, D.G., (1999) Pattern Classification, , John Wiley & Sons; Von Bünau, P., Meinecke, F.C., Király, F.C., Müller, K.-R., Finding stationary subspaces in multivariate time series (2009) Physical Review Letters, 103 (21), p. 214101; Samek, W., Vidaurre, C., Müller, K.-R., Kawanabe, M., Stationary common spatial patterns for brain-computer interfacing (2012) Journal of Neural Engineering, 9 (2), p. 026013; Lotte, F., Guan, C., Regularizing common spatial patterns to improve bci designs: Unified theory and new algorithms (2011) Biomedical Engineering, IEEE Transactions on, 58 (2), pp. 355-362; Lotte, F., Congedo, M., Lécuyer, A., Lamarche, F., Arnaldi, B., A review of classification algorithms for EEG-based brain-computer interfaces (2007) Journal of Neural Engineering, 4},
sponsors={},
publisher={IEEE Computer Society},
issn={19483546},
isbn={9781467363891},
language={English},
abbrev_source_title={Int. IEEE/EMBS Conf. Neural Eng., NER},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Heger20151131,
author={Heger, D. and Herff, C. and De Pesters, A. and Telaar, D. and Brunner, P. and Schalk, G. and Schultz, T.},
title={Continuous speech recognition from ECoG},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2015},
volume={2015-January},
pages={1131-1135},
note={cited By 3; Conference of 16th Annual Conference of the International Speech Communication Association, INTERSPEECH 2015 ; Conference Date: 6 September 2015 Through 10 September 2015;  Conference Code:118697},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959177532&partnerID=40&md5=aa8bb5317305bd242597beb189b0ac25},
affiliation={Cognitive Systems Lab, Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Germany; National Center for Adaptive Neurotechnologies, Wadsworth Center, New York State Department of Health, Albany, NY, United States; Department of Neurology, Albany Medical College, Albany, United States; Department of Biomedical Sciences, State University of New York at Albany, Albany, NY, United States},
abstract={Continuous speech production is a highly complex process involving many parts of the human brain. To date, no fundamental representation that allows for decoding of continuous speech from neural signals has been presented. Here we show that techniques from automatic speech recognition can be applied to decode a textual representation of spoken words from neural signals. We model phones as the fundamental unit of the speech process in invasively measured brain activity (intracranial electrocorticographic (ECoG)) recordings. These phone models give insights into timings and locations of neural processes associated with the continuous production of speech and can be used in a speech recognizer to decode the neural data into their textual representations. When restricting the dictionary to small subsets, Word Error Rates as low as 25% can be achieved. As the brain activity data sets are fairly small, alternative approaches to Gaussian models are investigated by relying on robust, regularized discriminative models. Copyright © 2015 ISCA.},
author_keywords={Brain-computer interface;  ECoG;  Electrocorticography;  Speech recognition},
keywords={Brain;  Brain computer interface;  Continuous speech recognition;  Decoding;  Electroencephalography;  Electrophysiology;  Interfaces (computer);  Neurophysiology;  Speech;  Speech communication;  Speech processing;  Telephone sets, Automatic speech recognition;  Continuous production;  Continuous speech;  Discriminative models;  ECoG;  Electrocorticography;  Fundamental units;  Textual representation, Speech recognition},
references={Chang, E.F., Rieger, J.W., Johnson, K., Berger, M.S., Barbaro, N.M., Knight, R.T., Categorical speech representation in human superior temporal gyrus (2010) Nature Neuroscience, 13 (11), pp. 1428-1432; Mesgarani, N., Cheung, C., Johnson, K., Chang, E.F., Phonetic feature encoding in human superior temporal gyrus (2014) Science, p. 1245994; Crone, N., Hao, L., Hart, J., Boatman, D., Lesser, R., Irizarry, R., Gordon, B., Electrocorticographic gamma activity during word production in spoken and sign language (2001) Neurology, 57 (11), pp. 2045-2053; Crone, N.E., Boatman, D., Gordon, B., Hao, L., Induced electrocorticographic gamma activity during auditory perception (2001) Clinical Neurophysiology, 112 (4), pp. 565-582; Leuthardt, E.C., Gaona, C., Sharma, M., Szrama, N., Roland, J., Freudenberg, Z., Solis, J., Schalk, G., Using the electrocorticographic speech network to control a brain-computer interface in humans (2011) Journal of Neural Engineering, 8 (3), p. 036004; Guenther, F.H., Brumberg, J.S., Wright, E.J., Nieto-Castanon, A., Tourville, J.A., Panko, M., Law, R., Andreasen, D.S., A wireless brain-machine interface for real-time speech synthesis (2009) PloS One, 4 (12), p. e8218; Formisano, E., De Martino, F., Bonte, M., Goebel, R., Who is saying what"? Brain-based decoding of human voice and speech (2008) Science, 322 (5903), pp. 970-973; Blakely, T., Miller, K.J., Rao, R.P., Holmes, M.D., Ojemann, J.G., Localization and classification of phonemes using high spatial resolution electrocorticography (ECoG) grids (2008) Engineering in Medicine and Biology Society, 2008. EMBS 2008. 30th Annual International Conference of the IEEE. IEEE, pp. 4964-4967; Pei, X., Barbour, D.L., Leuthardt, E.C., Schalk, G., Decoding vowels and consonants in spoken and imagined words using electrocorticographic signals in humans (2011) Journal of Neural Engineering, 8 (4), p. 046028; Kellis, S., Miller, K., Thomson, K., Brown, R., House, P., Greger, B., Decoding spoken words using local field potentials recorded from the cortical surface (2010) Journal of Neural Engineering, 7 (5), p. 056007; Brumberg, J.S., Wright, E.J., Andreasen, D.S., Guenther, F.H., Kennedy, P.R., Classification of intended phoneme production from chronic intracortical microelectrode recordings in speechmotor cortex (2011) Frontiers in Neuroscience, 5; Mugler, E.M., Patton, J.L., Flint, R.D., Wright, Z.A., Schuele, S.U., Rosenow, J., Shih, J.J., Slutzky, M.W., Direct classification of all American english phonemes using signals from functional speech motor cortex (2014) Journal of Neural Engineering, 11 (3), p. 035015. , http://stacks.iop.org/1741-2552/11/i=3/a=035015; Herff, C., Heger, D., De Pesters, A., Telaar, D., Brunner, P., Schalk, G., Schultz, T., Brain-to-text: Decoding spoken phrases from phone representations in the brain (2015) Frontiers in Neuroscience, 9 (217); Heger, D., Herff, C., Putze, F., Schultz, T., Joint optimization for discriminative, compact and robust brain-computer interfacing (2015) Neural Engineering (NER), 2015 7th International IEEE/EMBS Conference on, , April; Talairach, J., Tournoux, P., Co-planar stereotaxic atlas of the human brain. 3-Dimensional proportional system: An approach to cerebral imaging (1988) Thieme; Kubanek, J., Schalk, G., Neuralact: A tool to visualize electrocortical (ecog) activity on a three-dimensional model of the cortex (2014) Neuroinformatics, pp. 1-8; Schalk, G., McFarland, D.J., Hinterberger, T., Birbaumer, N., Wolpaw, J.R., Bci2000: A general-purpose brain-computer interface (bci) system (2004) Biomedical Engineering, IEEE Transactions on, 51 (6), pp. 1034-1043; Roy, E., Basler, P., (1955) The Gettysburg Address, in the Collected Works of Abraham Lincoln, , New Brunswick, NJ: Rutgers University Press; Kennedy, J.F., (1989) Inaugural Addresses of the Presidents of the United States, , www.bartleby.com/124/, Washington, DC; Crane, W., Gilbert, S., John McConnell, W., Tenniel, S., John Weir, H., Zwecker, J.B., Gooses Nursery Rhymes, M., (1867) A Collection of Alphabets, Rhymes, Tales and Jingles, , London: George Routledge and Sons; (2009) Traitor among Us and Split Feelings, , https://www.fanfiction.net/, unknown; Telaar, D., Wand, M., Gehrig, D., Putze, F., Amma, C., Heger, D., Vu, N.T., Schultz, T., BioKit-Real-time decoder for biosignal processing (2014) Interspeech; Sahin, N.T., Pinker, S., Cash, S.S., Schomer, D., Halgren, E., Sequential processing of lexical, grammatical, and phonological information within brocas area (2009) Science, 326 (5951), pp. 445-449; Mugler, E., Goldrick, M., Slutzky, M., Cortical encoding of phonemic context during word production (2014) Engineering in Medicine and Biology Society, 2014. EMBS 2014. 36th Annual International Conference of the IEEE. IEEE; Pulvermüller, F., Huss, M., Kherif, F., Del Prado Martin, F.M., Hauk, O., Shtyrov, Y., Motor cortex maps articulatory features of speech sounds (2006) Proceedings of the National Academy of Sciences, 103 (20), pp. 7865-7870; Bouchard, K.E., Mesgarani, N., Johnson, K., Chang, E.F., Functional organization of human sensorimotor cortex for speech articulation (2013) Nature, 495 (7441), pp. 327-332; Lotte, F., Brumberg, J.S., Brunner, P., Gunduz, A., Ritaccio, A.L., Guan, C., Schalk, G., Electrocorticographic representations of segmental features in continuous speech (2015) Frontiers in Human Neuroscience, 9 (97); Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A.-R., Jaitly, N., Senior, A., Sainath, T.N., Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups (2012) Signal Processing Magazine, IEEE, 29 (6), pp. 82-97; Von Bünau, P., Meinecke, F.C., Király, F.C., Müller, K.-R., Finding stationary subspaces in multivariate time series (2009) Physical Review Letters, 103 (21), p. 214101},
editor={Noth E., Steidl S., Moller S., Ney H., Mobius B.},
sponsors={Alibaba Group; Amazon; et al.; Facebook; Google; Telekom Innovation Laboratories},
publisher={International Speech and Communication Association},
issn={2308457X},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Herff20155,
author={Herff, C. and Fortmann, O. and Tse, C.-Y. and Cheng, X. and Putze, F. and Heger, D. and Schultz, T.},
title={Hybrid fNIRS-EEG based discrimination of 5 levels of memory load},
journal={International IEEE/EMBS Conference on Neural Engineering, NER},
year={2015},
volume={2015-July},
pages={5-8},
doi={10.1109/NER.2015.7146546},
art_number={7146546},
note={cited By 11; Conference of 7th International IEEE/EMBS Conference on Neural Engineering, NER 2015 ; Conference Date: 22 April 2015 Through 24 April 2015;  Conference Code:113593},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940389191&doi=10.1109%2fNER.2015.7146546&partnerID=40&md5=8c58ea067014adb3bd17f5da505053a1},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology, Karlsruhe, Germany; Department of Psychology, Center for Cognition and Brain Studies, Chinese University of Hong Kong, Hong Kong; Department of Psychology, LSI Neurobiology/Aging Programme, National University of Singapore, Singapore, Singapore},
abstract={In this study, we show that both electroencephalograhy (EEG) and functional Near-Infrared Spectroscopy (fNIRS) can be used to discriminate between 5 levels of memory load. We induce memory load with the memory updating task, which is known to robustly generate memory load and allows us to define 5 different levels of load. Typical experiments only discriminate between low and high workload or up to a maximum of three classes. To the best of our knowledge, the memory updating task has not been used in combination with brain activity measurements before. Here, accuracies of up to 93% are achieved for the binary classification between very high and very low workload. On average, two levels of workload could be discriminated with 74% accuracy. Classification between the full five classes yielded 44% accuracy on average. Despite the fact that EEG results consistently outperformed the results obtained with fNIRS, we could show that the feature-level fusion of both modalities increased robustness of classification results. A reliable discrimination between different levels of memory load could be used to adapt user interfaces or present the right amount of information to a learner. © 2015 IEEE.},
keywords={Brain;  Infrared devices;  Near infrared spectroscopy, 5-level;  Amount of information;  Binary classification;  Brain activity;  Classification results;  Feature level fusion;  Functional near-infrared spectroscopy (fnirs);  Memory load, User interfaces},
references={Fairclough, S.H., Fundamentals of physiological computing (2009) Interacting with Computers, 21 (1), pp. 133-145; Frey, J., Mühl, C., Lotte, F., Hachet, M., Review of the use of electroencephalography as an evaluation method for humancomputer interaction (2014) PhyCS 2014-International Conference on Physiological Computing Systems; Hogervorst, M.A., Brouwer, A.-M., Van Erp, J.B., Combining and comparing EEG, peripheral physiology and eye-related measures for the assessment of mental workload (2014) Frontiers in Neuroscience, 8; Zander, T.O., Kothe, C., Towards passive braincomputer interfaces: Applying braincomputer interface technology to humanmachine systems in general (2011) Journal of Neural Engineering, 8 (2), p. 025005; Kothe, C., Makeig, S., Estimation of task workload from EEG data: New and current tools and perspectives (2011) Engineering in Medicine and Biology Society, EMBC, 2011 Annual International Conference of the IEEE, pp. 6547-6551; Heger, D., Mutter, R., Herff, C., Putze, F., Schultz, T., Continuous recognition of affective states by functional near infrared spectroscopy signals (2013) Affective Computing and Intelligent Interaction. Springer, pp. 436-446; Herff, C., Heger, D., Putze, F., Hennrich, J., Fortmann, O., Schultz, T., Classification of mental tasks in the prefrontal cortex using fnirs (2013) Engineering in Medicine and Biology Society (EMBC), 2013 35th Annual International Conference of the IEEE, pp. 2160-2163; Ayaz, H., Willems, B., Bunce, B., Shewokis, P.A., Izzetoglu, K., Hah, S., Deshmukh, A., Onaral, B., Cognitive workload assessment of air traffic controllers using optical brain imaging sensors (2010) Advances in Understanding Human Performance: Neuroergonomics, Human Factors Design, and Special Populations, pp. 21-31; Ferrari, M., Quaresima, V., A brief review on the history of human functional near-infrared spectroscopy (fnirs) development and fields of application (2012) Neuroimage, 63 (2), pp. 921-935; Strait, M., Scheutz, M., What we can and cannot (yet) do with functional near infrared spectroscopy (2014) Frontiers in Neuroscience, 8 (117); Pfurtscheller, G., Allison, B.Z., Brunner, C., Bauernfeind, G., Solis-Escalante, T., Scherer, R., Zander, T.O., Birbaumer, N., The hybrid bci (2010) Frontiers in Neuroscience, 4; Putze, F., Hesslinger, S., Tse, C.-Y., Huang, Y., Herff, C., Guan, C., Schultz, T., Hybrid fnirs-EEG based classification of auditory and visual perception processes (2014) Frontiers in Neuroscience, 8 (373); Fazli, S., Mehnert, J., Steinbrink, J., Curio, G., Villringer, A., Müller, K.-R., Blankertz, B., Enhanced performance by a hybrid nirs-EEG brain computer interface (2012) Neuroimage, 59 (1), pp. 519-529; Herff, C., Heger, D., Fortmann, O., Hennrich, J., Putze, F., Schultz, T., Mental workload during n-back task-quantified in the prefrontal cortex using fnirs (2014) Frontiers in Human Neuroscience, 7 (935); Brouwer, A.-M., Hogervorst, M.A., Van Erp, J.B., Heffelaar, T., Zimmerman, P.H., Oostenveld, R., Estimating workload using EEG spectral power and erps in the n-back task (2012) Journal of Neural Engineering, 9 (4), p. 045008; Coffey, E.B., Brouwer, A.-M., Van Erp, J.B., Measuring workload using a combination of electroencephalography and near infrared spectroscopy (2012) Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 56 (1), pp. 1822-1826. , SAGE Publications; Salthouse, T.A., Babcock, R.L., Shaw, R.J., Effects of adult age on structural and operational capacities in working memory (1991) Psychology and Aging, 6 (1), p. 118; Oberauer, K., Süß, H.-M., Schulze, R., Wilhelm, O., Wittmann, W., Working memory capacity-facets of a cognitive ability construct (2000) Personality and Individual Differences, 29 (6), pp. 1017-1045; Lewandowsky, S., Oberauer, K., Yang, L.-X., Ecker, U.K., A working memory test battery for matlab (2010) Behavior Research Methods, 42 (2), pp. 571-585; Schlögl, A., Keinrath, C., Zimmermann, D., Scherer, R., Leeb, R., Pfurtscheller, G., A fully automated correction method of EOG artifacts in EEG recordings (2007) Clinical Neurophysiology, 118 (1), pp. 98-104; Huppert, T.J., Diamond, S.G., Franceschini, M.A., Boas, D.A., Homer: A review of time-series analysis methods for near-infrared spectroscopy of the brain (2009) Applied Optics, 48 (10), pp. D280-D298; Cui, X., Bray, S., Reiss, A., Functional near infrared spectroscopy (NIRS) signal improvement based on negative correlation between oxygenated and deoxygenated hemoglobin dynamics (2010) NeuroImage, 49 (4), pp. 3039-3046. , Feb; Ledoit, O., Wolf, M., A well-conditioned estimator for largedimensional covariance matrices (2004) Journal of Multivariate Analysis, 88 (2), pp. 365-411; Blankertz, B., Lemm, S., Treder, M., Haufe, S., Müller, K.-R., Single-trial analysis and classification of erp components- A tutorial (2011) NeuroImage, 56 (2), pp. 814-825; Bauernfeind, G., Steyrl, D., Brunner, C., Muller-Putz, G.R., Single trial classification of fnirs-based brain-computer interface mental arithmetic data: A comparison between different classifiers (2014) Engineering in Medicine and Biology Society (EMBC), 2014 36th Annual International Conference of the IEEE. IEEE, pp. 2004-2007; Heger, D., Herff, C., Schultz, T., Combining feature extraction and classification for fnirs bcis by regularized least squares optimization (2014) Engineering in Medicine and Biology Society (EMBC), 2014 36th Annual International Conference of the IEEE. IEEE, pp. 2012-2015},
sponsors={},
publisher={IEEE Computer Society},
issn={19483546},
isbn={9781467363891},
language={English},
abbrev_source_title={Int. IEEE/EMBS Conf. Neural Eng., NER},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Herff2015,
author={Herff, C. and Heger, D. and de Pesters, A. and Telaar, D. and Brunner, P. and Schalk, G. and Schultz, T.},
title={Brain-to-text: Decoding spoken phrases from phone representations in the brain},
journal={Frontiers in Neuroscience},
year={2015},
volume={9},
number={MAY},
doi={10.3389/fnins.2015.00217},
art_number={6},
note={cited By 98},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931262369&doi=10.3389%2ffnins.2015.00217&partnerID=40&md5=e66a60ccf18b315fea0b0704af1a331c},
affiliation={Cognitive Systems Lab, Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Adenauerring 4, Karlsruhe, 76131, Germany; National Center for Adaptive Neurotechnologies, Wadsworth Center, New York State Department of Health, Albany, NY, United States; Department of Neurology, Albany Medical College, Albany, United States; Department of Biomedical Sciences, State University of New York at Albany, Albany, NY, United States},
abstract={It has long been speculated whether communication between humans and machines based on natural speech related cortical activity is possible. Over the past decade, studies have suggested that it is feasible to recognize isolated aspects of speech from neural signals, such as auditory features, phones or one of a few isolated words. However, until now it remained an unsolved challenge to decode continuously spoken speech from the neural substrate associated with speech and language processing. Here, we show for the first time that continuously spoken speech can be decoded into the expressed words from intracranial electrocorticographic (ECoG) recordings. Specifically, we implemented a system, which we call Brain-To-Text that models single phones, employs techniques from automatic speech recognition (ASR), and thereby transforms brain activity while speaking into the corresponding textual representation. Our results demonstrate that our system achieved word error rates as low as 25% and phone error rates below 50%. Additionally, our approach contributes to the current understanding of the neural basis of continuous speech production by identifying those cortical regions that hold substantial information about individual phones. In conclusion, the Brain-To-Text system described in this paper represents an important step towards human-machine communication based on imagined speech. © 2015 Herff, Heger, De_pesters, Telaar, Brunner, Schalk and Schultz.},
author_keywords={Automatic speech recognition;  Brain-computer interface;  Broadband gamma;  ECoG;  Electrocorticography;  Pattern recognition;  Speech decoding;  Speech production},
keywords={Article;  automatic speech recognition;  brain electrophysiology;  brain to text;  clinical article;  controlled study;  diagonal band of Broca;  electrocorticography;  human;  human computer interaction;  sensorimotor cortex;  superior temporal gyrus},
funding_details={Deutsche ForschungsgemeinschaftDeutsche Forschungsgemeinschaft, DFG},
funding_details={National Institutes of HealthNational Institutes of Health, NIH, EB006356},
funding_details={National Institutes of HealthNational Institutes of Health, NIH, EB00856},
funding_details={National Institutes of HealthNational Institutes of Health, NIH, EB018783},
references={Blakely, T., Miller, K.J., Rao, R.P., Holmes, M.D., Ojemann, J.G., Localization and classification of phonemes using high spatial resolution electrocorticography (ECoG) grids, in Engineering in Medicine and Biology Society, 2008 (2008), pp. 4964-4967. , EMBS 2008. 30th Annual International Conference of the IEEE (IEEE); Bouchard, K., Chang, E., Neural decoding of spoken vowels from human sensory-motor cortex with high-density electrocorticography, in Engineering in Medicine and Biology Society, 2014 (2014), EMBS 2014. 36th Annual International Conference of the IEEE (IEEE); Bouchard, K.E., Mesgarani, N., Johnson, K., Chang, E.F., Functional organization of human sensorimotor cortex for speech articulation (2013) Nature, 495 (7441), pp. 327-332; Brumberg, J.S., Wright, E.J., Andreasen, D.S., Guenther, F.H., Kennedy, P.R., Classification of intended phoneme production from chronic intracortical microelectrode recordings in speech-motor cortex (2011) Frontiers in neuroscience, 5; Canolty, R.T., Soltani, M., Dalal, S.S., Edwards, E., Dronkers, N.F., Nagarajan, S.S., Spatiotemporal dynamics of word processing in the human brain (2007) Frontiers in neuroscience, 1, p. 14; Chang, E.F., Rieger, J.W., Johnson, K., Berger, M.S., Barbaro, N.M., Knight, R.T., Categorical speech representation in human superior temporal gyrus (2010) Nature neuroscience, 13 (11), pp. 1428-1432; Crane, W., Gilbert, S., John McConnell, W., Tenniel, S., John Weir, H., Zwecker, J.B., (1867) Mother Gooses Nursery Rhymes, , A Collection of Alphabets, Rhymes, Tales and Jingles (London: George Routledge and Sons); Crone, N., Hao, L., Hart, J., Boatman, D., Lesser, R., Irizarry, R., Electrocorticographic gamma activity during word production in spoken and sign language (2001) Neurology, 57 (11), pp. 2045-2053; Crone, N.E., Boatman, D., Gordon, B., Hao, L., Induced electrocorticographic gamma activity during auditory perception (2001) Clinical Neurophysiology, 112 (4), pp. 565-582; Deng, S., Srinivasan, R., Lappas, T., D'Zmura, M., Eeg classification of imagined syllable rhythm using hilbert spectrum methods (2010) Journal of neural engineering, 7 (4); Farwell, L.A., Donchin, E., Talking off the top of your head: toward a mental prosthesis utilizing event-related brain potentials (1988) Electroencephalography and clinical Neurophysiology, 70 (6), pp. 510-523; Formisano, E., De Martino, F., Bonte, M., Goebel, R., " who" is saying" what"? brain-based decoding of human voice and speech (2008) Science, 322 (5903), pp. 970-973; Fukuda, M., Rothermel, R., Juhász, C., Nishida, M., Sood, S., Asano, E., Cortical gamma oscillations modulated by listening and overt repetition of phonemes (2010) Neuroimage, 49 (3), pp. 2735-2745; Gales, M., Young, S., The application of hidden markov models in speech recognition (2008) Foundations and Trends in Signal Processing, 1 (3), pp. 195-304; Gales, M.J., Maximum likelihood linear transformations for hmm-based speech recognition (1998) Computer speech & language, 12 (2), pp. 75-98; Gasser, T., Bächer, P., Möcks, J., Transformations towards the normal distribution of broad band spectral parameters of the eeg (1982) Electroencephalography and clinical neurophysiology, 53 (1), pp. 119-124; Guenther, F.H., Brumberg, J.S., Wright, E.J., Nieto-Castanon, A., Tourville, J.A., Panko, M., A wireless brain-machine interface for real-time speech synthesis (2009) PloS one, 4 (12); Haeb-Umbach, R., Ney, H., Linear discriminant analysis for improved large vocabulary continuous speech recognition, in Acoustics, Speech, and Signal Processing, 1992 (1992), 1. , ICASSP-92., 1992 IEEE International Conference on, volume 1, volume 1, 13-16; Huang, X., Acero, A., Hon, H.-W., (2001) Spoken Language Processing: A Guide to Theory, , Algorithm and System Development (Prentice Hall PTR); Jelinek, F., Statistical methods for speech recognition (MIT press) (1997); Kellis, S., Miller, K., Thomson, K., Brown, R., House, P., Greger, B., Decoding spoken words using local field potentials recorded from the cortical surface (2010) Journal of neural engineering, 7 (5); Kennedy, J.F., (1989) Inaugural Addresses of the Presidents of the United States, , www.bartleby.com/124/, (Washington, DC); Kubanek, J., Brunner, P., Gunduz, A., Poeppel, D., Schalk, G., The tracking of speech envelope in the human cortex (2013) PloS one, 8 (1); Kubanek, J., Schalk, G., Neuralact: A tool to visualize electrocortical (ecog) activity on a three-dimensional model of the cortex (2014) Neuroinformatics, pp. 1-8; Lee, K.-F., Context-dependent phonetic hidden markov models for speaker-independent continuous speech recognition (1990) Acoustics, Speech and Signal Processing, IEEE Transactions on, 38 (4), pp. 599-609; Leuthardt, E.C., Gaona, C., Sharma, M., Szrama, N., Roland, J., Freudenberg, Z., Using the electrocorticographic speech network to control a brain-computer interface in humans (2011) Journal of neural engineering, 8 (3); Leuthardt, E.C., Pei, X.-M., Breshears, J., Gaona, C., Sharma, M., Freudenberg, Z., Temporal evolution of gamma activity in human cortex during an overt and covert word repetition task (2011) Frontiers in human neuroscience, 6, pp. 99-99; Lotte, F., Brumberg, J.S., Brunner, P., Gunduz, A., Ritaccio, A.L., Guan, C., Electrocorticographic representations of segmental features in continuous speech (2015) Frontiers in Human Neuroscience, 9 (97); Martin, S., Brunner, P., Holdgraf, C., Heinze, H.-J., Crone, N.E., Rieger, J., Decoding spectrotemporal features of overt and covert speech from the human cortex (2014) Frontiers in Neuroengineering, 7 (14); McFarland, D.J., Miner, L.A., Vaughan, T.M., Wolpaw, J.R., Mu and beta rhythm topographies during motor imagery and actual movements (2000) Brain topography, 12 (3), pp. 177-186; Mesgarani, N., Cheung, C., Johnson, K., Chang, E.F., Phonetic feature encoding in human superior temporal gyrus (2014) Science; Miller, K.J., Leuthardt, E.C., Schalk, G., Rao, R.P., Anderson, N.R., Moran, D.W., Spectral changes in cortical surface potentials during motor movement (2007) The Journal of neuroscience, 27 (9), pp. 2424-2432; Mugler, E., Goldrick, M., Slutzky, M., Cortical encoding of phonemic context during word production (2014) Engineering in Medicine and Biology Society, 2014, , EMBS 2014. 36th Annual International Conference of the IEEE (IEEE); Mugler, E.M., Patton, J.L., Flint, R.D., Wright, Z.A., Schuele, S.U., Rosenow, J., Direct classification of all american english phonemes using signals from functional speech motor cortex (2014) Journal of Neural Engineering, 11 (3); Pasley, B.N., David, S.V., Mesgarani, N., Flinker, A., Shamma, S.A., Crone, N.E., Reconstructing speech from human auditory cortex (2012) PLoS biology, 10 (1); Pei, X., Barbour, D.L., Leuthardt, E.C., Schalk, G., Decoding vowels and consonants in spoken and imagined words using electrocorticographic signals in humans (2011) Journal of neural engineering, 8 (4); Pei, X., Leuthardt, E.C., Gaona, C.M., Brunner, P., Wolpaw, J.R., Schalk, G., Spatiotemporal dynamics of electrocorticographic high gamma activity during overt and covert word repetition (2011) Neuroimage, 54 (4), pp. 2960-2972; Potes, C., Gunduz, A., Brunner, P., Schalk, G., Dynamics of electrocorticographic (ecog) activity in human temporal and frontal cortical areas during music listening (2012) NeuroImage, 61 (4), pp. 841-848. , http://dx.doi.org/10.1016/j.neuroimage.2012.04.022; Pulvermüller, F., Huss, M., Kherif, F., del Prado Martin, F.M., Hauk, O., Shtyrov, Y., Motor cortex maps articulatory features of speech sounds (2006) Proceedings of the National Academy of Sciences, 103 (20), pp. 7865-7870; Rabiner, L., A tutorial on hidden markov models and selected applications in speech recognition (1989) Proceedings of the IEEE, 77 (2), pp. 257-286; Roy, E., Basler, P., (1955) The gettysburg address, , The Collected Works of Abraham Lincoln (New Brunswick, NJ: Rutgers University Press); Sahin, N.T., Pinker, S., Cash, S.S., Schomer, D., Halgren, E., Sequential processing of lexical (2009) grammatical, and phonological information within brocas area, Science, 326 (5951), pp. 445-449; Schalk, G., McFarland, D.J., Hinterberger, T., Birbaumer, N., Wolpaw, J.R., Bci2000: a general-purpose brain-computer interface (bci) system (2004) Biomedical Engineering, IEEE Transactions on, 51 (6), pp. 1034-1043; Schultz, T., Kirchhoff, K., (2006), Multilingual Speech Processing (Elsevier, Academic Press); Stolcke, A., Srilman extensible language modeling toolkit (2002) Proceedings of the 7th International Conference on Spoken Language Processing (ICSLP 2002; Sutter, E.E., The brain response interface: communication through visually-induced electrical brain responses (1992) Journal of Microcomputer Applications, 15 (1), pp. 31-45; Talairach, J., Tournoux, P., (1988) Co-planar stereotaxic atlas of the human brain, , 3-Dimensional proportional system: an approach to cerebral imaging (Thieme); Telaar, D., Wand, M., Gehrig, D., Putze, F., Amma, C., Heger, D., BioKIT-real time decoder for biosignal processing (2014), The 15th Annual Conference of the International Speech Communication Association (Interspeech 2014); Towle, V.L., Yoon, H.-A., Castelle, M., Edgar, J.C., Biassou, N.M., Frim, D.M., Ecog gamma activity during a language task: differentiating expressive and receptive speech areas (2008) Brain, 131 (8), pp. 2013-2027; "Traitor among us" and "Split Feelings" (2009), https://www.fanfiction.net/, unknown; Wolpaw, J.R., Birbaumer, N., McFarland, D.J., Pfurtscheller, G., Vaughan, T.M., Brain- computer interfaces for communication and control (2002) Clinical neurophysiology, 113 (6), pp. 767-791},
correspondence_address1={Herff, C.; Cognitive Systems Lab, Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Adenauerring 4, Germany},
publisher={Frontiers Media S.A.},
issn={16624548},
language={English},
abbrev_source_title={Front. Neurosci.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cliquet2015V,
author={Cliquet, A., Jr. and Secca, M.F. and Schier, J. and Pastor, O. and Sinoquet, C. and Loose, H. and Bienkiewicz, M. and Verdier, C. and Plantier, G. and Schultz, T. and Fred, A. and Gamboa, H.},
title={Preface},
journal={Communications in Computer and Information Science},
year={2015},
volume={511},
pages={V-VI},
note={cited By 0; Conference of 7th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2014 ; Conference Date: 3 March 2014 Through 6 March 2014;  Conference Code:160609},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955300350&partnerID=40&md5=f240af053d3d76417e4baa80f56d4f1c},
editor={Schultz T., Fred A., Plantier G., Gamboa H.},
sponsors={Institute for Systems and Technologies of Information, Control and Communication (INSTICC)},
publisher={Springer Verlag},
issn={18650929},
isbn={9783319261287},
language={English},
abbrev_source_title={Commun. Comput. Info. Sci.},
document_type={Editorial},
source={Scopus},
}

@CONFERENCE{Putze2014275,
author={Putze, F. and Schultz, T.},
title={Investigating intrusiveness of workload adaptation},
journal={ICMI 2014 - Proceedings of the 2014 International Conference on Multimodal Interaction},
year={2014},
pages={275-281},
doi={10.1145/2663204.2663279},
note={cited By 0; Conference of 16th ACM International Conference on Multimodal Interaction, ICMI 2014 ; Conference Date: 12 November 2014 Through 16 November 2014;  Conference Code:109514},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947280338&doi=10.1145%2f2663204.2663279&partnerID=40&md5=7f69352bf5d137027ef3ab8d67c8768a},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={In this paper, we investigate how an automatic task assistant which can detect and react to a user's workload level is able to support the user in a complex, dynamic task. In a user study, we design a dispatcher scenario with low and high workload conditions and compare the effect of four support strategies with different levels of intrusiveness using objective and subjective metrics. We see that a more intrusive strategy results in higher efficiency and effectiveness, but is also less accepted by the participants. We also show that the benefit of supportive behavior depends on the user's workload level, i.e. adaptation to its changes are necessary. We describe and evaluate a Brain Computer Interface that is able to provide the necessary user state detection. Copyright 2014 ACM.},
author_keywords={Brain computer interface;  Intrusiveness;  User study;  Workload adaptive assistance},
keywords={Brain computer interface;  Interactive computer systems;  Interfaces (computer), Dynamic tasks;  Higher efficiency;  Intrusiveness;  State Detection;  Task assistant;  User study;  Workload adaptations;  Workload adaptive assistance, Interface states},
references={Bailey, N.R., Scerbo, M.W., Freeman, F.G., Mikulka, P.J., Scott, L.A., Comparison of a brain-based adaptive system and a manual adaptable system for invoking automation (2006) Human Factors: The Journal of the Human Factors and Ergonomics Society, 48 (4), pp. 693-709; Berka, C., Levendowski, D.J., Ramsey, C.K., Davis, G., Lumicao, M.N., Stanney, K., Reeves, L., Stibler, K., Evaluation of an EEG workload model in an aegis simulation environment (2005) Defense and Security, 5797, pp. 90-99; Chen, D., Vertegaal, R., Using mental load for managing interruptions in physiologically attentive user interfaces (2004) Extended Abstracts on Human Factors in Computing Systems, pp. 1513-1516. , USA; Christensen, J.C., Estepp, J.R., Coadaptive aiding and automation enhance operator performance (2013) Human Factors: The Journal of the Human Factors and Ergonomics Society, , 0018720813476883; Dijksterhuis, C., Waard, D.D., Mulder, B.L.J.M., Classifying visuomotor workload in a driving simulator using subject specific spatial brain patterns (2013) Frontiers in Neuroprosthetics, 7; Fairclough, S.H., Fundamentals of physiological computing (2009) Interacting with Computers, 21 (1-2), pp. 133-145; Frøkjær, E., Hertzum, M., Hornbæk, K., Measuring usability: Are effectiveness, efficiency, and satisfaction really correlated? (2000) Proceedings of the Conference on Human Factors in Computing Systems, , New York, USA; Gajos, K.Z., Czerwinski, M., Tan, D.S., Weld, D.S., Exploring the design space for adaptive graphical user interfaces (2006) Proceedings of the Working Conference on Advanced Visual Interfaces, , New York, USA; Hart, S.G., Staveland, L.E., Development of NASA-TLX (task load index): Results of empirical and theoretical research (1988) Advances in Psychology, 52, pp. 139-183. , of Human Mental Workload. North-Holland; Heger, D., Putze, F., Schultz, T., An EEG adaptive information system for an empathic robot (2011) International Journal of Social Robotics, 3 (4), pp. 415-425; Hornbaek, K., Law, E.L.-C., Meta-analysis of correlations among usability measures (2007) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 617-626. , CHI '07, ACM (New York, NY, USA); Jarvis, J., Putze, F., Heger, D., Schultz, T., Multimodal person independent recognition of workload related bio signal patterns (2011) Proceedings of the 13th International Conference on Multimodal Interfaces, pp. 205-208. , ICMI '11, ACM (New York, NY, USA); Kohlmorgen, J., Dornhege, G., Braun, M., Blankertz, B., Müller, K.-R., Curio, G., Hagemann, K., Kincses, W., Improving human performance in a real operating environment through real-time mental workload detection Toward Brain-Computer Interfacing, 2007, pp. 409-422; Kothe, C., Makeig, S., Estimation of task workload from EEG data: New and current tools and perspectives (2011) Proceedings of the Engineering in Medicine and Biology Society, pp. 6547-6551; Lei, S., Rötting, M., Influence of task combination on EEG spectrum modulation for driver workload estimation (2011) Human Factors, 53 (2), pp. 168-179; Murata, A., An attempt to evaluate mental workload using wavelet transform of EEG (2005) Human Factors, 47 (3), pp. 498-508; Wang, Z., Hope, R.M., Wang, Z., Ji, Q., Gray, W.D., Cross-subject workload classification with a hierarchical bayes model (2012) Neuro Image, 59 (1), pp. 64-69; Wilson, G.F., Lambert, J.D., Russell, C.A., Performance enhancement with real-time physiologically controlled adaptive aiding (2000) Proc. of the Human Factors and Ergonomics Society Annual Meeting, 44 (13), pp. 61-64; Wilson, G.F., Russell, C.A., Performance enhancement in an uninhabited air vehicle task using psycho physiologically determined adaptive aiding (2007) Human Factors, 49 (6), pp. 1005-1018},
sponsors={ACM SIGCHI; Openstream; SAP Innovation Center Turkey; US National Science Foundation (NSF)},
publisher={Association for Computing Machinery, Inc},
isbn={9781450328852},
language={English},
abbrev_source_title={ICMI - Proc. Int. Conf. Multimodal Interact.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wand20142515,
author={Wand, M. and Janke, M. and Schultz, A.T.},
title={Tackling speaking mode varieties in EMG-based speech recognition},
journal={IEEE Transactions on Biomedical Engineering},
year={2014},
volume={61},
number={10},
pages={2515-2526},
doi={10.1109/TBME.2014.2319000},
art_number={2319000},
note={cited By 31},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907468717&doi=10.1109%2fTBME.2014.2319000&partnerID=40&md5=27b0ec39f9573f67c035e8be2d32d370},
affiliation={Cognitive Systems Laboratory, Karlsruhe Institute of Technology, Karlsruhe, 76227, Germany},
abstract={An electromyographic (EMG) silent speech recognizer is a system that recognizes speech by capturing the electric potentials of the human articulatory muscles, thus enabling the user to communicate silently. After having established a baseline EMG-based continuous speech recognizer, in this paper, we investigate speaking mode variations, i.e., discrepancies between audible and silent speech that deteriorate recognition accuracy. We introduce multimode systems that allow seamless switching between audible and silent speech, investigate different measures which quantify speaking mode differences, and present the spectral mapping algorithm, which improves the word error rate (WER) on silent speech by up to 14.3% relative. Our best average silent speech WER is 34.7%, and our best WER on audibly spoken speech is 16.8%. © 1964-2012 IEEE.},
author_keywords={Electromyography (EMG);  EMG-based speech recognition;  silent speech interfaces (SSI)},
keywords={Conformal mapping;  Electric potential;  Electromyography;  Photomapping;  Speech, Continuous speech;  Electromyographic;  Multimode system;  Recognition accuracy;  Seamless switching;  Silent speech interfaces;  Spectral mappings;  Word error rate, Speech recognition, Article;  electrode;  electromyography;  electrophysiology;  human;  speech and language assessment;  speech discrimination;  word error rate;  adult;  algorithm;  electromyography;  face muscle;  female;  male;  physiology;  procedures;  signal processing;  speech;  young adult, Adult;  Algorithms;  Electromyography;  Facial Muscles;  Female;  Humans;  Male;  Signal Processing, Computer-Assisted;  Speech;  Young Adult},
references={Schultz, T., Wand, M., Modeling coarticulation in large vocabulary EMG-based speech recognition (2010) Speech Commun, 52 (4), pp. 341-353; Janke, M., Wand, M., Schultz, T., A spectral mapping method for EMG-based recognition of silent speech (2010) Proc. B-Interface, pp. 22-31; Florescu, V.-M., Crevier-Buchman, L., Denby, B., Hueber, T., Colazo-Simon, A., Pillot-Loiseau, C., Roussel, P., Quattrocchi, S., Silent vs vocalized articulation for a portable ultrasound-based silent speech interface (2010) Proc. Interspeech, pp. 450-453; Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Silent speech interfaces (2010) Speech Commun, 52 (4), pp. 270-287; Merletti, R., Parker, P.A., (2004) Electromyography: Physiology Engineering and Noninvasive Applications, , New York NY USA Wiley; Deng, Y., Patel, R., Heaton, J.T., Colby, G., Gilmore, L.D., Cabrera, J., Roy, S.H., Meltzner, G.S., Disordered speech recognition using acoustic and sEMG signals (2009) Proc. Interspeech, pp. 644-647; Sugie, N., Tsunoda, K., A speech prosthesis employing a speech synthesizer-Vowel discrimination from perioral muscle activities and vowel production (1985) IEEE Trans. Biomed. Eng., Vol. BME-32, (7), pp. 485-490. , Jul; Morse, M.S., Obrien, E.M., Research summary of a scheme to ascertain the availability of speech information in the myoelectric signals of neck and head muscles using surface electrodes (1986) Comput. Biol. Med, 16 (6), pp. 399-410; Jorgensen, C., Binsted, K., Web browser control using EMG based sub vocal speech recognition (2005) Proc. 38th Annu. Hawaii Int. Conf. Syst. Sci, p. 294c; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proc. Interspeech, pp. 573-576; Jou, S.-C., Maier-Hein, L., Schultz, T., Waibel, A., Articulatory feature classification using surface electromyography (2006) Proc. Int.Conf. Acoust., Speech, Signal Process, pp. I605-I608; Jou, S.-C., Schultz, T., Waibel, A., Continuous electromyographic speech recognition with a multi-stream decoding architecture (2007) Proc. Int. Conf. Acoust., Speech, Signal Process, , IV-401-IV-404; Janke, M., Wand, M., Schultz, T., Impact of lack of acoustic feedback in EMG-based silent speech recognition (2010) Proc. Interspeech, pp. 2686-2689; Wand, M., Janke, M., Schultz, T., Investigations on speaking mode discrepancies in emg-based speech recognition (2011) Proc. Interspeech, pp. 601-604; Wand, M., Janke, M., Schultz, T., Decision-Tree based analysis of speakingmode discrepancies in EMG-based speech recognition (2012) Proc. Biosignals, pp. 101-109; Wand, M., Schultz, T., Session-independent emg-based speech recognition (2011) Proc. Biosignals, pp. 295-300; Wand, M., Schulte, C., Janke, M., Schultz, T., Array-based electromyographic silent speech interface (2013) Proc. Biosignals, pp. 89-96; Wand, M., Himmelsbach, A., Heistermann, T., Janke, M., Schultz, T., Artifact removal algorithm for an EMG-based silent speech interface (2013) Proc. 35th Ann. Int. Conf. Eng. Med. Biol. Soc, pp. 5750-5753; Jorgensen, C., Dusan, S., Speech interfaces based upon surface electromyography (2010) Speech Commun, 52, pp. 354-366; Lopez-Larraz, E., Mozos, O.M., Antelis, J.M., Minguez, J., Syllablebased speech recognition using EMG (2010) Proc. Ann. Int. Conf. Eng. Med. Biol. Soc, pp. 4699-4702; Meltzner, G.S., Colby, G., Deng, Y., Heaton, J.T., Signal acquisition and processing techniques for sEMG based silent speech recognition (2011) Proc. Ann. Int. Conf. IEEE Eng. Med. Biol. Soc, pp. 4848-4851; Freitas, J., Teixeira, A., Dias, M.S., Towards a silent speech interface for portuguese (2012) Proc. Biosignals, pp. 91-100; Toth, A., Wand, A., Schultz, T., Synthesizing speech from electromyography using voice transformation techniques (2009) Proc. Interspeech, pp. 652-655; Lee, K.-S., Prediction of acoustic feature parameters using myoelectric signals (2010) IEEE Trans. Biomed. Eng, 57 (7), pp. 1587-1595. , Jul; Johner, C., Janke, M., Wand, M., Schultz, T., Inferring prosody from facial cues for EMG-based synthesis of silent speech (2012) Proc. 4th Int. Conf. Appl. Human Factors Ergon, pp. 5317-5326; Ito, T., Takeda, K., Itakura, F., Analysis and recognition of whispered speech (2005) Speech Commun, 45, pp. 139-152; Nakajima, Y., Kashioka, H., Campbell, N., Shikano, K., Non-Audible Murmur (NAM) recognition (2006) IEICE Trans. Inf. Syst., Vol. E89-D, (1), pp. 1-4. , Jan; Frowen, J., Perry, A.R., Reasons for success or failure in surgical voice restoration after total laryngectomy-An Australian study (2001) J. Laryngol. Otol, 115 (5), pp. 393-399; Meltzner, G.S., Hillman, R.E., Impact of aberrant acoustic properties on the perception of sound quality in electrolarynx speech (2005) J. Speech, Lang., Hear. Res, 48, pp. 766-779. , Aug; Stepp, C.E., Heaton, J.T., Rolland, R.G., Hillman, R.E., Neck and face surface electromyography for prosthetic voice control after total laryngectomy (2009) IEEE Trans. Neural Syst. Rehabil. Eng, 17 (2), pp. 146-155. , Apr; Milner-Brown, H.S., Stein, R.B., Yemm, R., Changes in firing rate of human motor units during linearly changing voluntary contractions (1973) J. Physiol, 230, pp. 371-390; De Luca, C.J., Physiology andmathematics of myoelectric signals (1979) IEEE Trans. Biomed. Eng., Vol. BME-26, (6), pp. 313-325. , Jun; Nawab, S.H., Wotiz, R.P., Luca, C.J.D., Decomposition of indwelling EMG signals (2008) J. Appl. Physiol, 105, pp. 700-710; De Luca, C.J., Adam, A., Wotiz, R., Gilmore, L.D., Nawab, S.H., Decomposition of surfaceEMGsignals (2006) J. Neurophysiol, 96, pp. 1646-1657; Schünke, M., Schulte, E., Schumacher, U., (2006) Prometheus-Lernatlas der Anatomie. Kopf und, 3. , Neuroanatomie Germany: Thieme Verlag; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-Audible speech recognition using surface electromyography (2005) Proc. IEEE Workshop Autom. Speech Recog. Understanding, pp. 331-336; Cavanagh, P.R., Komi, P.V., Electromechanical delay in human skeletal muscle under concentric and eccentric contractions (1979) Eur. J. Appl. Physiol. Occupat. Physiol, 42 (3), pp. 159-163; Heistermann, T., Janke, M., Wand, M., Schultz, T., Spatial artifact detection for multi-channel EMG-based speech recognition (2014) Proc. Biosignals, pp. 189-196; Wand, M., Schultz, T., Towards speaker-Adaptive speech recognition based on surface electromyography (2009) Proc. Biosignals, pp. 155-162; (1999) Handbook of the International Phonetic Association, , International Phonetic Association Cambridge, U.K.: Cambridge Univ. Press; Metze, F., Waibel, A., A flexible stream architecture for ASR using articulatory features (2002) Proc. Int. Conf. Spoken Lang. Process, pp. 2133-2136; Kirchhoff, K., Robust speech recognition using articulatory information" PhD dissertation (1999) Dept. Appl. Comput. Sci, , Univ. Bielefeld, Bielefeld, Germany; Frankel, J., Wester, M., King, S., Articulatory feature recognition using dynamic Bayesian networks (2004) Proc. Int. Conf. Spoken Lang. Process, pp. 660-663; Finke, M., Rogina, I., Wide context acoustic modeling in read versus spontaneous speech (1997) Proc. IEEE Int. Conf. Acoust., Speech, Signal Process, pp. 1743-1746; Lee, K.-F., Large-vocabulary speaker-independent continuous speech recognition: The sphinx system (1988) Dept. Comput. Sci, , Ph.D. dissertation Carnegie Mellon Univ., Pittsburgh, PA, USA; Ueda, N., Nakano, R., Ghahramani, Z., Hinton, G.E., Split and merge em algorithm for improving Gaussian mixture density estimates (2000) J. VLSI Signal Process, 26, pp. 133-140; Duda, R.O., Hart, P.E., Stork, D.G., (2000) Pattern Classification, , New York NY USA Wiley; Wand, M., Jou, S.-C., Toth, A.R., Schultz, T., Impact of different speakingmodes on EMG-based speech recognition (2009) Proc. Interspeech, pp. 648-651; Lombard, E., Le signe de lelevation de la voix (1911) Ann. Mal. Oreille Larynx, 37, pp. 101-119; Tourville, J.A., Reilly, K.J., Guenther, F.H., Neural mechanisms underlying auditory feedback control of speech (2008) NeuroImage, 32, pp. 1429-1443; Perkell, J., Matthies, M., Lane, H., Guenther, F., Wilhelms-Tricaricoa, R., Wozniaka, J., Guioda, P., Speech motor control: Acoustic goals, saturation effects, auditory feedback and internal models (1997) Speech Commun, 22, pp. 227-250; Guenther, F.H., Ghosh, S.S., Tourville, J.A., Neural modeling and imaging of the cortical interactions underlying syllable production (2006) Brain Lang, 96, pp. 280-301; Herff, C., Janke, M., Wand, M., Schultz, T., Impact of different feedback mechanisms in EMG-based speech recognition (2011) Proc. Interspeech, pp. 2213-2216; Schultz, T., Waibel, A., Language independent and language adaptive acoustic modeling for speech recognition (2001) Speech Commun, 35, pp. 31-51; Eide, E., Gish, H., Jeanrenaud, P., Mielke, A., Understanding and improving speech recognition performance through the use of diagnostic tools (1995) Proc. Int. Conf. Acoust., Speech, Signal Process, pp. 221-224; Schaaf, T., Metze, F., Analysis of gender normalization using MLP and VTLN features (2010) Proc. Interspeech, pp. 306-309; Welch, P., The use of fast fourier transform for the estimation of power spectra: A method based on time averaging over short, modified periodograms (1967) IEEE Trans. Audio Electroacoust., Vol. AU-15, (2), pp. 70-73. , Jun; Gales, M.J.F., Maximum likelihood linear transformations for HMMbased speech recognition (1997) Eng. Dept., Cambridge Univ, , Cambridge, MA, USA},
correspondence_address1={Wand, M.; Cognitive Systems Laboratory, Karlsruhe Institute of TechnologyGermany},
publisher={IEEE Computer Society},
issn={00189294},
coden={IEBEA},
pubmed_id={24760900},
language={English},
abbrev_source_title={IEEE Trans. Biomed. Eng.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Putze2014108,
author={Putze, F. and Schultz, T.},
title={Adaptive cognitive technical systems},
journal={Journal of Neuroscience Methods},
year={2014},
volume={234},
pages={108-115},
doi={10.1016/j.jneumeth.2014.06.029},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905368115&doi=10.1016%2fj.jneumeth.2014.06.029&partnerID=40&md5=959ff95f9b37c484411781154181d269},
affiliation={Karlsruhe Institute of Technology, Cognitive Systems Lab, Karlsruhe, Germany},
abstract={Adaptive cognitive technical systems are capable of sensing the internal state of its user and of adapting its behavior appropriately to those measurements to improve the usability of the system. One important example of such user state is the user's mental workload level. This paper gives an introduction to the topic of workload recognition and adaptation. It reviews the literature on recognition of workload from physiological signals and on how those user state estimates are employed to improve human-machine interaction. © 2014 .},
author_keywords={Adaptation;  Passive brain-computer interface;  User studies;  Workload},
keywords={adaptation;  cognition;  electroencephalogram;  human;  priority journal;  recognition;  review;  workload;  bibliographic database;  brain;  brain computer interface;  cognition;  electroencephalography;  man machine interaction;  physiology;  statistics and numerical data;  workload, Brain;  Brain-Computer Interfaces;  Cognition;  Databases, Bibliographic;  Electroencephalography;  Humans;  Man-Machine Systems;  Workload},
references={Allison, B.Z., Polich, J., Workload assessment of computer gaming using a single-stimulus event-related potential paradigm (2008) Biol Psychol, 77 (3), pp. 277-283; Bailey, N.R., Scerbo, M.W., Freeman, F.G., Mikulka, P.J., Scott, L.A., Comparison of a brain-based adaptive system and a manual adaptable system for invoking automation (2006) Hum Factors: J Hum Factors Ergon Soc, 48 (4), pp. 693-709; Batrinca, L., Lepri, B., Mana, N., Pianesi, F., Multimodal recognition of personality traits in human-computer collaborative tasks (2012) Proceedings of the 14th ACM international conference on multimodal interaction, pp. 39-46; Berka, C., Levendowski, D.J., Ramsey, C.K., Davis, G., Lumicao, M.N., Stanney, K., Evaluation of an EEG workload model in an aegis simulation environment (2005) In: Defense and security, 5797, pp. 90-99; Biessmann, F., Plis, S., Meinecke, F., Eichele, T., Müller, K., Analysis of multimodal neuroimaging data (2011) Rev Biomed Eng, 4, pp. 26-58; Breazeal, C.L., (2004) Designing Sociable Robots, , MIT Press, Cambridge; Brouwer, A.-M., Hogervorst, M.A., van Erp, J.B.F., Heffelaar, T., Zimmerman, P.H., Oostenveld, R., Estimating workload using EEG spectral power and ERPs in the n-back task (2012) J Neural Eng, 9 (4), p. 045008; Bunce, S.C., Izzetoglu, K., Ayaz, H., Shewokis, P., Izzetoglu, M., Pourrezaei, K., Implementation of fNIRS for monitoring levels of expertise and mental workload (2011) Foundations of augmented cognition. Directing the future of adaptive systems, no. 6780 in lecture notes in computer science, pp. 13-22. , Springer, Berlin/Heidelberg; Charlton, S.G., Driving while conversing: cell phones that distract and passengers who react (2009) Accid Anal Prev, 41 (1), pp. 160-173; Chen, D., Vertegaal, R., Using mental load for managing interruptions in physiologically attentive user interfaces (2004) Extended abstracts on human factors in computing systems, pp. 1513-1516; Christensen, J.C., Estepp, J.R., Coadaptive aiding and automation enhance operator performance (2013) Hum Factors: J Hum Factors Ergon Soc, , 0018720813476883; Coffey, E.B.J., Brouwer, A.-M., Erp, J., Measuring workload using a combination of electroencephalography and near infrared spectroscopy (2012) Proc Hum Factors Ergonom Soc Ann Meet, 56 (1), pp. 1822-1826; Dijksterhuis, C., de Waard, D., Brookhuis, K.A., Mulder, B.L., de Jong, R., Classifying visuomotor workload in a driving simulator using subject specific spatial brain patterns (2014) Front Neuroprosthet, 7; D'Mello, S., Kory, J., Consistent but modest: a meta-analysis on unimodal and multimodal affect detection accuracies from 30 studies (2012) Proceedings of the 14th international conference on multimodal interaction; Fairclough, S.H., Fundamentals of physiological computing (2009) Interact Comput, 21 (1-2), pp. 133-145; Fazli, S., Mehnert, J., Steinbrink, J., Curio, G., Villringer, A., Müller, K.-R., Enhanced performance by a hybrid NIRS-EEG brain computer interface (2012) NeuroImage, 59 (1), pp. 519-529; Feigh, K.M., Dorneich, M.C., Hayes, C.C., Toward a characterization of adaptive systems a framework for researchers and system designers (2012) Hum Factors: J Hum Factors Ergon Soc, 54 (6), pp. 1008-1024; Gašić, M., Tsiakoulis, P., Henderson, M., Thomson, B., Yu, K., Tzirkel, E., The effect of cognitive load on a statistical dialogue system (2012) Proceedings of the 13th annual meeting of the special interest group on discourse and dialogue, SIGDIAL'12, association for computational linguistics, pp. 74-78; Gajos, K.Z., Czerwinski, M., Tan, D.S., Weld, D.S., Exploring the design space for adaptive graphical user interfaces (2006) Proceedings of the working conference on advanced visual interfaces; Gajos, K.Z., Everitt, K., Tan, D.S., Czerwinski, M., Weld, D.S., Predictability and accuracy in adaptive user interfaces (2008) Proceedings of the conference on human factors in computing systems, pp. 1271-1274; (2001) Stress, workload, and fatigue, human factors in transportation, , Lawrence Erlbaum Associates Publishers, Mahwah, USA, P.A. Hancock, P.A. Desmond (Eds.); Hart, S.G., Staveland, L.E., Development of NASA-TLX (task load index): results of empirical and theoretical research (1988) Advances in psychology, vol. 52 of human mental workload, pp. 139-183; Healey, J., Picard, R., Detecting stress during real-world driving tasks using physiological sensors (2005) IEEE Trans Intell Transport Syst, 6 (2), pp. 156-166; Heger, D., Putze, F., Schultz, T., An EEG adaptive information system for an empathic robot (2011) Int J Soc Robot, 3 (4), pp. 415-425; Hirshfield, L.M., Chauncey, K., Gulotta, R., Girouard, A., Solovey, E.T., Jacob, R.J.K., Combining electroencephalograph and functional near infrared spectroscopy to explore users' mental workload (2009) Foundations of augmented cognition. neuroergonomics and operational neuroscience, no. 5638 in lecture notes in computer science, pp. 239-247. , Springer, Berlin/Heidelberg; Ickes, W., Empathic accuracy (1993) J Pers, 61 (4), pp. 587-610; Jameson, A.D., Understanding and dealing with usability side effects of intelligent processing (2009) AI Mag, 30 (4), pp. 23-40; Jarvis, J., Putze, F., Heger, D., Schultz, T., Multimodal person independent recognition of workload related biosignal patterns (2011) Proceedings of the 13th international conference on multimodal interfaces, ICMI'11, ACM, pp. 205-208; Ji, Q., Yang, X., Real-time eye, gaze, and face pose tracking for monitoring driver vigilance (2002) Real-Time Imaging, 8 (5), pp. 357-377; Kohlmorgen, J., Dornhege, G., Braun, M., Blankertz, B., Müller, K.-R., Curio, G., Improving human performance in a real operating environment through real-time mental workload detection (2007) Toward brain-computer interfacing, pp. 409-422; Kothe, C., Makeig, S., Estimation of task workload from EEG data: new and current tools and perspectives (2011) Proceedings of the engineering in medicine and biology society, pp. 6547-6551; Lansdown, T.C., Brook-Carter, N., Kersloot, T., Distraction from multiple in-vehicle secondary tasks: vehicle performance and mental workload implications (2004) Ergonomics, 47 (1), pp. 91-104; Lei, S., Rötting, M., Influence of task combination on EEG spectrum modulation for driver workload estimation (2011) Hum Factors, 53 (2), pp. 168-179; Li, X., Dong, Y., Yao, L., Jiang, Y., Wang, W., An evaluation system of mental workloadbased on the QN-MHP under multi-taskcondition (2013) 2013 IEEE international symposium on assembly and manufacturing (ISAM), pp. 240-244; Li, N., Jain, J., Busso, C., Modeling of driver behavior in real world scenarios using multiple noninvasive sensors (2013) Trans Multimed, 15 (5), pp. 1213-1225; Merat, N., Jamson, A.H., Shut up i'm driving!: is talking to an inconsiderate passenger the same as talking on a mobile telephone? (2005) Proceedings of the 3rd international driving symposium on human factors in driver assessment, training, and vehicle design, pp. 426-432; Murata, A., An attempt to evaluate mental workload using wavelet transform of EEG (2005) Hum Factors, 47 (3), pp. 498-508; Nass, C., Jonsson, I.-M., Harris, H., Reaves, B., Endo, J., Brave, S., Improving automotive safety by pairing driver emotion and car voice emotion (2005) Proceedings of the human factors in computing systems; Pan, S.J., Yang, Q., A survey on transfer learning (2010) IEEE Trans Knowl Data Eng, 22 (10), pp. 1345-1359; Pauzie, A., A method to assess the driver mental workload: the driving activity load index (DALI) (2008) IET Intell Transp Syst, 2 (4), p. 315; Picard, R., Vyzas, E., Healey, J., Toward machine emotional intelligence: analysis of affective physiological state (2001) IEEE Trans Pattern Anal Mach Intell, 23 (10), pp. 1175-1191; Picard, R.W., (2000) Affective computing, , MIT Press, Cambridge; Reeves, B., Nass, C.I., (1996) The media equation: how people treat computers, television, and new media like real people and places, 16. , Cambridge University Press, New York, USA; Robert, G., Hockey, J., Compensatory control in the regulation of human performance under stress and high workload: a cognitive-energetical framework (1997) Biol Psychol, 45 (1-3), pp. 73-93; Rubio, S., Díaz, E., Martín, J., Puente, J.M., Evaluation of subjective mental workload: a comparison of SWAT, NASA-TLX, and workload profile methods (2004) Appl Psychol, 53 (1), pp. 61-86; Sassaroli, A., Zheng, F., Hirshfield, L.M., Girouard, A., Solovey, E.T., Jacob, R.J.K., Discrimination of mental workload levels in human subjects with functional near-infrared spectroscopy (2008) J Innovat Opt Health Sci, 1 (2), pp. 227-237; van Winsun, W., Sergeant, J., Geuze, R., The functional significance of event-related desynchronization of alpha rhythm in attentional and activating tasks (1984) Electroencephalogr Clin Neurophysiol, 58 (6), pp. 519-524; Vetek, A., Lemmelä, S., Could a dialog save your life? Analyzing the effects of speech interaction strategies while driving (2011) Proceedings of the 13th international conference on multimodal interfaces; Wang, Z., Hope, R.M., Wang, Z., Ji, Q., Gray, W.D., Cross-subject workload classification with a hierarchical bayes model (2012) NeuroImage, 59 (1), pp. 64-69; Wickens, C.D., Multiple resources and mental workload (2008) Hum Factors: J Hum Factors Ergonom Soc, 50 (3), pp. 449-455; Wilson, G.F., Russell, C.A., Performance enhancement in an uninhabited air vehicle task using psychophysiologically determined adaptive aiding (2007) Hum Factors, 49 (6), pp. 1005-1018; Wilson, G.F., Lambert, J.D., Russell, C.A., Performance enhancement with real-time physiologically controlled adaptive aiding (2000) Proc Hum Factors Ergonom Soc Ann Meet, 44 (13), pp. 61-64; Wolpaw, J.R., Birbaumer, N., McFarland, D.J., Pfurtscheller, G., Vaughan, T.M., Brain-computer interfaces for communication and control (2002) Clin Neurophysiol, 113 (6), pp. 767-791; Zander, T.O., Kothe, C., Towards passive brain-computer interfaces: applying brain-computer interface technology to human-machine systems in general (2011) J Neural Eng, 8 (2), p. 025005; Zeng, Z., Pantic, M., Roisman, G., Huang, T., A survey of affect recognition methods: audio, visual, and spontaneous expressions (2009) IEEE Trans Pattern Anal Mach Intell, 31 (1), pp. 39-58},
correspondence_address1={Putze, F.; Karlsruhe Institute of Technology, Cognitive Systems Lab, Karlsruhe, Germany; email: felix.putze@kit.edu},
publisher={Elsevier},
issn={01650270},
coden={JNMED},
pubmed_id={24997342},
language={English},
abbrev_source_title={J. Neurosci. Methods},
document_type={Review},
source={Scopus},
}

@ARTICLE{Heger2014113,
author={Heger, D. and Herff, C. and Putze, F. and Mutter, R. and Schultz, T.},
title={Continuous affective states recognition using functional near infrared spectroscopy},
journal={Brain-Computer Interfaces},
year={2014},
volume={1},
number={2},
pages={113-125},
doi={10.1080/2326263X.2014.912884},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929465244&doi=10.1080%2f2326263X.2014.912884&partnerID=40&md5=5a07d4a8237cd80a3621ab8f5e9728b2},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={Monitoring the affective states of a person can be highly relevant for numerous disciplines, including adaptive user interfaces, entertainment, ergonomics, medicine and therapy. In many situations, the affective state of a user is not easily observable from outside by audio or video, but may be identified by a brain-computer interface (BCI). Functional near-infrared spectroscopy (fNIRS) is a brain imaging modality gaining rising attention in the BCI community. However, fNIRS emotion recognition studies have only analyzed stimulus-locked effects. For realistic human-machine interaction scenarios, the point of time of an emotion-triggering event and the time span of an affective state are unknown. In this paper, we investigate a BCI that monitors the affective states of the user continuously over time (i.e. asynchronous BCI). In our study, fNRIS signals from eight subjects have been recorded at eight prefrontal locations in response to three different classes of affect induction by emotional audio-visual stimuli plus a neutral class. Our system evaluates short windows of 5 s length to continuously recognize affective states. We analyze hemodynamic responses, present a careful evaluation of binary classification tasks, compare time-domain and wavelet-based signal features, and investigate classification accuracies over time. © 2014, © 2014 Taylor & Francis.},
author_keywords={affect recognition;  asynchronous;  brain-computer interfaces;  continuous;  emotion recognition;  functional near-infrared spectroscopy},
funding_text 1={Christian Herff studied Computer Science at the Karlsruhe Institute of Technology, fully funded by a scholarship provided by the “Association of German Businesses”. During his studies, Christian Herff was a visiting research student at IIT Delhi in India. He acquired his Diploma (Master equivalent) in 2011, which was partly carried out during an invited research stay at the Brain Computer Interface Group, I2R, Singapore. He is currently a research assistant and Ph.D. student at Karlsruhe Institute of Technology at the Cognitive Systems Lab with a research focus on Brain-Computer Interfaces and fNIRS.},
references={Picard, R.W., (2000) Affective computing, , MIT press; D’Mello, S.K., Craig, S.D., Witherspoon, A., Mcdaniel, B., Graesser, A., Automatic detection of learners affect from conversational cues (2008) User Modeling and User-Adapted Interaction, 18, pp. 45-80; Nasoz, F., Lisetti, C.L., Vasilakos, A.V., Affectively intelligent and adaptive car interfaces (2010) Inf Sci, 180, pp. 3817-3836; Heger, D., Putze, F., Schultz, T., An eeg adaptive information system for an empathic robot (2011) Int J Soc Rob, 3, pp. 415-425; Moore, D., Cheng, Y., McGrath, P., Powell, N.J., Collaborative virtual environment technology for people with autism (2005) Focus on Autism and Other Developmental Disabilities, 20, pp. 231-243; Kaliouby, R., Picard, R., Baron-Cohen, S., Affective computing and autism (2006) Ann NY Acad Sci, 1093, pp. 228-248; Rolls, E.T., The orbitofrontal cortex and reward (2000) Cerebral cortex, 10, pp. 284-294; LeDoux, J.E., Emotion circuits in the brain (2000) Annu Rev Neurosci, 23, pp. 155-184; Bush, G., Luu, P., Posner, M.I., Cognitive and emotional influences in anterior cingulate cortex (2000) Trends in Cognitive Sci, 4, pp. 215-222; Damasio, A.R., Grabowski, T.J., Bechara, A., Damasio, H., Ponto, L.L., Parvizi, J., Hichwa, R.D., Subcortical and cortical brain activity during the feeling of self-generated emotions (2000) Nat Neurosci, 3, pp. 1049-1056; Davidson, R.J., Ekman, P., Saron, C.D., Senulis, J.A., Friesen, W.V., Approach-withdrawal and cerebral asymmetry: Emotional expression and brain physiology: I (1990) J Personality Soc Psychol, 58, pp. 330-341; Doi, H., Nishitani, S., Shinohara, K., Nirs as a tool for assaying emotional function in the prefrontal cortex (2013) Front Human Neurosci, 7; Bechara, A., Damasio, H., Damasio, A.R., Emotion, decision making and the orbitofrontal cortex (2000) Cerebral cortex, 10, pp. 295-307; Zander, T.O., Kothe, C., Towards passive brain–computer interfaces: applying brain–computer interface technology to human–machine systems in general (2011) J Neural Eng, 8, p. 025005; Garcia-Molina, G., Tsoneva, T., Nijholt, A., Emotional brain–computer interfaces (2013) Int J of Auton Adapt Commun Syst, 6, pp. 9-25; Sassaroli, A., Fantini, S., Comment on the modified beerlambert law for cattering media (2004) Phys Med Biol, 49, pp. N255-N257; Heger, D., Mutter, R., Herff, C., Putze, F., Schultz, T., “Continuous recognition of affective states by functional near infrared spectroscopy signals”, in Affective Computing and Intelligent Interaction (ACII) (2013) IEEE, pp. 832-837; Jerritta, S., Murugappan, M., Nagarajan, R., Wan, K., “Physiological signals based human emotion recognition: a review,” in Signal Processing and its Applications (CSPA), 2011 IEEE 7th International Colloquium on (2011) IEEE, pp. 410-415; El Ayadi, M., Kamel, M.S., Karray, F., Survey on speech emotion recognition: Features, classification schemes, and databases (2011) Pattern Recognit, 44, pp. 572-587; Lang, P.J., Bradley, M.M., Cuthbert, B.N., (1999) International affective picture system (iaps): Technical manual and affective ratings, , Gainesville, FL: Technical report of University of Florida; Ekman, P., Facial expression and emotion (1993) Am Psycho, 48, pp. 384-392; Russell, J.A., Mehrabian, A., Evidence for a three-factor theory of emotions (1977) J Res Personality, 11, pp. 273-294; Lang, P., Bradley, M.M., The international affective picture system (iaps) in the study of emotion and attention (2007) Handbook of emotion elicitation and assessment, pp. 29-46; Bradley, M.M., Lang, P.J., (2007) The international affective digitized sounds (; iads-2): Affective ratings of sounds and instruction manual, , University of Florida, Gainesville, FL: Tech. Rep. B-3; Herrmann, M., Ehlis, A., Fallgatter, A., Prefrontal activation through task requirements of emotional induction measured with nirs (2003) Biol Psycho, 64, pp. 255-263; Hoshi, Y., Huang, J., Kohri, S., Iguchi, Y., Naya, M., Okamoto, T., Ono, S., Recognition of human emotions from cerebral blood flow changes in the frontal region: A study with event-related near-infrared spectroscopy (2011) J Neuroimaging, 21, pp. e94-e101; Morinaga, K., Akiyoshi, J., Matsushita, H., Ichioka, S., Tanaka, Y., Tsuru, J., Hanada, H., Anticipatory anxiety-induced changes in human lateral prefrontal cortex activity (2007) Biol Psycho, 74, pp. 34-38; Kreplin, U., Fairclough, S.H., Activation of the rostromedial prefrontal cortex during the experience of positive emotion in the context of aesthetic experience. an fnirs study (2013) Front Human Neurosci, 7; Leon-Carrion, J., Damas, J., Izzetoglu, K., Pourrezai, K., Martin-Rodriguez, J.F., Martin, J., Dominguez-Morales, M.R., Differential time course and intensity of pfc activation for men and women in response to emotional stimuli: a functional near-infrared spectroscopy (fnirs) study (2006) Neurosci Lett, 403, pp. 90-95; Yang, H., Zhou, Z., Liu, Y., Ruan, Z., Gong, H., Luo, Q., Lu, Z., Gender difference in hemodynamic responses of prefrontal area to emotional stress by near-infrared spectroscopy (2007) Behav Brain Res, 178, pp. 172-176; Glotzbach, E., Mühlberger, A., Gschwendtner, K., Fallgatter, A.J., Pauli, P., Herrmann, M.J., Prefrontal brain activation during emotional processing: A functional near infrared spectroscopy study (fnirs) (2011) The open neuroimaging journal, 5, pp. 33-39; Sitaram, R., Zhang, H., Guan, C., Thulasidas, M., Hoshi, Y., Ishikawa, A., Shimizu, K., Birbaumer, N., Temporal classification of multichannel near-infrared spectroscopy signals of motor imagery for developing a brain–computer interface (2007) NeuroImage, 34, pp. 1416-1427; Coyle, S.M., Ward, T.E., Markham, C.M., Brain–computer interface using a simplified functional near-infrared spectroscopy system (2007) J Neural Eng, 4, p. 219; Girouard, A., (2009) “Adaptive brain-computer interface,” in CHI ‘09 Extended Abstracts on Human Factors in Computing Systems, ser. CHI EA ’09, pp. 3097-3100; Ang, K.K., Guan, C., Lee, K., Lee, J.Q., Nioka, S., Chance, B., A brain-computer interface for mental arithmetic task from singletrial near-infrared spectroscopy brain signals, in Pattern Recognition (ICPR) 20th International Conference on. IEEE, pp. 3764-3767. , 2010; Herff, C., Putze, F., Heger, D., Guan, C., Schultz, T., Speaking mode recognition from functional near infrared spectroscopy, in Engineering in Medicine and Biology Society (EMBC), pp. 1715-1718. , 2012 Annual International Conference of the IEEE. IEEE, 2012; Herff, C., Heger, D., Putze, F., Hennrich, J., Fortmann, O., Schultz, T., Classification of mental tasks in the prefrontal cortex using fnirs, in Engineering in Medicine and Biology Society (EMBC) (2013) 2013 35th Annual International Conference of the IEEE. IEEE, pp. 2160-2163; Tai, K., Chau, T., Single-trial classification of nirs signals during emotional induction tasks: towards a corporeal machine interface (2009) J Neuroengineering and Rehabil, 6, p. 39; Hosseini, S., Mano, Y., Rostami, M., Takahashi, M., Sugiura, M., Kawashima, R., Decoding what one likes or dislikes from single-trial fnirs measurements (2011) Neuroreport, pp. 269-273; Moghimi, S., Kushki, A., Power, S., Guerguerian, A.M., Chau, T., Automatic detection of a prefrontal cortical response to emotionally rated music using multi-channel near-infrared spectroscopy (2012) J Neural Eng, 9, p. 026022; Asano, H., Sagami, T., Ide, H., The evaluation of the emotion by near-infrared spectroscopy (2013) Artif Life Rob, 17, pp. 452-456; Amma, C., Fischer, A., Stein, T., Schwameder, H., Schultz, T., Emotionserkennung auf der Basis von Gangmustern, in Sportinformatik trifft Sporttechnologie Tagung der dvs-Sektion Sportinformatik, , 2010; Matthews, F., Pearlmutter, B., Ward, T., Soraghan, C., Markham, C., Hemodynamics for brain-computer interfaces (2008) Signal Processing Magazine, IEEE, 25, pp. 87-94; Cooper, R., Selb, J., Gagnon, L., Phillip, D., Schytz, H., Iversen, H., Ashina, M., Boas, D., A systematic comparison of motion artifact correction techniques for functional near-infrared spectroscopy (2012) Front Neurosci, 6, p. 147; Molavi, B., Dumont, G.A., Wavelet-based motion artifact removal for functional near-infrared spectroscopy (2012) Physiol Meas, 33, p. 259; Khoa, T.Q.D., Nakagawa, M., Functional near infrared spectroscope for cognition brain tasks by wavelets analysis and neural networks (2008) Int J Biol Med Sci, 1, pp. 28-33; Tsunashima, H., Yanagisawa, K., Measurement of brain function of car driver using functional near-infrared spectroscopy (fnirs) (2009) Comput Intell Neurosci, 2009; Abibullaev, B., An, J., Classification of frontal cortex haemodynamic responses during cognitive tasks using wavelet transforms and machine learning algorithms (2012) Med Eng Phys, 34, pp. 1394-1410; Mallat, S.G., A theory for multiresolution signal decomposition: the wavelet representation (1989) Pattern Analysis and Machine Intelligence, IEEE Transactions on, 11, pp. 674-693; Daubechies, I., Ten lectures on wavelets (1992) SIAM, 61, pp. 1-357; Ang, K.K., Chin, Z.Y., Zhang, H., Guan, C., Mutual information based selection of optimal spatial–temporal patterns for single-trial eeg based bcis (2012) Pattern Recognit, 45, pp. 2137-2144; Marathe, A.R., Ries, A.J., McDowell, K., A novel method for single-trial classification in the face of temporal variability in Foundations of Augmented Cognition. Springer, pp. 345-352. , 2013; Bradley, M.M., Sabatinelli, D., Lang, P.J., Fitzsimmons, J.R., King, W., Desai, P., Activation of the visual cortex in motivated attention (2003) Behav Neurosci, 117, pp. 369-380; Rauschecker, J.P., Scott, S.K., Maps and streams in the auditory cortex: nonhuman primates illuminate human speech processing (2009) Nat Neurosci, 12, pp. 718-724; Grill-Spector, K., Malach, R., The human visual cortex (2004) Annu Rev Neurosci, 27, pp. 649-677},
correspondence_address1={Heger, D.; Cognitive Systems Lab, Karlsruhe Institute of TechnologyGermany; email: dominic.heger@kit.edu},
publisher={Taylor and Francis Ltd.},
issn={2326263X},
language={English},
abbrev_source_title={Brain-Computer Interfaces},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Srisuwan2014,
author={Srisuwan, N. and Wand, M. and Janke, M. and Phukpattaranont, P. and Schultz, T. and Limsakul, C.},
title={Enhancement of EMG-based Thai number words classification using frame-based time domain features with stacking filter},
journal={2014 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA 2014},
year={2014},
doi={10.1109/APSIPA.2014.7041549},
art_number={7041549},
note={cited By 0; Conference of 2014 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA 2014 ; Conference Date: 9 December 2014 Through 12 December 2014;  Conference Code:110955},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949925588&doi=10.1109%2fAPSIPA.2014.7041549&partnerID=40&md5=30ea51cf79d471fc1fb22592e602cc3d},
affiliation={Department of Electrical Engineering, Faculty of Engineering, Prince of Songkla University, Songkhla, Thailand; Cognitive Systems Laboratory, Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={In order to overcome a problem existing in a classical automatic speech recognition (e.g. ambient noise and loss of privacy), Electromyography (EMG) from speech production muscles was used in place of a human speech signal. We aim to investigate the EMG speech recognition based on Thai language. The earlier work, we used five channels of the EMG from the facial and neck muscles to classify 11 Thai number words based on Neural Network Classification. 15 features in time domain and frequency domain were employed for feature extraction. We obtained an average accuracy rate of 89.45% for audible speech and 78.55% for silent speech. However, it needs to be enhanced to get the best result. This paper proposes to improve an accuracy rate of EMG-based Thai number words classification. The ten subjects uttered 11 words in both an audible and a silent speech while five channels of the EMG signal were captured. Frame-based time domain features with a stacking filter was performed for feature extraction stage. After that, LDA was used to lessen a dimension of the feature vector. Hidden Markov Model (HMM) was employed in classification stage. The results show that using above techniques of feature extraction, feature dimensionality reduction and classification can improve an average accuracy rate by 3% absolute for audible speech when were compared to earlier work. We achieved an average classification rate of 92.45% and 75.73% for audible and silent speech respectively. © 2014 Asia-Pacific Signal and Information Processing Ass.},
keywords={Biomedical signal processing;  Classification (of information);  Electromyography;  Extraction;  Feature extraction;  Frequency domain analysis;  Hidden Markov models;  Markov processes;  Muscle;  Neural networks;  Speech;  Time domain analysis, Automatic speech recognition;  Classification rates;  Dimensionality reduction;  Feature vectors;  Frequency domains;  Neural network classification;  Speech production;  Time domain features, Speech recognition},
references={Srisuwan, N., Phukpattaranont, P., Limsakul, C., Feature selection for Thai tones classification based on surface EMG (2012) Proc. I-SEEC, 2012, pp. 253-259; Srisuwan, N., Phukpattaranont, P., Limsakul, C., Three steps of neuron network classification for EMG-based Thai tones speech recognition (2013) Proc. ECTI-CON, 2013, pp. 1-6; Sugie, N., Tsunoda, K., A speech prosthesis employing a speech synthesizer-vowel discrimination from perioral muscle activities and vowel production (1985) IEEE Trans. Biomed, Eng., 32, pp. 485-490. , July; Morse, M.S., O'Brien, E.M., Research summary of a scheme to ascertain the availability of speech information in the myoelectric signal of neck and head muscles using surface electrodes (1986) Comput. Biol. Med., 16, pp. 399-410; Chan, A.D.C., Englehart, K.B., Lovely, D.F., Myo-electric signals to augment speech recognition (2001) Med. Biol. Eng. Comput, 39, pp. 500-504; Manabe, H., Hiraiwa, A., Sugimura, T., Unvoice speech recognition using EMG-mime speech recognition (2003) Proc. Conference of Computer Human Interaction, pp. 794-795; Marier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) Proc. 2005 IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 331-336; Jou, S.C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proc. Interspeech, pp. 573-576; Wand, M., Schultz, T., Towards Speaker-adaptive speech recognition based on surface electromyography (2009) Proc. BIOSIGNALS, pp. 155-162; Wand, M., Schultz, T., Session-independent EMG-based speech recognition (2011) Proc. BIOSIGNALS; Phinyomark, A., Hirunviriya, S., Limsakul, C., Phukpattaranont, P., Evaluation of EMG feature extraction for hand movement recognition based on Euclidean distance and standard deviation (2010) Proc. International Conference Electrical Engineering/Electronics Computer Telecommunications and Information Technology, pp. 856-860; (2012) Human Facial Muscles, , http://www.corbisimages.com/stock-photo/rights-managed/42-17799652/human-facial-muscles, July 6; Omohyoid Muscle, , http://en.wikipedia.org/wiki/Omohyoid_muscle, October 20,2010; Balakrishnama, S., Ganapathi, A., Linear discriminant analysis-a brief tutorial Department of Electrical Engineering and Computer Engineering, , Missisipi State University, Missisipi},
sponsors={},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9786163618238},
language={English},
abbrev_source_title={Asia-Pac. Signal Inf. Process. Assoc. Annu. Summit Conf., APSIPA},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Herff2014,
author={Herff, C. and Heger, D. and Fortmann, O. and Hennrich, J. and Putze, F. and Schultz, T.},
title={Mental workload during n-back task-quantified in the prefrontal cortex using fNIRS},
journal={Frontiers in Human Neuroscience},
year={2014},
volume={7},
number={JAN},
doi={10.3389/fnhum.2013.00935},
art_number={935},
note={cited By 150},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893022713&doi=10.3389%2ffnhum.2013.00935&partnerID=40&md5=8267332cc2bbd6769affbcd901616d75},
affiliation={Cognitive Systems Lab, Institute for Anthropomatics, Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={When interacting with technical systems, users experience mental workload. Particularly in multitasking scenarios (e.g., interacting with the car navigation system while driving) it is desired to not distract the users from their primary task. For such purposes, human-machine interfaces (HCIs) are desirable which continuously monitor the users' workload and dynamically adapt the behavior of the interface to the measured workload. While memory tasks have been shown to elicit hemodynamic responses in the brain when averaging over multiple trials, a robust single trial classification is a crucial prerequisite for the purpose of dynamically adapting HCIs to the workload of its user. The prefrontal cortex (PFC) plays an important role in the processing of memory and the associated workload. In this study of 10 subjects, we used functional Near-Infrared Spectroscopy (fNIRS), a non-invasive imaging modality, to sample workload activity in the PFC. The results show up to 78% accuracy for single-trial discrimination of three levels of workload from each other. We use an n-back task (n ∈ {1, 2, 3}) to induce different levels of workload, forcing subjects to continuously remember the last one, two, or three of rapidly changing items. Our experimental results show that measuring hemodynamic responses in the PFC with fNIRS, can be used to robustly quantify and classify mental workload. Single trial analysis is still a young field that suffers from a general lack of standards. To increase comparability of fNIRS methods and results, the data corpus for this study is made available online. © 2014 Herff, Heger, Fortmann, Hennrich, Putze and Schultz.},
author_keywords={fNIRS;  Mental states;  n-back;  Near-infrared spectroscopy;  Passive BCI;  Prefrontal cortex;  User state monitoring;  Workload},
keywords={accuracy;  adult;  article;  controlled study;  female;  hemodynamics;  human;  human experiment;  male;  mental performance;  near infrared spectroscopy;  normal human;  prefrontal cortex;  quantitative analysis;  task performance;  young adult},
references={Ang, K., Guan, C., Lee, K., Lee, J., Nioka, S., Chance, B., A brain-computer interface for mental arithmetic task from single-trial near-infrared spectroscopy brain signals (2010) International Conference on Pattern Recognition, pp. 3764-3767. , in, (Istanbul); Ang, K.K., Guan, C., Lee, K., Lee, J.Q., Nioka, S., Chance, B., Application of rough set-based neuro-fuzzy system in nirs-based bci for assessing numerical cognition in classroom (2010) The 2010 International Joint Conference on Neural Networks (IJCNN), pp. 1-7. , in, (Barcelone); Ang, K.K., Chin, Z.Y., Zhang, H., Guan, C., Filter bank common spatial pattern (fbcsp) in brain-computer interface (2008) IEEE International Joint Conference on Neural Networks, 2008. IJCNN 2008. (IEEE World Congress on Computational Intelligence), pp. 2390-2397. , in, (Hong Kong); Ayaz, H., Izzetoglu, M., Bunce, S., Heiman-Patterson, T., Onaral, B., Detecting cognitive activity related hemodynamic signal for brain computer interface using functional near infrared spectroscopy (2007) 3rd International IEEE/EMBS Conference on Neural Engineering, 2007. CNE '07, pp. 342-345. , in, (Kohala Coast, HI); Ayaz, H., Shewokis, P.A., Bunce, S., Izzetoglu, K., Willems, B., Onaral, B., Optical brain monitoring for operator training and mental workload assessment (2012) Neuroimage, 59, pp. 36-47. , doi: 10.1016/j.neuroimage.2011.06.023; Ayaz, H., Willems, B., Bunce, B., Shewokis, P.A., Izzetoglu, K., Hah, S., Cognitive workload assessment of air traffic controllers using optical brain imaging sensors (2010) Advances in Understanding Human Performance: Neuroergonomics, Human Factors Design, and Special Populations, pp. 21-31. , in, eds T. Marek, W. Karwowski, and V. Rice (CRC Press Taylor & Francis Group); Baldwin, C.L., Penaranda, B., Adaptive training using an artificial neural network and eeg metrics for within-and cross-task workload classification (2012) Neuroimage, 59, pp. 48-56. , doi: 10.1016/j.neuroimage.2011.07.047; Berka, C., Levendowski, D.J., Lumicao, M.N., Yau, A., Davis, G., Zivkovic, V.T., Eeg correlates of task engagement and mental workload in vigilance, learning, and memory tasks (2007) Aviat. Space Environ. Med, 78, pp. B231-B244. , Available online at: Brouwer, A.-M., Hogervorst, M. A., Van Erp, J. B., Heffelaar, T., Zimmerman, P. H., and Oostenveld, R. (2012). Estimating workload using eeg spectral power and erps in the n-back task. J. Neural Eng. 9:045008. doi: 10.1088/1741-2560/9/4/045008; Cohen, J., Perlstein, W., Braver, T., Nystrom, L., Noll, D., Jonides, J., Temporal dynamics of brain activation during a working memory task (1997) Nature, 386, p. 604. , doi: 10.1038/386604a0; Cooper, R., Selb, J., Gagnon, L., Phillip, D., Schytz, H.W., Iversen, H.K., A systematic comparison of motion artifact correction techniques for functional near-infrared spectroscopy (2012) Front. Neurosci, 6, p. 147. , doi: 10.3389/fnins.2012.00147; Coyle, S.M., Ward, T.E., Markham, C.M., Brain-computer interface using a simplified functional near-infrared spectroscopy system (2007) J. Neural Eng, 4, p. 219. , doi: 10.1088/1741-2560/4/3/007; Cui, X., Bray, S., Reiss, A., Functional near infrared spectroscopy (NIRS) signal improvement based on negative correlation between oxygenated and deoxygenated hemoglobin dynamics (2010) Neuroimage, 49, pp. 3039-3046. , doi: 10.1016/j.neuroimage.2009.11.050; Cui, X., Bray, S., Reiss, A.L., Speeded near infrared spectroscopy (nirs) response detection (2010) PLoS ONE, 5, pp. e15474. , doi: 10.1371/journal.pone.0015474; Cutrell, E., Tan, D., Bci for passive input in hci (2008) Proceedings of CHI, 8, pp. 1-3. , in, (Citeseer); Duda, R.O., Hart, P.E., Stork, D.G., (2012) Pattern Classification, , New York, NY: John Wiley & Sons; Girouard, A., Solovey, E., Hirshfield, L., Chauncey, K., Sassaroli, A., Fantini, S., Distinguishing difficulty levels with non-invasive brain activity measurements (2009) Human-Computer Interaction INTERACT 2009, Volume 5726 of Lecture Notes in Computer Science, pp. 440-452. , in, eds T. Gross, J. Gulliksen, P. Kotz, L. Oestreicher, P. Palanque, R. Prates, and M. Winckler (Berlin; Heidelberg: Springer); Heger, D., Mutter, R., Herff, C., Putze, F., Schultz, T., Continuous recognition of affective states by functional near infrared spectroscopy signals (2013) Affective Computing and Intelligent Interaction (ACII), 2013 Humaine Association Conference on, pp. 832-837. , in, (Geneva: IEEE); Herff, C., Heger, D., Putze, F., Hennrich, J., Fortmann, O., Schultz, T., Classification of mental tasks in the prefrontal cortex using fnirs (2013) Engineering in Medicine and Biology Society (EMBC), 2013 35th Annual International Conference of the IEEE, pp. 2160-2163. , in, (San Diego); Herff, C., Putze, F., Heger, D., Guan, C., Schultz, T., Speaking mode recognition from functional near infrared spectroscopy (2012) Engineering in Medicine and Biology Society (EMBC), 2012 Annual International Conference of the IEEE, pp. 1715-1718. , in, (Osaka); Hirshfield, L.M., Gulotta, R., Hirshfield, S., Hincks, S., Russell, M., Ward, R., This is your brain on interfaces: Enhancing usability testing with functional near-infrared spectroscopy (2011) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 373-382. , in, (ACM), (Vancouver, BC); Hoshi, Y., Tsou, B.H., Billock, V.A., Tanosaki, M., Iguchi, Y., Shimada, M., Spatiotemporal characteristics of hemodynamic changes in the human lateral prefrontal cortex during working memory tasks (2003) Neuroimage, 20, pp. 1493-1504. , doi: 10.1016/S1053-8119(03)00412-9; Izzetoglu, K., Bunce, S., Izzetoglu, M., Onaral, B., Pourrezaei, K., fNIR spectroscopy as a measure of cognitive task load (2003) Engineering in Medicine and Biology Society, 2003. Proceedings of the 25th Annual International Conference of the IEEE, 4, pp. 3431-3434. , in, (Cancun); Jarvis, J., Putze, F., Heger, D., Schultz, T., Multimodal person independent recognition of workload related biosignal patterns (2011) Proceedings of the 13th International Conference on Multimodal Interfaces, ICMI '11, pp. 205-208. , in, (New York, NY: ACM); Kothe, C., Makeig, S., Estimation of task workload from eeg data: New and current tools and perspectives (2011) Engineering in Medicine and Biology Society, EMBC, 2011 Annual International Conference of the IEEE, pp. 6547-6551. , in, (Boston); Molavi, B., Dumont, G., Wavelet based motion artifact removal for functional near infrared spectroscopy (2010) Engineering in Medicine and Biology Society (EMBC), 2010 Annual International Conference of the IEEE, pp. 5-8. , in, (Buenos Aires); Oldfield, R., The assessment and analysis of handedness: The Edinburgh inventory (1971) Neuropsychologia, 9, pp. 97-113. , doi: 10.1016/0028-3932(71)90067-4; Owen, A.M., McMillan, K.M., Laird, A.R., Bullmore, E., N-back working memory paradigm: A meta-analysis of normative functional neuroimaging studies (2005) Hum. Brain Mapp, 25, pp. 46-59. , doi: 10.1002/hbm.20131; Power, S.D., Falk, T.H., Chau, T., Classification of prefrontal activity due to mental arithmetic and music imagery using hidden markov models and frequency domain near-infrared spectroscopy (2010) J. Neural Eng, 7, p. 026002. , doi: 10.1088/1741-2560/7/2/026002; Power, S.D., Kushki, A., Chau, T., Intersession consistency of single-trial classification of the prefrontal response to mental arithmetic and the no-control state by nirs (2012) PLoS ONE, 7, pp. e37791. , doi: 10.1371/journal.pone.0037791; Sassaroli, A., Fantini, S., Comment on the modified beerlambert law for scattering media (2004) Phys. Med. Biol, 49, pp. N255. , doi: 10.1088/0031-9155/49/14/N07; Sitaram, R., Zhang, H., Guan, C., Thulasidas, M., Hoshi, Y., Ishikawa, A., Temporal classification of multichannel near-infrared spectroscopy signals of motor imagery for developing a braincomputer interface (2007) Neuroimage, 34, pp. 1416-1427. , doi: 10.1016/j.neuroimage.2006.11.005; Smith, E.E., Jonides, J., Working memory: A view from neuroimaging (1997) Cogn. Psychol, 33, pp. 5-42. , doi: 10.1006/cogp.1997.0658; Wolpaw, J.R., Birbaumer, N., McFarland, D.J., Pfurtscheller, G., Vaughan, T.M., Brain-computer interfaces for communication and control (2002) Clin. Neurophysiol, 113, pp. 767-791. , doi: 10.1016/S1388-2457(02)00057-3; Zander, T.O., Kothe, C., Towards passive braincomputer interfaces: Applying braincomputer interface technology to humanmachine systems in general (2011) J. Neural Eng, 8, p. 025005. , doi: 10.1088/1741-2560/8/2/025005},
correspondence_address1={Herff, C.; Cognitive Systems Lab, Institute for Anthropomatics, Karlsruhe Institute of Technology, Adenauerring 4, 76131 Karlsruhe, Germany; email: christian.herff@kit.edu},
publisher={Frontiers Media S. A.},
issn={16625161},
language={English},
abbrev_source_title={Front. Human Neurosci.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Schultz2014337,
author={Schultz, T. and Schlippe, T.},
title={GlobalPhone: Pronunciation dictionaries in 20 languages},
journal={Proceedings of the 9th International Conference on Language Resources and Evaluation, LREC 2014},
year={2014},
pages={337-341},
note={cited By 7; Conference of 9th International Conference on Language Resources and Evaluation, LREC 2014 ; Conference Date: 26 May 2014 Through 31 May 2014;  Conference Code:131726},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010765237&partnerID=40&md5=28ab77a56f00589421d98fa3f9aa0620},
affiliation={Cognitive Systems Lab., Karlsruhe Institute of Technology (KIT), Germany},
abstract={This paper describes the advances in the multilingual text and speech database GLOBALPHONE a multilingual database of high-quality read speech with corresponding transcriptions and pronunciation dictionaries in 20 languages. GLOBALPHONE was designed to be uniform across languages with respect to the amount of data, speech quality, the collection scenario, the transcription and phone set conventions. With more than 400 hours of transcribed audio data from more than 2000 native speakers GLOBALPHONE supplies an excellent basis for research in the areas of multilingual speech recognition, rapid deployment of speech processing systems to yet unsupported languages, language identification tasks, speaker recognition in multiple languages, multilingual speech synthesis, as well as monolingual speech recognition in a large variety of languages. Very recently the GLOBALPHONE pronunciation dictionaries have been made available for research and commercial purposes by the European Language Resources Association (ELRA).},
author_keywords={Dictionary resources for multilingual speech processing;  Speech;  Text},
keywords={Audio systems;  Speech;  Speech processing;  Speech synthesis;  Transcription, Language identification;  Multilingual database;  Multilingual speech processing;  Multilingual speech recognition;  Multilingual texts;  Pronunciation dictionaries;  Speaker recognition;  Text, Speech recognition},
references={(2012) Speech and Language Resources 2012, , Appen Butler Hill Pty Ltd. Appen Butler Hill Speech and Language Resources 2012 - Product Catalogue; Bisani, M., Ney, H., Joint-sequence models for grapheme-to-phoneme conversion (2008) Speech Communication, 50, pp. 434-451; (2012) European Language Resources Association (ELRA), , http://catalog.elra.info, ELRA catalogue. Retrieved November 30, 2012; (1999) Handbook of the International Phonetic Association. A Guide to the use of the International Phonetic Alphabet, , International Phonetic Association Cambridge University Press, Cambridge; Lewis, M., Simons, G., Fennis, C., (2013) Ethnologue: Languages of the World (17th Ed.), , SIL International, Dallas, TX; (2012) Benchmark GlobalPhone Language Models, , http://csl.ira.uka.de/GlobalPhone, LM-BM Retrieved November 30, 2012; Schlippe, T., Volovyk, M., Yurchenko, K., Schultz, T., Rapid bootstrapping of a ukrainian large vocabulary continuous speech recognition system (2013) Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), , Vancouver, BC, Canada, May 26-31; Schlippe, T., Ochs, S., Schultz, T., Web-based tools and methods for rapid pronunciation dictionary creation (2014) Speech Communication, 56, pp. 101-118; Schultz, T., Kirchhoff, K., (2006) Multilingual Speech Processing, , Elsevier Academic Press; Schultz, T., Vu, N.T., Schlippe, T., Globalphone: A multilingual text and speech database in 20 languages (2013) Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 8126-8130. , Vancouver, BC, Canada, May 26-31; Schultz, T., Globalphone: A multilingual speech and text database developed at karlsruhe University (2002) Proceedings of the International Conference on Spoken Language Processing (ICSLP), pp. 345-348. , Denver, Co; Schultz, T., Towards rapid language portability of speech processing systems (2004) Conference on Speech and Language Systems for Human Communication (SPLASH), 1. , Delhi, India, November; Stolcke, A., SRILM - An extensible language modeling toolkit (2002) Proceedings of the International Conference on Spoken Language Processing (ICSLP), , Denver, Co; Vu, N.T., Schlippe, T., Kraus, F., Schultz, T., Rapid bootstrapping of five Eastern European languages using the rapid language adaptation toolkit (2010) Proceedings of InterSpeech, pp. 865-868. , Makuhari, Japan},
editor={Calzolari N., Choukri K., Goggi S., Declerck T., Mariani J., Maegaard B., Moreno A., Odijk J., Mazo H., Piperidis S., Loftsson H.},
sponsors={European Media Laboratory GmbH (EML); Holmes Semantic Solutions; IMMI; KDictionaries; VoiceBox Technologies},
publisher={European Language Resources Association (ELRA)},
isbn={9782951740884},
language={English},
abbrev_source_title={Int. Conf. Lang. Resourc. and Eval. - LREC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wand20144200,
author={Wand, M. and Schultz, T.},
title={Pattern learning with deep neural networks in EMG-based speech recognition},
journal={2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2014},
year={2014},
pages={4200-4203},
doi={10.1109/EMBC.2014.6944550},
art_number={6944550},
note={cited By 18; Conference of 2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2014 ; Conference Date: 26 August 2014 Through 30 August 2014;  Conference Code:109045},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929501278&doi=10.1109%2fEMBC.2014.6944550&partnerID=40&md5=452cee46541f48082ddd71f4bf880c29},
affiliation={Swiss AI Lab IDSIA, Istituto Dalle Molle di Studi sull'Intelligenza Artificiale, University of Lugano and SUPSI, Manno-Lugano, Switzerland; Cognitive Systems Lab, Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={We report on classification of phones and phonetic features from facial electromyographic (EMG) data, within the context of our EMG-based Silent Speech interface. In this paper we show that a Deep Neural Network can be used to perform this classification task, yielding a significant improvement over conventional Gaussian Mixture models. Our central contribution is the visualization of patterns which are learned by the neural network. With increasing network depth, these patterns represent more and more intricate electromyographic activity. © 2014 IEEE.},
keywords={artificial intelligence;  artificial neural network;  automated pattern recognition;  electromyography;  face;  face muscle;  human;  human experiment;  language;  pattern recognition;  phonetics;  physiology;  procedures;  speech, Artificial Intelligence;  Electromyography;  Face;  Facial Muscles;  Humans;  Language;  Neural Networks (Computer);  Nontherapeutic Human Experimentation;  Pattern Recognition, Automated;  Pattern Recognition, Physiological;  Phonetics;  Speech},
references={Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Silent speech interfaces (2010) Speech Communication, 52 (4), pp. 270-287; Wand, M., Schulte, C., Janke, M., Schultz, T., Array-based electromyographic silent speech interface (2013) Proc. Biosignals, pp. 89-96; Schultz, T., Wand, M., Modeling coarticulation in large vocabulary EMG-based speech recognition (2010) Speech Communication, 52 (4), pp. 341-353; Wand, M., Schultz, T., Session-independent EMG-based speech recognition (2011) Proc. Biosignals, pp. 295-300; Janke, M., Wand, M., Schultz, T., A spectral mapping method for EMG-based recognition of silent speech (2010) Proc. B-INTERFACE, pp. 22-31; Wand, M., Schultz, T., Analysis of phone confusion in EMG-based speech recognition (2011) Proc. ICASSP, pp. 757-760; Deng, Y., Patel, R., Heaton, J.T., Colby, G., Gilmore, L.D., Cabrera, J., Roy, S.H., Meltzner, G.S., Disordered speech recognition using acoustic and sEMG signals (2009) Proc. Interspeech, pp. 644-647; Freitas, J., Teixeira, A., Dias, M.S., Towards a silent speech interface for portuguese (2012) Proc. Biosignals, pp. 91-100; Srisuwan, N., Phukpattaranont, P., Limsakul, C., Feature selection for thai tone classification based on surface EMG (2012) Procedia Engineering, 32, pp. 253-259; Toth, A., Wand, M., Schultz, T., Synthesizing speech from electromyography using voice transformation techniques (2009) Proc. Interspeech, pp. 652-655; Johner, C., Janke, M., Wand, M., Schultz, T., Inferring prosody from facial cues for EMG-based synthesis of silent speech (2012) Proc. AHFE, pp. 5317-5326; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proc. Interspeech, pp. 573-576; Hinton, G.E., Salakhutdinov, R.R., Reducing the dimensionality of data with neural networks (2006) Science, 313, pp. 504-507; Cho, K., Raiko, T., Ilin, A., Gaussian-bernoulli deep boltzmann machine (2013) Proc. IJCNN; http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html, Available online; Erhan, D., Bengio, Y., Courville, A., Vincent, P., Visualizing higher-layer features of a deep network (2009) University of Montreal, , Tech. Rep; Heistermann, T., Janke, M., Wand, M., Schultz, T., Spatial artifact detection for multi-channel EMG-based speech recognition (2014) Proc. Biosignals, pp. 189-196},
sponsors={},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781424479290},
pubmed_id={25570918},
language={English},
abbrev_source_title={Annu. Int. Conf. IEEE Eng. Med. Biol. Soc., EMBC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wand20141189,
author={Wand, M. and Schultz, T.},
title={Towards real-life application of EMG-based speech recognition by using unsupervised adaptation},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2014},
pages={1189-1193},
note={cited By 13; Conference of 15th Annual Conference of the International Speech Communication Association: Celebrating the Diversity of Spoken Languages, INTERSPEECH 2014 ; Conference Date: 14 September 2014 Through 18 September 2014;  Conference Code:108771},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910093118&partnerID=40&md5=a0e03cd9acbd24eee3f7a6e28725b050},
affiliation={Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={This paper deals with a Silent Speech Interface based on Surface Electromyography (EMG), where electrodes capture the electric activity generated by the articulatory muscles from a user's face in order to decode the underlying speech, allowing speech to be recognized even when no sound is heard or created. So far, most EMG-based speech recognizers described in literature do not allow electrode reattachment between system training and usage, which we consider unsuitable for practical applications. In this study we report on our research on unsupervised session adaptation: A system is pre-trained with data from multiple recording sessions and then adapted towards the current recording session using data accruable during normal use, without requiring a time-consuming specific enrollment phase. We show that considerable accuracy improvements can be achieved with this method, paving the way towards real-life applications of the technology. Copyright © 2014 ISCA.},
author_keywords={Emg;  Emg-based speech recognition;  Silent speech interfaces;  Unsupervised adaptation},
keywords={Audio recordings;  Electrodes;  Electromyography;  Speech;  Speech communication, Accuracy Improvement;  Electric activity;  Emg;  Multiple recordings;  Real-life applications;  Silent speech interfaces;  Surface electromyography;  Unsupervised adaptation, Speech recognition},
references={Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Silent speech interfaces (2010) Speech Communication, 52 (4), pp. 270-287; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) Proc. ASRU, pp. 331-336; Wand, M., Schultz, T., Session-independent EMG-based speech recognition (2011) Proc. Biosignals, pp. 295-300; Sugie, N., Tsunoda, K., A speech prosthesis employing a speech synthesizer - vowel discrimination from perioral muscle activities and vowel production (1985) IEEE Transactions on Biomedical Engineering, 32 (7), pp. 485-490; Morse, M.S., O'Brien, E.M., Research summary of a scheme to ascertain the availability of speech information in the myoelectric signals of neck and head muscles using surface electrodes (1986) Computers in Biology and Medicine, 16 (6), pp. 399-410; Jorgensen, C., Lee, D.D., Agabon, S., Sub auditory speech recognition based on EMG/EPG signals (2003) Proc. IJCNN, pp. 3128-3133. , Portland, Oregon; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proc. Interspeech, pp. 573-576. , Pittsburgh, PA, Sep; Schultz, T., Wand, M., Modeling coarticulation in large vocabulary EMG-based speech recognition (2010) Speech Communication, 52 (4), pp. 341-353; Meltzner, G.S., Colby, G., Deng, Y., Heaton, J.T., Signal acquisition and processing techniques for sEMG based silent speech recognition (2011) Proc. EMBC, pp. 4848-4851; Wand, M., Schulte, C., Janke, M., Schultz, T., Array-based electromyographic silent speech interface (2013) Proc. Biosignals, pp. 89-96; Janke, M., Wand, M., Schultz, T., A spectral mapping method for EMG-based recognition of silent speech (2010) Proc. B-INTERFACE, pp. 22-31; Janke, M., Wand, M., Schultz, T., Impact of lack of acoustic feedback in EMG-based silent speech recognition (2010) Proc. Interspeech, pp. 2686-2689; Wand, M., Janke, M., Schultz, T., Investigations on speaking mode discrepancies in EMG-based speech recognition (2011) Proc. Interspeech, pp. 601-604; Wand, M., Janke, M., Schultz, T., Decision-tree based analysis of speaking mode discrepancies in EMG-based speech recognition (2012) Proc. Biosignals, pp. 101-109; Freitas, J., Teixeira, A., Silva, S., Oliveira, C., Dias, M.S., Velum movement detection based on surface electromyography for speech interface (2014) Proc. Biosignals, pp. 13-20; Toth, A., Wand, M., Schultz, T., Synthesizing speech from electromyography using voice transformation techniques (2009) Proc. Interspeech, pp. 652-655; Lee, K.-S., Prediction of acoustic feature parameters using myoelectric signals (2010) IEEE Transactions on Biomedical Engineering, 57, pp. 1587-1595; Janke, M., Wand, M., Nakamura, K., Schultz, T., Further investigations on EMG-to-speech conversion (2012) Proc. ICASSP, pp. 365-368; Wand, M., Janke, M., Schultz, T., The EMG-UKA corpus for electromyographic speech processing (2014) Proc. Interspeech; Schünke, M., Schulte, E., Schumacher, U., (2006) Prometheus - Lernatlas Der Anatomie, 3. , Stuttgart, New York: Thieme Verlag, Kopf und Neuroanatomie; Maier-Hein, L., (2005) Speech Recognition Using Surface Electromyography, , Diploma thesis, Interactive Systems Labs, University of Karlsruhe; Gales, M.J.F., Woodland, P.C., Mean and variance adaptation within the MLLR framework (1996) Computer Speech and Language, 10, pp. 249-264; Gales, M.J.F., The generation and use of regression class trees for MLLR adaptation (1996) Cambridge University Engineering Department, Tech. Rep.; Kemp, T., Schaaf, T., Estimating confidence using word lattices (1997) Proc. Eurospeech, pp. 827-830},
correspondence_address1={Wand, M.; Karlsruhe Institute of TechnologyGermany},
editor={Chng E.S., Li H., Meng H., Ma B., Xie L.},
sponsors={Amazon; Baidu; et al.; Google; Temasek Laboratories at Nanyang Technological University (TL at NTU); WeChat},
publisher={International Speech and Communication Association},
issn={2308457X},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Heger20142012,
author={Heger, D. and Herff, C. and Schultz, T.},
title={Combining feature extraction and classification for fNIRS BCIs by regularized least squares optimization},
journal={2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2014},
year={2014},
pages={2012-2015},
doi={10.1109/EMBC.2014.6944010},
art_number={6944010},
note={cited By 4; Conference of 2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2014 ; Conference Date: 26 August 2014 Through 30 August 2014;  Conference Code:109045},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929493486&doi=10.1109%2fEMBC.2014.6944010&partnerID=40&md5=15fdc244ca410808f14c65da89ee1a59},
affiliation={Cognitive Systems Lab, Institute for Anthro-pomatics and Robotics, Karlsruhe Institute of Technology, Adenauerring 4, Karlsruhe, 76131, Germany},
abstract={In this paper, we show that multiple operations of the typical pattern recognition chain of an fNIRS-based BCI, including feature extraction and classification, can be unified by solving a convex optimization problem. We formulate a regularized least squares problem that learns a single affine transformation of raw HbO<inf>2</inf> and HbR signals. We show that this transformation can achieve competitive results in an fNIRS BCI classification task, as it significantly improves recognition of different levels of workload over previously published results on a publicly available n-back data set. Furthermore, we visualize the learned models and analyze their spatio-temporal characteristics. © 2014 IEEE.},
keywords={algorithm;  brain computer interface;  female;  human;  male;  near infrared spectroscopy;  procedures;  regression analysis;  theoretical model, Algorithms;  Brain-Computer Interfaces;  Female;  Humans;  Least-Squares Analysis;  Male;  Models, Theoretical;  Spectroscopy, Near-Infrared},
references={Sassaroli, A., Fantini, S., Comment on the modified beerlambert law for scattering media (2004) Physics in Medicine and Biology, 49 (14); Tai, K., Chau, T., Single-trial classification of nirs signals during emotional induction tasks: Towards a corporeal machine interface (2009) Journal of Neuroengineering and Rehabilitation, 6, p. 39; Moghimi, S., Kushki, A., Power, S., Guerguerian, A.M., Chau, T., Automatic detection of a prefrontal cortical response to emotionally rated music using multi-channel near-infrared spectroscopy (2012) Journal of Neural Engineering, 9 (2), p. 026022; Herff, C., Heger, D., Fortmann, O., Hennrich, J., Putze, F., Schultz, T., Mental workload during n-back task quantified in the prefrontal cortex using fnirs (2013) Frontiers in Human Neuroscience, 7; Heger, D., Herff, C., Putze, F., Mutter, R., Schultz, T., Continuous affective states recognition using functional near infrared spectroscopy (2014) Brain-Computer Interfaces, 1 (2), pp. 113-125; Khoa, T.Q.D., Nakagawa, M., Functional near infrared spectroscope for cognition brain tasks by wavelets analysis and neural networks (2008) Int J Biol Med Sci, 1, pp. 28-33; Sitaram, R., Zhang, H., Guan, C., Thulasidas, M., Hoshi, Y., Ishikawa, A., Shimizu, K., Birbaumer, N., Temporal classification of multichannel near-infrared spectroscopy signals of motor imagery for developing a brain-computer interface (2007) NeuroImage, 34 (4), pp. 1416-1427; Power, S.D., Kushki, A., Chau, T., Towards a system-paced near-infrared spectroscopy brain-computer interface: Differentiating prefrontal activity due to mental arithmetic and mental singing from the no-control state (2011) Journal of Neural Engineering, 8 (6), p. 066004; Rahnuma, K.S., Wahab, A., Kamaruddin, N., Majid, H., Eeg analysis for understanding stress based on affective model basis function (2011) Consumer Electronics (ISCE), pp. 592-597. , 2011 IEEE 15th International Symposium on. IEEE; Farquhar, J., Hill, J., Schölkopf, B., (2006) NIPS 2006 Workshop on Current Trends Brain-Computer Interfacing, , Whistler, Canada; Parra, L.C., Christoforou, C., Gerson, A.D., Dyrholm, M., Luo, A., Wagner, M., Philiastides, M.G., Sajda, P., Spatiotemporal linear decoding of brain state (2008) Signal Processing Magazine, 25 (1), pp. 107-115. , IEEE; Tomioka, R., Müller, K.-R., A regularized discriminative framework for eeg analysis with application to brain-computer interface (2010) Neuroimage, 49 (1), pp. 415-432; Blankertz, B., Tomioka, R., Lemm, S., Kawanabe, M., Muller, K.-R., Optimizing spatial filters for robust eeg single-trial analysis (2008) Signal Processing Magazine, 25 (1), pp. 41-56. , IEEE; Grant, M., Boyd, S., Ye, Y., (2008) Cvx: Matlab Software for Disciplined Convex Programming; Tomioka, R., Sugiyama, M., Dual-augmented lagrangian method for efficient sparse reconstruction (2009) Signal Processing Letters, 16 (12), pp. 1067-1070. , IEEE; Schmidt, M., Fung, G., Rosales, R., Fast optimization methods for l1 regularization: A comparative study and two new approaches (2007) Machine Learning: ECML 2007, pp. 286-297. , Springer; Boyd, S., Parikh, N., Chu, E., Peleato, B., Eckstein, J., Distributed optimization and statistical learning via the alternating direction method of multipliers (2011) Foundations and Trends® in Machine Learning, 3 (1), pp. 1-122; Muller, K., Anderson, C.W., Birch, G.E., Linear and nonlinear methods for brain-computer interfaces (2003) Neural Systems and Rehabilitation Engineering, 11 (2), pp. 165-169. , IEEE Transactions on; Tibshirani, R., Regression shrinkage and selection via the lasso (1996) Journal of the Royal Statistical Society, pp. 267-288. , Series B (Methodological); Haufe, S., Meinecke, F., Görgen, K., Dähne, S., Haynes, J.-D., Blankertz, B., Bießmann, F., On the interpretation of weight vectors of linear models in multivariate neuroimaging (2014) NeuroImage, 87, pp. 96-110},
sponsors={},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781424479290},
pubmed_id={25570378},
language={English},
abbrev_source_title={Annu. Int. Conf. IEEE Eng. Med. Biol. Soc., EMBC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schlippe20142867,
author={Schlippe, T. and Merz, M. and Schultz, T.},
title={Methods for efficient semi-automatic pronunciation dictionary bootstrapping},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2014},
pages={2867-2871},
note={cited By 0; Conference of 15th Annual Conference of the International Speech Communication Association: Celebrating the Diversity of Spoken Languages, INTERSPEECH 2014 ; Conference Date: 14 September 2014 Through 18 September 2014;  Conference Code:108771},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910068989&partnerID=40&md5=c44eedfa0ed905b66125506b9b8d8ca9},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology (KIT), Germany},
abstract={In this paper we propose efficient methods which contribute to a rapid and economic semi-automatic pronunciation dictionary development and evaluate them on English, German, Spanish, Vietnamese, Swahili, and Haitian Creole. First we determine optimal strategies for the word selection and the period for the grapheme-to-phoneme model retraining. In addition to the traditional concatenation of single phonemes most commonly associated with each grapheme, we show that web-derived pronunciations and cross-ligual grapheme-to-phoneme models can help to reduce the initial editing effort. Furthermore we show that our phoneme-level combination of the output of multiple grapheme-to-phoneme converters reduces the editing effort more than the best single converters. Totally, we report on average 15% relative editing effort reduction with our phonemelevel combination compared to conventional methods. An additional reduction of 6% relative was possible by including initial pronunciations from Wiktionary for English, German, and Spanish. Copyright © 2014 ISCA.},
author_keywords={Phoneme-level combination;  Pronunciation modeling;  Semi-automatic pronunciation generation;  Web-derived pronunciations},
keywords={Social networking (online);  Speech communication, Conventional methods;  Grapheme to phonemes;  Grapheme-to-phoneme converters;  Phoneme-level combination;  Pronunciation dictionaries;  Pronunciation modeling;  Semi-automatics;  Web-derived pronunciations, Automation},
references={Maskey, S.R., Black, A.W., Tomokiyo, L.M., Bootstrapping phonetic lexicons for new languages (2004) 8th International Conference on Spoken Language Processing (ICSLP), , Jeju, Korea, 4-8 October; Davel, M., Martirosian, O., Pronunciation dictionary development in resource-scarce environments (2009) 10th Annual Conference of the International Speech Communication Association (Interspeech), , Brighton, UK, 6-10 September; Schultz, T., Black, A.W., Badaskar, S., Hornyak, M., Kominek, J., SPICE: Web-based tools for rapid language adaptation in speech processing systems (2007) The 8th Annual Conference of the International Speech Communication Association (Interspeech 2007, , Antwerp, Belgium, 27-31 August; Kominek, J., (2006) TTS from Zero: Building Synthetic Voices for New Languages, , Ph.D. thesis, Language Technologies Institute, School of Computer Science, Carnegie Mellon University; Vu, N.T., Schlippe, T., Kraus, F., Schultz, T., Rapid bootstrapping of five eastern european languages using the rapid language adaptation toolkit (2010) The 11th Annual Conference of the International Speech Communication Association (Interspeech 2010), , Makuhari, Japan, 26-30 September; Kominek, J., Black, A.W., Learning pronunciation dictionaries: Language complexity and word selection strategies (2006) Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, , New York, New York, USA, 4-6 June; Davel, M., Barnard, E., Efficient generation of pronunciation dictionaries: Human factors during bootstrapping (2004) 8th International Conference on Spoken Language Processing (ICSLP), , Jeju Island, Korea, 4-8 October; Davel, M., Barnard, E., The efficient creation of pronunciation dictionaries: Machine learning factors in bootstrapping (2004) 8th International Conference on Spoken Language Processing, , Jeju Island, Korea, 4-8 October; Davel, M., Barnard, E., Bootstrapping pronunciation dictionaries: Practical issues (2005) The Annual Conference of the International Speech Communication Association (Interspeech 2005), , Lisbon, Portugal, 4-8 September; Davis, S.L., Fetters, S., Gustafson, B., Loney, L., Schulz, D.E., System and method for preparing a pronunciation dictionary for a text-to-speech voice (2005) Tech. Rep., AT&T, , September; Ghoshal, A., Jansche, M., Khudanpur, S., Riley, M., Ulinski, M., Web-derived pronunciations (2009) IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), , Taipei, Taiwan, 19-24 April; Schlippe, T., Ochs, S., Schultz, T., Wiktionary as a source for automatic pronunciation extraction (2010) The 11th Annual Conference of the International Speech Communication Association (Interspeech), , Makuhari, Japan, 26-30 September; Schlippe, T., Quaschningk, W., Schultz, T., Combining grapheme-to-phoneme converter outputs for enhanced pronunciation generation in low-resource scenarios (2014) The 4th Workshop on Spoken Language Technologies for Under-resourced Languages (SLTU 2014), , St. Petersburg, Russia, 14-16 May; Vu, N.T., Schultz, T., Vietnamese large vocabulary continuous speech recognition (2009) 2009 IEEE Automatic Speech Recognition and Understanding (ASRU), , Merano, Italy, 13-17 December; Schultz, T., Schlippe, T., Global phone: Pronunciation dictionaries in 20 languages (2014) The 9th Edition of the Language Resources and Evaluation Conference (LREC 2014), , Reykjavik, Iceland, 26-31 May; Schultz, T., Vu, N.T., Schlippe, T., Global phone: A multilingual text & speech database in 20 languages (2013) The 38th International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2013), , Vancouver, Canada, 26-31 May; Bisani, M., Ney, H., Joint-sequence models for grapheme-to-phoneme conversion (2008) Speech Communication, 50 (5), pp. 434-451. , May; Novak, J.R., (2011) Phonetisaurus: A WFST-driven Phoneticizer, , http://code.google.com/p/phonetisaurus/; Novak, J.R., Minematsu, N., Hirose, K., WFST-based grapheme-to-phoneme conversion: Open source tools for alignment, model-building and decoding (2012) The 10th InternationalWorkshop on Finite State Methods and Natural Language Processing, , Donostia-San Sebastin, July, Association for Computational Linguistics; Davel, M., (2005) The Default & Refine Algorithm, A Rule-based Learning Algorithm, , http://code.google.com/p/defaultrefine/, August; Davel, M., Barnard, E., A default-and-refinement approach to pronunciation prediction (2004) Symposium of the Pattern Recognition Association of South Africa, , South Africa; Davel, M., Barnard, E., Pronunciation prediction with default & refine (2008) Computer Speech and Language, 22 (4), pp. 374-393. , October; Lenzo, K., (1997) T2p: Text-to-Phoneme Converter Builder, , http://www.cs.cmu.edu/afs/cs.cmu.edu/user/lenzo/html/areas/t2p/; Stolcke, A., SRILM - An extensible language modeling toolkit (2002) International Conference on Spoken Language Processing (ICSLP), , Denver, Colerado, 16-20 September; Schlippe, T., Ochs, S., Schultz, T., Web-based tools and methods for rapid pronunciation dictionary creation (2014) Speech Communication, 56, pp. 101-118; Schlippe, T., Ochs, S., Schultz, T., Grapheme-to- phoneme model generation for indo-european languages (2012) The 37th International Conference on Acoustics, Speech, and Signal Processing (ICASSP), , Kyoto, Japan, 25-30 March; Schlippe, T., Ochs, S., Schultz, T., Automatic error recovery for pronunciation dictionaries (2012) The 13th Annual Conference of the International Speech Communication Association (Interspeech 2012), , Portland, Oregon, 9-13 September; Schlippe, T., Volovyk, M., Yurchenko, K., Schultz, T., Rapid bootstrapping of a ukrainian large vocabulary continuous speech recognition system (2013) The 38th International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2013), , Vancouver, Canada, 26-31 May; Hakkani-Tür, D., Riccardi, G., Gorin, A., Active learning for automatic speech recognition (2002) The 27th International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2002), , Orlando, Florida, 13 - 17 May},
correspondence_address1={Schlippe, T.; Cognitive Systems Lab, Karlsruhe Institute of Technology (KIT)Germany},
editor={Chng E.S., Li H., Meng H., Ma B., Xie L.},
sponsors={Amazon; Baidu; et al.; Google; Temasek Laboratories at Nanyang Technological University (TL at NTU); WeChat},
publisher={International Speech and Communication Association},
issn={2308457X},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wand20141593,
author={Wand, M. and Janke, M. and Schultz, T.},
title={The EMG-UKA corpus for electromyographic speech processing},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2014},
pages={1593-1597},
note={cited By 10; Conference of 15th Annual Conference of the International Speech Communication Association: Celebrating the Diversity of Spoken Languages, INTERSPEECH 2014 ; Conference Date: 14 September 2014 Through 18 September 2014;  Conference Code:108771},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910029927&partnerID=40&md5=ef0f33423035466051f08ef9ec32a917},
affiliation={Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={This article gives an overview of the EMG-UKA corpus, a corpus of electromyographic (EMG) recordings of articulatory activity enabling speech processing (in particular speech recognition and synthesis) based on EMG signals, with the purpose of building Silent Speech interfaces. Data is available in multiple speaking modes, namely audibly spoken, whispered, and silently articulated speech. Besides the EMG data, synchronous acoustic data was additionally recorded to serve as a reference. The corpus comprises 63 recorded sessions from 8 speakers, the total amount of data is 7:32 hours. A trial subset, consisting of 1:52 hours of data, is freely available for download. Copyright © 2014 ISCA.},
author_keywords={Electromyography;  Silent speech interfaces;  Speech data corpus},
keywords={Electromyography;  Speech communication;  Speech processing, Acoustic data;  Electromyographic;  Electromyographic recordings;  EMG signal;  Silent speech interfaces;  Speech data, Speech recognition},
references={Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Silent speech interfaces (2010) Speech Communication, 52 (4), pp. 270-287; Schultz, T., Wand, M., Modeling coarticulation in large vocabulary EMG-based speech recognition (2010) Speech Communication, 52 (4), pp. 341-353; Nakamura, K., Janke, M., Wand, M., Schultz, T., Estimation of fundamental frequency from surface Electromyographic data: EMG-to-F0 (2011) Proc. ICASSP, pp. 573-576; Wand, M., Schultz, T., Session-independent EMG-based speech recognition (2011) Proc. Biosignals, pp. 295-300; Janke, M., Wand, M., Schultz, T., A spectral mapping method for Emg-based recognition of silent speech (2010) Proc. B-INTERFACE, pp. 22-31; Janke, M., Wand, M., Schultz, T., Impact of lack of acoustic feedback in EMG-based silent speech recognition (2010) Proc. Inter Speech, pp. 2686-2689; Wand, M., Janke, M., Schultz, T., Investigations on speaking mode discrepancies in EMG-based speech recognition (2011) Proc. Inter Speech, pp. 601-604; Wand, M., Janke, M., Schultz, T., Decision-tree based analysis of speaking mode discrepancies in EMG-based speech recognition (2012) Proc. Biosignals, pp. 101-109; Maier-Hein, L., (2005) Speech Recognition Using Surface Electromyography, , Diploma thesis, Interactive Systems Labs, University of Karlsruhe; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) Proc. ASRU, pp. 331-336; Janke, M., (2010) Spektrale Methoden Zur EMG-basierten Erkennung Lautloser Sprache, , Diploma Thesis, Cognitive Systems Lab, Karlsruhe Institute of Technology; (2002) Dissection of the Speech Production Mechanism, , http://www.linguistics.ucla.edu/people/ladefoge/manual.htm, Department of Linguistics, University of California, Tech. Rep; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proc. Inter Speech, pp. 573-576; Mayer, C., UKA EMG/EEG Studio V2.0; Schunke, M., Schulte, E., Schumacher, U., (2006) Prometheus - Lernatlas Der Anatomie, 3. , Stuttgart, New York: Thieme Verlag, Kopf und Neuroanatomie; Kirchhoff, K., (1999) Robust Speech Recognition Using Articulatory Information, , Dissertation, University of Bielefeld; Metze, F., (2005) Articulatory Features for Conversational Speech Recognition, , Dissertation, University of Karlsruhe; Wand, M., Schultz, T., Towards real-life application of EMG based speech recognition by using unsupervised adaptation (2014) Proc. Inter Speech; Wand, M., (2014) Advancing Electromyographic Continuous Speech Recognition: Signal Preprocessing and Modeling, , Dissertation, Karlsruhe Institute of Technology},
correspondence_address1={Wand, M.; Karlsruhe Institute of TechnologyGermany},
editor={Chng E.S., Li H., Meng H., Ma B., Xie L.},
sponsors={Amazon; Baidu; et al.; Google; Temasek Laboratories at Nanyang Technological University (TL at NTU); WeChat},
publisher={International Speech and Communication Association},
issn={2308457X},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vu2014825,
author={Vu, N.T. and Weiner, J. and Schultz, T.},
title={Investigating the learning effect of multilingual bottle-neck features for ASR},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2014},
pages={825-829},
note={cited By 12; Conference of 15th Annual Conference of the International Speech Communication Association: Celebrating the Diversity of Spoken Languages, INTERSPEECH 2014 ; Conference Date: 14 September 2014 Through 18 September 2014;  Conference Code:108771},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910027407&partnerID=40&md5=2efedc0807744d1091f9292a73ed64a4},
affiliation={Karlsruhe Institute of Technology, Germany},
abstract={Deep neural networks (DNNs) have become state-of-the-art techniques of automatic speech recognition in the last few years. They can be used at the preprocessing level (Tandem or Bottle- Neck features) or at the acoustic model level (hybrid Hidden Markov Model/DNN). Moreover, they allow exploiting multilingual data to improve monolingual systems. This paper presents our investigation of the learning effect of neural networks in the context of multilingual Bottle-Neck features. For this, we perform a visual analysis of the output of the Bottle- Neck layer of a neural network using t-Distributed Stochastic Neighbor Embedding. Our results show that multilingual Bottle-Neck features seem to learn phoneme characteristics, such as the F1 and F2 formants which characterize different vowels, and other articulatory features, such as fricatives and nasals which characterize consonants. Furthermore, they seem to normalize language dependent variations and transfer the learned representation to unseen languages. Copyright © 2014 ISCA.},
author_keywords={Multilingual bottle-neck features;  Visualization},
keywords={Flow visualization;  Hidden Markov models;  Linguistics;  Speech communication;  Speech recognition;  Stochastic systems, Articulatory features;  Automatic speech recognition;  Deep neural networks;  Learning effects;  Monolingual systems;  Multilingual bottle-neck features;  State-of-the-art techniques;  Stochastic neighbor embedding, Bottles},
references={Schultz, T., Waibel, A., Language independent and language adaptive acoustic modeling for speech recognition (2001) Speech Communication, 35 (1-2), pp. 31-51; Hermansky, H., Wellis, D., Sharma, S., Tandem connectionist feature extraction for conventional HMM systems (2000) Proc. ICASSP, , Turkey; Grezl, F., Probabilistic and bottle-neck features for LVCSR of meetings (2007) Proc. ICASSP, , USA; Morgan, N., Bourlard, H., Continuous speech recognition: An introduction to the hybrid HMM/connectionist approach (1995) IEEE Signal Processing Magazine, 12 (3), p. 2442; Seide, F., Li, G., Yu, D., Conversational speech transcription using context-dependent deep neural networks (2011) Proc. of Interspeech; Dahl, G., Yu, D., Deng, L., Acero, A., Context-dependent pretrained deep neural networks for large vocabulary speech recognition (2012) Audio, Speech, and Language Processing, IEEE Transactions, 20 (1), p. 3042; Mohamed, A., Dahl, G., Hinton, G., Acoustic modeling using deep belief networks (2012) Audio, Speech, and Language Processing, IEEE Transactions, 20 (1), p. 1422; Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A., Jaitly, N., Senior, A., Kingsbury, B., Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups (2012) IEEE Signal Processing Magazine, 29 (6), p. 8297; Stolcke, A., Grezl, F., Hwang, M.-Y., Lei, X., Morgan, N., Vergyri, D., Cross-domain and cross-lingual portability of acoustic features estimated by multilayer perceptrons (2006) Proc. ICASSP; Toth, L., Frankel, J., Gosztolya, G., King, S., Cross-lingual portability of MLP-based tandem features - A case study for English and Hungarian (2008) Proc. Interspeech; Cetin, O., Magimai-Doss, M., Livescu, K., Kantor, A., King, S., Bartels, C., Frankel, J., Monolingual and crosslingual comparison of tandem features derived from articulatory and phone MLPs (2007) Proc. ASRU; Imseng, D., Bourlard, H., Magimai-Doss, M., Towards mixed language speech recognition systems (2010) Proc. Interspeech, , Japan; Plahl, C., Schlueter, R., Ney, H., Cross-lingual portability of Chinese and English neural network features for French and German LVCSR (2011) Proc. ASRU, , USA; Thomas, S., Ganapathy, S., Jansen, A., Hermansky, H., Datadriven posterior features for low resource speech recognition applications (2012) Proc. Interspeech, , USA; Vesely, K., Karafiat, M., Grezl, F., Janda, M., Egorova, E., The language-independent bottleneck features (2012) Proc. SLT, , USA; Vu, N.T., Metze, F., Schultz, T., Multilingual bottle-neck feature for under resourced languages (2012) Proc. SLTU, South Africa; Vu, N.T., Schultz, T., Multilingual multilayer perceptron for rapid language adaptation between and across language families (2013) Proc. Interspeech, , France; Swietojanski, P., Ghoshal, A., Renals, S., Unsupervised crosslingual knowledge transfer in DNN-based LVCSR (2012) Proc. SLT, , USA; Huang, J.T., Li, J., Yu, D., Deng, L., Gong, Y., Crosslanguage knowledge transfer using multilingual deep neural network with shared hidden layers (2013) Proc. ICASSP; Heigold, G., Vanhoucke, V., Senior, A., Nguyen, P., Ranzato, M., Devin, M., Dean, J., Multilingual acoustic models using distributed deep neural networks (2013) Proc. ICASSP; Ghoshal, A., Swietojanski, P., Renals, S., Multilingual training of deep neural networks (2013) Proc. ICASSP; Vu, N.T., Imseng, D., Povey, D., Motlicek, P., Schultz, T., Bourlard, H., Multilingual deep neural network based acoustic modeling for rapid language adaptation (2014) Proc. ICASSP; Mohamed, A., Hinton, G., Penn, G., Understanding how deep belief networks perform acoustic modelling (2012) Proc. ICASSP; Davies, D., Bouldin, D., A cluster separation measure (1979) IEEE Transactions on Pattern Analysis and Machine Intelligence; Schultz, T., Vu, N.T., Schlippe, T., GlobalPhone: A multilingual text & speech database in 20 languages (2013) Proc. ICASSP, , Canada; Vu, N.T., Schlippe, T., Kraus, F., Schultz, T., Rapid bootstrapping of five eastern European languages using the rapid language adaptation toolkit (2010) Proc. Interspeech, , Japan; Vu, N.T., Schultz, T., Vietnamese large vocabulary continuous speech recognition (2009) Proc. ASRU, , Italy; http://www.icsi.berkeley.edu/Speech/qn; Van Der Maaten, L., Hinton, G.E., Visualizing data using t- SNE (2008) Journal of Machine Learning Research, 9, pp. 2579-2605; Hinton, G., Sam, R., Stochastic neighbor embedding (2002) NIPS, 2; http://homepage.tudelft.nl/19j49/t-SNE.html},
correspondence_address1={Vu, N.T.; Karlsruhe Institute of TechnologyGermany},
editor={Chng E.S., Li H., Meng H., Ma B., Xie L.},
sponsors={Amazon; Baidu; et al.; Google; Temasek Laboratories at Nanyang Technological University (TL at NTU); WeChat},
publisher={International Speech and Communication Association},
issn={2308457X},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Heger20142064,
author={Heger, D. and Terziyska, E. and Schultz, T.},
title={Connectivity based feature-level filtering for single-trial EEG BCIs},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2014},
pages={2064-2068},
doi={10.1109/ICASSP.2014.6853962},
art_number={6853962},
note={cited By 2; Conference of 2014 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2014 ; Conference Date: 4 May 2014 Through 9 May 2014;  Conference Code:106632},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905281039&doi=10.1109%2fICASSP.2014.6853962&partnerID=40&md5=8d50d1b5eee50f61f0ccba51db7efd93},
affiliation={Karlsruhe Institute of Technology (KIT), Cognitive Systems Lab, Germany},
abstract={EEG-based Brain Computer interfaces (BCIs) often rely on power spectral density features to represent relevant aspects of brain activity. The information flow within human brain networks and the corresponding connectivity patterns may contain useful information to improve BCI performance, however they are typically not leveraged in current systems. In this paper, analyzes of information flow between independent sources of brain activity have been incorporated into the feature extraction stage of a BCI. For this purpose, connectivity measures based on multivariate autoregressive models have been estimated and are applied as filters to power spectral density based features. Two publicly available data sets have been used to evaluate the proposed feature extraction method: a two-back task and a motor imagery task. The results demonstrate significant performance improvements of the proposed method over band-power features and indicate that connectivity in brain networks can be used as powerful feature-level filters for BCIs. © 2014 IEEE.},
author_keywords={brain-computer interfaces;  Connectivity;  direct directed transfer function;  electroencephalography;  Granger causality},
keywords={Brain;  Electroencephalography;  Electrophysiology;  Feature extraction;  Neurophysiology;  Power spectral density;  Signal processing, Brain computer interfaces (BCIs);  Connectivity;  Connectivity pattern;  Directed transfer functions;  Feature extraction methods;  Granger Causality;  Motor imagery tasks;  Multivariate autoregressive models, Brain computer interface},
references={Sporns, O., The human connectome: A complex network (2011) Annals of the New York Academy of Sciences, 1224 (1), pp. 109-125; Baccalá, L.A., Sameshima, K., Partial directed coherence: A new concept in neural structure determination (2001) Biological Cybernetics, 84 (6), pp. 463-474; Kaminski, M.J., Blinowska, K.J., A new method of the description of the information flow in the brain structures (1991) Biological Cybernetics, 65 (3), pp. 203-210; Makeig, S., Bell, A.J., Jung, T., Sejnowski, T.J., Independent component analysis of electroencephalographic data (1996) Advances in Neural Information Processing Systems, pp. 145-151; Haufe, S., Nikulin, V.V., Müller, K., Nolte, G., A critical assessment of connectivity measures for eeg data: A simulation study (2012) Neuroimage; Lim, J.H., Hwang, H.J., Jung, Y.L., Im, C.H., Feature extraction for brain-computer interface (bci) based on the functional causality analysis of brain signals (2011) 5th Int. BCI Conference, pp. 36-39; Liu, M., Kuo, C., Chiu, A.W.L., Statistical threshold for nonlinear granger causality in motor intention analysis (2011) Engineering in Medicine and Biology Society, EMBC, 2011 Annual International Conference of the IEEE. IEEE, pp. 5036-5039; Daly, I., Nasuto, S.J., Warwick, K., Brain computer interface control via functional connectivity dynamics (2012) Pattern Recognition, 45 (6), pp. 2123-2136; Chen, D., Xiang, J., Guo, H., Deng, H., Li, H., Chen, J., (2012) Motor Imagery Classification with Source Analysis and Network Dynamics for Brain-computer Interface; Billinger, M., Brunner, C., Rmüller-Putz, G., Single-trial connectivity estimation for classification of motor imagery data (2013) Journal of Neural Engineering, 10 (4), p. 046006; Mullen, T., Delorme, A., Kothe, C., Makeig, S., An electrophysiological information flow toolbox for eeglab (2010) Biol. Cybern, 83, pp. 35-45; Delorme, A., Makeig, S., Eeglab: An open source toolbox for analysis of single-trial eeg dynamics including independent component analysis (2004) Journal of Neuroscience Methods, 134 (1), pp. 9-21; Granger, C.W.J., Investigating causal relations by econometric models and cross-spectral methods (1969) Econometrica: Journal of the Econometric Society, pp. 424-438; Liao, W., Mantini, D., Zhang, Z., Pan, Z., Ding, J., Gong, Q., Yang, Y., Chen, H., Evaluating the effective connectivity of resting state networks using conditional granger causality (2010) Biological Cybernetics, 102 (1), pp. 57-69; Roebroeck, A., Formisano, E., Goebel, R., Mapping directed influence over the brain using granger causality and fmri (2005) Neuroimage, 25 (1), pp. 230-242; Wang, X., Chen, Y., Bressler, S.L., Ding, M., Granger causality between multiple interdependent neurobiological time series: Blockwise versus pairwise methods (2007) International Journal of Neural Systems, 17 (2), pp. 71-78; Geweke, J.F., Measures of conditional linear dependence and feedback between time series (1984) Journal of the American Statistical Association, 79 (388), pp. 907-915; Mullen, T., (2010) Source Information Flow Toolbox (SIFT) Theoretical Handbook and User Manual, Swartz Center for Computational Neuroscience, Institute for Neural Computation, and Department of Cognitive Science University of California, , San Diego, release 0.9.7a edition, December; Nolte, G., Bai, O., Wheaton, L., Mari, Z., Vorbach, S., Hallett, M., Identifying true brain interaction from eeg data using the imaginary part of coherency (2004) Clinical Neurophysiology, 115 (10), pp. 2292-2307; Schlögl, A., Supp, G., Analzying event-related eeg data with multivariate autoregressive parameters (2006) Progress in Brain Research, 159; Rosenberg, J.R., Halliday, D.M., Breeze, P., Conway, B.A., Identification of patterns of neuronal connectivity, partial spectra, partial coherence, and neuronal interactions (1998) Journal of Neuroscience Methods, 83 (1), pp. 57-72; Korzeniewska, A., Mánczak, M., Kamínski, M., Blinowska, K.J., Kasicki, S., Determination of information flow direction among brain structures by a modified directed transfer function (ddtf) method (2003) Journal of Neuroscience Methods, 125 (1), pp. 195-207; Blankertz, B., Müller, K.-R., Krusienski, D.J., Schalk, G., Wolpaw, J.R., Schlögl, A., Pfurtscheller, G., Birbaumer, N., The bci competition iii: Validating alternative approaches to actual bci problems (2006) Neural Systems and Rehabilitation Engineering, IEEE Transactions on, 14 (2), pp. 153-159; Mognon, A., Jovicich, J., Bruzzone, L., Buiatti, M., Adjust: An automatic eeg artifact detector based on the joint use of spatial and temporal features (2011) Psychophysiology, 48 (2), pp. 229-240; Morf, M., Vieira, A., Lee, D.Tl., Kailath, T., Recursive multichannel maximum entropy spectral estimation (1978) Geoscience Electronics, IEEE Transactions on, 16 (2), pp. 85-94; Ding, M., Bressler, S.L., Yang, W., Liang, H., Short-window spectral analysis of cortical event-related potentials by adaptive multivariate autoregressive modeling: Data preprocessing, model validation, and variability assessment (2000) Biological Cybernetics, 83 (1), pp. 35-45; Welch, P., The use of fast fourier transform for the estimation of power spectra: A method based on time averaging over short, modified periodograms (1967) Audio and Electroacoustics, IEEE Transactions on, 15 (2), pp. 70-73; Keng Ang, K., Yang Chin, Z., Zhang, H., Guan, C., Mutual information-based selection of optimal spatial-temporal patterns for single-trial eeg-based bcis (2012) Pattern Recognition, 45 (6), pp. 2137-2144},
sponsors={},
publisher={Institute of Electrical and Electronics Engineers Inc.},
address={Florence},
issn={15206149},
isbn={9781479928927},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Schlippe2014101,
author={Schlippe, T. and Ochs, S. and Schultz, T.},
title={Web-based tools and methods for rapid pronunciation dictionary creation},
journal={Speech Communication},
year={2014},
volume={56},
number={1},
pages={101-118},
doi={10.1016/j.specom.2013.06.015},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893706991&doi=10.1016%2fj.specom.2013.06.015&partnerID=40&md5=d922f73e14337832064db7d54ab5fc07},
affiliation={Institute for Anthropomatics, Cognitive Systems Lab. (CSL), Karlsruhe Institute of Technology (KIT), Adenauerring 4, 76131 Karlsruhe, Germany},
abstract={In this paper we study the potential as well as the challenges of using the World Wide Web as a seed for the rapid generation of pronunciation dictionaries in new languages. In particular, we describe Wiktionary, a community-driven resource of pronunciations in IPA notation, which is available in many different languages. First, we analyze Wiktionary in terms of language and vocabulary coverage and compare it in terms of quality and coverage with another source of pronunciation dictionaries in multiple languages (GlobalPhone). Second, we investigate the performance of statistical grapheme-to-phoneme models in ten different languages and measure the model performance for these languages over the amount of training data. The results show that for the studied languages about 15k phone tokens are sufficient to train stable grapheme-to-phoneme models. Third, we create grapheme-to-phoneme models for ten languages using both the GlobalPhone and the Wiktionary resources. The resulting pronunciation dictionaries are carefully evaluated along several quality checks, i.e. in terms of consistency, complexity, model confidence, grapheme n-gram coverage, and phoneme perplexity. Fourth, as a crucial prerequisite for a fully automated process of dictionary generation, we implement and evaluate methods to automatically remove flawed and inconsistent pronunciations from dictionaries. Last but not least, speech recognition experiments in six languages evaluate the usefulness of the dictionaries in terms of word error rates. Our results indicate that the web resources of Wiktionary can be successfully leveraged to fully automatically create pronunciation dictionaries in new languages. © 2013 Elsevier B.V. All rights reserved.},
author_keywords={Multilingual speech recognition;  Pronunciation modeling;  Rapid bootstrapping;  Web-derived pronunciations},
keywords={Automation;  Deep neural networks;  Quality control;  Web crawler, Grapheme to phonemes;  Model performance;  Multilingual speech recognition;  Multiple languages;  Pronunciation dictionaries;  Pronunciation modeling;  Rapid bootstrapping;  Web-derived pronunciations, Speech recognition},
funding_text 1={This work was partly realized as part of the Quaero Programme, funded by OSEO, French State agency for innovation.},
references={Schultz, T., Black, A.W., Badaskar, S., Hornyak, M., Kominek, J., SPICE: Web-based tools for rapid language adaptation in speech processing systems (2007) Interspeech; Martirosian, O., Davel, M., Error analysis of a public domain pronunciation dictionary (2007) PRASA; Vu, N.T., Schlippe, T., Kraus, F., Schultz, T., Rapid bootstrapping of five eastern european languages using the rapid language adaptation toolkit (2010) Interspeech; (1999) Handbook of the International Phonetic Association: A Guide to the use of the International Phonetic Alphabet, , IPA Cambridge University Press; Zhu, X., Rosenfeld, R., Improving trigram language modeling with the world wide web (2001) ICASSP; Black, A.W., Lenzo, K., Pagel, V., Issues in building general letter to sound rules (1998) ESCA Workshop on Speech Synthesis; Kominek, J., Black, A.W., Learning pronunciation dictionaries: Language complexity and word selection strategies (2006) HLT Conference of the NAACL; Davel, M., Barnard, E., The efficient generation of pronunciation dictionaries: Human factors during bootstrapping (2004) ICSLP; Ghoshal, A., Jansche, M., Khudanpurv, S., Riley, M., Ulinski, M., Web-derived pronunciations (2009) ICASSP; Can, D., Cooper, E., Ghoshal, A., Jansche, M., Khudanpur, S., Ramabhadran, B., Riley, M., White, C., Web derived pronunciations for spoken term detection (2009) 32nd Annual International ACM SIGIR Conference; Schlippe, T., Ochs, S., Schultz, T., Wiktionary as a source for automatic pronunciation extraction (2010) Interspeech; Schlippe, T., Ochs, S., Schultz, T., Grapheme-to-phoneme model generation for indo-european languages (2012) ICASSP; Kaplan, R.M., Kay, M., Regular models of phonological rule systems (1994) Computational Linguistics; Besling, S., Heuristical and statistical methods for grapheme-to-phoneme conversion (1994) Konvens; Kneser, R., Grapheme-to-phoneme study (2000) Tech. Rep. WYT-P4091/00002, , Philips Speech Processing, Germany; Chen, S.F., Conditional and joint models for grapheme-to-phoneme conversion (2003) Eurospeech; Vozila, P., Adams, J., Lobacheva, Y., Ryan, T., Grapheme to phoneme conversion and dictionary verification using graphonemes (2003) Eurospeech; Jiampojamarn, S., Kondrak, G., Sherif, T., Applying many-to-many alignments and hidden Markov models to letter-to-phoneme conversion (2007) HLT; Novak, J., (2011) Phonetisaurus: A WFST-driven Phoneticizer, , http://code.google.com/p/phonetisaurus/; Novak, J., Minematsu, N., Hirose, K., WFST-based grapheme-to-phoneme conversion: Open source tools for alignment, model-building and decoding (2012) International Workshop on Finite State Methods and Natural Language Processing; Gerosa, M., Federico, M., Coping with out-of-vocabulary words: Open versus huge vocabulary ASR (2009) Proceedings of the 2009 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP'09, , ISBN 978-1-4244-2353-8. http://dx.doi.org/10.1109/ICASSp.2009.4960583; Laurent, A., Deléglise, P., Meignier, S., Grapheme to phoneme conversion using an SMT system (2009) Interspeech; Karanasou, P., Lamel, L., Comparing SMT methods for automatic generation of pronunciation variants (2010) Proceedings of the 7th International Conference on Advances in Natural Language Processing, IceTAL'10, , ISBN 3-642-14769-0, 978-3-642-14769-2; Bisani, M., Ney, H., Joint-sequence models for grapheme-to-phoneme conversion (2008) Speech Communication; Hahn, S., Vozila, P., Bisani, M., Comparison of grapheme-to-phoneme methods on large pronunciation dictionaries and LVCSR tasks (2012) Interspeech, , 2012; Davel, M., Martirosian, O., Pronunciation dictionary development in resource-scarce environments (2009) Interspeech; Davel, M., De Wet, F., Verifying pronunciation dictionaries using conflict analysis (2010) Interspeech; Wolff, M., Eichner, M., Hoffmann, R., Measuring the quality of pronunciation dictionaries (2002) PMLA; Davel, M., Barnard, E., Developing consistent pronunciation models for phonemic variants (2006) Interspeech; Kominek, J., (2009) TTS from Zero - Building Synthetic Voices for New Languages, , Doctoral Thesis; Schultz, T., GlobalPhone: A multilingual speech and text database developed at karlsruhe university (2002) ICSLP; Schlippe, T., Ochs, S., Schultz, T., Automatic error recovery for pronunciation dictionaries (2012) Interspeech; Wells, J.C., SAMPA computer readable phonetic alphabet (1997) Handbook of Standards and Resources for Spoken Language Systems, , Gibbon, D., Moore, R., Winski, R. (Eds.) Mouton de Gruyter, Berlin, New York; (2012) List of Wiktionary Editions, Ranked by Article Count, , http://meta.wikimedia.org/wiki/ListofWiktionaries; Llitjos, A.F., Black, A.W., Evaluation and collection of proper name pronunciations online (2002) LREC; Kanthak, S., Ney, H., Context-dependent acoustic modeling using graphemes for large vocabulary speech recognition (2002) ICASSP; Killer, M., Stueker, S., Schultz, T., Grapheme based speech recognition (2003) Eurospeech; Stueker, S., Schultz, T., A grapheme based speech recognition system for Russian (2004) SPECOM; Schlippe, T., Djomgang, E.G.K., Vu, N.T., Ochs, S., Schultz, T., Hausa large vocabulary continuous speech recognition (2012) SLT-U},
correspondence_address1={Schlippe, T.; Institute for Anthropomatics, Cognitive Systems Lab. (CSL), Karlsruhe Institute of Technology (KIT), Adenauerring 4, 76131 Karlsruhe, Germany; email: tim.schlippe@kit.edu},
publisher={Elsevier B.V.},
issn={01676393},
coden={SCOMD},
language={English},
abbrev_source_title={Speech Commun},
document_type={Article},
source={Scopus},
}

@ARTICLE{Amma2014191,
author={Amma, C. and Georgi, M. and Schultz, T.},
title={Airwriting: A wearable handwriting recognition system},
journal={Personal and Ubiquitous Computing},
year={2014},
volume={18},
number={1},
pages={191-203},
doi={10.1007/s00779-013-0637-3},
note={cited By 37},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891853288&doi=10.1007%2fs00779-013-0637-3&partnerID=40&md5=99423246b1ddd38a9f52310cd2851f48},
affiliation={Cognitive Systems Lab, Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany},
abstract={We present a wearable input system which enables interaction through 3D handwriting recognition. Users can write text in the air as if they were using an imaginary blackboard. The handwriting gestures are captured wirelessly by motion sensors applying accelerometers and gyroscopes which are attached to the back of the hand. We propose a two-stage approach for spotting and recognition of handwriting gestures. The spotting stage uses a support vector machine to identify those data segments which contain handwriting. The recognition stage uses hidden Markov models (HMMs) to generate a text representation from the motion sensor data. Individual characters are modeled by HMMs and concatenated to word models. Our system can continuously recognize arbitrary sentences, based on a freely definable vocabulary. A statistical language model is used to enhance recognition performance and to restrict the search space. We show that continuous gesture recognition with inertial sensors is feasible for gesture vocabularies that are several orders of magnitude larger than traditional vocabularies for known systems. In a first experiment, we evaluate the spotting algorithm on a realistic data set including everyday activities. In a second experiment, we report the results from a nine-user experiment on handwritten sentence recognition. Finally, we evaluate the end-to-end system on a small but realistic data set. © 2013 Springer-Verlag London.},
author_keywords={Gesture recognition;  Handwriting recognition;  Hidden Markov models;  Inertial sensors;  Wearable computing},
keywords={Gesture vocabularies;  Handwriting recognition;  Hidden markov models (HMMs);  Inertial sensor;  Recognition of handwriting;  Sentence recognition;  Statistical language modeling;  Wearable computing, Experiments;  Gesture recognition;  Hidden Markov models;  Inertial navigation systems;  Natural language processing systems;  Sensors;  Wearable computers, Character recognition},
references={Amft, O.A., Gesture-controlled user input to complete questionnaires on wrist-worn watches (2009) Human-computer Interaction. Novel Interaction Methods and Techniques. Lecture Notes in Computer Science, 5611, pp. 131-140. , Springer, Heidelberg; Amma, C., Gehrig, D., Schultz, T., Airwriting recognition using wearable motion sensors (2010) Proceedings of the 1st Augmented Human International Conference (AH'10), , doi: 10.1145/1785455.1785465; Amma, C., Georgi, M., Schultz, T., Airwriting: Hands-free mobile text input by spotting and continuous recognition of 3D-space handwriting with inertial sensors (2012) IEEE 16th International Symposium on Wearable Computers (ISWC), pp. 52-59; Amma, C., Schultz, T., Airwriting: Demonstrating mobile text input by 3D-space handwriting (2012) Proceedings of the ACM International Conference on Intelligent User Interfaces (IUI'12); Bang, W.C., Chang, W., Kang, K.H., Choi, E.S., Potanin, A., Kim, D.Y., Self-contained spatial input device for wearable computers (2003) Proceedings of the IEEE International Symposium on Wearable Computers (ISWC'03); Chen, M., Alregib, G., Juang, B., 6D motion gesture recognition using spatio-temporal features (2012) IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2341-2344. , IEEE; Ephraim, Y., Merhav, N., Hidden Markov processes (2002) IEEE Transactions on Information Theory, 48 (6), pp. 1518-1569. , DOI 10.1109/TIT.2002.1003838, PII S0018944802040142; Gustafson, S., Bierwirth, D., Baudisch, P., Imaginary interfaces: Spatial interaction with empty hands and without visual feedback (2010) Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology (UIST'10); Hein, A., Hoffmeyer, A., Kirste, T., Utilizing an accelerometric bracelet for ubiquitous gesture-based interaction (2009) Universal Access in Human-computer Interaction. Intelligent and Ubiquitous Interaction Environments. Lecture Notes in Computer Science, 5615, pp. 519-527. , Springer, Heidelberg; Huang, X., Acero, A., Hon, H., (2001) Spoken Language Processing, , Prentice Hall NJ; Junker, H., Amft, O., Lukowicz, P., Tröster, G., Gesture spotting with body-worn inertial sensors to detect user activities (2008) Pattern Recognit, 41 (6), pp. 2010-2024. , 10.1016/j.patcog.2007.11.016 1203.68180; Kallio, S., Kela, J., Mantyjarvi, J., Online gesture recognition system for mobile interaction (2003) Proceedings of the IEEE International Conference on Systems, Man and Cybernetics (ICSMC'03); Kim, D.H., Choi, H.I., Kim, J.H., 3D space handwriting recognition with ligature model (2006) Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 4239, pp. 41-56. , Ubiquitous Computing Systems - Third International Symposium, UCS 2006, Proceedings; Lee Hyeon-Kyu, Kim Jin, H., HMM-based threshold model approach for gesture recognition (1999) IEEE Transactions on Pattern Analysis and Machine Intelligence, 21 (10), pp. 961-973; Lyons, K., Starner, T., Plaisted, D., Fusia, J., Lyons, A., Drew, A., Looney, E.W., Twiddler typing: One-handed chording text entry for mobile phones (2004) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI'04); Mackenzie, I.S., Soukoreff, R.W., Helga, J., 1 thumb, 4 buttons, 20 words per minute: Design and evaluation of h4-writer (2011) Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology (UIST'11); McGuire, R., Hernandez-Rebollar, J., Starner, T., Henderson, V., Brashear, H., Ross, D., Towards a one-way american sign language translator (2004) Proceedings of the Sixth IEEE International Conference on Automatic Face and Gesture Recognition (FGR'04); Mistry, P., Maes, P., Chang, L., Wuw-wear ur world: A wearable gestural interface (2009) Proceedings of the 27th International Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA '09); Mitra, S., Acharya, T., Gesture recognition: A survey (2007) IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews, 37 (3), pp. 311-324. , DOI 10.1109/TSMCC.2007.893280; Odell, J., Valtchev, V., Woodland, P., Young, S., A one pass decoder design for large vocabulary recognition (1994) Proceedings of the Workshop on Human Language Technology. Association for Computational Linguistics, pp. 405-410; Plamondon, R., Srihari, S.N., On-line and off-line handwriting recognition: A comprehensive survey (2000) IEEE Transactions on Pattern Analysis and Machine Intelligence, 22 (1), pp. 63-84. , DOI 10.1109/34.824821; Rabiner, L., A tutorial on hidden markov models and selected applications in speech recognition (1989) Proc IEEE, 77 (2), pp. 257-286; Raffa, G., Lee, J., Nachman, L., Song, J., Don't slow me down: Bringing energy efficiency to continuous gesture recognition (2010) Proceedings of the International Symposium on Wearable Computers (ISWC'10); Schultz, T., GlobalPhone: A multilingual speech and text database developed at Karlsruhe University (2002) Proceedings of the International Conference on Spoken Language Processing (ICSLP'02); Soltau, H.M., A one-pass decoder based on polymorphic linguistic context assignment (2001) IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU '01); Stiefmeier, T., Roggen, D., Ogris, G., Lukowicz, P., Troster, G., Wearable activity tracking in car manufacturing (2008) IEEE Pervasive Computing, 7 (2), pp. 42-50. , DOI 10.1109/MPRV.2008.40, 4487087; Stolcke, A., Srilm - An extensible language modeling toolkit (2002) International Conference on Spoken Language Processing; Tamaki, E., Miyaki, T., Rekimoto, J., Brainy hand: An ear-worn hand gesture interaction device (2009) Proceedings of the 27th International Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA '09); Woodman, O.J., (2007) An Introduction to Inertial Navigation, , Technical report. University of Cambridge},
correspondence_address1={Amma, C.; Cognitive Systems Lab, Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; email: christoph.amma@kit.edu},
issn={16174909},
language={English},
abbrev_source_title={Pers. Ubiquitous Comp.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Putze2014387,
author={Putze, F. and Holt, D.V. and Schultz, T. and Funke, J.},
title={Model-based identification of EEG markers for learning opportunities in an associative learning task with delayed feedback},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2014},
volume={8681 LNCS},
pages={387-394},
doi={10.1007/978-3-319-11179-7_49},
note={cited By 1; Conference of 24th International Conference on Artificial Neural Networks, ICANN 2014 ; Conference Date: 15 September 2014 Through 19 September 2014;  Conference Code:107079},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958537369&doi=10.1007%2f978-3-319-11179-7_49&partnerID=40&md5=fe61d7f7b2bc398084a26388f3189859},
affiliation={Institute of Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute of Psychology, University of Heidelberg, Heidelberg, Germany},
abstract={This paper combines a reinforcement learning (RL) model and EEG data analysis to identify learning situations in a associative learning task with delayed feedback. We investigated neural correlates in occipital alpha and prefrontal theta band power of learning opportunities, identified by the RL model. We show that those parameters can also be used to differentiate between learning opportunities which lead to correct learning and those which do not. Finally, we show that learning situations can also be identified on a single trial basis. © 2014 Springer International Publishing Switzerland.},
author_keywords={EEG;  Frequency Analysis;  learning situations;  Reinforcement Learning},
keywords={Electroencephalography;  Reinforcement learning, Associative learning;  Delayed feedback;  Frequency Analysis;  Learning opportunity;  Learning situation;  Model-based identification;  Neural correlate;  Reinforcement learning models, Neural networks},
references={Anderson, J.R., Betts, S., Ferris, J.L., Fincham, J.M., Neural imaging to track mental states while using an intelligent tutoring system (2010) Proceedings of the National Academy of Sciences, 107 (15), pp. 7018-7023; Cavanagh, J.F., Frank, M.J., Klein, T.J., Allen, J.J.B., Frontal theta links prediction errors to behavioral adaptation in reinforcement learning (2010) NeuroImage, 49 (4), pp. 3198-3209; Collins, A.G.E., Frank, M.J., How much of reinforcement learning is working memory, not reinforcement learning? a behavioral, computational, and neurogenetic analysis (2012) European Journal of Neuroscience, 35 (7), pp. 1024-1035; Fu, W.-T., Anderson, J.R., From recurrent choice to skill learning: A reinforcementlearning model (2006) Journal of Experimental Psychology: General, 135 (2), pp. 184-206; Jensen, O., Tesche, C.D., Frontal theta activity in humans increases with memory load in a working memory task (2002) The European Journal of Neuroscience, 15 (8), pp. 1395-1399; Klimesch, W., Doppelmayr, M., Schwaiger, J., Auinger, P., Winkler, T., 'Paradoxical' alpha synchronization in a memory task (1999) Cognitive Brain Research, 7 (4), pp. 493-501; Klimesch, W., Memory processes, brain oscillations and EEG synchronization (1996) International Journal of Psychophysiology, 24 (1-2), pp. 61-100; Klimesch, W., EEG alpha and theta oscillations reflect cognitive and memory performance: A review and analysis (1999) Brain Research Reviews, 29 (2-3), pp. 169-195; Klimesch, W., Sauseng, P., Hanslmayr, S., EEG alpha oscillations: The inhibition- timing hypothesis (2007) Brain Research Reviews, 53 (1), pp. 63-88; Osipova, D., Takashima, A., Oostenveld, R., Fernández, G., Maris, E., Jensen, O., Theta and gamma oscillations predict encoding and retrieval of declarative memory (2006) The Journal of Neuroscience, 26 (28), pp. 7523-7531. , PMID: 16837600; Sauseng, P., Klimesch, W., Doppelmayr, M., Pecherstorfer, T., Freunberger, R., Hanslmayr, S., EEG alpha synchronization and functional coupling during topdown processing in a working memory task (2005) Human Brain Mapping, 26 (2), pp. 148-155; Sutton, R.S., Barto, A.G., (1998) Introduction to Reinforcement Learning, 1st Edn., , MIT Press, Cambridge; Thomson, D.J., Spectrum estimation and harmonic analysis (1982) Proceedings of the IEEE, 70 (9), pp. 1055-1096; Tuladhar, A.M., Ter Huurne, N., Schoffelen, J.-M., Maris, E., Oostenveld, R., Jensen, O., Parieto-occipital sources account for the increase in alpha activity with working memory load (2007) Human Brain Mapping, 28 (8), pp. 785-792; Walsh, M.M., Anderson, J.R., Learning from delayed feedback: Neural responses in temporal credit assignment (2011) Cognitive, Affective, & Behavioral Neuroscience, 11 (2), pp. 131-143; Weiss, S., Müller, H.M., Rappelsberger, P., Theta synchronization predicts efficient memory encoding of concrete and abstract nouns (2000) Neuroreport, 11 (11), pp. 2357-2361},
sponsors={},
publisher={Springer Verlag},
address={Hamburg},
issn={03029743},
isbn={9783319111780},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zahner20141184,
author={Zahner, M. and Janke, M. and Wand, M. and Schultz, T.},
title={Conversion from facial myoelectric signals to speech: A unit selection approach},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2014},
pages={1184-1188},
note={cited By 9; Conference of 15th Annual Conference of the International Speech Communication Association: Celebrating the Diversity of Spoken Languages, INTERSPEECH 2014 ; Conference Date: 14 September 2014 Through 18 September 2014;  Conference Code:108771},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910091101&partnerID=40&md5=a0a65fc5cec350207269417a2520080e},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology, Germany},
abstract={This paper reports on our recent research on surface electromyographic (EMG) speech synthesis: A direct conversion of the EMG signals of the articulatory muscle movements to the acoustic speech signal. In this work we introduce a unit selection approach which compares segments of the input EMG signal to a database of simultaneously recorded EMG/audio unit pairs and selects the best matching audio unit based on target and concatenation cost, which will be concatenated to synthesize an acoustic speech output. We show that this approach is feasible to generate a proper speech output from the input EMG signal. We evaluate different properties of the units and investigate what amount of data is necessary for an initial transformation. Prior work on EMG-to-speech conversion used a framebased approach from the voice conversion domain, which struggles with the generation of a natural F0 contour. This problem may also be tackled by our unit selection approach. Copyright © 2014 ISCA.},
author_keywords={Electromyography;  Silent speech interface;  Unit selection},
keywords={Audio acoustics;  Electromyography;  Metadata;  Speech analysis;  Speech processing;  Speech synthesis, Direct conversion;  Electromyographic;  Myoelectric signals;  Recent researches;  Silent speech interfaces;  Speech conversion;  Unit selection;  Unit selection approach, Speech communication},
references={Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Silent speech interfaces (2010) Speech Communication, 52 (4), pp. 270-287; Chan, A.D.C., Englehart, K., Hudgins, B., Lovely, D.F., Hidden markov model classification of myolectric signals in speech (2002) IEEE Engineering in Medicine and Biology Society, 21 (5), pp. 143-146; Toth, A.R., Wand, M., Schultz, T., Synthesizing speech from electromyography using voice transformation techniques (2009) Proceedings of Interspeech 2009, pp. 652-655; Toda, T., Saruwatari, H., Shikano, K., Voice conversion algorithm based on Gaussian mixture model with dynamic frequency warping of STRAIGHT spectrum (2001) IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 841-844; Nakamura, K., Janke, M., Wand, M., Schultz, T., Estimation of fundamental frequency from surface electromyographic data: EMG-to-f0 (2011) IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 573-576; Hunt, A.J., Black, A.W., Unit selection in a concatenative speech synthesis system using a large speech database (1996) IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 373-376; Toda, T., Shikano, K., NAM-to-speech conversion with Gaussian mixture models (2005) European Conference on Speech Communication and Technology (Interspeech), pp. 1957-1960; Janke, M., Wand, M., Nakamura, K., Schultz, T., Further investigations on EMG-to-speech conversion (2012) IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 365-368; Wand, M., Schulte, C., Janke, M., Schultz, T., Array-based electromyographic silent speech interface (2013) International Conference on Bio-inspired Systems and Signal Processing (BIOSIGNALS), pp. 89-96; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 331-336; Schultz, T., Wand, M., Modeling coarticulation in large vocabulary EMG-based speech recognition (2010) Speech Communication, 52 (4), pp. 341-353; Viterbi, A.J., Error bounds for convolutional codes and an asymptotically optimum decoding algorithm (1967) IEEE Transactions on Information Theory, 13 (2), pp. 260-269; Wu, Z., Virtanen, T., Kinnunen, T., Chng, E.S., Li, H., Exemplar-based unit selection for voice conversion utilizing temporal information (2013) Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), pp. 3057-3061; Fukada, T., Tokuda, K., Kobayashi, T., Imai, S., An adaptive algorithm for mel-cepstral analysis of speech (1992) IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 137-140; Imai, S., Cepstral analysis synthesis on the mel frequency scale (1983) IEEE International Conference on Acoustics, Speech, and Signal Processing, 8, pp. 93-96; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), pp. 573-576; Jou, S.-C., Maier-Hein, L., Schultz, T., Waibel, A., Articulatory feature classification using surface electromyography (2006) IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 605-608; Scheme, E.J., Hudgins, B., Parker, P.A., Myoelectric signal classification for phoneme-based speech recognition (2007) IEEE Transactions on Biomedical Engineering, 54 (4), pp. 694-699; Kubichek, R.F., Mel-cepstral distance measure for objective speech quality assessment (1993) IEEE Pacific Rim Conference on Communications, Computers and Signal Processing, pp. 125-128},
correspondence_address1={Zahner, M.; Cognitive Systems Lab, Karlsruhe Institute of TechnologyGermany},
editor={Chng E.S., Li H., Meng H., Ma B., Xie L.},
sponsors={Amazon; Baidu; et al.; Google; Temasek Laboratories at Nanyang Technological University (TL at NTU); WeChat},
publisher={International Speech and Communication Association},
issn={2308457X},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wand20142094,
author={Wand, M. and Schulte, C. and Janke, M. and Schultz, T.},
title={Compensation of recording position shifts for a myoelectric Silent Speech Recognizer},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2014},
pages={2094-2098},
doi={10.1109/ICASSP.2014.6853968},
art_number={6853968},
note={cited By 5; Conference of 2014 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2014 ; Conference Date: 4 May 2014 Through 9 May 2014;  Conference Code:106632},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905281040&doi=10.1109%2fICASSP.2014.6853968&partnerID=40&md5=27ed169f1eef075c814b260ca7aa7dd1},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology, Germany},
abstract={A myoelectric Silent Speech Recognizer is a system which recognizes speech by capturing the electrical activity of the human articulatory muscles, thus enabling the user to communicate silently. We recently devised a recording setup based on electrode arrays with multiple measuring points. In this study we show that this allows to compensate for shifts of the recording position, which happen when the array is removed and reattached between system training and application. We present a method which determines the amount of recording position shift; compensation is performed by linear interpolation. We evaluate our method by running recognition experiments across recording sessions and obtain a Word Error Rate improvement of 14.3% relative on the development set and 12.9% relative on the evaluation set, compared to using classical session adaptation. © 2014 IEEE.},
author_keywords={Adaptation;  EMG;  EMG-based Speech Recognition;  Signal Interpolation;  Silent Speech Interfaces},
keywords={Signal processing;  Speech recognition, Adaptation;  Electrical activities;  Electrode arrays;  EMG;  Linear Interpolation;  Measuring points;  Signal interpolation;  Silent speech interfaces, Audio recordings},
references={Schultz, T., Wand, M., Modeling coarticulation in large vocabulary emg-based speech recognition (2010) Speech Communication, 52 (4), pp. 341-353; Wand, M., Schulte, C., Janke, M., Schultz, T., Array-based electromyographic silent speech interface (2013) Proc. Biosignals, pp. 89-96; Wand, M., Schultz, T., Session-independent emg-based speech recognition (2011) Proc. Biosignals, pp. 295-300; Hofe, R., Ell, S.R., Fagan, M.J., Gilbert, J.M., Green, P.D., Moore, R.K., Rybchenko, S.I., Small-vocabulary speech recognition using a silent speech interface based on magnetic sensing (2013) Speech Communication, 55, pp. 22-32; Hueber, T., Bailly, G., Denby, B., Continuous articulatory-to-acoustic mapping using phone-based trajectory hmm for a silent speech interface (2012) Proc. Interspeech; Tran, V., Bailly, G., Loevenbruck, H., Toda, T., Improvement to a nam-captured whisper-to-speech system (2010) Speech Communication, 52, pp. 314-326; Jorgensen, C., Dusan, S., Speech interfaces based upon surface electromyography (2010) Speech Communication, 52, pp. 354-366; Deng, Y., Patel, R., Heaton, J.T., Colby, G., Donald Gilmore, L., Cabrera, J., Roy, S.H., Meltzner, G.S., Disordered speech recognition using acoustic and semg signals (2009) Proc. Interspeech, pp. 644-647; Freitas, J., Teixeira, A., Sales Dias, M., Towards a silent speech interface for portuguese (2012) Proc. Biosignals, pp. 91-100; Wand, M., Himmelsbach, A., Heistermann, T., Janke, M., Schultz, T., Artifact removal algorithm for an emg-based silent speech interface (2013) Proc. EMBC, pp. 5750-5753; Janke, M., Wand, M., Schultz, T., A spectral mapping method for emg-based recognition of silent speech (2010) Proc. B-INTERFACE, pp. 22-31; Janke, M., Wand, M., Schultz, T., Impact of lack of acoustic feedback in emg-based silent speech recognition (2010) Proc. Interspeech, pp. 2686-2689; Wand, M., Janke, M., Schultz, T., Investigations on speaking mode discrepancies in emgbased speech recognition (2011) Proc. Interspeech, pp. 601-604; Wand, M., Janke, M., Schultz, T., Decision-tree based analysis of speaking mode discrepancies in emg-based speech recognition (2012) Proc. Biosignals, pp. 101-109; Jou, S., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proc. Interspeech, pp. 573-576. , Pittsburgh, PA, Sep; Schulte, C., (2013) Kompensation Unterschiedlicher Elektrodenpositionierungen in der EMG-basierten Sprachverarbeitung, , M.S. thesis, Karlsruhe Institute of Technology; Gales, M.J.F., Woodland, P.C., Mean and variance adaptation within the mllr framework (1996) Computer Speech and Language, 10, pp. 249-264},
sponsors={},
publisher={Institute of Electrical and Electronics Engineers Inc.},
address={Florence},
issn={15206149},
isbn={9781479928927},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Plantier2014,
author={Plantier, G. and Schultz, T. and Fred, A. and Gamboa, H.},
title={Foreword},
journal={BIOSIGNALS 2014 - 7th Int. Conference on Bio-Inspired Systems and Signal Processing, Proceedings; Part of 7th Int. Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2014},
year={2014},
pages={XI-XII},
note={cited By 0; Conference of 7th International Conference on Bio-Inspired Systems and Signal Processing, BIOSIGNALS 2014 - Part of 7th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2014 ; Conference Date: 3 March 2014 Through 6 March 2014;  Conference Code:105594},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902345088&partnerID=40&md5=ea16df0e6ec2884dc29d2f2d04d17f71},
affiliation={ESEO, GSII, France; Karlsruhe Institute of Technology, Germany; Instituto de Telecomunicações / IST, Portugal; CEFITEC, FCT, New University of Lisbon, Portugal},
sponsors={Control and Communication (INSTICC); Institute for Systems and Technologies of Information,},
publisher={SciTePress},
address={Angers, Loire Valley},
isbn={9789897580116},
language={English},
abbrev_source_title={BIOSIGNALS - Int. Conf. Bio-Inspired Syst. Signal Process., Proc.; Part Int. Jt. Conf. Biomed. Eng. Syst. Technol., BIOSTEC},
document_type={Editorial},
source={Scopus},
}

@CONFERENCE{Plantier2014,
author={Plantier, G. and Schultz, T. and Fred, A. and Gamboa, H.},
title={Foreword},
journal={HEALTHINF 2014 - 7th International Conference on Health Informatics, Proceedings; Part of 7th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2014},
year={2014},
pages={XI-XII},
note={cited By 0; Conference of 7th International Conference on Health Informatics, HEALTHINF 2014 - Part of 7th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2014 ; Conference Date: 3 March 2014 Through 6 March 2014;  Conference Code:105595},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902345038&partnerID=40&md5=95a99713c3cc9908e5ac353cd03a03a5},
affiliation={ESEO, GSII, France; Karlsruhe Institute of Technology, Germany; Instituto de Telecomunicações / IST, Portugal; CEFITEC, FCT, New University of Lisbon, Portugal},
sponsors={Control and Communication (INSTICC); Institute for Systems and Technologies of Information,},
publisher={SciTePress},
address={Angers, Loire Valley},
isbn={9789897580109},
language={English},
abbrev_source_title={HEALTHINF - Int. Conf. Health Informatics, Proc.; Part Int. Jt. Conf. Biomed. Eng. Syst. Technol., BIOSTEC},
document_type={Editorial},
source={Scopus},
}

@CONFERENCE{Heistermann2014189,
author={Heistermann, T. and Janke, M. and Wand, M. and Schultz, T.},
title={Spatial artifact detection for multi-channel EMG-based speech recognition},
journal={BIOSIGNALS 2014 - 7th Int. Conference on Bio-Inspired Systems and Signal Processing, Proceedings; Part of 7th Int. Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2014},
year={2014},
pages={189-196},
doi={10.5220/0004793901890196},
note={cited By 4; Conference of 7th International Conference on Bio-Inspired Systems and Signal Processing, BIOSIGNALS 2014 - Part of 7th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2014 ; Conference Date: 3 March 2014 Through 6 March 2014;  Conference Code:105594},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902331531&doi=10.5220%2f0004793901890196&partnerID=40&md5=203b6ad3432999a240e2a53818f3679e},
affiliation={Cognitive Systems Lab, Institute for Anthropomatics, Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={We introduce a spatial artifact detection method for a surface electromyography (EMG) based speech recognition system. The EMG signals are recorded using grid-shaped electrode arrays affixed to the speakers face. Continuous speech recognition is performed on the basis of these signals. As the EMG data are highdimensional, Independent Component Analysis (ICA) can be applied to separate artifact components from the content-bearing signal. The proposed artifact detection method classifies the ICA components by their spatial shape, which is analyzed using the spectra of the spatial patterns of the independent components. Components identified as artifacts can then be removed. Our artifact detection method reduces the word error rates (WER) of the recognizer significantly. We observe a slight advantage in terms of WER over the temporal signal based artifact detection method by (Wand et al., 2013a). Copyright © 2014 SCITEPRESS - Science and Technology Publications. All rights reserved.},
author_keywords={Array Processing;  Artifact Removal;  EMG;  ICA;  Silent Speech Interfaces;  Speech Recognition},
keywords={Array processing;  Biomedical engineering;  Biomimetics;  Continuous speech recognition;  Independent component analysis;  Signal detection;  Speech;  Speech recognition, Artifact detection;  Artifact removal;  High-dimensional;  Independent component analysis(ICA);  Independent components;  Silent speech interfaces;  Speech recognition systems;  Surface electromyography, Biomedical signal processing},
references={Bell, A.J., Sejnowski, T.I., An information-maximization approach to blind separation and blind deconvolution (1995) Neural Computation, 7, pp. 1129-1159; Blankertz, B., Tomioka, R., Lemm, S., Kawanabe, M., Müller, K., Optimizing spatial filters for robust eeg single-trial analysis (2008) Signal Processing Magazine, IEEE, 25 (1), pp. 41-56; Cardoso, J.-F., Blind signal separation: Statistical principles (1998) Proc. IEEE, 9 (10), pp. 2009-2025; Cavanagh, P., Komi, P., Electromechanical ddelay in human skeletal muscle under concentric and eccentric contractions (1979) European Journal of Applied Physiology and Occupational Physiology, 42 (3), pp. 159-163; Delorme, A., Makeig, S., EEGLAB: An open source toolbox for analysis of single-trial eeg dynamics including independent component analysis (2004) Journal of Neuroscience Methods, 134 (1), pp. 9-21; Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Silent speech interfaces (2010) Speech Communication, 52 (4), pp. 270-287; Deng, Y., Colby, G., Heaton, J.T., Meltzner, G.S., Signal processing advances for the mute SEMG-based silent speech recognition system (2012) Military Communication Conference, MILCOM 2012, pp. 1-6. , IEEE; Freitas, J., Teixeira, A., Dias, M.S., Towards a silent speech interface for portuguese (2012) Proc. Biosignals, pp. 91-100; Hyv̈arinen, A., Oja, E., Independent component analysis: Algorithms and applications (2000) Neural Networks, 13, pp. 411-430; Jorgensen, C., Dusan, S., Speech interfaces based upon surface electromyography (2010) Speech Communication, 52, pp. 354-366; Jorgensen, C., Lee, D., Agabon, S., Sub auditory speech recognition based on EMG/EPG signals (2003) Proceedings of International Joint Conference on Neural Networks (IJCNN), pp. 3128-3133; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Andwaibel, A., Towards continuous speech recognition using surface electromyography (2006) Proc. Interspeech, pp. 573-576; Jou, S.S.-C., Schultz, T., Waibel, A., Continuous electromyographic speech recognition with a multi-stream decoding architecture (2007) Proc. ICASSP, pp. IV401-IV404; Jung, T.-P., Makeig, S., Humphries, C., Lee, T.-W., McKeown, M.J., Iragui, V., Sejnowski, T.J., Removing electroencephalographic artifacts by blind source separation (2000) Psychophysiology, 37 (2), pp. 163-178; Kubo, T., Yoshida, M., Hattori, T., Ikeda, K., Shift Invariant Feature Extraction for sEMG-Based Speech Recognition with Electrode Grid (2013) Engineering in Medicine and Biology Society (EMBC), 2013 35th Annual International Conference of the IEEE, pp. 5797-5800; Lee, K.-F., (1989) Automatic Speech Recognition: The Development of the SPHINX System, , Kluwer Academic Publishers; Metze, F., Waibel, A., A flexible stream architecture for asr using articulatory features (2002) Proc. ICSLP, pp. 2133-2136; Nakamura, H., Yoshida, M., Kotani, M., Akazawa, K., Moritani, T., The application of independent component analysis to the multi-channel surface electromyographic signals for separation ofmotor unit action potential trains: Part ii-modelling interpretation (2004) Journal of Electromyography and Kinesiology, 14 (4), pp. 433-441; Qiao, Z., Zhou, L., Huang, J.Z., Sparse linear discriminant analysis with applications to high dimensional low sample size data (2009) International Journal of Applied Mathematics, 39, pp. 48-60; Ren, X., Hu, X., Wang, Z., Yan, Z., MUAP extraction and classification based onwavelet transform and ica for emg decomposition (2006) Medical and Biological Engineering and Computing, 44 (5), pp. 371-382; Schultz, T., Wand, M., Modeling coarticulation in large vocabulary emg-based speech recognition (2010) Speech Communication, 52 (4), pp. 341-353; Viola, F., Thorne, J., Edmonds, B., Schneider, T., Eichele, T., Debener, S., Semi-Automatic identification of independent components representing eeg artifact (2009) Clinical Neurophysiology, 120 (5), pp. 868-877; Wand, M., Himmelsbach, A., Heistermann, T., Janke, M., Schultz, T., Artifact removal algorithm for an EMG-based silent speech interface (2013) Proc. of the 2013 IEEE Engineering in Medicine and Biology 35th Annual Conference; Wand, M., Schulte, C., Janke, M., Schultz, T., Array-based electromyographic silent speech interface (2013) Proc. Biosignals; Wand, M., Schultz, T., Session-independent emg-based speech recognition (2011) Proc. Biosignals, pp. 295-300; Zhao, H., Xu, G., The research on surface electromyography signal effective feature extraction (2011) Proc. of the 6th International Forum on Strategic Technology},
sponsors={Control and Communication (INSTICC); Institute for Systems and Technologies of Information,},
publisher={SciTePress},
address={Angers, Loire Valley},
isbn={9789897580116},
language={English},
abbrev_source_title={BIOSIGNALS - Int. Conf. Bio-Inspired Syst. Signal Process., Proc.; Part Int. Jt. Conf. Biomed. Eng. Syst. Technol., BIOSTEC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Plantier2014,
author={Plantier, G. and Schultz, T. and Fred, A. and Gamboa, H.},
title={Foreword},
journal={BIODEVICES 2014 - 7th Int. Conference on Biomedical Electronics and Devices, Proceedings; Part of 7th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2014},
year={2014},
pages={IX-X},
note={cited By 0; Conference of 7th International Conference on Biomedical Electronics and Devices, BIODEVICES 2014 - Part of 7th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2014 ; Conference Date: 3 March 2014 Through 6 March 2014;  Conference Code:105590},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902320636&partnerID=40&md5=21072293ec9ea3eed9fb404c649a8c48},
affiliation={ESEO, GSII, France; Karlsruhe Institute of Technology, Germany; Instituto de Telecomunicações / IST, Portugal; CEFITEC, FCT, New University of Lisbon, Portugal},
sponsors={Control and Communication (INSTICC); Institute for Systems and Technologies of Information,},
publisher={SciTePress},
address={Angers, Loire Valley},
isbn={9789897580130},
language={English},
abbrev_source_title={BIODEVICES - Int. Conf. Biomed. Electron. Devices, Proc.; Part Int. Jt. Conf. Biomed. Eng. Syst. Technol., BIOSTEC},
document_type={Editorial},
source={Scopus},
}

@CONFERENCE{Plantier2014,
author={Plantier, G. and Schultz, T. and Fred, A. and Gamboa, H.},
title={Foreword},
journal={BIOINFORMATICS 2014 - 5th Int. Conf. on Bioinformatics Models, Methods and Algorithms, Proceedings; Part of 7th Int. Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2014},
year={2014},
pages={XI-XII},
note={cited By 0; Conference of 5th International Conference on Bioinformatics Models, Methods and Algorithms, BIOINFORMATICS 2014 - Part of 7th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2014 ; Conference Date: 3 March 2014 Through 6 March 2014;  Conference Code:105593},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902303861&partnerID=40&md5=5886c5ddb64cc71430050db7939c6e35},
affiliation={ESEO, GSII, France; Karlsruhe Institute of Technology, Germany; Instituto de Telecomunicações / IST, Portugal; CEFITEC, FCT, New University of Lisbon, Portugal},
sponsors={Control and Communication (INSTICC); Institute for Systems and Technologies of Information,},
publisher={SciTePress},
address={Angers, Loire Valley},
isbn={9789897580123},
language={English},
abbrev_source_title={BIOINFORMATICS - Int. Conf. Bioinformatics Models, Methods Algorithms, Proc.; Part Int. Jt. Conf. Biomed. Eng. Syst. Technol.},
document_type={Editorial},
source={Scopus},
}

@ARTICLE{Besacier201485,
author={Besacier, L. and Barnard, E. and Karpov, A. and Schultz, T.},
title={Automatic speech recognition for under-resourced languages: A survey},
journal={Speech Communication},
year={2014},
volume={56},
number={1},
pages={85-100},
doi={10.1016/j.specom.2013.07.008},
note={cited By 174},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893667016&doi=10.1016%2fj.specom.2013.07.008&partnerID=40&md5=d73f09d012c9fcd9ff23e4ab0854d492},
affiliation={Laboratory of Informatics of Grenoble, Grenoble, France; North-West University, Vanderbijlpark, South Africa; St. Petersburg Institute for Informatics and Automation, Russian Academy of Sciences, St. Petersburg, Russian Federation; Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany},
abstract={Speech processing for under-resourced languages is an active field of research, which has experienced significant progress during the past decade. We propose, in this paper, a survey that focuses on automatic speech recognition (ASR) for these languages. The definition of under-resourced languages and the challenges associated to them are first defined. The main part of the paper is a literature review of the recent (last 8 years) contributions made in ASR for under-resourced languages. Examples of past projects and future trends when dealing with under-resourced languages are also presented. We believe that this paper will be a good starting point for anyone interested to initiate research in (or operational development of) ASR for one or several under-resourced languages. It should be clear, however, that many of the issues and approaches presented here, apply to speech technology in general (text-to-speech synthesis for instance). © 2013 Published by Elsevier B.V.},
author_keywords={Automatic pronunciation generation;  Automatic speech recognition (ASR);  Crosslingual acoustic modeling and adaptation;  Language portability;  Lexical modeling;  Speech and language resources acquisition;  Statistical language modeling;  Under-resourced languages},
keywords={Deep neural networks;  Modeling languages;  Natural language processing systems;  Speech;  Speech processing;  Speech synthesis;  Surveys, Acoustic model;  Automatic pronunciation generation;  Automatic speech recognition;  Language portabilities;  Language resources;  Statistical language modeling;  Under-resourced languages, Speech recognition},
funding_text 1={See for instance “Sorosoro” program funded by the Chirac Foundation http://www.sorosoro.org/ . 26 So, while there is commercial interest in enabling the ∼300 most widely spoken languages in the digital domain (if digital technologies work for this group of languages that represents 95% of humanity), there are other reasons to work on the other ∼6500 languages that are not of commercial interest: to provide access to information, to provide a critical new domain of use for endangered languages, for better linguistic knowledge of them, for response in a crisis (“surge languages”), etc. We are convinced that using automatic speech recognition technologies would be particularly useful for computer assisted language learning of the endangered languages. In addition, the development of tools for field linguists (automatic annotation tools, forced alignment and segmentation, etc.) seems important for revitalizing or at least for documenting endangered languages. The idea here is to evaluate the analysis capabilities of existing automatic speech processing systems to investigate phonetic characteristics of languages. For instance, Gelas et al. (2010) showed the relevance of multilingual acoustic models to study, at a large scale, particular phenomena of rare languages.},
references={Abdillahi, N., Nocera, P., Bonastre, J.-F., Automatic transcription of somali language (2006) ICSLP'06, pp. 289-292. , Pittsburgh, PA, USA; Ablimit, M., Neubig, G., Mimura, M., Mori, S., Kawahara, T., Hamdulla, A., Uyghur morpheme-based language models and ASR (2010) Proc. IEEE 10th International Conference on Signal Processing (ICSP), pp. 581-584. , Beijing, China; Adda-Decker, M., A corpus-based decompounding algorithm for german lexical modeling in LVCSR (2003) Proc. Eurospeech-2003, pp. 257-260. , Geneva, Switzerland; Arisoy, E., Dutagaci, H., Arslan, L., A unified language model for large vocabulary continuous speech recognition of turkish (2006) Signal Processing, 86 (10), pp. 2844-2862; Arisoy, E., Sainath, T.N., Kingsbury, B., Ramabhadran, B., Deep neural network language models (2012) Proc. NAACL-HLT 2012 Workshop, pp. 20-28. , Montreal, Canada; Barnard, E., Davel, M., Van Heerden, C., ASR corpus design for resource-scarce languages (2009) Proc. Interspeech, pp. 2847-2850; Barnard, E., Davel, M., Van Huyssteen, G.B., Speech technology for information access: A South African case study (2010) Proceedings of the AAAI Spring Symposium on Artificial Intelligence for Development (AI-D), pp. 8-13. , Palo Alto, California, March 2010; Barnett, J., Corrada, A., Gao, G., Gillik, L., Ito, Y., Lowe, S., Manganaro, L., Peskin, B., Multilingual speech recognition at dragon systems (1996) Proc. ICSLP, pp. 2191-2194. , Philadelphia; Berment, V., (2004) Méthodes pour Informatiser des Langues et des Groupes de Langues Peu Dotées, , Ph.D. Thesis, J. Fourier University - Grenoble I, May 2004; Besacier, L., Zhou, B., Gao, Y., Towards speech translation of non written languages (2006) IEEE/ACL SLT 2006, , Aruba, December 2006; Bhanuprasad, K., Svenson, M., Errgrams - A way to improving ASR for highly inflective dravidian languages (2008) Proc. 3rd International Joint Conf. on Natural Language Processing IJCNLP'08, pp. 805-810. , India; Billa, J., Ma, K., McDonough, J., Zavaliagkos, G., Miller, D.R., Ross, K.N., El-Jaroudi, A., Multilingual speech recognition: The 1996 byblos callhome system (1997) Proc. Eurospeech-1997, pp. 363-366. , Rhodes, Greece; Cai, J., Transcribing southern min speech corpora with a web-based language learning system (2008) SLTU'08, , Hanoi, Vietnam; Carki, K., Geutner, P., Schultz, T., Turkish LVCSR: Towards better speech recognition for agglutinative languages (2000) IEEE ICASSP; Cetin, O., Unsupervised adaptive speech technology for limited resource languages: A case study for tamil (2008) SLTU'08, , Hanoi, Vietnam; Chan, H.Y., Rosenfeld, R., Discriminative pronunciation learning for speech recognition for resource scarce languages (2012) Proceedings of the 2nd ACM Symposium on Computing for Development, , Article No. 12; Charniak, E., Knight, K., Yamada, K., Syntax-based language models for machine translation (2003) Proc. IX MT Summit, pp. 40-46. , New Orleans, USA; Charoenpornsawat, P., Hewavitharana, S., Schultz, T., Thai grapheme-based speech recognition (2006) Human Language Technology Conference (HLT); Chelba, C., Jelinek, F., Structured language model (2000) Computer Speech and Language, 10, pp. 283-332; Cohen, P., Dharanipragada, S., Gros, J., Monkowski, M., Neti, C., Roukos, S., Ward, T., Towards a universal speech recognizer for multiple languages (1997) Proc. Automatic Speech Recognition and Understanding (ASRU), pp. 591-598. , St. Barbara CA; Constantinescu, A., Chollet, G., On cross-language experiments and data-driven units for ALISP (1997) Proc. Automatic Speech Recognition and Understanding (ASRU), pp. 606-613. , St. Barbara CA; Creutz, M., Lagus, K., Unsupervised morpheme segmentation and morphology induction from text corpora using morfessor 1.0 (2005) Computer and Information Science, Report A81, , Helsinki University of Technology, Finland; Creutz, M., Hirsimaki, T., Kurimo, M., Puurula, A., Pylkkonen, J., Siivola, V., Varjokallio, M., Stolcke, A., Morph-based speech recognition and modeling of out-of-vocabulary words across languages (2007) ACM Transactions on Speech and Language Processing, 5 (1). , Article No. 3; Crystal, D., (2000) Language Death, , Cambridge CUP; Cucu, H., Besacier, L., Burileanu, C., Buzo, A., Investigating the role of machine translated text in ASR domain adaptation: Unsuper-vised and semi-supervised methods (2011) Proc. ASRU 2011, , Hawaii, USA; Cucu, H., Besacier, L., Burileanu, C., Buzo, A., ASR domain adaptation methods for low-resourced languages: Application to Romanian language (2012) EUSIPCO'2012, , Bucarest, Romania; Cucu, H., Buzo, A., Besacier, L., Burileanu, C., SMT-based ASR domain adaptation methods for under-resourced languages: Application to Romanian (2013) Speech Communication, , http://dx.doi.org/10.1016/j.specom.2013.05.003; Davel, M.H., Van Heerden, C., Kleynhans, N., Barnard, E., Efficient harvesting of internet audio for resource-scarce ASR (2011) Proc. Interspeech, pp. 3153-3156; De Vries, N.J., Badenhorst, J., Davel, M.H., Barnard, E., De Waal, A., Woefzela - An open-source platform for ASR data collection in the developing world (2011) Proc. Interspeech, pp. 3177-3180; De Vries, N.J., Davel, M.H., Badenhorst, J., Basson, W.D., De Wet, F., Barnard, E., De Waal, A., A smartphone-based ASR data collection tool for under-resourced languages (2013) Speech Communication, , http://dx.doi.org/10.1016/j.specom.2013.07.001; Denoual, E., Lepage, Y., The character as an appropriate unit of processing for non-segmenting languages (2006) NLP Annual Meeting, pp. 731-734. , Tokyo, Japan; Do, T., Besacier, L., Castelli, E., Unsupervised SMT for a low-resourced language pair (2010) Workshop on Spoken Language Technologies for Under-resourced Languages (SLTU), , Penang, Malaysia; Dugast, C., Aubert, X., Kneser, R., The philips large-vocabulary recognition system for American english, french, and german (1995) Proc. Eurospeech, pp. 197-200. , Madrid; Ekpenyong, M., Urua, E.-A., Watts, O., King, S., Yamagishi, J., Statistical parametric speech synthesis for ibibio (2013) Speech Communication, , http://dx.doi.org/10.1016/j.specom.2013.02.003; Ganapathiraju, A., Hamaker, J., Picone, J., Hybrid SVM/HMM architectures for speech recognition (2000) Proceedings of Speech Transcription Workshop, pp. 504-507; Gebreegziabher, M., Besacier, L., English-amharic statistical machine translation (2012) SLTU - Workshop on Spoken Language Technologies for Under-Resourced Languages, , Cape-Town, South Africa; Gelas, H., Besacier, L., Rossato, S., Pellegrino, F., Using automatic speech recognition for phonological purposes: Study of vowel length in punu (Bantu B40) (2010) Laphon 12, , New Mexico (US), July 2010; Gelas, H., Teferra Abate, S., Besacier, L., Pellegrino, F., Quality assessment of crowdsourcing transcriptions for african languages (2011) Interspeech 2011 Florence, , Italy, 28-31 August 2011; Gemmeke, J.F., Van Hamme, H., A hierarchical exemplar-based sparse model of speech with an application to ASR (2011) IEEE ASRU 2011, , HI, USA; Ghoshal, A., Jansche, M., Khudanpur, S., Riley, M., Ulinski, M., Web-derived pronunciations (2009) IEEE ICASSP; Gizaw, S., Multiple pronunciation model for amharic speech recognition system (2008) SLTU 2008, , Hanoi, Vietnam; Glass, J., Flammia, G., Goodine, D., Phillips, M., Polifroni, J., Sakai, S., Seneff, S., Zue, V., Multi-lingual spoken language understanding in the MIT voyager system (1995) Speech Communication, 17, pp. 1-18; Godfrey, J.J., Holliman, E.C., McDaniel, J., SWITCHBOARD: Telephone speech corpus for research and development (1992) IEEE International Conference on Acoustics, Speech, and Signal Processing, 1, pp. 517-520; Gokcen, S., Gokcen, J., A multilingual phoneme and model set: Towards a universal base for automatic speech recognition (1997) Proc. Automatic Speech Recognition and Understanding (ASRU), pp. 599-603. , St. Barbara CA; Grezl, F., Probabilistic and bottle-neck features for LVCSR of meetings (2007) Proc. ICASSP, , USA; Hermansky, H., Wellis, D., Sharma, S., Tandem connectionist feature extraction for conventional HMM systems (2000) Proc. ICASSP, , Turkey; Huang, C., Chang, E., Zhou, J., Lee, K.-F., Accent modeling based on pronunciation dictionary adaptation for large vocabulary mandarin speech recognition (2000) Proc. INTERSPEECH-2000, pp. 818-821. , Beijing, China; Huet, S., Gravier, G., Sebillot, P., Morpho-syntactic postprocessing of N-best lists for improved french automatic speech recognition (2010) Computer Speech and Language, 24 (4), pp. 663-684; Hughes, T., Nakajima, K., Ha, L., Moreno, P., LeBeau, M., Building transcribed speech corpora quickly and cheaply for many languages (2010) Proc. Interspeech, pp. 1914-1917. , Makuhari, Japan; (1999) Handbook of the International Phonetic Association: A Guide to the use of the International Phonetic Alphabet, , IPA Cambridge University Press; Jensson, A., Development of a speech recognition system for Icelandic using machine translated text (2008) SLTU'08, , Hanoi, Vietnam; Jing, Z., Min, Z., Speech recognition system based improved DTW algorithm (2010) Proc. Int. Conf. on Computer, Mechatronics, Control And, Electronic Engineering CMCE-2010, 5, pp. 320-323; Kanejiya, D.P., Kumar, A., Prasad, S., Statistical language modeling using syntactically enhanced LSA (2003) Proc. TIFR Workshop on Spoken Language Processing, pp. 93-100. , Mumbai, India; Kanthak, S., Ney, H., Multilingual acoustic modeling using graphemes (2003) Eurospeech-2003, pp. 1145-1148. , Geneva, Switzerland; Karanasou, P., Lamel, L., Comparing SMT methods for automatic generation of pronunciation variants (2010) IceTAL 2010, p. 167. , Reykjavik, Iceland; Karpov, A., Kipyatkova, I., Ronzhin, A., Very large vocabulary ASR for spoken Russian with syntactic and morphemic analysis (2011) Proc. Interspeech'2011, pp. 3161-3164. , Florence, Italy; Karpov, A., Markov, K., Kipyatkova, I., Vazhenina, D., Ronzhin, A., Large vocabulary Russian speech recognition using syntactico-statistical language modeling (2013) Speech Communication, , http://dx.doi.org/10.1016/j.specom.2013.07.004; Kiecza, D., Schultz, T., Waibel, A., Data-driven determination of appropriate dictionary units for Korean LVCSR (1999) Proceedings of the International Conference on Speech Processing, pp. 323-327; Killer, M., Stüker, S., Schultz, T., Grapheme based speech recognition (2003) Interspeech; Kipyatkova, I., Karpov, A., Verkhodanova, V., Zelezny, M., Analysis of long-distance word dependencies and pronunciation variability at conversational Russian speech recognition (2012) Proc. FedCSIS-2012, pp. 719-725. , Wroclav, Poland; Köhler, J., Language adaptation of multilingual phone models for vocabulary independent speech recognition tasks (1998) Proc. ICASSP, pp. 417-420. , Seattle; Krauwer, S., The basic language resource kit (BLARK) as the first milestone for the language resources roadmap (2003) Proceedings of the 2003 International Workshop Speech and Computer SPECOM-2003, pp. 8-15. , Moscow, Russia; Kuo, H.-K.J., Mangu, L., Emami, A., Zitouni, I., Lee, Y.-S., Syntactic features for arabic speech recognition (2009) Proc. International Workshop ASRU'2009, pp. 327-332. , Merano, Italy; Kurimo, M., Puurula, A., Arisoy, E., Siivola, V., Hirsimaki, T., Pylkkonen, J., Alumae, T., Saraclar, M., Unlimited vocabulary speech recognition for agglutinative languages (2006) Proc. HLT-NAACL, , NY, USA; Kurimo, M., Unsupervised segmentation of words into morphemes - Morpho challenge. Application to automatic speech recognition (2006) Proc. Interspeech'06, pp. 1021-1024. , Pittsburgh, PA, USA; Lamel, L., Adda-Decker, M., Gauvain, J.L., Issues in large vocabulary multilingual speech recognition (1995) Proc. Eurospeech, pp. 185-189. , Madrid; Laurent, A., Deléglise, P., Meignier, S., Grapheme to phoneme conversion using an SMT system (2009) Interspeech 2009, pp. 708-711. , Brighton, UK; Le, V.-B., Besacier, L., Automatic speech recognition for under-resourced languages: Application to vietnamese language (2009) IEEE Transactions on Audio, Speech and Language Processing, 17 (8), pp. 1471-1482; Le, V.B., Bigi, B., Besacier, L., Castelli, E., Using the web for fast language model construction in minority languages (2003) Euro-speech'03, pp. 3117-3120. , Geneva, Switzerland; Lee, D.-G., Rim, H.-C., Probabilistic modeling of Korean morphology (2009) IEEE Transactions on Audio, Speech & Language Processing, 17 (5), pp. 945-955; Loof, J., Gollan, C., Ney, H., Cross-language bootstrapping for unsupervised acoustic model training: Rapid development of a polish speech recognition system (2009) Interspeech 2009, , Brighton, UK; Lopatková, M., Plátek, M., Kuboň, V., Modeling syntax of free word-order languages: Dependency analysis by reduction (2005) Proc. TSD'2005, pp. 140-147. , Springer LNAI 3658, Karlovy Vary, Czech Republic; Mihajlik, P., Fegyó, T., Tüske, Z., Ircing, P., Morpho-graphemic approach for the recognition of spontaneous speech in agglutinative languages - Like hungarian (2007) Interspeech'07, , Antwerp, Belgium; Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., Khudanpur, S., Recurrent neural network based language model (2010) Proc. INTER-SPEECH-2010, pp. 1045-1048. , Makuhari, Japan; Mohamed, A., Dahl, G.E., Hinton, G., Acoustic modeling using deep belief networks (2012) IEEE Transactions on Audio, Speech, and Language Processing, 20 (1), pp. 14-22; Muthusamy, Y.K., Cole, R.A., Automatic segmentation and identification of ten languages using telephone speech (1992) Second International Conference on Spoken Language Processing; Nakajima, H., Yamamoto, H., Watanabe, T., Language model adaptation with additional text generated by machine translation (2002) COLING 2002, 2, pp. 716-722. , Taipei, Taiwan; Nanjo, H., Kawahara, T., A new ASR evaluation measure and minimum bayes-risk decoding for open-domain speech understanding (2005) Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing ICASSP-2005, pp. 1053-1056. , PA, USA; (2009) Rich Transcription Meeting Recognition Evaluation Plan, , The US NIST 2009 (RT-09); Oparin, I., Glembek, O., Burget, L., Černocký, J., Morphological random forests for language modeling of inflectional languages (2008) Proc. IEEE Workshop on Spoken Language Technology SLT'08, , Goa, India; Parent, G., Eskenazi, M., Toward better crowdsourced transcription: Transcription of a year of the let's go bus information system data (2010) Proceedings of IEEE Workshop on Spoken Language Technology, pp. 312-317. , Berkeley, California, December 2010; Patel, N., Agarwal, S., Rajput, N., Nanavati, A., Dave, P., Parikh, T.S., A comparative study of speech and dialed input voice interfaces in rural India (2009) CHI'09: Proceedings of the 27th International Conference on Human Factors in Computing Systems, pp. 51-54. , ACM, New York, NY, USA; Patel, N., Chittamuru, D., Jain, A., Dave, P., Parikh, T.S., Avaaj otalo: A field study of an interactive voice forum for small farmers in rural India (2010) CHI. ACM, pp. 733-742; Pellegrini, T., Lamel, L., Investigating automatic decomposition for ASR in less represented languages (2006) ICSLP'06, , Pittsburgh; Pellegrini, T., Lamel, L., Are audio or textual training data more important for ASR in less-represented languages? (2008) SLTU'08, , Hanoi, Vietnam; Pellegrini, T., Lamel, L., Automatic word decompounding for ASR in a morphologically rich language: Application to amharic (2009) IEEE Transactions on Audio, Speech & Language Processing, 17 (5), pp. 863-873; Plahl, C., Schlueter, R., Ney, H., Cross-lingual portability of Chinese and english neural network features for french and german LVCSR (2011) Proc. ASRU, , USA; Rastrow, A., Dredze, M., Khudanpur, S., Fast syntactic analysis for statistical language modeling via substructure sharing and uptraining (2012) Proc. 50th Annual Meeting of Association for Computational Linguistics ACL'2012, pp. 175-183. , Jeju, Korea; Ronzhin, A.L., Karpov, A.A., Russian voice interface (2007) Pattern Recognition and Image Analysis, 17 (2), pp. 321-336. , DOI 10.1134/S1054661807020216; Rotovnik, T., Maucec, M.S., Kacic, Z., Large vocabulary continuous speech recognition of an inflected language using stems and endings (2007) Speech Communication, 49 (6), pp. 437-452. , DOI 10.1016/j.specom.2007.02.010, PII S0167639307000428; Roux, J.C., Botha, E.C., Du Preez, J.A., Developing a multilingual telephone based information retrieval system in african languages (2000) Proceedings of the Second International Conference on Language Resources and Evaluation, pp. 975-980; Sak, H., Saraclar, M., Güngör, T., Morphology-based and subword language modeling for turkish speech recognition (2010) ICASSP 2010, pp. 5402-5405; Sarikaya, R., Afify, M., Gao, Y., Joint morphological-lexical language modeling (JMLLM) for arabic (2007) Proc. ICASSP'07, 4, pp. 181-184; Schlippe, T., Ochs, S., Schultz, T., Wiktionary as a source for automatic pronunciation extraction (2010) Interspeech 2010, , Makuhari, Japan, 26-30 September 2010; Schlippe, T., Ochs, S., Schultz, T., Grapheme-to-phoneme model generation for indo-european languages (2012) ICASSP 2012, , Kyoto, Japan, 25-30 March 2012; Schlippe, T., Ochs, S., Vu, N.T., Schultz, T., Automatic error recovery for pronunciation dictionaries (2012) Interspeech 2012, , Portland, Oregon, 9-13 September 2012; Schlippe, T., Ochs, S., Schultz, T., Web-based tools and methods for rapid pronunciation dictionary creation (2013) Speech Communication, , http://dx.doi.org/10.1016/j.specom.2013.06.015; Schultz, T., GlobalPhone: A multilingual speech and text database developed at karlsruhe university (2002) ICSLP, pp. 345-348; Schultz, T., (2006) Multilingual Speech Processing, , Tanja Schultz, Katrin Kirchhoff (Eds.), Elsevier, Academic Press, ISBN 13: 978-0-12-088501-5, 2006; Schultz, T., Black, A.W., Badaskar, S., Hornyak, M., Kominek, J., SPICE: Web-based tools for rapid language adaptation in speech processing systems (2007) Interspeech 2007, , Antwerp, Belgium; Schultz, T., Vu, N.T., Schlippe, T., GlobalPhone: A multilingual text & speech database in 20 languages (2013) ICASSP 2013, , Vancouver, Canada; Schultz, T., Waibel, A., Language independent and language adaptive LVCSR (1998) Proc. ICSLP, pp. 1819-1822. , Sydney; Schultz, T., Waibel, A., Language-independent and language-adaptive acoustic modeling for speech recognition (2001) Speech Communication, 35 (1-2), pp. 31-51. , DOI 10.1016/S0167-6393(00)00094-7, PII S0167639300000947; Seide, F., Li, G., Chen, X., Yu, D., Feature engineering in context-dependent deep neural networks for conversational speech transcription (2011) Proc. ASRU-2011 International Workshop, pp. 24-29. , HI, USA; Siniscalchi, S.M., Reed, J., Svendsen, T., Lee, C.-H., Universal attribute characterization of spoken languages for automatic spoken language recognition (2013) Computer Speech & Language, 27 (1), pp. 209-227; Solera-Urena, R., Martin-Iglesias, D., Gallardo-Antolin, A., Pelaez-Moreno, C., Diaz-de-Maria, F., Robust ASR using Support Vector Machines (2007) Speech Communication, 49 (4), pp. 253-267. , DOI 10.1016/j.specom.2007.01.013, PII S0167639307000246; Stahlberg, F., Schlippe, T., Vogel, S., Schultz, T., Word segmentation through cross-lingual word-to-phoneme alignment (2012) Proceedings of the Fourth IEEE Workshop on Spoken Language Technology (SLT 2012), , Miami, Florida, 2-5 December 2012; Stahlberg, F., Schlippe, T., Vogel, S., Schultz, T., Pronunciation extraction from phoneme sequences through cross-lingual word-to-phoneme alignment (2013) Proceedings of the 1st International Conference on Statistical Language and Speech Processing (SLSP 2013), , Tarragona, Spain, 29-31 July 2013; Stephenson, T.A., Escofet, J., Magimai-Doss, M., Bourlard, H., Dynamic Bayesian network based speech recognition with pitch and energy as auxiliary variables (2002) Technical Report Idiap-RR-24-2002, p. 10; Stolcke, A., Grezl, F., Hwang, M.-Y., Lei, X., Morgan, N., Vergyri, D., Cross-domain and cross-lingual portability of acoustic features estimated by multilayer perceptrons (2006) Proc. ICASSP 2006; Stüker, S., Integrating thai grapheme based acoustic models into the ML-mix framework - For language independent and cross-language ASR (2008) SLTU'08, , Hanoi, Vietnam; Stüker, S., Schultz, T., Metze, F., Waibel, A., Multilingual articulatory features (2003) ICASSP 2003; Stuker, S., Schultz, T., Metze, F., Waibel, A., Multilingual articulatory features (2003) Proceedings. ICASSP'03 IEEE International Conference on Acoustics, Speech, And, Signal Processing; Stüker, S., Besacier, L., Waibel, A., Human translations guided language discovery for ASR systems (2009) InterSpeech-2009, , Brighton, UK; Suenderman, K., Liscombe, J., Localization of speech recognition in spoken dialog systems: How machine translation can make our lives (2009) Interspeech 2009, pp. 1475-1478. , Brighton, UK; Szarvas, M., Furui, S., Finite-state transducer based modeling of morphosyntax with applications to hungarian LVCSR (2003) Proc. ICASSP, pp. 368-371. , HongKong, China; Tachbelie, M., Abate, S.T., Besacier, L., Rossato, S., Syllable-based and hybrid acoustic models for amharic speech recognition (2012) SLTU - Workshop on Spoken Language Technologies for Under-Resourced Languages, , Cape-Town, South Africa; Tachbelie, M., Abate, S.T., Besacier, L., Using different acoustic, lexical and language modeling units for ASR of an under-resourced language - Amharic (2013) Speech Communication, , http://dx.doi.org/10.1016/j.specom.2013.01.008; Tarjan, B., Mihajlik, P., On morph-based LVCSR improvements (2010) Proc. 2nd Int. Workshop on Spoken Languages Technologies for Under-resourced Languages SLTU-2010, pp. 10-16. , Malaysia; Thomas, S., Ganapathy, S., Hermansky, H., Multilingual MLP features for low-resource LVCSR systems (2012) Proc. ICASSP, , Japan; Thomas, S., Ganapathy, S., Jansen, A., Hermansky, H., Data-driven posterior features for low resource speech recognition applications (2012) Proc. Interspeech, , USA; Toth, L., Frankel, J., Gosztolya, G., King, S., Cross-lingual portability of MLP-based tandem features - A case study for english and hungarian (2008) Proc. Interspeech; Trentin, E., Gori, M., A survey of hybrid ANN/HMM models for automatic speech recognition (2001) Neurocomputing, 37 (1), pp. 91-126; Van Heerden, C., Kleynhans, N., Barnard, E., Davel, M., Pooling ASR data for closely related languages (2010) Proceedings of the Workshop on Spoken Languages Technologies for Under-Resourced Languages (SLTU 2010), pp. 17-23. , Penang, Malaysia, May 2010; Van Niekerk, D.R., Barnard, E., Predicting utterance pitch targets in yoruba for tone realisation in speech synthesis (2013) Speech Communication, , http://dx.doi.org/10.1016/j.specom.2013.01.009; Vergyri, D., Kirchhoff, K., Duh, K., Stolcke, A., Morphology-based language modeling for arabic speech recognition (2004) Proc. ICSLP'04, pp. 2245-2248; Vesely, K., Karafiat, M., Grezl, F., Janda, M., Egorova, E., The language-independent bottleneck features (2012) Proc. SLT, , USA; Vu, N.T., Kraus, F., Schultz, T., Multilingual A-stabil: A new confidence score for multilingual unsupervised training (2010) Proc. SLT, , USA; Vu, N.T., Kraus, F., Schultz, T., Rapid building of an ASR system for under-resourced languages based on multilingual unsupervised training (2011) Proc. Interspeech, , Italy; Vu, N.T., Metze, F., Schultz, T., Multilingual bottle-neck feature for under resourced languages (2012) Proc. SLTU, , South Africa; Vu, N.T., Breiter, W., Metze, F., Schultz, T., An investigation on initialization schemes for multilayer perceptron training using multilingual data and their effect on ASR performance (2012) Proc. Interspeech, , USA; Wheatley, B., Kondo, K., Anderson, W., Muthusamy, Y., An evaluation of cross-language adaptation for rapid HMM development in a new language (1994) Proc. ICASSP, pp. 237-240. , Adelaide; Whittaker, E.W.D., (2000) Statistical Language Modelling for Automatic Speech Recognition of Russian and English, p. 140. , Ph.D. thesis, Cambridge Univ; Whittaker, E.W.D., Woodland, P.C., Efficient class-based language modelling for very large vocabularies (2001) ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings, 1, pp. 545-548; Wissing, D., Barnard, E., Vowel variations in southern sotho: An acoustical investigation (2008) Southern African Linguistics and Applied Language Studies, 26 (2), pp. 255-265; Young, S.J., Adda-Dekker, M., Aubert, X., Dugast, C., Gauvain, J.-L., Kershaw, D.J., Lamel, L., Woodland, P.C., Multilingual large vocabulary speech recognition: The European SQALE project (1997) Computer Speech and Language, 11 (1), pp. 73-89; Young, S., HMMs and related speech recognition technologies (2008) Springer Handbook of Speech Processing, pp. 539-557. , Springer-Verlag, Berlin Heidelberg; Yu, D., Siniscalchi, S.M., Deng, L., Lee, C.-H., Boosting attribute and phone estimation accuracies with deep neural networks for detection-based speech recognition (2012) Proc. ICASSP-2012, pp. 4169-4172},
publisher={Elsevier B.V.},
issn={01676393},
coden={SCOMD},
language={English},
abbrev_source_title={Speech Commun},
document_type={Article},
source={Scopus},
}

@ARTICLE{Besacier201483,
author={Besacier, L. and Barnard, E. and Karpov, A. and Schultz, T.},
title={Introduction to the special issue on processing under-resourced languages},
journal={Speech Communication},
year={2014},
volume={56},
number={1},
pages={83-84},
doi={10.1016/j.specom.2013.09.001},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893085966&doi=10.1016%2fj.specom.2013.09.001&partnerID=40&md5=5fccf5270d39ef682878a2c9cde018a5},
affiliation={Laboratory of Informatics of Grenoble, Grenoble, France; North-West University, Vanderbijlpark, South Africa; St. Petersburg Institute for Informatics and Automation, Russian Academy of Sciences, St. Petersburg, Russian Federation; Karlsruhe Institute of Technology, Karlsruhe, Germany},
publisher={Elsevier B.V.},
issn={01676393},
coden={SCOMD},
language={English},
abbrev_source_title={Speech Commun},
document_type={Editorial},
source={Scopus},
}

@CONFERENCE{Adel2014651,
author={Adel, H. and Kirchhoff, K. and Vu, N.T. and Telaar, D. and Schultz, T.},
title={Comparing approaches to convert recurrent neural networks into backoff language models for efficient decoding},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2014},
pages={651-655},
note={cited By 13; Conference of 15th Annual Conference of the International Speech Communication Association: Celebrating the Diversity of Spoken Languages, INTERSPEECH 2014 ; Conference Date: 14 September 2014 Through 18 September 2014;  Conference Code:108771},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910090415&partnerID=40&md5=c2e37bca544f3ace61d727aeb94a4ad7},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology, Germany; Department of Electrical Engineering, University of Washington, United States},
abstract={In this paper, we investigate and compare three different possibilities to convert recurrent neural network language models (RNNLMs) into backoff language models (BNLM). While RNNLMs often outperform traditional n-gram approaches in the task of language modeling, their computational demands make them unsuitable for an efficient usage during decoding in an LVCSR system. It is, therefore, of interest to convert them into BNLMs in order to integrate their information into the decoding process. This paper compares three different approaches: A text based conversion, a probability based conversion and an iterative conversion. The resulting language models are evaluated in terms of perplexity and mixed error rate in the context of the Code-Switching data corpus SEAME. Although the best results are obtained by combining the results of all three approaches, the text based conversion approach alone leads to significant improvements on the SEAME corpus as well while offering the highest computational efficiency. In total, the perplexity can be reduced by 11.4% relative on the evaluation set and the mixed error rate by 3.0% relative on the same data set. Copyright © 2014 ISCA.},
author_keywords={Code switching;  Decoding with neural network language models;  Language modeling;  Recurrent neural networks},
keywords={Computational efficiency;  Computational linguistics;  Decoding;  Speech communication, Backoff;  Code-switching;  Computational demands;  Data set;  Decoding process;  Language model;  Mixed errors;  Network language, Recurrent neural networks},
references={Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., Khudanpur, S., Recurrent neural network based language model (2010) Proc. of Interspeech; Mikolov, T., Kombrink, S., Burget, L., Cernocky, J.H., Khudanpur, S., Extensions of recurrent neural network language model (2011) Proc. of ICASSP, , IEEE; Adel, H., Vu, N.T., Kraus, F., Schlippe, T., Li, H., Schultz, T., Recurrent neural network language modeling for code switching conversational speech (2013) Proc. of ICASSP, , IEEE; Shi, Y., Wiggers, P., Jonker, C.M., Towards recurrent neural networks language models with linguistic and contextual features (2012) Proc. of Interspeech; Deoras, A., Mikolov, T., Kombrink, S., Karafiat, M., Khudanpur, S., Variational approximation of long-span language models for LVCSR (2011) Proc. of ICASSP, , IEEE; Arisoy, E., Chen, S.F., Ramabhadran, B., Sethy, A., Converting neural network language models into back-off language models for efficient decoding in automatic speech recognition (2013) , Proc. of ICASSP, , IEEE; Stolcke, A., SRILM - An extensible language modeling toolkit (2002) Proc. of SLP; Chen, S.F., Goodman, J., (1998) An Empirical Study of Smoothing Techniques for Language Modeling, , Technical Report; Lyu, D.C., Tan, T.P., Chng, E.S., Li, H., An analysis of a Mandarin-English code-switching speech corpus: Seame (2010) Oriental COCOSDA; Telaar, D., Wand, M., Gehrig, D., Putze, F., Amma, C., Heger, D., Vu, N.T., Schultz, T., Biokit - real-time decoder for biosignal processing (2014) Proc. of Interspeech; Vu, N.T., Metze, F., Schultz, T., Multilingual bottleneck features and its application for under-resourced languages (2012) Proc. of SLTU; CMU Pronouncation Dictionary for English, , http://www.speech.cs.cmu.edu/cgi-bin/cmudict, Online; Hsiao, R., Fuhs, M., Tam, Y., Jin, Q., Schultz, T., The CMU-interact 2008 mandarin transcription system (2008) Proc. of ICASSP, , IEEE; Chen, W., Tan, Y., Chng, E., Li, H., The development of a Singapore English call resource (2010) Oriental COCOSDA; Vu, N.T., Lyu, D.C., Weiner, J., Telaar, D., Schlippe, T., Blaicher, F., Chng, E.S., Li, H., A first speech recognition system for mandarin- English code-switch conversational speech (2012) Proc. of ICASSP, , IEEE},
correspondence_address1={Adel, H.; Cognitive Systems Lab, Karlsruhe Institute of TechnologyGermany},
editor={Chng E.S., Li H., Meng H., Ma B., Xie L.},
sponsors={Amazon; Baidu; et al.; Google; Temasek Laboratories at Nanyang Technological University (TL at NTU); WeChat},
publisher={International Speech and Communication Association},
issn={2308457X},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Adel20141415,
author={Adel, H. and Telaar, D. and Vu, N.T. and Kirchhoff, K. and Schultz, T.},
title={Combining recurrent neural networks and factored language models during decoding of code-switching speech},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2014},
pages={1415-1419},
note={cited By 8; Conference of 15th Annual Conference of the International Speech Communication Association: Celebrating the Diversity of Spoken Languages, INTERSPEECH 2014 ; Conference Date: 14 September 2014 Through 18 September 2014;  Conference Code:108771},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910049659&partnerID=40&md5=4df271815691a3acdbd6014e1733486a},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology, Germany; Department of Electrical Engineering, University of Washington, United States},
abstract={In this paper, we present our latest investigations of language modeling for Code-Switching. Since there is only little text material for Code-Switching speech available, we integrate syntactic and semantic features into the language modeling process. In particular, we use part-of-speech tags, language identifiers, Brown word clusters and clusters of open class words. We develop factored language models and convert recurrent neural network language models into backoff language models for an efficient usage during decoding. A detailed error analysis reveals the strengths and weaknesses of the different language models. When we interpolate the models linearly, we reduce the perplexity by 15.6% relative on the SEAME evaluation set. This is even slightly better than the result of the unconverted recurrent neural network. We also combine the language models during decoding and obtain a mixed error rate reduction of 4.4% relative on the SEAME evaluation set. Copyright © 2014 ISCA.},
author_keywords={Code-switching;  Factored language models;  Language modeling;  Recurrent neural network language models},
keywords={Computational linguistics;  Decoding;  Error analysis;  Semantics;  Speech communication;  Switching, Backoff;  Code-switching;  Language model;  Mixed errors;  Part-of-speech tags;  Semantic features;  Text materials;  Word-clusters, Recurrent neural networks},
references={Mikolov, T., Kombrink, S., Burget, L., Cernocky, J.H., Khudanpur, S., Extensions of recurrent neural network language model (2011) Proc. of ICASSP, , IEEE; Kirchhoff, K., Bilmes, J.A., Duh, K., Factored language models tutorial (2007) Tech. Rep. University of Washington, , EE Department, UWEETR-2008-004; Adel, H., Vu, N.T., Kraus, F., Schlippe, T., Li, H., Schultz, T., Recurrent neural network language modeling for code switching conversational speech (2013) Proc. of ICASSP, , IEEE; Adel, H., Vu, N.T., Schultz, T., Combination of recurrent neural networks and factored language models for Code- Switching language modeling (2013) Proc. of ACL; Solorio, T., Liu, Y., Learning to predict code-switching points (2008) Proc. of EMNLP, , ACL; Chan, J.Y.C., Ching, P.C., Lee, T., Cao, H., Automatic speech recognition of Cantonese-English code-mixing utterances (2006) Proc. of Inter Speech; Schultz, T., Fung, P., Burgmer, C., Detecting Code-Switch Events Based on Textual Features; Adel, H., Kirchhoff, K., Telaar, D., Vu, N.T., Schultz, T., Features for factored language models for Code- Switching speech (2014) Proc. of SLTU; Mikolov, T., Karafiát, M., Burget, L., Cernocky, J., Khudanpur, S., Recurrent neural network based language model (2010) Proc. of Inter Speech; Shi, Y., Wiggers, P., Jonker, C.M., Towards recurrent neural networks language models with linguistic and contextual features (2012) Proc. of Inter Speech; Mikolov, T., Yih, W.-T., Zweig, G., Linguistic regularities in continuous space word representations (2013) Proc. of HLT-NAACL. ACL; Arisoy, E., Chen, S.F., Ramabhadran, B., Sethy, A., Converting neural network language models into back-off language models for efficient decoding in automatic speech recognition (2013) Proc. of ICASSP, , IEEE; Deoras, A., Mikolov, T., Kombrink, S., Karafiát, M., Khudanpur, S., Variational approximation of long-span language models for LVCSR (2011) Proc. of ICASSP, , IEEE; El-Desoky, A., Schlüter, R., Ney, H., A hybrid morphologically decomposed factored language model for Arabic LVCSR (2010) Proc. of HLT-NAACL, , ACL; Adel, H., Kirchhoff, K., Vu, N.T., Telaar, D., Schultz, T., Comparing approaches to convert recurrent neural networks into backoff language models for efficient decoding (2014) Proc. of Inter Speech; Toutanova, K., Klein, D., Manning, C.D., Singer, Y., Feature-rich part-of-speech tagging with a cyclic dependency network (2003) Proc. HLT-NAACL, , ACL; Tseng, H., Chang, P., Andrew, G., Jurafsky, D., Manning, C.D., A conditional random field word segmenter (2005) SIGHAN Workshop on Chinese Language Processing; Brown, P.F., Desouza, P.V., Mercer, R.L., Pietra, V.J.D., Lai, J.C., Class-based n-gram models of natural language (1992) Computational Linguistics, 18 (4), pp. 467-479; Fromkin, V., An introduction to language (2013) Cengage Learning; Dhillon, I.S., Guan, Y., Kulis, B., Weighted graph cuts without eigenvectors: A multilevel approach (2007) IEEE Transactions on Pattern Analysis and Machine Intelligence, 29 (11), pp. 1944-1957; Von Luxburg, U., A tutorial on spectral clustering (2007) Statistics and Computing, 17 (4), pp. 395-416; Lyu, D.C., Tan, T.P., Chng, E.S., Li, H., (2010) An Analysis of a Mandarin-English Code-switching Speech Corpus: SEAME, , Oriental COCOSDA; Stolcke, A., SRILM - An extensible language modeling toolkit (2002) Proc. of SLP; Telaar, D., Wand, M., Gehrig, D., Putze, F., Amma, C., Heger, D., Vu, N.T., Schultz, T., BioKIT - Real-time decoder for biosignal processing (2014) Proc. of Inter Speech; Vu, N.T., Lyu, D.C., Weiner, J., Telaar, D., Schlippe, T., Blaicher, F., Chng, E.S., Li, H., A first speech recognition system for Mandarin- English code-switch conversational speech (2012) Proc. of ICASSP, , IEEE},
correspondence_address1={Adel, H.; Cognitive Systems Lab, Karlsruhe Institute of TechnologyGermany},
editor={Chng E.S., Li H., Meng H., Ma B., Xie L.},
sponsors={Amazon; Baidu; et al.; Google; Temasek Laboratories at Nanyang Technological University (TL at NTU); WeChat},
publisher={International Speech and Communication Association},
issn={2308457X},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vu201411,
author={Vu, N.T. and Wang, Y. and Klose, M. and Mihaylova, Z. and Schultz, T.},
title={Improving ASR performance on non-native speech using multilingual and crosslingual information},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2014},
pages={11-15},
note={cited By 5; Conference of 15th Annual Conference of the International Speech Communication Association: Celebrating the Diversity of Spoken Languages, INTERSPEECH 2014 ; Conference Date: 14 September 2014 Through 18 September 2014;  Conference Code:108771},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910047207&partnerID=40&md5=b54c3b77e10e572de74626b11b22c251},
affiliation={Karlsruhe Institute of Technology, Germany},
abstract={This paper presents our latest investigation of automatic speech recognition (ASR) on non-native speech. We first report on a non-native speech corpus - An extension of the GlobalPhone database - which contains English with Bulgarian, Chinese, German and Indian accent and German with Chinese accent. In this case, English is the spoken language (L2) and Bulgarian, Chinese, German and Indian are the mother tongues (L1) of the speakers. Afterwards, we investigate the effect of multilingual acoustic modeling on non-native speech. Our results reveal that a bilingual L1-L2 acoustic model significantly improves the ASR performance on non-native speech. For the case that L1 is unknown or L1 data is not available, a multilingual ASR system trained without L1 speech data consistently outperforms the monolingual L2 ASR system. Finally, we propose a method called crosslingual accent adaptation, which allows using English with Chinese accent to improve the German ASR on German with Chinese accent and vice versa. Without using any intra lingual adaptation data, we achieve 15.8% relative improvement in average over the baseline system. Copyright © 2014 ISCA.},
author_keywords={Multilingual ASR;  Non-native speech},
keywords={Speech;  Speech communication, Automatic speech recognition;  Baseline systems;  Cross-lingual information;  Mother tongues;  Multilingual acoustic models;  Multilingual ASR;  Non-native speech;  Spoken languages, Speech recognition},
references={Schultz, T., Vu, N.T., Schlippe, T., GlobalPhone: A multilingual text & speech database in 20 languages (2013) Proc. ICASSP, , Canada; Livescu, K., (1999) Analysis and Modeling of Non-native Speech for Automatic Speech Recognition, , Masters thesis, MIT; Tomokiyo, L.M., Waibel, A., Adaptation methods for nonnative speech (2000) Multilinguality in Spoken Language Processing, 2001, , SR: A preliminary study. In InSTIL; Wang, Z., Schultz, T., Waibel, A., Comparison of acoustic model adaptation techniques on non-native speech (2003) Proc. Interspeech; Raab, M., Gruhn, R., Noth, E., Multilingual weighted codebooks for non-native speech recognition (2008) Proc. TSD, pp. 485-492; Tan, T.P., Besacier, L., Acoustic model interpolation for non-native speech recognition (2007) Proc. ICASSP; Bouselmi, G., Fohr, D., Illina, I., Haton, J.P., Multilingual Non-native speech recognition using phonetic confusion-based acoustic model modification and graphemic constraints (2006) Proc. of ICSLP; Mihaylova, Z., (2011) Lexical and Acoustic Adaptation for Multiple Non-Native English Accents, , Diplomarbeit in Karlsruhe Institute of Technology (KIT; Vu, N.T., Metze, F., Schultz, T., Multilingual bottle-neck feature and its application on new languages (2012) Proc. of SLTU; Vu, N.T., Breiter, W., Metze, F., Schultz, T., An investigation on initialization schemes for multilayer perceptron training using multilingual data and their effect on asr performance (2012) Proc. of Interspeech; Schultz, T., Waibel, A., Language independent and language adaptive acoustic modeling for speech recognition (2001) Speech Communication August, 35 (1-2), pp. 31-51; Vu, N.T., Schlippe, T., Kraus, F., Schultz, T., Rapid bootstrapping of five eastern european languages using the rapid language adaptation toolkit (2010) Proc. Interspeech, , Japan; Flege, J.E., Phonetic approximation in second language acquisition (1980) Language Learning, 30, pp. 117-134; Flege, J.E., The production of new and similar phones in a foreign language: Evidence for the effect of Equivalence Classification (1987) Journal of Phonetics, 15, pp. 47-65; Gonzlez-Bueno, M., The effects of formal instruction on the acquisition of Spanish stop consonants (1997) Contemporary Perspectives on the Acquisition of Spanish. Volume 2: Production, Processing, and Comprehension, pp. 57-75; Flege, J.E., Frieda, E.M., Nozawa, T., Amount of nativelanguage (L1) use affects the pronunciation of an L2 (1997) Journal of Phonetics, 25, pp. 169-186; Kullback, S., Letter to the Editor: The KullbackLeibler distance (1987) The American Statistician, 41 (4), p. 340341},
correspondence_address1={Vu, N.T.; Karlsruhe Institute of TechnologyGermany},
editor={Chng E.S., Li H., Meng H., Ma B., Xie L.},
sponsors={Amazon; Baidu; et al.; Google; Temasek Laboratories at Nanyang Technological University (TL at NTU); WeChat},
publisher={International Speech and Communication Association},
issn={2308457X},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Janke20142579,
author={Janke, M. and Wand, M. and Heistermann, T. and Schultz, T. and Prahallad, K.},
title={Fundamental frequency generation for whisper-to-audible speech conversion},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2014},
pages={2579-2583},
doi={10.1109/ICASSP.2014.6854066},
art_number={6854066},
note={cited By 16; Conference of 2014 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2014 ; Conference Date: 4 May 2014 Through 9 May 2014;  Conference Code:106632},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905233520&doi=10.1109%2fICASSP.2014.6854066&partnerID=40&md5=fa7c980ffed0de696a9ef6a440210960},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology, Germany; International Institute of Information Technology, Hyderabad, India},
abstract={In this work, we address the issues involved in whisper-to-audible speech conversion. Spectral mapping techniques using Gaussian mixture models or Artificial Neural Networks borrowed from voice conversion have been applied to transform whisper spectral features to normally phonated audible speech. However, the modeling and generation of fundamental frequency (F0) and its contour in the converted speech is a major issue. Whispered speech does not contain explicit voicing characteristics and hence it is hard to derive a suitable F0, making it difficult to generate a natural prosody after conversion. Our work addresses the F0 modeling in whisper-to-speech conversion. We show that F0 contours can be derived from the mapped spectral vectors, which can be used for the synthesis of a speech signal. We also present a hybrid unit selection approach for whisper-to-speech conversion. Unit selection is performed on the spectral vectors, where F0 and its contour can be obtained as a byproduct without any additional modeling. © 2014 IEEE.},
author_keywords={F0 generation;  Silent speech interface;  voice conversion;  whisper-to-speech conversion},
keywords={Natural frequencies;  Neural networks;  Photomapping;  Speech communication;  Speech processing, F0 generation;  Fundamental frequencies;  Gaussian Mixture Model;  Silent speech interfaces;  Spectral mappings;  Speech conversion;  Unit selection approach;  Voice conversion, Audio signal processing},
references={Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J.M., Brumberg, J.S., Silent speech interfaces (2010) Speech Communication, 52 (4), pp. 270-287; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 331-336; Denby, B., Stone, M., Speech synthesis from real time ultrasound images of the tongue (2004) IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP, pp. I685-I688; Fagan, M.J., Ell, S.R., Gilbert, J.M., Sarrazin, E., Chapman, P.M., Development of a (silent) speech recognition system for patients following laryngectomy (2008) Medical Engineering and Physics, 30 (4), pp. 419-425; Nakajima, Y., Kashioka, H., Shikano, K., Campbell, N., Non-audible murmur recognition input interface using stethoscopic microphone attached to the skin (2003) IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP, pp. 708-711; Tran, V.A., Bailly, G., Loevenbruck, H., Toda, T., Predicting f0 and voicing from nam-captured whispered speech (2008) Proc. Speech Prosody, pp. 133-136; Sharifzadeh, H.R., McLoughlin, I.V., Ahamdi, F., Voiced speech from whispers for post-laryngectomised patients (2009) IAENG International Journal of Computer Science, 36 (4), pp. 367-377; Passos, A.P., A lightweight processing for conversion of whispering voice into normal speech (2010) International Conferenceon Audio Language and Image Processing (ICALIP, pp. 74-79; Toda, T., Shikano, K., NAM-to-speech conversion with gaussian mixture models (2005) 9th European Conference on Speech Communication and Technology (Interspeech, pp. 1957-1960; Meyer-Eppler, W., Realization of prosodic features in whispered speech (1957) The Journal of the Acoustical Society of America, 29, pp. 104-106; Itoh, T., Takeda, K., Itakura, F., Acoustic analysis and recognition of whispered speech (2002) IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP, pp. 389-293; Parlikar, A., TestVox: Web-based framework for subjective evaluation of speech synthesis (2012) Opensource Software, , https://bitbucket.org/happyalu/testvox; Fukada, T., Tokuda, K., Kobayashi, T., Imai, S., An adaptive algorithm for mel-cepstral analysis of speech (1992) IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP, pp. 137-140; Desai, S., Raghavendra, E.V., Yegnanarayana, B., Black, A.W., Prahallad, K., Voice conversion using artificial neural networks (2009) IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP, pp. 3893-3896; Greenberg, S., Carvey, H., Hitchcock, L., Chang, S., Temporal properties of spontaneous speech-a syllable-centric perspective (2003) Journal of Phonetics, 31 (3), pp. 465-485; Imai, S., Cepstral analysis synthesis on the mel frequency scale (1983) IEEE International Conference on Acoustics, Speech, and Signal Processing, 8, pp. 93-96; Black, A.W., Bennett, C.L., Blanchard, B.C., Kominek, J., Langner, B., Prahallad, K., Toth, A., CMU blizzard 2007: A hybrid acoustic unit selection system from statistically predicted parameters (2007) Blizzard Challenge Workshop, , Bonn, Germany; Verhelst, W., Overlap-add methods for time-scaling of speech (2000) Speech Communication, 30 (4), pp. 207-221; Toda, T., Black, A.W., Tokuda, K., Mapping from articulatory movements to vocal tract spectrum with gaussian mixture model for articulatory speech synthesis (2004) Fifth ISCA Workshop on Speech Synthesis, pp. 31-36},
sponsors={},
publisher={Institute of Electrical and Electronics Engineers Inc.},
address={Florence},
issn={15206149},
isbn={9781479928927},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wand2014300,
author={Wand, M. and Janke, M. and Heistermann, T. and Schulte, C. and Himmelsbach, A. and Schultz, T.},
title={Application of electrode arrays for artifact removal in an electromyographic silent speech interface},
journal={Communications in Computer and Information Science},
year={2014},
volume={452},
pages={300-312},
doi={10.1007/978-3-662-44485-6_21},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922024181&doi=10.1007%2f978-3-662-44485-6_21&partnerID=40&md5=25b7cf9b8ae70e296a49e5c525803504},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology, Adenauerring 4, Karlsruhe, 76137, Germany},
abstract={An electromygraphic (EMG) Silent Speech Interface is a system which recognizes speech by capturing the electric potentials of the human articulatory muscles, thus enabling the user to communicate silently. This study deals with the introduction of multi-channel electrode arrays to the EMG recording system, which requires meticulous dealing with the resulting high-dimensional data. As a first application of the technology, Independent Component Analysis (ICA) is applied for automated artifact detection and removal. Without the artifact removal component, the system achieves optimal average Word Error Rates of 40.1#x0025; for 40 training sentences and 10.9#x0025; for 160 training sentences on EMG signals of audible speech. On a subset of the corpus, we evaluate the ICA artifact removal method, improving the Word Error Rate by 10.7#x0025; relative. © Springer-Verlag Berlin Heidelberg 2014.},
author_keywords={Electrode array;  Electromyography;  EMG;  Emg-based speech recognition;  Silent speech interface},
keywords={Biomedical engineering;  Clustering algorithms;  Electric potential;  Electrodes;  Electromyography;  Independent component analysis;  Speech, Artifact detection;  Electrode arrays;  Electromyographic;  EMG;  High dimensional data;  Independent component analysis(ICA);  Multichannel electrodes;  Silent speech interfaces, Speech recognition},
references={Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Silent speech interfaces (2010) Speech Commun, 52 (4), pp. 270-287; Schultz, T., Wand, M., Modeling coarticulation in large vocabulary EMG-based speech recognition (2010) Speech Commun, 52 (4), pp. 341-353; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) Proceedings of IEEE ASRU, , San Juan, Puerto Rico; Freitas, J., Teixeira, A., Dias, M.S., Towards a silent speech interface for Portuguese (2012) Proceedings of Biosignals, pp. 91-100; Jorgensen, C., Dusan, S., Speech interfaces based upon surface electromyography (2010) Speech Commun, 52 (4), pp. 354-366; Lopez-Larraz, E., Mozos, O.M., Antelis, J.M., Minguez, J., Syllable-based speech recognition using EMG (2010) Proceedings of IEEE EMBS; Winter, B.B., Webster, J.G., Driven-right-leg circuit design (1983) IEEE Trans. Biomed. Eng. BME–30, pp. 62-66; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proceedings of Interspeech, Pittsburgh, PA; Wand, M., Schultz, T., Speaker-adaptive speech recognition based on surface electromyography (2010) BIOSTEC 2009. CCIS, pp. 271-285. , In: Fred, A., Filipe, J., Gamboa, H. (eds.), Springer, Heidelberg; Ueda, N., Nakano, R., Ghahramani, Z., Hinton, G.E., Split and merge EM algorithm for improving Gaussian mixture density estimates (2000) J. VLSI Sig. Proc, 26, pp. 133-140; Qiao, Z., Zhou, L., Huang, J.Z., Sparse linear discriminant analysis with applications to high dimensional low sample size data (2009) Int. J. Appl. Math, 39, pp. 48-60; Belhumeur, P.N., Hespanha, J.P., Kriegman, D.J., Eigenfaces vs fisherface: Recognition using class-specific linear projection (1997) IEEE Trans. Pattern Anal. Mach. Intell, 19, pp. 711-720; Hyvrinen, A., Oja, E., (2000) Independent Component Analysis: Algorithms and Applications, 13, pp. 411-430; Bell, A.J., Sejnowski, T.I., An Information-Maximization Approach to Blind Separation and Blind Deconvolution (1995) Neural Comput, 7, pp. 1129-1159; Makeig, S., (2000), www.sccn.ucsd.edu/eeglab/, EEGLAB: ICA Toolbox for Psychophysiological Research. WWW Site, Swartz Center for Computational Neuroscience, Institute of Neural Computation, University of San Diego California, (World Wide Web Publication); Wand, M., Himmelsbach, A., Heistermann, T., Janke, M., Schultz, T., Artifact removal algorithm for an EMG-based silent speech interface (2013) Proceedings of IEEE EMBC; Zhao, H., Xu, G., The research on surface electromyography signal effective feature extraction (2011) Proceedings of 6Th International Forum on Strategic Technology; Wand, M., Schulte, C., Janke, M., Schultz, T., Array-based electromyographic silent speech interface (2013) Proceedings of Biosignals},
correspondence_address1={Wand, M.; Cognitive Systems Lab, Karlsruhe Institute of Technology, Adenauerring 4, Germany; email: michael.wand@kit.edu},
editor={Fred A., Gamboa H., Fernandes P.L., Sole-Casals J., Fernandez-Chimeno M., Alvarez S., Stacey D.},
publisher={Springer Verlag},
issn={18650929},
isbn={9783662444849},
language={English},
abbrev_source_title={Commun. Comput. Info. Sci.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Hild20143,
author={Hild, J. and Putze, F. and Kaufman, D. and Kühnle, C. and Schultz, T. and Beyerer, J.},
title={Spatio-temporal event selection in basic surveillance tasks using eye tracking and EEG},
journal={GazeIn 2014 - Proceedings of the 7th ACM Workshop on Eye Gaze in Intelligent Human Machine Interaction: Eye-Gaze and Multimodality, Co-located with ICMI 2014},
year={2014},
pages={3-8},
doi={10.1145/2666642.2666645},
note={cited By 2; Conference of 7th Workshop on Eye Gaze in Intelligent Human Machine Interaction, GazeIn 2014 ; Conference Date: 16 November 2014 Through 16 November 2014;  Conference Code:109471},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919395097&doi=10.1145%2f2666642.2666645&partnerID=40&md5=f0acb44b5ac0578ff67bf54469d8346a},
affiliation={Fraunhofer Institute for Optronics, System Technologies and Image Exploitation (IOSB), Karlsruhe, 76131, Germany; Cognitive Systems Lab Karlsruhe Institute of Technology, Karlsruhe, 76131, Germany; Vision and Fusion Laboratory Karlsruhe Institute of Technology, Karlsruhe, 76131, Germany},
abstract={In safety- and security-critical applications like video surveillance it is crucial that human operators detect task-relevant events in the continuous video streams and select them for report or dissemination to other authorities. Usually, the selection operation is performed using a manual input device like a mouse or a joystick. Due to the visually rich and dynamic input, the required high attention, the long working time, and the challenging manual selection of moving objects, it occurs that relevant events are missed. To alleviate this problem we propose adding another event selection process, using eye-brain input. Our approach is based on eye tracking and EEG, providing spatio-temporal event selection without any manual intervention. We report ongoing research, building on prior work where we showed the general feasibility of the approach. In this contribution, we extend our work testing the feasibility of the approach using more advanced and less artificial experimental paradigms simulating frequently occurring, basic types of real surveillance tasks. The paradigms are much closer to a real surveillance task in terms of the used visual stimuli, the more subtle cues for event indication, and the required viewing behavior. As a methodology we perform an experiment (N=10) with non-experts. The results confirm the feasibility of the approach for event selection in the advanced tasks. We achieve spatio-temporal event selection accuracy scores of up to 77% and 60% for different stages of event indication. © 2014 ACM.},
author_keywords={EEG;  Event localization;  Event selection;  Experiment;  Eye tracking;  Video surveillance task},
keywords={Electroencephalography;  Experiments;  Monitoring;  Video streaming, Event localizations;  Event selection;  Eye-tracking;  Manual intervention;  Security-critical;  Spatio-temporal events;  Surveillance task;  Video surveillance, Security systems},
references={Chang, C.-C., Lin, C.-J., (2011) LIBSVM: A Library for Support Vector Machines, 2-3, pp. 271-2727. , ACM Transactions on Intelligent Systems and Technology; Fischer, Y., Krempel, E., Birnstill, P., Unmüßig, G., Monari, E., Moßgraber, J., Schenk, M., Beyerer, J., Privacy-aware smart video surveillance revisited (2014) Proceedings of the 9th Security Research Conference (Future Security), , (accepted for publication); Heinze, N., Esswein, M., Krüger, W., Saur, G., Image exploitation algorithms for reconnaissance and surveillance with UAV (2010) Proceedings of SPIE Defense, Security, and Sensing, International Society for Optics and Photonics, pp. 76680U-76680U. , 2010; Hild, J., Brüstle, S., Heinze, N., Peinsipp-Byma, E., Gaze interaction in UAS video exploitation (2013) Proceedings of SPIE Defense, Security, and Sensing, International Society for Optics and Photonics, pp. 87400H-87400H. , 2013; Hild, J., Müller, E., Klaus, E., Peinsipp-Byma, E., Beyerer, J., Evaluating multi-modal eye gaze interaction for moving object selection (2013) Proceedings of the Sixth International Conference on Advances in Computer-Human Interactions, pp. 454-459. , ACHI 2013. IARIA; IEEE, Huang, B., Lo, A., Integrating EEG information improves performance of gaze based cursor control (2013) Proceedings of Neural Engineering., pp. 415-418; Irwin, D., Visual memory within and across fixations (1992) Eye Movements and Visual Cognition, pp. 146-165. , Springer; Jacob, R.J.K., Karn, K.S., Eye tracking in humancomputer interaction and usability research: Ready to deliver the promises (2003) Mind, 2 (3), pp. 573-605; Just, M.A., Carpenter, P.A., Eye fixations and cognitive processes (1976) Cognitive Psychology, 8 (4), pp. 441-480; Krusienski, D.J., Sellers, E.W., McFarland, D.J., Vaughan, T.M., Wolpaw, J.R., Toward enhanced p300 speller performance (2008) Journal of Neuroscience Methods, 167 (1), pp. 15-21; ACM, Putze, F., Hild, J., Kärgel, R., Herff Ch. Redmann, A., Beyerer, J., Schulz, T., Locating user attention using eye tracking and eeg for spatio-temporal event selection (2013) Proceedings of the 2013 International Conference on Intelligent User Interfaces, pp. 129-136. , IUI 2013; Schlögl, A., Keinrath, C., Zimmermann, D., Scherer, R., Leeb, R., Pfurtscheller, G.A., Fully automated correction method of eog artifacts in EEG recordings (2007) Clinical Neurophysiology, 118 (1), pp. 98-204. , Jan; ACM, Salvucci, D.D., Goldberg, J.H., Identifying fixations and saccades in eye-tracking protocols (2000) Proceedings of the 2000 Symposium on Eye Tracking Research & Applications, pp. 71-78. , ETRA 2000; APS, Von Bünau, P., Meinecke, F., Kiraly, F., Müller, K.-R., Finding stationary subspaces in multivariate time series (2009) Physical Review Letters, 103, p. 21; Teichner, W.H., The detection of a simple visual signal as a function of time on watch (1974) Human Factors, 16 (4), pp. 339-352. , Sage Publications; ACM, Ware, C., Mikaelian, H., An evaluation of an eye tracker as a device for computer input (1987) ACM SIGCHI Bulletin, 17, pp. 183-188. , SI; Yong, X., Fatourechi, M., Ward, R.K., Birch, G.E., The Design of a Point-and-Click System by Integrating a Self- Paced Brain Computer Interface with an Eye-Tracker (2011) IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 1 (4), pp. 590-602. , December 2011; Zander, T.O., Gaertner, M., Kothe, C., Vilimek, R., Combining eye gaze input with a brain-computer interface for touchless human-computer interaction (2010) International Journal of Human-Computer Interaction, 27 (1), pp. 38-51. , 2010},
sponsors={ACM SIGCHI},
publisher={Association for Computing Machinery, Inc},
isbn={9781450301251},
language={English},
abbrev_source_title={GazeIn - Proc. ACM Workshop Eye Gaze in Intell. Hum. Mach. Interact.: Eye-Gaze Multimodality, Co-located with ICMI},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wand2014300,
author={Wand, M. and Janke, M. and Heistermann, T. and Schulte, C. and Himmelsbach, A. and Schultz, T.},
title={Application of electrode arrays for artifact removal in an electromyographic silent speech interface},
journal={Communications in Computer and Information Science},
year={2014},
volume={452},
pages={300-312},
doi={10.1007/978-3-662-44485-6f21},
note={cited By 0; Conference of 6th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2013 ; Conference Date: 11 February 2013 Through 14 February 2013;  Conference Code:111339},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916204559&doi=10.1007%2f978-3-662-44485-6f21&partnerID=40&md5=76029d549474f9b9e38eb6391d5b4e94},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology, Adenauerring 4, Karlsruhe, 76137, Germany},
abstract={An electromygraphic (EMG) Silent Speech Interface is a system which recognizes speech by capturing the electric potentials of the human articulatory muscles, thus enabling the user to communicate silently. This study deals with the introduction of multi-channel electrode arrays to the EMG recording system, which requires meticulous dealing with the resulting high-dimensional data. As a first application of the technology, Independent Component Analysis (ICA) is applied for automated artifact detection and removal. Without the artifact removal component, the system achieves optimal average Word Error Rates of 40.1% for 40 training sentences and 10.9% for 160 training sentences on EMG signals of audible speech. On a subset of the corpus, we evaluate the ICA artifact removal method, improving the Word Error Rate by 10.7% relative. ©Springer-Verlag Berlin Heidelberg 2014.},
author_keywords={Electrode Array;  Electromyography;  Emg;  Emg-Based Speech Recognition;  Silent Speech Interface},
keywords={Biomedical engineering;  Clustering algorithms;  Electric potential;  Electrodes;  Electromyography;  Independent component analysis;  Speech, Artifact detection;  Channel electrodes;  Electrode arrays;  Electromyographic;  Emg;  High dimensional data;  Independent component analysis(ICA);  Silent speech interfaces, Speech recognition},
references={Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Silent speech interfaces (2010) Speech Commun, 52 (4), pp. 270-287; Schultz, T., Wand, M., Modeling coarticulation in large vocabulary EMG-based speech recognition (2010) Speech Commun, 52 (4), pp. 341-353; Maier-Hein, L., Metze, F., Schultz, T.,Waibel, A.: Session independent non-audible speech recognition using surface electromyography (2005) Proceedings of IEEE ASRU, San Juan, Puerto Rico; Freitas, J., Teixeira, A., Dias, M.S., Towards a silent speech interface for Portuguese (2012) Proceedings of Biosignals, pp. 91-100; Jorgensen, C., Dusan, S., Speech interfaces based upon surface electromyography (2010) Speech Commun, 52 (4), pp. 354-366; Lopez-Larraz, E., Mozos, O.M., Antelis, J.M., Minguez, J., Syllable-based speech recognition using EMG (2010) Proceedings of IEEE EMBS; Winter, B.B., Webster, J.G., Driven-right-leg circuit design (1983) IEEE Trans. Biomed. Eng. BME–30, pp. 62-66; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proceedings of Interspeech, , Pittsburgh, PA; Wand, M., Schultz, T., Speaker-adaptive speech recognition based on surface electromyography (2010) BIOSTEC 2009. CCIS, 52, pp. 271-285. , In: Fred, A., Filipe, J., Gamboa, H. (eds.), Springer, Heidelberg; Ueda, N., Nakano, R., Ghahramani, Z., Hinton, G.E., Split and merge EM algorithm for improving Gaussian mixture density estimates (2000) J. VLSI Sig. Proc, 26, pp. 133-140; Qiao, Z., Zhou, L., Huang, J.Z., Sparse linear discriminant analysis with applications to high dimensional low sample size data (2009) Int. J. Appl. Math, 39, pp. 48-60; Belhumeur, P.N., Hespanha, J.P., Kriegman, D.J., Eigenfaces vs fisherface: recognition using class-specific linear projection (1997) IEEE Trans. Pattern Anal. Mach. Intell, 19, pp. 711-720; Hyvrinen, A., Oja, E., Independent component analysis: Algorithms and applications (2000) neural networks, 13, pp. 411-430; Bell, A.J., Sejnowski, T.I., An Information-Maximization Approach to Blind Separation and Blind Deconvolution (1995) Neural Comput, 7, pp. 1129-1159; Makeig, S., (2000) EEGLAB: ICA Toolbox for Psychophysiological Research, , www.sccn.ucsd.edu/eeglab/, WWW Site, Swartz Center for Computational Neuroscience, Institute of Neural Computation, University of San Diego California, World Wide Web Publication; Wand, M., Himmelsbach, A., Heistermann, T., Janke, M., Schultz, T., Artifact removal algorithm for an EMG-based silent speech interface (2013) Proceedings of IEEE EMBC; Zhao, H., Xu, G., The research on surface electromyography signal effective feature extraction (2011) Proceedings of 6th International Forum on Strategic Technology; Wand, M., Schulte, C., Janke, M., Schultz, T., Array-based electromyographic silent speech interface (2013) Proceedings of Biosignals},
correspondence_address1={Wand, M.; Cognitive Systems Lab, Karlsruhe Institute of Technology, Adenauerring 4, Germany},
editor={Fred A., Gamboa H., Fernandes P.L., Sole-Casals J., Fernandez-Chimeno M., Alvarez S., Stacey D.},
sponsors={Institute for Systems and Technologies of Information, Control and Communication, INSTICC; University of Vic},
publisher={Springer Verlag},
issn={18650929},
isbn={9783662444849},
language={English},
abbrev_source_title={Commun. Comput. Info. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vu20147639,
author={Vu, N.T. and Imseng, D. and Povey, D. and Motlicek, P. and Schultz, T. and Bourlard, H.},
title={Multilingual deep neural network based acoustic modeling for rapid language adaptation},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2014},
pages={7639-7643},
doi={10.1109/ICASSP.2014.6855086},
art_number={6855086},
note={cited By 73; Conference of 2014 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2014 ; Conference Date: 4 May 2014 Through 9 May 2014;  Conference Code:106632},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905223329&doi=10.1109%2fICASSP.2014.6855086&partnerID=40&md5=617ab9c496388629fdba637b3b1caa22},
affiliation={Karlsruhe Institute of Technology, Karlsruhe, Germany; Idiap Research Institute, Martigny, Switzerland; Johns Hopkins University, Baltimore, United States},
abstract={This paper presents a study on multilingual deep neural network (DNN) based acoustic modeling and its application to new languages. We investigate the effect of phone merging on multilingual DNN in context of rapid language adaptation. Moreover, the combination of multilingual DNNs with Kullback-Leibler divergence based acoustic modeling (KL-HMM) is explored. Using ten different languages from the Globalphone database, our studies reveal that crosslingual acoustic model transfer through multilingual DNNs is superior to unsupervised RBM pre-training and greedy layer-wise supervised training. We also found that KL-HMM based decoding consistently outperforms conventional hybrid decoding, especially in low-resource scenarios. Furthermore, the experiments indicate that multilingual DNN training equally benefits from simple phoneset concatenation and manually derived universal phonesets. © 2014 IEEE.},
author_keywords={KL-HMM;  Multilingual DNN;  phone merging;  rapid language adaptation},
keywords={Decoding;  Merging;  Telephone sets, Acoustic model;  Deep neural networks;  ITS applications;  KL-HMM;  Kullback Leibler divergence;  Multilingual DNN;  rapid language adaptation;  Supervised trainings, Signal processing},
references={Seide, F., Li, G., Yu, D., Conversational speech transcription using context-dependent deep neural networks (2011) Proc. of Interspeech, pp. 437-440; Dahl, G., Yu, D., Deng, L., Acero, A., Contextdependent pre-trained deep neural networks for largevocabulary speech recognition (2012) Audio, Speech, and Language Processing, IEEE Transactions on, 20 (1), pp. 30-42; Mohamed, A., Dahl, G., Hinton, G., Acoustic modeling using deep belief networks (2012) Audio, Speech, and Language Processing, IEEE Transactions on, 20 (1), pp. 14-22; Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A., Jaitly, N., Senior, A., Kingsbury, B., Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups (2012) IEEE Signal Processing Magazine, 29 (6), pp. 82-97; Swietojanski, P., Ghoshal, A., Renals, S., Unsupervised cross-lingual knowledge transfer in DNN-based LVCSR (2012) Proc. of the IEEE Workshop on Spoken Language Technology (SLT), pp. 246-251; Huang, J.T., Li, J., Yu, D., Deng, L., Gong, Y., Crosslanguage knowledge transfer using multilingual deep neural network with shared hidden layers (2013) Proc. of ICASSP; Heigold, G., Vanhoucke, V., Senior, A., Nguyen, P., Ranzato, M., Devin, M., Dean, J., Multilingual acoustic models using distributed deep neural networks (2013) Proc. of ICASSP; Ghoshal, A., Swietojanski, P., Renals, S., Multilingual training of deep neural networks (2013) Proc. of ICASSP; Thomas, S., Ganapathy, S., Hermansky, H., Crosslingual and multi-stream posterior features for low resource lvcsr systems (2010) Prof. of Interspeech, pp. 877-880; Vu, N.T., Metze, F., Schultz, T., Multilingual bottleneck features and its application for under-resourced languages (2012) Proc. of SLTU, 12; Vesely, K., Karafiát, M., Grezl, F., Janda, M., Egorova, E., The language-independent bottleneck features (2012) Proc. of SLT, pp. 336-341; Tuske, Z., Pinto, J., Willett, D., Schluter, R., Investigation on cross-and multilingual mlp features under matched and mismatched acoustical conditions (2013) Proc. of ICASSP, pp. 7349-7353; Schultz, T., Waibel, A., Language-independent and language-adaptive acoustic modeling for speech recognition (2001) Speech Communication, 35 (1), pp. 31-51; Imseng, D., Motlicek, P., Garner, P.N., Bourlard, H., Impact of deep MLP architecture on different acoustic modeling techniques for under-resourced speech recognition (2013) Proc. of ASRU; Xiaohui, Z., Jan, T., Povey, D., Sanjeev, K., Improving deep neural network acoustic models using generalized maxout networks (2014) Proc. of ICASSP; Povey, D., Ghoshal, A., Boulianne, G., Burget, L., Glembek, O., Goel, N., Hannemann, M., Schwarz, P., The kaldi speech recognition toolkit (2011) Proc. of ASRU; Morgan, N., Bourlard, H., Continuous speech recognition: An introduction to the hybrid HMM/connectionist approach (1995) IEEE Signal Processing Magazine, 12 (3), pp. 24-42. , May; Vesely, K., Ghoshal, A., Burget, L., Povey, D., Sequence-discriminative training of deep neural networks (2013) Proc. of Interspeech; Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent, P., Bengio, S., Why does unsupervised pre-training help deep learning (2010) Journal of Machine Learning Research, pp. 625-660; Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H., Greedy layer-wise training of deep networks (2007) Advances in Neural Information Processing Systems, 19, p. 153; Niu, F., Recht, B., Ré, C., Wright, S.J., (2011) Hogwild: A Lock-free Approach to Parallelizing Stochastic Gradient Descent, , arXiv preprint arXiv:1106. 5730; Imseng, D., Motlicek, P., Bourlard, H., Garner, P.N., Using out-of-language data to improve an underresourced speech recognizer (2013) Speech Communication; Schultz, T., Vu, N.T., Schlippe, T., Globalphone: A multilingual text &speech database in 20 languages (2013) Proc. of ICASSP; (2013) Benchmark Globalphone Language Models, , http://csl.ira.uka.de/GlobalPhone, Retrieved November 3rd},
sponsors={},
publisher={Institute of Electrical and Electronics Engineers Inc.},
address={Florence},
issn={15206149},
isbn={9781479928927},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Putze2014,
author={Putze, F. and Hesslinger, S. and Tse, C.-Y. and Huang, Y. and Herff, C. and Guan, C. and Schultz, T.},
title={Hybrid fNIRS-EEG based classification of auditory and visual perception processes},
journal={Frontiers in Neuroscience},
year={2014},
volume={8},
number={OCT},
doi={10.3389/fnins.2014.00373},
art_number={Article 373},
note={cited By 54},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910077347&doi=10.3389%2ffnins.2014.00373&partnerID=40&md5=318b22965f53e3b33c5709bd18d90d64},
affiliation={Cognitive Systems Lab, Institute of Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Germany; Center for Cognition and Brain Studies, Department of Psychology, The Chinese University of Hong Kong, Shatin, Hong Kong; Temasek Laboratories, National University of Singapore, Kent Ridge, Singapore; Nuffield Department of Clinical Neurosciences, John Radcliffe Hospital, Oxford, United Kingdom; Institute for Infocomm Research (I2R), A STAR, Singapore},
abstract={For multimodal Human-Computer Interaction (HCI), it is very useful to identify the modalities on which the user is currently processing information. This would enable a system to select complementary output modalities to reduce the user's workload. In this paper, we develop a hybrid Brain-Computer Interface (BCI) which uses Electroencephalography (EEG) and functional Near Infrared Spectroscopy (fNIRS) to discriminate and detect visual and auditory stimulus processing. We describe the experimental setup we used for collection of our data corpus with 12 subjects. On this data, we performed cross-validation evaluation, of which we report accuracy for different classification conditions. The results show that the subject-dependent systems achieved a classification accuracy of 97.8% for discriminating visual and auditory perception processes from each other and a classification accuracy of up to 94.8% for detecting modality specific processes independently of other cognitive activity. The same classification conditions could also be discriminated in a subject-independent fashion with accuracy of up to 94.6% and 86.7%, respectively. We also look at the contributions of the two signal types and show that the fusion of classifiers using different features significantly increases accuracy. © 2014 Putze, Hesslinger, Tse, Huang, Herff, Guan and Schultz.},
author_keywords={Brain-computer interface;  EEG;  FNIRS;  Visual and auditory perception},
keywords={adult;  Article;  auditory stimulation;  brain computer interface;  cerebral oximeter;  classification;  cognition;  electroencephalography;  female;  functional neuroimaging;  hearing;  human;  human experiment;  male;  near infrared spectroscopy;  normal human;  validation study;  vision;  visual stimulation},
tradenames={Imagent, ISS, United States},
funding_details={National University of SingaporeNational University of Singapore, NUS},
references={Turk, M., Multimodal interaction: A review (2014) Pattern Recognition Letters, 36, pp. 189-195; Wickens, C.D., Multiple resources and mental workload (2008) Human Factors: The Journal of the Human Factors and Ergonomics Society, 50, pp. 449-455; Yang, Y., Reimer, B., Mehler, B., Wong, A., McDonald, M., Exploring differences in the impact of auditory and visual demands on driver behavior (2012) Proceedings of the 4th International Conference on Auto motive User Interfaces and Interactive Vehicular Applications, p. 173177. , (New York, NY, USA: ACM), AutomotiveUI '12; Cao, Y., Theune, M., Nijholt, A., Modality effects on cognitive load and performance in high-load infor mation presentation (2009) Proceedings of the 14th International Conference on Intelligent User Interfaces, p. 335344. , (New York, NY, USA: ACM), IUI '09; Wolpaw, J.R., Wolpaw, E.W., (2012) Brain-Computer Interfaces: Principles and Practice, , (Oxford; New York: Oxford Univ Pr); Wolpaw, J.R., McFarland, D.J., Neat, G.W., Forneris, C.A., An eeg-based brain-computer interface for cursor control (1991) Electroencephalography and clinical neurophysiology, 78, pp. 252-259; Sitaram, R., Zhang, H., Guan, C., Thulasidas, M., Hoshi, Y., Ishikawa, A., Temporal classification of multichannel near-infrared spectroscopy signals of motor imagery for developing a brain-computer interface (2007) NeuroImage, 34, pp. 1416-1427; Sitaram, R., Zhang, H., Guan, C., Thulasidas, M., Hoshi, Y., Ishikawa, A., Temporal classification of multichannel near-infrared spectroscopy signals of motor imagery for developing a brain-computer interface (2007) NeuroImage, 34, pp. 1416-1427; Pfurtscheller, G., Allison, B.Z., Brunner, C., Bauernfeind, G., Solis-Escalante, T., Scherer, R., The hybrid BCI (2010) Frontiers in neuroscience, 4; Spüler, M., Bensch, M., Kleih, S., Rosenstiel, W., Bogdan, M., Kübler, A., Online use of error-related poten tials in healthy users and people with severe motor impairment increases performance of a p300-BCI (2012) Clinical neurophysiology: official journal of the International Federation of Clinical Neurophysiology, 123, pp. 1328-1337; Fazli, S., Mehnert, J., Steinbrink, J., Curio, G., Villringer, A., Müller, K.R., Enhanced performance by a hybrid NIRS-EEG brain computer interface (2012) Neuroimage, 59, pp. 519-529; Zander, T.O., Kothe, C., Towards passive brain-computer interfaces: applying brain-computer interface technology to human-machine systems in general (2011) Journal of Neural Engineering, 8, p. 025005; Heger, D., Putze, F., Schultz, T., Online workload recognition from eeg data during cognitive tests and human-machine interaction (2010) KI 2010: Advances in Artificial Intelligence, pp. 410-417. , Springer; Kothe, C.A., Makeig, S., Estimation of task workload from eeg data: new and current tools and perspe ctives (2011) Engineering in Medicine and Biology Society, EMBC, 2011 Annual International Conference of the IEEE (IEEE), pp. 6547-6551; Allison, B.Z., Polich, J., Workload assessment of computer gaming using a single-stimulus event-related potential paradigm (2008) Biological psychology, 77, pp. 277-283. , PMID: 18093717 PMCID: PMC2443059; Brouwer, A.M., Hogervorst, M.A., van Erp, J.B.F., Heffelaar, T., Zimmerman, P.H., Oostenveld, R., Estimating workload using EEG spectral power and ERPs in the n-back task (2012) Journal of neural engineering, 9, p. 045008. , PMID: 22832068; Dijksterhuis, C., Waard, Dd., Mulder, B.L.J.M., Classifying visuomotor workload in a driving simulator using subject specific spatial brain patterns (2013) Frontiers in Neuroprosthetics, 7, p. 149; Sassaroli, A., Zheng, F., Hirshfield, L.M., Girouard, A., Solovey, E.T., Jacob, R.J.K., Discrimination of mental workload levels in human subjecs with functional near-infrared spectroscopy (2008) Journal of Innovative Optical Health Sciences, 1, pp. 227-237; Bunce, S.C., Izzetoglu, K., Ayaz, H., Shewokis, P., Izzetoglu, M., Pourrezaei, K., Implementation of fNIRS for monitoring levels of expertise and mental workload (2011) Foundations of Augmented Cognition. Directing the Future of Adaptive Systems, pp. 13-22. , Schmorrow DD, Fidopiastis CM, editors, (Springer Berlin Heidelberg), no. 6780 in Lecture Notes in Computer Science; Herff, C., Heger, D., Fortmann, O., Hennrich, J., Putze, F., Schultz, T., Mental workload during n-back taskquantified in the prefrontal cortex using fNIRS (2014) Frontiers in Human Neuroscience, 7; Hirshfield, L.M., Chauncey, K., Gulotta, R., Girouard, A., Solovey, E.T., Jacob, R.J.K., Combining electroencephalograph and functional near infrared spectroscopy to explore users mental workload (2009) Foundations of Augmented Cognition. Neuroer gonomics and Operational Neuroscience, pp. 239-247. , Schmorrow DD, Estabrooke IV, Grootjen M, editors, (Springer Berlin Heidelberg), no. 5638 in Lecture Notes in Computer Science; Coffey, E.B.J., Brouwer, A.M., Erp J.B., Fv., Measuring workload using a combination of electroencephalo graphy and near infrared spectroscopy (2012) Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 56, pp. 1822-1826; Heger, D., Putze, F., Schultz, T., An EEG adaptive information system for an empathic robot (2011) International Journal of Social Robotics, 3, pp. 415-425; Keitel, C., Maess, B., Schröger, E., Müller, M.M., Early visual and auditory processing rely on modality specific attentional resources (2012) NeuroImage, 70, pp. 240-249; Wolf, M., Wolf, U., Choi, J.H., Toronov, V., Adelina Paunescu, L., Michalos, A., Fast cerebral functi onal signal in the 100-ms range detected in the visual cortex by frequency-domain near-infrared spectrophotometry (2003) Psychophysiology, 40, p. 521528; Joseph, D.K., Huppert, T.J., Franceschini, M.A., Boas, D.A., Diffuse optical tomography system to image brain activation with improved spatial resolution and validation with functional magnetic resonance imaging (2006) Applied optics, 45, pp. 8142-8151; Whalen, C., Maclin, E.L., Fabiani, M., Gratton, G., Validation of a method for coregistering scalp recording locations with 3d structural mr images (2008) Human brain mapping, 29, pp. 1288-1301; Gratton, G., Corballis, P.M., Removing the heart from the brain: compensation for the pulse artifact in the photon migration signal (1995) Psychophysiology, 32, pp. 292-299; Sassaroli, A., Fantini, S., Comment on the modified BeerLambert law for scattering media (2004) Physics in Medicine and Biology, 49, p. N255; Huppert, T.J., Diamond, S.G., Franceschini, M.A., Boas, D.A., Homer: a review of time-series analysis methods for near-infrared spectroscopy of the brain (2009) Applied optics, 48, pp. D280-D298; Ang, K.K., Yu, J., Guan, C., Extracting effective features from high density nirs-based BCI for assessing numerical cognition (2012) Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on (IEEE), pp. 2233-2236; Delorme, A., Makeig, S., EEGLAB: an open source toolbox for analysis of single-trial eeg dynamics including independent component analysis (2004) Journal of neuroscience methods, 134, pp. 9-21; Jung, T.P., Makeig, S., Westerfield, M., Townsend, J., Courchesne, E., Sejnowski, T.J., Removal of eye activity artifacts from visual event-related potentials in normal and clinical subjects (2000) Clinical Neurophysiology, 111, pp. 1745-1758; Leamy, D.J., Collins, R., Ward, T.E., Combining fNIRS and EEG to improve motor cortex activity clas sification during an imagined movement-based task (2011) Foundations of Augmented Cognition. Directing the Future of Adaptive Systems, pp. 177-185. , (Springer); Schlogl, A., Brunner, C., Biosig: a free and open source software library for bci research (2008) Computer, 41, pp. 44-50; Chang, C.C., Lin, C.J., LIBSVM: A library for support vector machines (2011) ACM Transactions on Intelligent Systems and Technology, 2, pp. 1-27; D'Mello, S., Kory, J., Consistent but modest: a meta-analysis on unimodal and multimodal affect detection accuracies from 30 studies (2012) Proceedings of the 14th ACM international conference on Multimodal interaction (ACM), pp. 31-38; Mueller-Putz, G., Scherer, R., Brunner, C., Leeb, R., Pfurtscheller, G., Better than random: A closer look on BCI results (2008) International Journal of Bioelectromagnetism, 10, pp. 52-55; Molavi, B., Dumont, G.A., Wavelet-based motion artifact removal for functional near-infrared spectro scopy (2012) Physiological measurement, 33, pp. 259-270; Fairclough, S.H., Fundamentals of physiological computing (2009) Interacting with Computers, 21, pp. 133-145},
correspondence_address1={Putze, F.; Cognitive Systems Lab, Institute of Anthropomatics and Robotics, Karlsruhe Institute of Technology, Adenauerring 4, Germany},
publisher={Frontiers Research Foundation},
issn={16624548},
language={English},
abbrev_source_title={Front. Neurosci.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Telaar20142650,
author={Telaar, D. and Wand, M. and Gehrig, D. and Putze, F. and Amma, C. and Heger, D. and Vu, N.T. and Erhardt, M. and Schlippe, T. and Janke, M. and Herff, C. and Schultz, T.},
title={BioKIT - Real-time decoder for biosignal processing},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2014},
pages={2650-2654},
note={cited By 14; Conference of 15th Annual Conference of the International Speech Communication Association: Celebrating the Diversity of Spoken Languages, INTERSPEECH 2014 ; Conference Date: 14 September 2014 Through 18 September 2014;  Conference Code:108771},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910048355&partnerID=40&md5=306953e82654df15d818c007d5ffbe26},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany},
abstract={We introduce BioKIT, a new Hidden Markov Model based toolkit to preprocess, model and interpret biosignals such as speech, motion, muscle and brain activities. The focus of this toolkit is to enable researchers from various communities to pursue their experiments and integrate real-time biosignal interpretation into their applications. BioKIT boosts a flexible two-layer structure with a modular C++ core that interfaces with a Python scripting layer, to facilitate development of new applications. BioKIT employs sequence-level parallelization and memory sharing across threads. Additionally, a fully integrated error blaming component facilitates in-depth analysis. A generic terminology keeps the barrier to entry for researchers from multiple fields to a minimum. We describe our online-capable dynamic decoder and report on initial experiments on three different tasks. The presented speech recognition experiments employ Kaldi [1] trained deep neural networks with the results set in relation to the real time factor needed to obtain them. Copyright © 2014 ISCA.},
author_keywords={Biosignal processing;  Dynamic decoder;  Speech recognition;  Toolkit},
keywords={Brain;  C++ (programming language);  Decoding;  Experiments;  Hidden Markov models;  Speech communication, Bio-signal processing;  Biosignal interpretation;  Deep neural networks;  Fully integrated;  In-depth analysis;  New applications;  Toolkit;  Two-layer structures, Speech recognition},
references={Povey, D., Ghoshal, A., Boulianne, G., Burget, L., Glembek, O., Goel, N., Hannemann, M., Schwarz, P., The kaldi speech recognition toolkit (2011) IEEE Automatic Speech Recognition and Understanding (ASRU) Workshop, pp. 1-4; Chase, L.L., (1997) Error-Responsive Feedback Mechanisms for Speech Recognizers, , Ph.D. dissertation, Pittsburgh, PA: Carnegie Mellon University; Nanjo, H., Ri, A., Kawahara, T., Automatic diagnosis of recognition errors in large vocabulary continuous speech recognition system (1999) Joho Shori Gakkai Kenkyu Hokoku, 99 (64), pp. 41-48; Young, S.J., Evermann, G., Gales, M., Kershaw, D., Moore, G., Odell, J., Ollason, D., Woodland, P., (2006) The HTK Book Version 3.4; Walker, W., Lamere, P., Kwok, P., Raj, B., Singh, R., Gouvea, E., Wolf, P., Woelfel, J., Sphinx-4: A flexible open source framework for speech recognition (2004) Sun Microsystems Technical Report; Lee, A., Kawahara, T., Shikano, K., Julius - An open source real-time large vocabulary recognition engine (2001) Eurospeech, pp. 1691-1694; Rybach, D., Gollan, C., Heigold, G., Hoffmeister, B., Lööf, J., Schlüter, R., Ney, H., The RWTH aachen university open source speech recognition system (2009) Interspeech, pp. 2111-2114; Westeyn, T., Brashear, H., Atrash, A., Starner, T., Georgia tech gesture toolkit: Supporting experiments in gesture recognition (2003) International Conference on Multimodal Interfaces (ICMI), pp. 85-92; Finke, M., Geutner, P., Hild, H., Kemp, T., Ries, K., Westphal, M., The karlsruhe-verbmobil speech recognition engine (1997) IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 1, pp. 83-86; Soltau, H., Metze, F., Fügen, C., Waibel, A., A one-pass decoder based on polymorphic linguistic context assignment (2001) IEEE Automatic Speech Recognition and Understanding (ASRU) Workshop, pp. 214-217; Soltau, H., Saon, G., Kingsbury, B., The IBM attila speech recognition toolkit (2010) IEEE Workshop on Spoken Language Technology (SLT), pp. 97-102; Wand, M., Schulte, C., Janke, M., Schultz, T., Array-based electromyographic silent speech interface (2013) BIOSIGNALS, pp. 89-96; Jones, E., Oliphant, T., Peterson, P., (2001) SciPy: Open Source Scientific Tools for Python, , http://www.scipy.org/, [Online]; Amma, C., Georgi, M., Schulz, T., Airwriting: A wearable handwriting recognition system (2013) Personal and Ubiquitious Computing, pp. 1-13; Rebelo, D., Amma, C., Gamboa, H., Schultz, T., Human activity recognition for an intelligent knee orthosis (2013) International Joint Conference on Biomedical Engineering Systems and Technologies - Biosignals, pp. 368-371; Gales, M.J., Maximum likelihood linear transformations for HMM-based speech recognition (1998) Computer Speech & Language, 12 (2), pp. 75-98; Johnson, D., Ellis, D., Oei, C., Wooters, C., Faerber, P., (2005) Quicknet, , http://www.icsi.berkeley.edu/Speech/qn.html, [Online]; Gauvain, J.-L., Lee, C.-H., Maximum a posteriori estimation for multivariate gaussian mixture observations of markov chains (1994) Speech and Audio Processing, Ieee Transactions on, 2 (2), pp. 291-298; Stolcke, A., SRILM an extensible language modeling toolkit (2002) Interspeech, pp. 901-904; Bilmes, J.A., Kirchhoff, K., Factored language models and generalized parallel backoff (2003) HLT-NAACL, 2, pp. 4-6; Soltau, H., Saon, G., Dynamic network decoding revisited (2009) IEEE Automatic Speech Recognition and Understanding (ASRU) Workshop, pp. 276-281; Ortmanns, S., Eiden, A., Ney, H., Coenen, N., Look-ahead techniques for fast beam search (1997) IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 3, pp. 1783-1786; Garofalo, J., Graff, D., Paul, D., Pallett, D., Continous speech recognition (CSR-I) wall street journal (WSJ0) news, complete (1993) Linguistic Data Consortium, , Philadelphia, Tech. Rep; Schultz, T., Wand, M., Modeling coarticulation in large vocabulary EMG-based speech recognition (2010) Speech Communication, 52 (4), pp. 341-353; Wand, M., Janke, M., Schultz, T., The EMG-UKA corpus for EMG-based speech recognition (2014) Interspeech; Schultz, T., Vu, N.T., Schlippe, T., Global phone: A multilingual text & speech database in 20 languages (2013) IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP); Vu, N.T., Imseng, D., Povey, D., Motlicek, P., Schultz, T., Bourlard, H., Multilingual deep neural network based acoustic modeling for rapid language adaptation (2014) IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
correspondence_address1={Telaar, D.; Cognitive Systems Lab, Karlsruhe Institute of Technology (KIT)Germany},
editor={Chng E.S., Li H., Meng H., Ma B., Xie L.},
sponsors={Amazon; Baidu; et al.; Google; Temasek Laboratories at Nanyang Technological University (TL at NTU); WeChat},
publisher={International Speech and Communication Association},
issn={2308457X},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Putze2013419,
author={Putze, F. and Heger, D. and Schultz, T.},
title={Reliable subject-adapted recognition of EEG error potentials using limited calibration data},
journal={International IEEE/EMBS Conference on Neural Engineering, NER},
year={2013},
pages={419-422},
doi={10.1109/NER.2013.6695961},
art_number={6695961},
note={cited By 4; Conference of 2013 6th International IEEE EMBS Conference on Neural Engineering, NER 2013 ; Conference Date: 6 November 2013 Through 8 November 2013;  Conference Code:103024},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897723048&doi=10.1109%2fNER.2013.6695961&partnerID=40&md5=1e597332586eac0dafabf3bc536a0767},
affiliation={Cognitive System Lab, Faculty of Computer Science, Karlsruhe Institute of Technology, 76131 Karlsruhe, Germany},
abstract={For the development of efficient Brain Computer Interfaces (BCIs), recognizing when the system reacts erroneously to a user's input is a much desired functionality. In this paper, we investigate a system for the recognition of error potentials from single-trial Electroencephalography (EEG). Our focus here is the development of a system using only limited calibration data from the test subject, while exploiting available training data from other subjects. In an evaluation with 20 sessions, we show that we can achieve an average F-score of up to 0.86 for a system using ICA-based artifact correction and training data filtering which only requires few minutes of additional calibration data. © 2013 IEEE.},
keywords={Artifact correction;  Brain computer interfaces (BCIs);  Calibration data;  F-score;  Training data, Electrophysiology, Calibration},
references={Zander, T.O., Kothe, C., Towards passive braincomputer interfaces: Applying braincomputer interface technology to humanmachine systems in general (2011) Journal of Neural Engineering, 8 (2), p. 025005; Falkenstein, M., Hoormann, J., Christ, S., Hohnsbein, J., ERP components on reaction errors and their functional significance: A tutorial (2000) Biological Psychology, 51 (2-3), pp. 87-107; Ferrez, P., Del Millan, J.R., Error-related EEG potentials generated during simulated brain computer interaction (2008) IEEE Transactions on Biomedical Engineering, 55 (3), pp. 923-929; Schalk, G., Wolpaw, J.R., McFarland, D.J., Pfurtscheller, G., EEG-based communication: Presence of an error potential (2000) Clinical Neurophysiology, 111 (12), pp. 2138-2144; Forster, K., Biasiucci, A., Chavarriaga, R., Millan, J.D.R., Roggen, D., Troster, G., On the use of brain decoded signals for online user adaptive gesture recognition systems (2010) Pervasive Computing, Ser. Lecture Notes in Computer Science, (6030), pp. 427-444. , Springer Berlin Heidelberg; Vi, C., Subramanian, S., Detecting error-related negativity for interaction design (2012) Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, Ser. CHI '12, p. 493502. , New York, NY, USA: ACM; Iturrate, I., Chavarriaga, R., Montesano, L., Minguez, J., Millan, J.D.R., Latency correction of error potentials between different experiments reduces calibration time for single-trial classification (2012) Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society. 2012, pp. 3288-3291; Spuler, M., Bensch, M., Kleih, S., Rosenstiel, W., Bogdan, M., Kubler, A., Online use of error-related potentials in healthy users and people with severe motor impairment increases performance of a p300-BCI (2012) Clinical Neurophysiology, 123 (7), pp. 1328-1337; Palmer, J.A., Makeig, S., Delgado, K., Rao, B.D., Newton method for the ICA mixture model (2008) IEEE International Conference on Acoustics, Speech and Signal Processing, 2008. ICASSP 2008, pp. 1805-1808; Kailath, T., The divergence and bhattacharyya distance measures in signal selection (1967) IEEE Transactions on Communication Technology, 15 (1), pp. 52-60; Pan, S.J., Yang, Q., A survey on transfer learning (2010) IEEE Transactions on Knowledge and Data Engineering, 22 (10), pp. 1345-1359},
correspondence_address1={Cognitive System Lab, Faculty of Computer Science, Karlsruhe Institute of Technology, 76131 Karlsruhe, Germany},
sponsors={Battelle; Brain Vision LLC; CORTECH Solutions; System Neuroengineering IGERT Program - University of Minnesota; IOP Publishing},
address={San Diego, CA},
issn={19483546},
isbn={9781467319690},
language={English},
abbrev_source_title={Int. IEEE/EMBS Conf. Neural Eng., NER},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nallasamy201360,
author={Nallasamy, U. and Fuhs, M. and Woszczyna, M. and Metze, F. and Schultz, T.},
title={Neighbour selection and adaptation for rapid speaker-dependent ASR},
journal={2013 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2013 - Proceedings},
year={2013},
pages={60-65},
doi={10.1109/ASRU.2013.6707706},
art_number={6707706},
note={cited By 3; Conference of 2013 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2013 ; Conference Date: 8 December 2013 Through 13 December 2013;  Conference Code:102503},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893668797&doi=10.1109%2fASRU.2013.6707706&partnerID=40&md5=29761d3ed4ab9469dab49ece407e74bc},
affiliation={Language Technologies Institute, Carnegie Mellon University, Pittsburgh, United States; Speech Technology Group, M'Modal Inc., Pittsburgh, United States; Cognitive Systems Labs, Karlsruhe Institute of Technology, Germany},
abstract={Speaker dependent (SD) ASR systems have significantly lower word error rates (WER) compared to speaker independent (SI) systems. However, SD systems require sufficient training data from the target speaker, which is impractical to collect in a short time. We present a technique for training SD models using just few minutes of speaker's data. We compensate for the lack of adequate speaker-specific data by selecting neighbours from a database of existing speakers who are acoustically close to the target speaker. These neighbours provide ample training data, which is used to adapt the SI model to obtain an initial SD model for the new speaker with significantly lower WER. We evaluate various neighbour selection algorithms on a large-scale medical transcription task and report significant reduction in WER using only 5 mins of speaker-specific data. We conduct a detailed analysis of various factors such as gender and accent in the neighbour selection. Finally, we study neighbour selection and adaptation in the context of discriminative objective functions. © 2013 IEEE.},
author_keywords={acoustic modeling;  data selection approaches;  speaker adaptation;  Speech recognition},
keywords={Acoustic model;  Analysis of various;  Data Selection;  Medical transcription;  Neighbour selections;  Objective functions;  Speaker adaptation;  Speaker independents, Speech processing;  Speech recognition, Loudspeakers},
references={Zhan, P., Waibel, A., Vocal tract length normalization for large vocabulary continuous speech recognition (1997) Tech. Rep., Carnegie Mellon University; Gales, M.J.F., Maximum likelihood linear transformations for HMM-based speech recognition (1998) Computer Speech and Language, 12 (2), pp. 75-98; Shi, Y., Chang, E., Studies in massively speaker-specific speech recognition (2002) Interspeech; Gauvain, J.-L., Lee, C.-H., Maximum a posteriori estimation for multivariate gaussian mixture observations of markov chains (1994) IEEE Transactions on Speech and Audio Processing, 2 (2), pp. 291-298; Leggetter, C.J., Woodland, C.P., Speaker adaptation of continuous density hmms using multivariate linear regression (1994) ICSLP; Kuhn, R., Nguyen, P., Junqua, J.-C., Goldwasser, L., Niedzielski, N., Fincke, S., Field, K., Contolini, M., Eigenvoices for speaker adaptation (1998) ICSLP; Bacchiani, M., Rapid adaptation for mobile speech applications (2013) ICASSP; Chen, K.-T., Liau, W.-W., Wang, H.-M., Lee, L.-S., Fast speaker adaptation using eigenspace-based maximum likelihood linear regression (2000) Interspeech, pp. 742-745; Povey, D., Yao, K., A basis representation of constrained MLLR transforms for robust adaptation (2012) Computer Speech & Language, 26 (1), pp. 35-51; Sankar, A., Beaufays, F., Digalakis, V., Training data clustering for improved speech recognition (1995) EUROSPEECH; Beaufays, F., Vanhoucke, V., Strope, B., Unsupervised discovery and training of maximally dissimilar cluster models (2010) Interspeech, pp. 66-69; Gales, M.J.F., Cluster adaptive training of hidden markov models (2000) IEEE Transactions on Speech and Audio Processing, 8 (4), pp. 417-428; Padmanabhan, M., Bahl, L.R., Nahamoo, D., Picheny, M.A., Speaker clustering and transformation for speaker adaptation in speech recognition systems (1998) IEEE Transactions on Speech and Audio Processing, 6 (1), pp. 71-77. , PII S1063667698005896; Huang, C., Chen, T., Chang, E., Speaker selection training for large vocabulary continuous speech recognition (2002) ICASSP, pp. 609-612; Wu, J., Chang, E., Cohorts based custom models for rapid speaker and dialect adaptation (2001) Interspeech, pp. 1261-1264; Vipperla, R., Renals, S., Frankel, J., Augmentation of adaptation data (2010) Interspeech, pp. 530-533; Sainath, T.N., Ramabhadran, B., Nahamoo, D., Kanevsky, D., Compernolle, D.V., Demuynck, K., Gemmeke, J.F., Sundaram, S., Exemplar-based processing for speech recognition: An overview (2012) IEEE Signal Process. Mag., 29 (6), pp. 98-113; Huang, J., Padmanabhan, M., A study of adaptation techniques on a voicemail transcription task (1999) EUROSPEECH; Gales, M.J.F., The generation and use of regression class trees for MLLR adaptation (1996) Tech. Rep., Cambridge University; Mann, H.B., Whitney, D.R., On a test of whether one of two random variables is stochastically larger than the other (1947) Annals of Mathematical Statistics, 18 (1), pp. 50-60; Gibson, M., Hain, T., Hypothesis spaces for minimum bayes risk training in large vocabulary speech recognition (2006) Interspeech; Povey, D., Gales, M.J.F., Kim, D.Y., Woodland, P.C., Mmi-map and MPE-MAP for acoustic model adaptation (2003) Interspeech; Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, A.R., Jaitly, N., Andrew, S., Kingsbury, B., Deep neural networks for acoustic modeling in speech recognition (2012) Signal Processing Magazine; Sainath, T.N., Kingsbury, B., Ramabhadran, B., Auto-encoder bottleneck features using deep belief networks (2012) ICASSP, pp. 4153-4156},
correspondence_address1={Language Technologies Institute, Carnegie Mellon University, Pittsburgh, United States},
sponsors={The Institute of Electrical and Electronics Engineers (IEEE) Signal Processing Society},
address={Olomouc},
isbn={9781479927562},
language={English},
abbrev_source_title={IEEE Workshop Autom. Speech Recogn. Underst., ASRU - Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Heger2013832,
author={Heger, D. and Mutter, R. and Herff, C. and Putze, F. and Schultz, T.},
title={Continuous recognition of affective states by functional near infrared spectroscopy signals},
journal={Proceedings - 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction, ACII 2013},
year={2013},
pages={832-837},
doi={10.1109/ACII.2013.156},
art_number={6681548},
note={cited By 16; Conference of 2013 5th Humaine Association Conference on Affective Computing and Intelligent Interaction, ACII 2013 ; Conference Date: 2 September 2013 Through 5 September 2013;  Conference Code:102402},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892984788&doi=10.1109%2fACII.2013.156&partnerID=40&md5=5c4f261c64c974bb9403c0018b10f8f0},
affiliation={Karlsruhe Institute of Technology (KIT), Adenauerring 4, 76131 Karlsruhe, Germany},
abstract={Functional near infrared spectroscopy (fNIRS) is becoming more and more popular as an innovative imaging modality for brain computer interfaces. A continuous (i.e. asynchronous) affective state monitoring system using fNIRS signals would be highly relevant for numerous disciplines, including adaptive user interfaces, entertainment, biofeedback, and medical applications. However, only stimulus-locked emotion recognition systems have been proposed by now. fNRIS signals of eight subjects at eight prefrontal locations have been recorded in response to three different classes of affect induction by emotional audio-visual stimuli and a neutral class. Our system evaluates short windows of five seconds length to continuously recognize affective states. We analyze hemodynamic responses, present a careful evaluation of binary classification tasks and investigate classification accuracies over the time. © 2013 IEEE.},
author_keywords={Affective states;  Asynchronous;  Continuous recognition;  Emotion recognition;  FNIRS;  Functional Near Infrared Spectroscopy},
keywords={Affective state;  Asynchronous;  Continuous recognition;  Emotion recognition;  FNIRS;  Functional near infrared spectroscopy, Biofeedback;  Medical applications;  Near infrared spectroscopy, Association reactions},
references={Picard, R.W., (2000) Affective Computing, , MIT press; Dmello, S.K., Craig, S.D., Witherspoon, A., McDaniel, B., Graesser, A., Automatic detection of learners affect from conversational cues (2008) User Modeling and User-Adapted Interaction, 18 (1-2), pp. 45-80; Heger, D., Putze, F., Schultz, T., An eeg adaptive information system for an empathic robot (2011) International Journal of Social Robotics, 3 (4), pp. 415-425; Kaliouby, R.E., Picard, R., Baron-Cohen, S., Affective computing and autism (2006) Annals of the New York Academy of Sciences, 1093 (1), pp. 228-248; Ledoux, J.E., Emotion circuits in the brain (2000) Annual Review of Neuroscience, 23 (1), pp. 155-184; Bush, G., Luu, P., Posner, M.I., Cognitive and emotional influences in anterior cingulate cortex (2000) Trends in Cognitive Sciences, 4 (6), pp. 215-222; Damasio, A.R., Grabowski, T.J., Bechara, A., Damasio, H., Ponto, L.L., Parvizi, J., Hichwa, R.D., Subcortical and cortical brain activity during the feeling of self-generated emotions (2000) Nature Neuroscience, 3 (10), pp. 1049-1056; Bechara, A., Damasio, H., Damasio, A.R., Emotion, decision making and the orbitofrontal cortex (2000) Cerebral Cortex, 10 (3), pp. 295-307; Zander, T.O., Kothe, C., Towards passive brain-computer interfaces: Applying brain-computer interface technology to human-machine systems in general (2011) Journal of Neural Engineering; Molina, G.G., Tsoneva, T., Nijholt, A., Emotional brain-computer interfaces (2009) Affective Computing and Intelligent Interaction and Workshops. 3rd International Conference On., pp. 1-9. , IEEE; Sassaroli, A., Fantini, S., Comment on the modified beerlambert law for scattering media (2004) Physics in Medicine and Biology, 49 (14), pp. N255; Jerritta, S., Murugappan, M., Nagarajan, R., Wan, K., Physiological signals based human emotion recognition: A review (2011) Signal Processing and Its Applications (CSPA), 2011 IEEE 7th International Colloquium on, pp. 410-415. , IEEE; El Ayadi, M., Kamel, M.S., Karray, F., Survey on speech emotion recognition: Features, classification schemes, and databases (2011) Pattern Recognition, 44 (3), pp. 572-587; Lang, P.J., Bradley, M.M., Cuthbert, B.N., (1999) International Affective Picture System (Iaps): Technical Manual and Affective Ratings; Bradley, M.M., Lang, P.J., (2007) The International Affective Digitized Sounds (iads-2): Affective Ratings of Sounds and Instruction Manual, , University of Florida, Gainesville, FL, Tech. Rep. B-3; Ekman, P., Facial expression and emotion (1993) American Psychologist, 48, pp. 384-384; Russell, J.A., Mehrabian, A., Evidence for a three-factor theory of emotions (1977) Journal of Research in Personality; Lang, P., Bradley, M.M., The international affective picture system (iaps) in the study of emotion and attention (2007) Handbook of Emotion Elicitation and Assessment, p. 29; Herrmann, M., Ehlis, A., Fallgatter, A., Prefrontal activation through task requirements of emotional induction measured with nirs (2003) Biological Psychology, 64 (3), pp. 255-263; Hoshi, Y., Huang, J., Kohri, S., Iguchi, Y., Naya, M., Okamoto, T., Ono, S., Recognition of human emotions from cerebral blood flow changes in the frontal region: A study with event-related near-infrared spectroscopy (2011) Journal of Neuroimaging, 21 (2); Morinaga, K., Akiyoshi, J., Matsushita, H., Ichioka, S., Tanaka, Y., Tsuru, J., Hanada, H., Anticipatory anxiety-induced changes in human lateral prefrontal cortex activity (2007) Biological Psychology, 74 (1), pp. 34-38; Leon-Carrion, J., Damas, J., Izzetoglu, K., Pourrezai, K., Martín-Rodríguez, J.F., Martin, J.M.B.Y., Dominguez-Morales, M.R., Differential time course and intensity of pfc activation for men and women in response to emotional stimuli: A functional near-infrared spectroscopy (fnirs) study (2006) Neuroscience Letters, 403 (1), pp. 90-95; Yang, H., Zhou, Z., Liu, Y., Ruan, Z., Gong, H., Luo, Q., Lu, Z., Gender difference in hemodynamic responses of prefrontal area to emotional stress by near-infrared spectroscopy (2007) Behavioural Brain Research, 178 (1), pp. 172-176; Glotzbach, E., Mühlberger, A., Gschwendtner, K., Fallgatter, A.J., Pauli, P., Herrmann, M.J., Prefrontal brain activation during emotional processing: A functional near infrared spectroscopy study (fnirs) (2011) The Open Neuroimaging Journal, 5, p. 33; Ishikawa, A., Shimizu, K., Birbaumer, N., Temporal classification of multichannel near-infrared spectroscopy signals of motor imagery for developing a brain-computer interface (2007) NeuroImage; Coyle, S.M., Ward, T.E., Markham, C.M., Brain-computer interface using a simplified functional near-infrared spectroscopy system (2007) Journal of Neural Engineering, 4 (3), p. 219; Girouard, A., Adaptive brain-computer interface (2009) CHI'09 Extended Abstracts on Human Factors in Computing Systems, , ACM; Ang, K.K., Guan, C., Lee, K., Lee, J.Q., Nioka, S., Chance, B., A brain-computer interface for mental arithmetic task from singletrial near-infrared spectroscopy brain signals (2010) Pattern Recognition (ICPR), 20th International Conference on, pp. 3764-3767. , IEEE; Herff, C., Putze, F., Heger, D., Guan, C., Schultz, T., Speaking mode recognition from functional near infrared spectroscopy (2012) Engineering in Medicine and Biology Society (EMBC), 2012 Annual International Conference of the IEEE, pp. 1715-1718. , IEEE; Tai, K., Chau, T., Single-trial classification of nirs signals during emotional induction tasks: Towards a corporeal machine interface (2009) Journal of Neuroengineering and Rehabilitation, 6, p. 39; Hosseini, S., Mano, Y., Rostami, M., Takahashi, M., Sugiura, M., Kawashima, R., Decoding what one likes or dislikes from single-trial fnirs measurements (2011) Neuroreport, 22 (6), p. 269; Moghimi, S., Kushki, A., Power, S., Guerguerian, A.M., Chau, T., Automatic detection of a prefrontal cortical response to emotionally rated music using multi-channel near-infrared spectroscopy (2012) Journal of Neural Engineering, 9 (2), p. 026022; Asano, H., Sagami, T., Ide, H., The evaluation of the emotion by near-infrared spectroscopy (2013) Artificial Life and Robotics, pp. 1-5; Amma, C., Fischer, A., Stein, T., Schwameder, H., Schultz, T., Emotionserkennung auf der basis von gangmustern (2010) Sportinformatik Trifft Sporttechnologie, Tagung der Dvs-Sektion Sportinformatik; Matthews, F., Pearlmutter, B., Ward, T., Soraghan, C., Markham, C., Hemodynamics for brain-computer interfaces (2008) Signal Processing Magazine, IEEE, 25 (1), pp. 87-94; Cooper, R., Selb, J., Gagnon, L., Phillip, D., Schytz, H., Iversen, H., Ashina, M., Boas, D., A systematic comparison of motion artifact correction techniques for functional near-infrared spectroscopy (2012) Frontiers in Neuroscience, 6, p. 147; Molavi, B., Dumont, G.A., Wavelet-based motion artifact removal for functional near-infrared spectroscopy (2012) Physiological Measurement, 33 (2), p. 259; Chang, C.-C., Lin, C.-J., LIBSVM: A library for support vector machines (2011) ACM Transactions on Intelligent Systems and Technology, 2, pp. 1-27},
correspondence_address1={Karlsruhe Institute of Technology (KIT), Adenauerring 4, 76131 Karlsruhe, Germany},
address={Geneva},
isbn={9780769550480},
language={English},
abbrev_source_title={Proc. - Hum. Assoc. Conf. Affective Comput. Intelligent Interact., ACII},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Schultz2013760,
author={Schultz, T. and Amma, C. and Wand, M. and Heger, D. and Putze, F.},
title={Human-machine interfaces based on biosignals [Biosignale-basierte Mensch-Maschine Schnittstellen]},
journal={At-Automatisierungstechnik},
year={2013},
volume={61},
number={11},
pages={760-769},
doi={10.1524/auto.2013.1061},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889684453&doi=10.1524%2fauto.2013.1061&partnerID=40&md5=90e5f0aa4f3f04d7a841e06b2fa9429c},
affiliation={Cognitive Systems Lab, Institut für Anthropomatik, Karlsruher Institut für Technologie, Adenauerring 4, Karlsruhe-D-76131, Denmark},
abstract={Human communication relies on signals like speech, mimics, or gestures and the interpretation of these signals seems to be innate to human beings. Consequently, the research community investigates long-since the transfer of these capabilities to Man-Machine Interfaces (MMI). While significant progress has been made, the focus on mimicing human skills ignores the potential of emerging new sensor technologies to build innovative MMIs which exploit information that is not available to human beings. This paper discusses ongoing research in the area of "Biosignals and Interfaces", such as Airwriting, a motion based interfaces for text entry in wearable computing applications, silent speech interfaces that rely on articulatory muscle movement, as well as interfaces that use brain activity to determine users' mental states, such as task activity and cognitive workload in order to adapt the system's behavior accordingly. The goal of this research is to establish a new generation of human-centered systems, which are aware of the users' needs and provide intuitive, efficient, robust, and adaptive mechanisms for interaction and communication with and through machines. © 2013 by Walter de Gruyter Berlin Boston 2013.},
author_keywords={airwriting;  biosignals;  human-centered systems;  Human-maschine interfaces;  silent speech interfaces;  user states and activities},
references={Amma, C., Georgi, M., Schultz, T., Airwriting: Hands-free mobile text input by spotting and continuous recognition of 3d-space handwriting with inertial sensors (2012) IEEE 16th International Symposium on Wearable Computers (ISWC), pp. 52-59. , Best Paper Award ISWC; Amma, C., Georgi, M., Schultz, T., Airwriting: A wearable handwriting recognition system (2013) Journal of Personal and Ubiquitous Computing, , DOI 10.1007/s00779-013-0637-3 Springer; Chan, A.D.C., (2003) Multi-expert Automatic Speech Recognition System Using Myoelectric Signals, , Ph.D Dissertation, Department of Electrical and Computer Engineering, University of New Brunswick, Canada; Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Brumberg, J., Silent speech interfaces special issue on silent speech interfaces (2010) Speech Communication, 52 (4), pp. 270-287; Herff, C., Heger, D., Putze, F., Hennrich, J., Fortmann, O., Schultz, T., Classification of mental tasks in the prefrontal cortex using fNIRS (2013) Proceedings of the International Conference of the IEEE Engineering in Medicine and Biology Society, , Osaka, Japan; Heger, D., Putze, F., Schultz, T., An EEG adaptive information system for an empathic robot (2011) International Journal of Social Robotics, 3 (4), pp. 415-425. , Best Paper Nomination ICSR; Jarvis, J., Putze, F., Heger, D., Schultz, T., Multimodal person independent recognition of workload related biosignal patterns (2011) Proceedings of the 13th International Conference on Multimodal Interfaces, pp. 205-208. , ACM; Jorgensen, C., Lee, D.D., Agabon, S., Sub auditory speech recognition based on EMG signals (2003) Proc. Internat. Joint Conf. on Neural Networks (IJCNN), pp. 3128-3133; Jou, S.-C., Schultz, T., Waibel, A., Continuous electromyographic speech recognition with a multi-stream decoding architecture (2006) Proceedings of Interspeech, pp. 573-576. , Pittsburgh, PA; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session Independent non-audible speech recognition using surface electromyography (2005) Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 331-336. , Puerto Rico; Putze, F., Müller, M., Heger, D., Schultz, T., Session-independent EEG-based workload recognition (2013) Proceedings of the 13th Biosignals Conference, , Barcelona, Spain; Putze, F., Hild, J., Kärgel, R., Herff, C., Redmann, A., Beyerer, J., Schultz, T., Locating user attention using eye tracking and EEG for spatio-temporal event selection (2013) Proceedings of the 2013 International Conference on Intelligent User Interfaces, pp. 129-136. , Santa Monica, USA; Schultz, T., Wand, M., Modeling coarticulation in large vocabulary EMG-based speech recognition (2010) Special Issue on Silent Speech Interfaces, Speech Communication, 52 (4), pp. 341-353; Walliczek, M., Kraft, F., Jou, S.-C., Schultz, T., Waibel, A., Subword unit based non-audible speech recognition using surface electromyography (2006) Proceedings of Interspeech, pp. 1487-1490. , Pittsburgh, USA; Wand, M., Schultz, T., Speaker-adaptive speech recognition based on surface electromyography (2010) Biomedical Engineering Systems and Technologies, Communications in Computer and Information Science, 52, pp. 271-285. , Selected Best Paper BIOSTEC; Wand, M., Schulte, C., Janke, M., Schultz, T., Array-based electromyographic silent speech interface, best student paper award (2013) Proceedings of Biosignals 2013, , extended version to appear in: Biomedical Engineering Systems and Technologies, Communications in Computer and Information Science},
correspondence_address1={Schultz, T.; Cognitive Systems Lab, Institut für Anthropomatik, Karlsruher Institut für Technologie, Adenauerring 4, Karlsruhe-D-76131, Denmark; email: tanja.schultz@kit.edu},
issn={01782312},
language={German},
abbrev_source_title={At-Automatisierungstechnik},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Heger20135614,
author={Heger, D. and Putze, F. and Herff, C. and Schultz, T.},
title={Subject-to-subject transfer for CSP based BCIs: Feature space transformation and decision-level fusion},
journal={Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
year={2013},
pages={5614-5617},
doi={10.1109/EMBC.2013.6610823},
art_number={6610823},
note={cited By 5; Conference of 2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2013 ; Conference Date: 3 July 2013 Through 7 July 2013;  Conference Code:100170},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886463959&doi=10.1109%2fEMBC.2013.6610823&partnerID=40&md5=26ef5bf07b6d6030ac087c2f1249b43b},
affiliation={Cognitive Systems Lab, Institute for Anthropomatics, Karlsruhe Institute of Technology, Adenauerring 4, Karlsruhe 76131, Germany},
abstract={Modern Brain Computer Interfaces (BCIs) usually require a calibration session to train a machine learning system before each usage. In general, such trained systems are highly specialized to the subject's characteristic activation patterns and cannot be used for other sessions or subjects. This paper presents a feature space transformation that transforms features generated using subject-specific spatial filters into a subject-independent feature space. The transformation can be estimated from little adaptation data of the subject. Furthermore, we combine three different Common Spatial Pattern based feature extraction approaches using decision-level fusion, which enables BCI use when little calibration data is available, but also outperformed the subject-dependent reference approaches for larger amounts of training data. © 2013 IEEE.},
keywords={Activation patterns;  Brain computer interfaces (BCIs);  Calibration data;  Common spatial patterns;  Decision level fusion;  Spatial filters;  Subject-specific;  Training data, Brain computer interface;  Calibration;  Feature extraction;  Learning systems, Metadata, artificial intelligence;  automated pattern recognition;  brain computer interface;  calibration;  electroencephalography;  human;  imagination;  movement (physiology);  procedures, Artificial Intelligence;  Brain-Computer Interfaces;  Calibration;  Electroencephalography;  Humans;  Imagination;  Movement;  Pattern Recognition, Automated},
references={Ramoser, H., Müller-Gerking, J., Pfurtscheller, G., Optimal spatial filtering of single trial eeg during imagined hand movement (2000) Rehabilitation Engineering, IEEE Transactions on, 8 (4), pp. 441-446; Blankertz, B., Tomioka, R., Lemm, S., Kawanabe, M., Müller, K., Optimizing spatial filters for robust eeg single-trial analysis (2008) Signal Processing Magazine, 25 (1), pp. 41-56. , IEEE; Krauledat, M., Tangermann, M., Blankertz, B., Müller, K., Towards zero training for brain-computer interfacing (2008) PLoS One, 3 (8), pp. e2967; Fazli, S., Popescu, F., Dańoczy, M., Blankertz, B., Müller, K., Grozea, C., Subject-independent mental state classification in single trials (2009) Neural Networks, 22 (9), pp. 1305-1312; Kang, H., Nam, Y., Choi, S., Composite common spatial pattern for subject-to-subject transfer (2009) Signal Processing Letters, 16 (8), pp. 683-686. , IEEE; Lotte, F., Guan, C., Ang, K., Comparison of designs towards a subject-independent brain-computer interface based on motor imagery (2009) IEEE Engineering in Medicine and Biology Society, 2009, pp. 4543-4546. , IEEE; Lotte, F., Guan, C., Learning from other subjects helps reducing brain-computer interface calibration time (2010) Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on, pp. 614-617; Reuderink, B., Farquhar, J., Poel, M., Nijholt, A., A subjectindependent brain-computer interface based on smoothed, secondorder baselining (2011) IEEE Engineering in Medicine and Biology Society, 2011, pp. 4600-4604. , IEEE; Tu, W., Sun, S., A subject transfer framework for eeg classification (2011) Neurocomputing; Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P., Mark, R., Mietus, J., Stanley, H., Physiobank, physiotoolkit, and physionet: Components of a new research resource for complex physiologic signals (2000) Circulation, 101 (23), pp. e215-e220; Schalk, G., Mcfarland, D., Hinterberger, T., Birbaumer, N., Wolpaw, J., Bci2000: A general-purpose brain-computer interface (bci) system (2004) Biomedical Engineering, IEEE Transactions on, 51 (6), pp. 1034-1043; Lotte, F., Guan, C., Regularizing common spatial patterns to improve bci designs: Unified theory and new algorithms (2011) Biomedical Engineering, IEEE Transactions on, 58 (2), pp. 355-362; Breiman, L., Bagging predictors (1996) Machine Learning, 24 (2), pp. 123-140; Shenoy, P., Krauledat, M., Blankertz, B., Rao, R., Müller, K., Towards adaptive classification for bci (2006) Journal of Neural Engineering, 3 (1), pp. R13},
correspondence_address1={Cognitive Systems Lab, Institute for Anthropomatics, Karlsruhe Institute of Technology, Adenauerring 4, Karlsruhe 76131, Germany},
address={Osaka},
issn={1557170X},
isbn={9781457702167},
pubmed_id={24111010},
language={English},
abbrev_source_title={Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wand20135750,
author={Wand, M. and Himmelsbach, A. and Heistermann, T. and Janke, M. and Schultz, T.},
title={Artifact removal algorithm for an EMG-based Silent Speech Interface},
journal={Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
year={2013},
pages={5750-5753},
doi={10.1109/EMBC.2013.6610857},
art_number={6610857},
note={cited By 8; Conference of 2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2013 ; Conference Date: 3 July 2013 Through 7 July 2013;  Conference Code:100170},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886571895&doi=10.1109%2fEMBC.2013.6610857&partnerID=40&md5=e09484781ea180046326b199d4075c07},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology, Germany},
abstract={An electromygraphic (EMG) Silent Speech Interface is a system which recognizes speech by capturing the electric potentials of the human articulatory muscles, thus enabling the user to communicate silently. This study deals with improving the EMG signal quality by removing artifacts: The EMG signals are captured by electrode arrays with multiple measuring points. On the resulting high-dimensional signal, Independent Component Analysis is performed, and artifact components are automatically detected and removed. This method reduces the Word Error Rate of the silent speech recognizer by 9.9% relative on a development corpus, and by 13.9% relative on an evaluation corpus. © 2013 IEEE.},
keywords={Artifact removal;  Electrode arrays;  EMG signal;  High-dimensional;  Measuring points;  Silent speech;  Silent speech interfaces;  Word error rate, Electromyography;  Independent component analysis, Speech recognition, algorithm;  artifact;  electromyography;  face muscle;  human;  muscle contraction;  physiology;  speech, Algorithms;  Artifacts;  Electromyography;  Facial Muscles;  Humans;  Muscle Contraction;  Speech},
references={Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Silent speech interfaces (2010) Speech Communication, 52 (4), pp. 270-287; Schultz, T., Wand, M., Modeling coarticulation in large vocabulary emg-based speech recognition (2010) Speech Communication, 52 (4), pp. 341-353; Wand, M., Schulte, C., Janke, M., Schultz, T., Array-based electromyographic silent speech interface (2013) Proc. Biosignals; Chan, A., Englehart, K., Hudgins, B., Lovely, D., Myoelectric signals to augment speech recognition (2001) Medical and Biological Engineering and Computing, 39, pp. 500-506; Jorgensen, C., Lee, D., Agabon, S., Sub auditory speech recognition based on emg/epg signals (2003) Proceedings of International Joint Conference on Neural Networks (IJCNN), pp. 3128-3133. , Portland, Oregon; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) IEEE Workshop on Automatic Speech Recognition and Understanding, , San Juan, Puerto Rico; Freitas, J., Teixeira, A., Dias, M.S., Towards a silent speech interface for portuguese (2012) Proc. Biosignals; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proc. Interspeech, , Pittsburgh, PA, Sep; Jou, S.-C.S., Schultz, T., Waibel, A., Continuous electromyographic speech recognition with a multi-stream decoding architecture (2007) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP); Wand, M., Schultz, T., Session-independent emg-based speech recognition (2011) Proc. Biosignals; Wand, M., Janke, M., Schultz, T., Decision-tree based analysis of speaking mode discrepancies in emg-based speech recognition (2012) Proc. Biosignals; Bell, A.J., Sejnowski, T.I., An information-maximization approach to blind separation and blind deconvolution (1995) Neural Computation, 7, pp. 1129-1159; Delorme, A., Makeig, S., Eeglab: An open source toolbox for analysis of single-trial eeg dynamics including independent component analysis (2004) Journal of Neuroscience Methods, 134 (1), pp. 9-21; Zhao, H., Xu, G., The research on surface electromyography signal effective feature extraction (2011) Proc. of the 6th International Forum on Strategic Technology; Blankertz, B., Tomioka, R., Lemm, S., Kawanabe, M., Müller, K.-R., Optimizing spatial filters for robust eeg single-trial analysis (2008) IEEE Signal Processing Magazine, 25, pp. 41-56},
correspondence_address1={Cognitive Systems Lab, Karlsruhe Institute of TechnologyGermany},
address={Osaka},
issn={1557170X},
isbn={9781457702167},
pubmed_id={24111044},
language={English},
abbrev_source_title={Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Herff20132160,
author={Herff, C. and Heger, D. and Putze, F. and Hennrich, J. and Fortmann, O. and Schultz, T.},
title={Classification of mental tasks in the prefrontal cortex using fNIRS},
journal={Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
year={2013},
pages={2160-2163},
doi={10.1109/EMBC.2013.6609962},
art_number={6609962},
note={cited By 40; Conference of 2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2013 ; Conference Date: 3 July 2013 Through 7 July 2013;  Conference Code:100170},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886501485&doi=10.1109%2fEMBC.2013.6609962&partnerID=40&md5=b5f3777f0eabd6b0ceca431a7258b9e2},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology, Adenauerring 4, Karlsruhe 76131, Germany},
abstract={Functional near infrared spectroscopy (fNIRS) is rapidly gaining interest in both the Neuroscience, as well as the Brain-Computer-Interface (BCI) community. Despite these efforts, most single-trial analysis of fNIRS data is focused on motor-imagery, or mental arithmetics. In this study, we investigate the suitability of different mental tasks, namely mental arithmetics, word generation and mental rotation for fNIRS based BCIs. We provide the first systematic comparison of classification accuracies achieved in a sample study. Data was collected from 10 subjects performing these three tasks. © 2013 IEEE.},
keywords={Classification accuracy;  Functional near-infrared spectroscopy (fnirs);  Mental arithmetic;  Mental rotation;  Mental tasks;  Prefrontal cortex;  Single-trial analysis, Brain computer interface;  Near infrared spectroscopy, Functional neuroimaging},
references={Friedrich, E.V.C., Scherer, R., Neuper, C., The effect of distinct mental strategies on classification performance for braincomputer interfaces (2012) International Journal of Psychophysiology, 84 (1), pp. 86-94; Coyle, S.M., Ward, T.E., Markham, C.M., Brain-computer interface using a simplified functional near-infrared spectroscopy system (2007) Journal of Neural Engineering, 4 (3), p. 219; Tanida, M., Sakatani, K., Takano, R., Tagai, K., Relation between asymmetry of prefrontal cortex activities and the autonomic nervous system during a mental arithmetic task: Near infrared spectroscopy study (2004) Neuroscience Letters, 369 (1), pp. 69-74; Watanabe, E., Maki, A., Kawaguchi, F., Takashiro, K., Yamashita, Y., Koizumi, H., Mayanagi, Y., Non-invasive assessment of language dominance with near-infrared spectroscopic mapping (1998) Neuroscience Letters, 256 (1), pp. 49-52; Shimoda, N., Takeda, K., Imai, I., Kaneko, J., Kato, H., Cerebral laterality differences in handedness: A mental rotation study with nirs (2008) Neuroscience Letters, 430 (1), pp. 43-47; Ang, K.K., Guan, C., Lee, K., Lee, J.Q., Nioka, S., Chance, B., A brain-computer interface for mental arithmetic task from single- trial near-infrared spectroscopy brain signals (2010) Int. Conference on Pattern Recognition, pp. 3764-3767; Ogata, H., Mukai, T., Yagi, T., A study on the frontal cortex in cognitive tasks using near-infrared spectroscopy (2007) Engineering in Medicine and Biology Society, 2007 EMBS 2007 29th Annual International Conference of the IEEE, pp. 4731-4734. , Aug; Sassaroli, A., Fantini, S., Comment on the modified beerlambert law for scattering media (2004) Physics in Medicine and Biology, 49 (14), pp. N255; Oldfield, R.C., The assessment and analysis of handedness: The edinburgh inventory (1971) Neuropsychologia, 9, pp. 97-113; Cooper, R., Selb, J., Gagnon, L., Phillip, D., Schytz, H.W., Iversen, H.K., Ashina, M., Boas, D.A., A systematic comparison of motion artifact correction techniques for functional near-infrared spectroscopy (2012) Frontiers in Neuroscience, 6 (147); Molavi, B., Dumont, G.A., Wavelet based motion artifact removal for functional near infrared spectroscopy (2010) Engineering in Medicine and Biology Society (EMBC), 2010 Annual International Conference of the IEEE, 31, pp. 5-8. , 2010-sept, 4; Herff, C., Putze, F., Heger, D., Guan, C., Schultz, T., Speaking mode recognition from functional near infrared spectroscopy (2012) Engineering in Medicine and Biology Society (EMBC), 2012 Annual International Conference of the IEEE, pp. 1715-1718; Herff, C., Heger, D., Putze, F., Guan, C., Schultz, T., Cross-subject classification of speaking modes using fnirs (2012) Neural Information Processing, pp. 417-424. , T Huang, Z Zeng, C Li, and C Leung, Eds. vol. 7664 of Lecture Notes in Computer Science, Springer Berlin Heidelberg; Cui, X., Bray, S., Reiss, A.L., Functional near infrared spectroscopy (NIRS) signal improvement based on negative correlation between oxygenated and deoxygenated hemoglobin dynamics (2010) NeuroImage, 49 (4), pp. 3039-3046. , Feb; Power, S.D., Kushki, A., Chau, T., Intersession consistency of singletrial classification of the prefrontal response to mental arithmetic and the no-control state by nirs (2012) PLoS ONE, 7 (7), pp. e37791; Friedrich, E.V.C., Scherer, R., Neuper, C., Long-term evaluation of a 4-class imagery-based braincomputer interface (2013) Clinical Neurophysiology},
correspondence_address1={Cognitive Systems Lab, Karlsruhe Institute of Technology, Adenauerring 4, Karlsruhe 76131, Germany},
address={Osaka},
issn={1557170X},
isbn={9781457702167},
language={English},
abbrev_source_title={Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schultz20138126,
author={Schultz, T. and Vu, N.T. and Schlippe, T.},
title={GlobalPhone: A multilingual text & speech database in 20 languages},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2013},
pages={8126-8130},
doi={10.1109/ICASSP.2013.6639248},
art_number={6639248},
note={cited By 49; Conference of 2013 38th IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2013 ; Conference Date: 26 May 2013 Through 31 May 2013;  Conference Code:101421},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890463379&doi=10.1109%2fICASSP.2013.6639248&partnerID=40&md5=36df184b725962d50ec003305b5b21c8},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology (KIT), Germany},
abstract={This paper describes the advances in the multilingual text and speech database GlobalPhone, a multilingual database of high-quality read speech with corresponding transcriptions and pronunciation dictionaries in 20 languages. GlobalPhone was designed to be uniform across languages with respect to the amount of data, speech quality, the collection scenario, the transcription and phone set conventions. With more than 400 hours of transcribed audio data from more than 2000 native speakers GlobalPhone supplies an excellent basis for research in the areas of multilingual speech recognition, rapid deployment of speech processing systems to yet unsupported languages, language identification tasks, speaker recognition in multiple languages, multilingual speech synthesis, as well as monolingual speech recognition in a large variety of languages. © 2013 IEEE.},
author_keywords={Dictionary Resources for Multilingual Speech Processing;  Speech;  Text},
keywords={Language identification;  Multilingual database;  Multilingual speech processing;  Multilingual speech recognition;  Multiple languages;  Pronunciation dictionaries;  Speaker recognition;  Text, Computational linguistics;  Signal processing;  Speech;  Speech processing;  Speech synthesis;  Transcription, Speech recognition},
references={Gordon, R., (2005) Ethnologue: Languages of the World. Dallas: SIL International; Schultz, T., Towards rapid language portability of speech processing systems (2004) Conference on Speech and Language Systems for Human Communication (SPLASH), 1. , Delhi, India, November; Schultz, T., Kirchhoff, K., (2006) Multilingual Speech Processing, , Elsevier Academic Press; Schultz, T., Globalphone: A multilingual speech and text database developed at karlsruhe university (2002) Proceedings of the ICSLP, pp. 345-348; (2012) European Language Resources Association (ELRA), , http://catalog.elra.info, ELRA catalogue. Retrieved November 30, 2012; Butler, H., Pty, A., Speech and language resources 2012 (2012) Appen Butler Hill Speech and Language Resources 2012-Product Catalogue; (2012) Benchmark GlobalPhone Language Models, , http://csl.ira.uka.de/GlobalPhone, Retrieved November 30, 2012; (2012) Rapid Language Adaptation Toolkit (RLAT), , http://csl.ira.uka.de/rlat-dev, Retrieved November 30 2012; Schultz, T., Black, A.W., Badaskar, S., Hornyak, M., Kominek, J., Spice: Web-based tools for rapid language adaptation in speech processing systems (2007) Proceedings of Interspeech, , Antwerp, Belgium, August; Black, A.W., Lenzo, K., (2000) Building Voices in the Festival Speech Synthesis System, , http://festvox.org/bsv/, Festvox. Retrieved November 30 2012; IPA, The principles of the International Phonetic Association, 2nd ed. London, UK: University College of London, 1982; Vu, N.T., Schlippe, T., Kraus, F., Schultz, T., Rapid bootstrapping of five eastern european languages using the rapid language adaptation toolkit (2010) INTERSPEECH, pp. 865-868; Stolcke, A., SRilm-An extensible language modeling toolkit (2002) Intl. Conf. Spoken Language Processing (ICSLP); Vu, N.T., Breiter, W., Metze, F., Schultz, T., Initialization schemes for multilayer perceptron training and their impact on asr performance using multilingual data (2012) INTERSPEECH; Schultz, T., Waibel, A., Language independent and language adaptive acoustic modeling for speech recognition (2001) Speech Communication, 35, pp. 31-51},
correspondence_address1={Cognitive Systems Lab, Karlsruhe Institute of Technology (KIT)Germany},
sponsors={IEE Signal Processing Society},
address={Vancouver, BC},
issn={15206149},
isbn={9781479903566},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schlippe20138406,
author={Schlippe, T. and Zhu, C. and Lemcke, D. and Schultz, T.},
title={Statistical machine translation based text normalization with crowdsourcing},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2013},
pages={8406-8410},
doi={10.1109/ICASSP.2013.6639305},
art_number={6639305},
note={cited By 2; Conference of 2013 38th IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2013 ; Conference Date: 26 May 2013 Through 31 May 2013;  Conference Code:101421},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890508104&doi=10.1109%2fICASSP.2013.6639305&partnerID=40&md5=ed1c76f06964b951ac6da49f836a434d},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology (KIT), Germany},
abstract={In [1], we have proposed systems for text normalization based on statistical machine translation (SMT) methods which are constructed with the support of Internet users and evaluated those with French texts. Internet users normalize text displayed in a web interface in an annotation process, thereby providing a parallel corpus of normalized and non-normalized text. With this corpus, SMT models are generated to translate non-normalized into normalized text. In this paper, we analyze their efficiency for other languages. Additionally, we embedded the English annotation process for training data in Amazon Mechanical Turk and compare the quality of texts thoroughly annotated in our lab to those annotated by the Turkers. Finally, we investigate how to reduce the user effort by iteratively applying an SMT system to the next sentences to be edited, built from the sentences which have been annotated so far. © 2013 IEEE.},
author_keywords={crowdsourcing;  rapid language adaptation;  statistical machine translation;  text normalization},
keywords={Amazon mechanical turks;  Crowdsourcing;  Internet users;  Normalized texts;  Parallel corpora;  rapid language adaptation;  Statistical machine translation;  Text normalizations, Iterative methods;  Linguistics;  Signal processing;  Translation (languages);  World Wide Web, Speech transmission},
references={Schlippe, T., Zhu, C., Gebhardt, J., Schultz, T., Text normalization based on statistical machine translation and internet user support (2010) 11th Annual Conference of the International Speech Communication Association (Interspeech 2010), , Makuhari, Japan, 26-30 September; Thang Vu, N., Schlippe, T., Kraus, F., Schultz, T., Rapid bootstrapping of five eastern european languages using the RLAT (2010) 11th Annual Conference of the International Speech Communication Association (Interspeech 2010), , Makuhari, Japan, 26-30 September; Adda, G., Martine, A.-D., Gauvain, J.-L., Lamel, L., Text normalization and speech recognition in French (1997) ESCA Eurospeech, , Rhodes, Greece, 22-25 September; Schultz, T., Black, A.W., Badaskar, S., Hornyak, M., Kominek, J., SPICE: Webbased tools for rapid language adaptation in speech processing systems (2007) Annual Conference of the International Speech Communication Association (Interspeech 2007, , Antwerp, Belgium, August; Chris, C.-B., Dredze, M., Creating speech and language data with amazons mechanical turk (2010) NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazons Mechanical Turk, , Los Angeles, California, June; Denkowski, M., Lavie, A., Exploring normalization techniques for human judgments of machine translation adequacy collected using amazon mechanical turk (2010) NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazons Mechanical Turk, , Los Angeles, California, June; Ambati, V., Vogel, S., Can crowds build parallel corpora for machine translation systems (2010) NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazons Mechanical Turk, , Los Angeles, California, June; Gralinski, F., Jassem, K., Wagner, A., Wypych, M., Text normalization as a special case of machine translation Wisla, Poland, November 2006, International Multiconference on Computer Science and Information Technology; Henriquez, C.A., Hernandez, A., A ngram-based statistical machine translation approach for text normalization on chat-speak style communications (2009) CAW2 (Content Analysis in Web 2.0), , April; Aw, A., Zhang, M., Xiao, J., Su, J., A phrasebased statistical model for sms text normalization (2006) COLING/ACL 2006, , Sydney, Australia; Kobus, C., Yvon, F., Damnati, G., Normalizing sms: Are two metaphors better than one (2008) Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), Manchester, UK, August, pp. 441-448. , Coling 2008 Organizing Committee; Church, K.W., Gale, W.A., Probability scoring for spelling correction (1991) Statistics and Computing, 1 (2), pp. 93-103; Brill, E., Moore, R.C., An improved error model for noisy channel spelling correction (2000) The 38th Annual Meeting of the Association for Computational Linguistics (ACL), , October; Toutanova, K., Moore, R.C., Pronunciation modeling for improved spelling correction (2002) The 40th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 144-151. , Philadelphia, PA, USA, July; Koehn, P., Hoang, H., Birch, A., Chris, C.-B., Federico, M., Bertoldi, N., Cowan, B., Herbst, E., Moses: Open source toolkit for statistical machine translation (2007) Annual Meeting of ACL, Demonstration Session, , Prag, Czech Republic, June; Josef Och, F., Ney, H., A systematic comparison of various statistical alignment models (2003) Computational Linguistics, 29 (1), pp. 19-51; Stolcke, A., SRilm-an extensible language modeling toolkit (2002) 7th International Conference on Spoken Language Processing (Interspeech 2002), , Denver, USA; Ross, J., Irani, L., Six Silberman, M., Zaldivar, A., Tomlinson, B., Who are the Crowdworkers?. Shifting Demographics in Amazon Mechanical Turk (2010) NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazons Mechanical Turk, , Los Angeles, California, June},
correspondence_address1={Cognitive Systems Lab, Karlsruhe Institute of Technology (KIT)Germany},
sponsors={IEE Signal Processing Society},
address={Vancouver, BC},
issn={15206149},
isbn={9781479903566},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schlippe20137329,
author={Schlippe, T. and Volovyk, M. and Yurchenko, K. and Schultz, T.},
title={Rapid bootstrapping of a Ukrainian large vocabulary continuous speech recognition system},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2013},
pages={7329-7333},
doi={10.1109/ICASSP.2013.6639086},
art_number={6639086},
note={cited By 3; Conference of 2013 38th IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2013 ; Conference Date: 26 May 2013 Through 31 May 2013;  Conference Code:101421},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890467764&doi=10.1109%2fICASSP.2013.6639086&partnerID=40&md5=7af13edb585b7164640db32ad42463b4},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology (KIT), Germany},
abstract={We report on our efforts toward an LVCSR system for the Slavic language Ukrainian. We describe the Ukrainian text and speech database recently collected as a part of our GlobalPhone corpus [1] with our Rapid Language Adaptation Toolkit [2]. The data was complemented by a large collection of text data crawled from various Ukrainian websites. For the production of the pronunciation dictionary, we investigate strategies using grapheme-to-phoneme (g2p) models derived from existing dictionaries of other languages, thereby reducing severely the necessary manual effort. Russian and Bulgarian g2p models even decrease the number of pronunciation rules to one fifth. We achieve significant improvement by applying state-of-the art techniques for acoustic modeling and our day-wise text collection and language model interpolation strategy [3]. Our best system achieves a word error rate of 11.21% on the test set on read newspaper speech. © 2013 IEEE.},
author_keywords={pronunciation dictionary;  rapid language adaptation;  Slavic language;  speech recognition;  Ukrainian},
keywords={Grapheme to phonemes;  Large vocabulary continuous speech recognition;  Pronunciation dictionaries;  rapid language adaptation;  Slavic languages;  State of the art;  Text collection;  Ukrainian, Computational linguistics;  Continuous speech recognition;  Speech recognition, Signal processing},
references={Schultz, T., Vu, N.T., Schlippe, T., GlobalPhone: A multilingual text speech database in 20 languages (2013) ICASSP; Black, A.W., Schultz, T., Rapid language adaptation tools and technologies for multilingual speech processing (2008) ICASSP; Vu, N.T., Schlippe, T., Kraus, F., Schultz, T., Rapid bootstrapping of five eastern european languages using the rapid language adaptation toolkit (2010) Interspeech; Martirosian, O., Davel, M., Error Analysis of a Public Domain Pronunciation Dictionary (2007) PRASA; Schlippe, T., Ochs, S., Schultz, T., Graphemeto-phoneme model generation for indo-european languages (2012) ICASSP; Besling, S., Heuristical and statistical methods for grapheme-to-phoneme conversion (1994) Konvens; Black, A.W., Lenzo, K., Pagel, V., Issues in building general letter to sound rules (1998) ESCA Workshop on Speech Synthesis; Bisani, M., Ney, H., Joint-sequence models for grapheme-to-phoneme conversion (2008) Speech Communication; Ghoshal, A., Jansche, M., Khudanpurv, S., Riley, M., Ulinski, M., Web-derived pronunciations (2009) ICASSP; Schlippe, T., Ochs, S., Schultz, T., Wiktionary as a source for automatic pronunciation extraction (2010) Interspeech; (2001) Ukrainian Population Census 2001: Historical, Methodological, Social, Economic and Ethnic Aspects, , http://2001.ukrcensus.gov.ua; Kramar, O., Russification Via Bilingualism, , http://www.ethnologue.com, The Ukrainian Week, 2012, http://ukrainianweek.com/Society/47497.[l3] "Ethnologue; Karpov, A., Kipyatkova, I., Ronzhin, A., Speech recognition for east slavic languages: The case of russian (2012) SLT-U; Bilous, T., (2005) IPA for Ukrainian; Buk, S.N., Macutek, J., Rovenchak, A.A., Some properties of the ukrainian writing system (2008) CoRR; Pylypenko, V., Robeyko, V., Experimental system of computerized stenographer for ukrainian speech (2009) SPECOM; Pylypenko, V., Robeiko, V., Sazhok, M., Vasylieva, N., Radoutsky, O., Ukrainian broadcast speech corpus development (2011) SPECOM; Lyudovyk, T., Robeiko, V., Pylypenko, V., Automatic recognition of spontaneous ukrainian speech based on the ukrainian broadcast speech corpus (2011) Dialog'Ii Conference; Robeiko, V., Sazhok, M., Real-time spontaneous ukrainian speech recognition system based on word acoustic composite models (2012) UkrObraz; Sazhok, M., Robeiko, V., Bidirectional text-topronunciation conversion with word stress prediction for ukrainian (2012) UkrObraz; Lytvynov, S., Prodeus, A., Modeling of ukrainian speech recognition system using htk tools Electronics and Communications, 1; Vazhenina, D., Markov, K., Phoneme set selection for russian speech recognition (2011) NLPKE' i 1; Schlippe, T., Komng, E.G., Vu, N.T., Ochs, S., Schultz, T., Hausa large vocabulary continuous speech recognition (2012) SLT-U; (2011) East Central and South-East Europe Division of the United Nations Group of Experts on Geographical Names, , Romanization System in Ukraine; (1999) International Phonetic Association, Handbook of the International Phonetic Association: A Guide to the Use of the International Phonetic Alphabet, , Cambridge University Press; Kanthak, S., Ney, H., Context-dependent acoustic modeling using graphemes for large vocabulary speech recognition (2002) ICASSP; Killer, M., Stueker, S., Schultz, T., Grapheme based speech recognition (2003) Eurospeech; Stueker, S., Schultz, T., A grapheme based speech recognition system for russian (2004) SPECOM; Vu, N.T., Schultz, T., Vietnamese large vocabulary continuous speech recognition (2009) ASRU; Soltau, H., Metze, F., Fuegen, C., Waibel, A., A one pass-decoder based on polymorphic linguistic context assignment (2001) ASRU},
correspondence_address1={Cognitive Systems Lab, Karlsruhe Institute of Technology (KIT)Germany},
sponsors={IEE Signal Processing Society},
address={Vancouver, BC},
issn={15206149},
isbn={9781479903566},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Adel20138411,
author={Adel, H. and Vu, N.T. and Kraus, F. and Schlippe, T. and Li, H. and Schultz, T.},
title={Recurrent neural network language modeling for code switching conversational speech},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2013},
pages={8411-8415},
doi={10.1109/ICASSP.2013.6639306},
art_number={6639306},
note={cited By 38; Conference of 2013 38th IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2013 ; Conference Date: 26 May 2013 Through 31 May 2013;  Conference Code:101421},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890519772&doi=10.1109%2fICASSP.2013.6639306&partnerID=40&md5=b86611848328d9309e2cb2390eb205d4},
affiliation={Cognitive Systems Lab, Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT), Germany; School of Computer Engineering, Nanyang Technological University (NTU), Singapore},
abstract={Code-switching is a very common phenomenon in multilingual communities. In this paper, we investigate language modeling for conversational Mandarin-English code-switching (CS) speech recognition. First, we investigate the prediction of code switches based on textual features with focus on Part-of-Speech (POS) tags and trigger words. Second, we propose a structure of recurrent neural networks to predict code-switches. We extend the networks by adding POS information to the input layer and by factorizing the output layer into languages. The resulting models are applied to our task of code-switching language modeling. The final performance shows 10.8% relative improvement in perplexity on the SEAME development set which transforms into a 2% relative improvement in terms of Mixed Error Rate and a relative improvement of 16.9% in perplexity on the evaluation set which leads to a 2.7% relative improvement of MER. © 2013 IEEE.},
author_keywords={code-switching;  recurrent neural network language model},
keywords={Code-switching;  Conversational speech;  Input layers;  Language model;  Mixed errors;  Network language;  Part-of-speech tags;  Textual features, Computational linguistics;  Recurrent neural networks;  Signal processing;  Speech recognition, Switching},
references={Auer, P., (1999) Code-switching in Conversation, , Routledge; Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., Khudanpur, S., Recurrent neural network based language model (2010) Proceedings of Interspeech; Mikolov, T., Kombrink, S., Burget, L., Cernocky, J.H., Khudanpur, S., Extensions of recurrent neural network language model (2011) Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference On. IEEE, pp. 5528-5531; Shi, Y., Wiggers, P., Jonker, C.M., Towards recurrent neural networks language models with linguistic and contextual features (2012) Interspeech; Poplack, S., (1978) Syntactic Structure and Social Function of codeswitching2, , Centro de Estudios Puertorriquenos,[City University of New York]; Bokamba, E.G., Are there syntactic constraints on codemixing (1989) World Englishes, 8 (3), pp. 277-292; Muysken, P., (2000) Bilingual Speech: A Typology of Code-mixing 11, , Cambridge University Press; Auer, P., From codeswitching via language mixing to fused lects toward a dynamic typology of bilingual speech (1999) International Journal of Bilingualism, 3 (4), pp. 309-332; Poplack, S., Sometimes ill start a sentence in spanish y termino en espanol: Toward a typology of code-switching1 (1980) Linguistics, 18 (7-8), pp. 581-618; Solorio, T., Liu, Y., Learning to predict code-switching points (2008) Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pp. 973-981; Chan, J.Y.C., Ching, P.C., Lee, T., Cao, H., Automatic speech recognition of cantonese-english code-mixing utterances (2006) Proceedings of Interspeech; Vu, N.T., Lyu, D.C., Weiner, J., Telaar, D., Schlippe, T., Blaicher, F., Chng, E.S., Li, H., A first speech recognition system for mandarin-english code-switch conversational speech (2012) Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference On. IEEE, pp. 4889-4892; Mikolov, T., Kombrink, S., Deoras, A., Burget, L., Cernock, Y.J., RNNLM-recurrent neural network language modeling toolkit (2011) Proceedings of the 2011 ASRUWorkshop, pp. 196-201; Lyu, D.C., Tan, T.P., Chng, E.S., Li, H., An analysis of a mandarin-english code-switching speech corpus: Seame (2010) Age, 21, pp. 25-8; Schultz, I.T., Fung, P., Burgmer, C., (2010) Detecting Code-switch Events Based on Textual Features; Toutanova, K., Klein, D., Manning, C.D., Singer, Y., Feature-rich part-of-speech tagging with a cyclic dependency network (2003) Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1. Association for Computational Linguistics, pp. 173-180; Toutanova, K., Manning, C.D., Enriching the knowledge sources used in a maximum entropy part-of-speech tagger (2000) Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora: Held in Conjunction with the 38th Annual Meeting of the Association for Computational Linguistics-Volume 13. Association for Computational Linguistics, pp. 63-70; Xue, N., Xia, F., Chiou, F.D., Palmer, M., The penn chinese treebank: Phrase structure annotation of a large corpus (2005) Natural Language Engineering, 11 (2), p. 207; Marcus, M.P., Marcinkiewicz, M.A., Santorini, B., Building a large annotated corpus of english: The penn treebank (1993) Computational Linguistics, 19 (2), pp. 313-330; Vuw. Breiter, N.T., Metze, F., Schultz, T., An investigation on initialization schemes for multilayer perceptron training using multilingual data and their effect on asr performance (2012) Interspeech; Weiner, J., Vu, N.T., Telaar, D., Metze, F., Schultz, T., Lyu, D.-C., Chng, E.-S., Li, H., Integration of language identification into a recognition system for spoken conversations containing code-switches (2012) SLTU; http://www.speech.cs.cmu.edu/cgi-bin/cmudict, CMU pronouncation dictionary for english; Hsiao, R., Fuhs, M., Tam, Y.C., Jin, Q., Schultz, T., The CMU-InterACT 2008 mandarin transcription system (2008) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing; Chen, W., Tan, Y., Chng, E., Li, H., The development of a singapore english call resource (2010) Oriental COCOSDA, , Nepal; Stolcke, A., SRILM-an extensible language modeling toolkit (2002) Proceedings of the International Conference on Spoken Language Processing, 2, pp. 901-904},
correspondence_address1={Cognitive Systems Lab, Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT)Germany},
sponsors={IEE Signal Processing Society},
address={Vancouver, BC},
issn={15206149},
isbn={9781479903566},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Amma2013147,
author={Amma, C. and Volk, H. and Schultz, T.},
title={Compressed signal representation for inertial sensor signals},
journal={UbiComp 2013 Adjunct - Adjunct Publication of the 2013 ACM Conference on Ubiquitous Computing},
year={2013},
pages={147-150},
doi={10.1145/2494091.2494140},
note={cited By 0; Conference of 2013 ACM Conference on Ubiquitous Computing, UbiComp 2013 ; Conference Date: 8 September 2013 Through 12 September 2013;  Conference Code:99854},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885210727&doi=10.1145%2f2494091.2494140&partnerID=40&md5=f95ca9c0b8ac59db1f8704f6b18a1f47},
affiliation={Karlsruhe Institute of Technology (KIT), Kronenstraße 12, Karlsruhe, 76131, Germany},
abstract={We present and evaluate a method to generate a compressed representation of multi-dimensional inertial sensor signals using a piecewise linear approximation. The representation can be computed on small sensor nodes and thus allows for a reduction of the amount of data that needs to be transmitted to the main processing node. On an existing gesture database, we present the compression rate that is reached and evaluate the quality of the representation in terms of the accuracy reached for gesture classification. We compare the results to our baseline system using a simpler approach for data reduction.},
author_keywords={Gesture recognition;  Inertial sensors;  Signal compression},
keywords={Baseline systems;  Compression rates;  Gesture classifications;  Inertial sensor;  Piecewise linear approximations;  Processing nodes;  Signal compression;  Signal representations, Gesture recognition;  Inertial navigation systems;  Piecewise linear techniques;  Sensor nodes;  Ubiquitous computing, Quality control},
references={Amma, C., Georgi, M., Schulz, T., Airwriting: A wearable handwriting recognition system (2013) Personal and Ubiquitious Computing; Junker, H., Amft, O., Lukowicz, P., Tröster, G., Gesture spotting with body-worn inertial sensors to detect user activities (2008) Journal on Pattern Recognition, 41 (6), pp. 2010-2024; Keogh, E., Chu, S., Hart, D., Pazzani, M., Segmenting time series: A survey and novel approach (2001) IEEE International Conference on Data Mining},
correspondence_address1={Karlsruhe Institute of Technology (KIT), Kronenstraße 12, Karlsruhe, 76131, Germany},
sponsors={ACM SIGCHI; ACM SIGMOBILE},
address={Zurich},
isbn={9781450322157},
language={English},
abbrev_source_title={UbiComp Adjunct - Adjunct Publ. ACM Conf. Ubiquitous Comput.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Vu2013297,
author={Vu, N.T. and Adel, H. and Schultz, T.},
title={An investigation of code-switching attitude dependent language modeling},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2013},
volume={7978 LNAI},
pages={297-308},
doi={10.1007/978-3-642-39593-2_26},
note={cited By 6; Conference of 1st International Conference on Statistical Language and Speech Processing, SLSP 2013 ; Conference Date: 29 July 2013 Through 31 July 2013;  Conference Code:98969},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883192240&doi=10.1007%2f978-3-642-39593-2_26&partnerID=40&md5=bb010bcfc75069eb72e8c28ca7a9217b},
affiliation={Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT), Germany},
abstract={In this paper, we investigate the adaptation of language modeling for conversational Mandarin-English Code-Switching (CS) speech and its effect on speech recognition performance. First, we investigate the prediction of code switches based on textual features with focus on Part-of-Speech (POS) tags. We show that the switching attitude is speaker dependent and utilize this finding to cluster the training speakers into classes with similar switching attitude. Second, we apply recurrent neural network language models which integrate the POS information into the input layer and factorize the output layer into languages for modeling CS. Furthermore, we adapt the background N-Gram and RNN language model to the different Code-Switching attitudes of the speaker clusters which lead to significant reductions in terms of perplexity. Finally, using these adapted language models we rerun the speech recognition system for each speaker and achieve improvements in terms of mixed error rate. © 2013 Springer-Verlag.},
author_keywords={code switch attitude;  language model adaptation;  multilingual speech processing},
keywords={code switch attitude;  Language model adaptation;  Multilingual speech processing;  Part-of-speech tags;  Speaker dependents;  Speech recognition performance;  Speech recognition systems;  Textual features, Computational linguistics;  Recurrent neural networks;  Speech recognition;  Switching, Speech processing},
references={Adel, H., Vu, N.T., Kraus, F., Schlippe, T., Schultz, T., Li, H., Recurrent neural network language modeling for code switching conversational speech (2012) ICASSP; Auer, P., (1999) Code-switching in Conversation, , Routledge; Auer, P., From codeswitching via language mixing to fused lects toward a dynamic typology of bilingual speech (1999) International Journal of Bilingualism, 3 (4), pp. 309-332; Bokamba, E.G., Are there syntactic constraints on code-mixing? (1989) World Englishes, 8 (3), pp. 277-292; Chan, J.Y.C., Ching, P., Lee, T., Cao, H., Automatic speech recognition of Cantonese-English code-mixing utterances Proc. Of ICSLP (2006); Kombrink, S., Mikolov, T., Karafiát, M., Burget, L., Recurrent neural network based language modeling in meeting recognition Proc. Of ICSLP (2011); Lyu, D.C., Tan, T.P., Chng, E.S., Li, H., An analysis of a Mandarin-English code-switching speech corpus: Seame (2010) ICSLP; Marcus, M.P., Marcinkiewicz, M.A., Santorini, B., Building a large annotated corpus of English: The penn treebank (1993) Computational Linguistics, 19 (2), pp. 313-330; Mikolov, T., Karafiát, M., Burget, L., Cernocky, J., Khudanpur, S., Recurrent neural network based language model Proc. Of ICSLP (2010); Mikolov, T., Kombrink, S., Burget, L., Cernocky, J., Khudanpur, S., Extensions of recurrent neural network language model (2011) ICASSP, pp. 5528-5531; Mikolov, T., Kombrink, S., Deoras, A., Burget, L., Cernockỳ, J., Rnnlm-recurrent neural network language modeling toolkit (2011) Proc. Of the 2011 ASRU Workshop, pp. 196-201; Muysken, P., (2000) Bilingual Speech: A Typology of Code-mixing, 11. , Cambridge University Press; Poplack, S., (1978) Syntactic Structure and Social Function of Code-switching, 2. , Centro de Estudios Puertorriqueños, City University of New York; Poplack, S., Sometimes i'll start a sentence in spanish y termino en español: Toward a typology of code-switching (1980) Linguistics, 18 (7-8), pp. 581-618; Shi, Y., Wiggers, P., Jonker, C.M., Towards recurrent neural networks language models with linguistic and contextual features (2012) ICSLP; Solorio, T., Liu, Y., Learning to predict code-switching points (2008) Proc. Of the EMNLP, pp. 973-981. , Association for Computational Linguistics; Steinbach, M., Karypis, G., Kumar, V., A comparison of document clustering techniques (2000) KDD Workshop on Text Mining, 400, pp. 525-526; Stolcke, A., Srilm-an extensible language modeling toolkit (2002) Proc. Of ICSLP, 2, pp. 901-904; Toutanova, K., Klein, D., Manning, C.D., Singer, Y., Feature-rich part-of-speech tagging with a cyclic dependency network (2003) Proc. Of the HLT-NAACL, pp. 173-180; Toutanova, K., Manning, C.D., Enriching the knowledge sources used in a maximum entropy part-of-speech tagger (2000) Proc. Of the 2000 Joint SIGDAT Conference EMNLP/VLC, pp. 63-70; Vu, N.T., Lyu, D.C., Weiner, J., Telaar, D., Schlippe, T., Blaicher, F., Chng, E., Li, H., A first speech recognition system for Mandarin-English code-switch conversational speech (2012) ICASSP, pp. 4889-4892; Xue, N., Xia, F., Chiou, F.D., Palmer, M., The Penn Chinese treebank: Phrase structure annotation of a large corpus (2005) Natural Language Engineering, 11 (2), p. 207},
correspondence_address1={Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT)Germany},
address={Tarragona},
issn={03029743},
isbn={9783642395925},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Stahlberg2013260,
author={Stahlberg, F. and Schlippe, T. and Vogel, S. and Schultz, T.},
title={Pronunciation extraction from phoneme sequences through cross-lingual word-to-phoneme alignment},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2013},
volume={7978 LNAI},
pages={260-272},
doi={10.1007/978-3-642-39593-2_23},
note={cited By 3; Conference of 1st International Conference on Statistical Language and Speech Processing, SLSP 2013 ; Conference Date: 29 July 2013 Through 31 July 2013;  Conference Code:98969},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883149008&doi=10.1007%2f978-3-642-39593-2_23&partnerID=40&md5=3068a127889b329737f9bd1ab7f3a90f},
affiliation={Karlsruhe Institute of Technology, Cognitive Systems Lab., Adenauerring 4, 76131 Karlsruhe, Germany; Qatar Foundation, Qatar Computing Research Institute, Al-Nasr Tower A, Doha, Qatar},
abstract={With the help of written translations in a source language, we cross-lingually segment phoneme sequences in a target language into word units using our new alignment model Model 3P [17]. From this, we deduce phonetic transcriptions of target language words, introduce the vocabulary in terms of word IDs, and extract a pronunciation dictionary. Our approach is highly relevant to bootstrap dictionaries from audio data for Automatic Speech Recognition and bypass the written form in Speech-to-Speech Translation, particularly in the context of under-resourced languages, and those which are not written at all. Analyzing 14 translations in 9 languages to build a dictionary for English shows that the quality of the resulting dictionary is better in case of close vocabulary sizes in source and target language, shorter sentences, more word repetitions, and formal equivalent translations. © 2013 Springer-Verlag.},
author_keywords={pronunciation dictionary;  speech-to-speech translation;  under-resourced languages;  word segmentation},
keywords={Automatic speech recognition;  Phonetic transcriptions;  Pronunciation dictionaries;  Speech-to-speech translation;  Target language;  Under-resourced languages;  Vocabulary size;  Word segmentation, Alignment;  Computational linguistics;  Speech processing, Translation (languages)},
references={Achtert, E., Goldhofer, S., Kriegel, H.P., Schubert, E., Zimek, A., Evaluation of Clusterings-Metrics and Visual Support (2012) ICDE; Besacier, L., Zhou, B., Gao, Y., Towards Speech Translation of Non-Written Languages (2006) SLT; Borland, J.A., (2003) The English Standard Version-A Review Article, p. 162. , Faculty Publications and Presentations; Brown, P.F., Pietra, V.J.D., Pietra, S.A.D., Mercer, R.L., The Mathematics of Statistical Machine Translation: Parameter Estimation (1993) Computational Linguistics, 19 (2), pp. 263-311; (2001) The Holy Bible: English Standard Version, , Crossway; Ester, M., Kriegel, H.P., Sander, J., Xu, X., A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases With Noise (1996) KDD; Gollan, C., Bisani, M., Kanthak, S., Schlüter, R., Ney, H., Cross Domain Automatic Transcription on the TC-STAR EPPS Corpus (2005) ICASSP; Gordon, R.G., Grimes, B.F., (2005) Ethnologue: Languages of the World, , 15th edn. SIL International; Johnson, M., Goldwater, S., Improving Non-Parameteric Bayesian Inference: Experiments on Unsupervised Word Segmentation with Adaptor Grammars (2009) HLT-NAACL; Kikui, G., Sumita, E., Takezawa, T., Yamamoto, S., Creating Corpora for Speech-to-Speech Translation (2003) Eurospeech; (1986) La Biblia de Las Américas, , http://www.lockman.org/lblainfo/, accessed on February 28, 2013; Martirosian, O., Davel, M., Error Analysis of a Public Domain Pronunciation Dictionary (2007) PRASA; Nettle, D., Romaine, S., (2000) Vanishing Voices: The Extinction of the World's Languages, , Oxford University Press; Och, F.J., Ney, H., A Systematic Comparison of Various Statistical Alignment Models (2003) Computational Linguistics, 29 (1), pp. 19-51; Rodgers, J.L., Nicewander, W.A., Thirteen Ways to Look at the Correlation Coefficient (1988) The American Statistician, 42 (1), pp. 59-66; Schultz, T., Kirchhoff, K., (2006) Multilingual Speech Processing, , Academic Press, Amsterdam; Stahlberg, F., Schlippe, T., Vogel, S., Schultz, T., Word Segmentation Through Cross-Lingual Word-to-Phoneme Alignment (2012) SLT; Stolcke, A., Konig, Y., Weintraub, M., Explicit Word Error Minimization in N-best List Rescoring (1997) Eurospeech; Stüker, S., Waibel, A., Towards Human Translations Guided Language Discovery for ASR Systems (2008) SLTU; Stüker, S., Besacier, L., Waibel, A., Human Translations Guided Language Discovery for ASR Systems (2009) Interspeech; Thomas, R.L., Bible Translations: The Link Between Exegesis and Expository Preaching (1990) The Masters Seminary Journal, 1, pp. 53-74; (2004) International Vocabulary of Basic and General Terms in Metrology, pp. 09-14. , VIM: International Organization; Vu, N.T., Kraus, F., Schultz, T., Rapid Building of an ASR System for Under-Resourced Languages Based on Multilingual Unsupervised Training (2011) Interspeech; Weide, R., (2005) The Carnegie Mellon Pronouncing Dictionary 0.6},
correspondence_address1={Karlsruhe Institute of Technology, Cognitive Systems Lab., Adenauerring 4, 76131 Karlsruhe, Germany},
address={Tarragona},
issn={03029743},
isbn={9783642395925},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wand201389,
author={Wand, M. and Schulte, C. and Janke, M. and Schultz, T.},
title={Array-based electromyographic silent speech interface},
journal={BIOSIGNALS 2013 - Proceedings of the International Conference on Bio-Inspired Systems and Signal Processing},
year={2013},
pages={89-96},
note={cited By 28; Conference of International Conference on Bio-Inspired Systems and Signal Processing, BIOSIGNALS 2013 ; Conference Date: 11 February 2013 Through 14 February 2013;  Conference Code:97038},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878000240&partnerID=40&md5=9fcefdcf8471147a2800cb3f0e405bfe},
affiliation={Cognitive Systems Lab., Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={An electromygraphic (EMG) Silent Speech Interface is a system which recognizes speech by capturing the electric potentials of the human articulatory muscles, thus enabling the user to communicate silently. This study is concerned with introducing an EMG recording system based on multi-channel electrode arrays. We first present our new system and introduce a method to deal with undertraining effects which emerge due to the high dimensionality of our EMG features. Second, we show that Independent Component Analysis improves the classification accuracy of the EMG array-based recognizer by up to 22.9% relative, which is a first example of an EMG signal processing method which is specifically enabled by our new array-based system. We evaluate our system on recordings of audible speech; achieving an optimal average word error rate of 10.9% with a training set of less than 10 minutes on a vocabulary of 108 words.},
author_keywords={Electrode array;  EMG;  EMG-based speech recognition;  Silent speech interface},
keywords={Classification accuracy;  Electrode arrays;  Electromyographic;  EMG;  EMG signal processing;  High dimensionality;  Recording systems;  Silent speech interfaces, Electromyography;  Independent component analysis;  Signal processing;  Speech recognition, Speech},
references={Belhumeur, P.N., Hespanha, J.P., Kriegman, D.J., Eigenfaces vs fisherface: Recognition using class-specific linear projection (1997) IEEE Transactions on Pattern Analysis and Machine Intelligence, 19, pp. 711-720; Bell, A.J., Sejnowski, T.I., An information-maximization approach to blind separation and blind deconvolution (1995) Neural Computation, 7, pp. 1129-1159; Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Silent speech interfaces (2010) Speech Communication, 52 (4), pp. 270-287; Freitas, J., Teixeira, A., Dias, M.S., Towards a silent speech interface for portuguese (2012) Proc. Biosignals; Hyvrinen, A., Oja, E., Independent component analysis: Algorithms and applications (2000) Neural Networks, 13, pp. 411-430; Jorgensen, C., Dusan, S., Speech interfaces based upon surface electromyography (2010) Speech Communication, 52, pp. 354-366; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proc. Inter-speech, pp. 573-576. , Pittsburgh, PA; Jung, T., Makeig, S., Humphries, C., Lee, T., Mckeown, M., Iragui, V., Sejnowski, T., Removing elec-troencephalographic artifacts by blind source separation (2000) Psychophysiology, 37, pp. 163-178; Lopez-Larraz, E., Mozos, O.M., Antelis, J.M., Minguez, J., Syllable-based speech recognition using EMG (2010) Proc. IEEE EMBS; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 331-336. , San Juan, Puerto Rico; Makeig, S., (2000) EEGLAB: Ica Toolbox for Psychophysiological Research, , www.sccn.ucsd.edu/eeglab/, WWW Site, Swartz Center for Computational Neuroscience, Institute of Neural Computation, University of San Diego California; Qiao, Z., Zhou, L., Huang, J.Z., Sparse linear discriminant analysis with applications to high dimensional low sample size data (2009) International Journal of Applied Mathematics, 39, pp. 48-60; Schultz, T., Wand, M., Modeling coarticulation in large vocabulary EMG-based speech recognition (2010) Speech Communication, 52 (4), pp. 341-353; Ueda, N., Nakano, R., Ghahramani, Z., Hinton, G.E., Split and merge EM algorithm for improving Gaussian mixture density estimates (2000) Journal of VLSI Signal Processing, 26, pp. 133-140; Wand, M., Schultz, T., Speaker-adaptive speech recognition based on surface electromyography (2010) Biomedical Engineering Systems and Technologies, Volume 52 of Communications in Computer and Information Science, pp. 271-285. , Fred, A., Filipe, J., and Gamboa, H., editors Springer Berlin Heidelberg; Winter, B.B., Webster, J.G., Driven-right-leg circuit design (1983) IEEE Trans. Biomed. Eng., BME-30, pp. 62-66},
correspondence_address1={Cognitive Systems Lab., Karlsruhe Institute of Technology, Karlsruhe, Germany},
sponsors={Inst. Syst. Technol. Inf., Control Commun. (INSTICC)},
address={Barcelona},
isbn={9789898565365},
language={English},
abbrev_source_title={BIOSIGNALS - Proc. Int. Conf. Bio-Inspired Syst. Signal Process.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Putze2013360,
author={Putze, F. and Müller, M. and Heger, D. and Schultz, T.},
title={Session-independent EEG-based workload recognition},
journal={BIOSIGNALS 2013 - Proceedings of the International Conference on Bio-Inspired Systems and Signal Processing},
year={2013},
pages={360-363},
note={cited By 1; Conference of International Conference on Bio-Inspired Systems and Signal Processing, BIOSIGNALS 2013 ; Conference Date: 11 February 2013 Through 14 February 2013;  Conference Code:97038},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877971081&partnerID=40&md5=ff4c46154a0a18d574e64adc25a60fb6},
affiliation={Institute of Anthropomatics, Cognitive Systems Lab., Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany},
abstract={In this paper, we investigate the development of a session-independent EEG-based workload recognition system with minimal calibration time. On a corpus of ten sessions with the same subject, we investigate three different approaches: Accumulation of training data, an adaptive classifier (adaptive LDA) and feature selection algorithm (based on Mutual Information) to improve generalizability of the classifier. In a detailed evalution, we investigate how each approach performs under different conditions and show how we can use those methods to improve classification accuracy by more than 22% and make transfer of models between sessions more reliable.},
author_keywords={Adaptation;  EEG;  Feature selection;  Session independence;  Workload recognition},
keywords={Adaptation;  Adaptive classifiers;  Classification accuracy;  Feature selection algorithm;  Mutual informations;  Recognition systems;  Session independence;  Workload recognition, Electroencephalography;  Feature extraction;  Signal processing, Classification (of information)},
references={Ang, K.K., Chin, Z.Y., Zhang, H., Guan, C., Filter bank common spatial pattern (FBCSP) in brain-computer interface (2008) IEEE International Joint Conference on Neural Networks. IJCNN, pp. 2390-2397. , IEEE; Clercq, W.D., Vergult, A., Vanrumste, B., Van Paesschen, W., Van Huffel, S., Canonical correlation analysis applied to remove muscle artifacts from the electroencephalogram (2006) IEEE Transactions on Biomedical Engineering, 53 (12), pp. 2583-2587; Heger, D., Putze, F., Schultz, T., An adaptive information system for an empathic robot using EEG data (2010) Social Robotics, Volume 6414 of Lecture Notes in Computer Science, pp. 151-160. , Ge, S., Li, H., Cabibihan, J.-J., and Tan, Y., editors Springer Berlin / Heidelberg; Jarvis, J., Putze, F., Heger, D., Schultz, T., (2011) Multimodal Person Independent Recognition of Workload Related Biosignal Patterns, p. 205. , ACM Press; Kothe, C., Makeig, S., Estimation of task workload from EEG data: New and current tools and perspectives (2011) 2011 Annual International Conference of the IEEE Engineering in Medicine and Biology So-ciety, EMBC, pp. 6547-6551; Lansdown, T., Brook-Carter, N., Kersloot, T., Distraction from multiple in-vehicle secondary tasks: Vehicle performance and mental workload implications (2004) Ergonomics, 47 (1), pp. 91-104; Mattes, S., The lane-change-task as a tool for driver distraction evaluation (2003) Proceedings of IGfA; Shenoy, P., Krauledat, M., Blankertz, B., Rao, R.P.N., Mller, K.-R., Towards adaptive classification for BCI (2006) Journal of Neural Engineering, 3 (1), pp. R13-R23; Vidaurre, C., Schloegl, A., Blankertz, B., Kawanabe, M., Mller, K.-R., Unsupervised adaptation of the lda classifier for brain-computer interfaces (2008) Proceedings of the 4th International Brain-Computer Interface Workshop and Training Course, pp. 122-127},
correspondence_address1={Institute of Anthropomatics, Cognitive Systems Lab., Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany},
sponsors={Inst. Syst. Technol. Inf., Control Commun. (INSTICC)},
address={Barcelona},
isbn={9789898565365},
language={English},
abbrev_source_title={BIOSIGNALS - Proc. Int. Conf. Bio-Inspired Syst. Signal Process.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rebelo2013368,
author={Rebelo, D. and Amma, C. and Gamboa, H. and Schultz, T.},
title={Human activity recognition for an intelligent knee orthosis},
journal={BIOSIGNALS 2013 - Proceedings of the International Conference on Bio-Inspired Systems and Signal Processing},
year={2013},
pages={368-371},
note={cited By 7; Conference of International Conference on Bio-Inspired Systems and Signal Processing, BIOSIGNALS 2013 ; Conference Date: 11 February 2013 Through 14 February 2013;  Conference Code:97038},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877964940&partnerID=40&md5=f7130de33fc0c76b1df73f480666c5b7},
affiliation={CEFITEC, Physics Department, FCT-UNL, 2829-516, Caparica, Portugal; Cognitive System Lab. (CSL), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; PLUX - Wireless Biosignals S.A, 1050-059, Lisbon, Portugal},
abstract={This paper investigates the possibility to classify isolated human activities from biosignal sensors integrated into a knee orthosis. An intelligent orthosis that is capable to recognize its wearers activity would be able to adapt itself to the users situation for enhanced comfort. We use a setup with three modalities: accelerometry, electromyography and goniometry to measure leg motion and muscle activity of the wearer. We segment signals in motion primitives and apply Hidden Markov Models to classify these isolated motion primitives. We discriminate between seven activities like for example walking stairs and ascend or descend a hill. In a small user study we reach an average person-dependent accuracy of 98% and a person-independent accuracy of 79%.},
author_keywords={Biosignals;  Hidden Markov models;  Human activity recognition;  Signal-processing},
keywords={Biosignal sensors;  Biosignals;  Human activities;  Human activity recognition;  Motion primitives;  Muscle activities;  Person-dependent;  Person-independent, Electromyography;  Hidden Markov models;  Pattern recognition;  Signal processing, Motion estimation},
references={Amma, C., Georgi, M., Schultz, T., Airwriting: Hands-free mobile text input by spotting and continuous recognition of 3d-space handwriting with inertial sensors (2012) Wearable Computers (ISWC), pp. 52-59. , 2012 16th International Symposium on IEEE; Lukowicz, P., Ward, J., Junker, H., Stger, M., Trster, G., Atrash, A., Starner, T., Recognizing workshop activity using body worn microphones and ac-celerometers (2004) Pervasive Computing, Volume 3001 of Lecture Notes in Computer Science, pp. 18-32. , Springer; Mathie, M., Coster, A., Lovell, N., Celler, B., Detection of daily physical activities using a triaxial accelerometer (2003) Medical and Biological Engineering and Computing, 41 (3), pp. 296-301; Rabiner, L., A tutorial on hidden Markov models and selected applications in speech recognition (1989) Proceedings of the IEEE, 77 (2), pp. 257-286; Rowe, P., Myles, C., Walker, C., Nutton, R., Knee joint kinematics in gait and other functional activities measured using flexible electrogoniometry: How much knee motion is sufficient for normal daily life? (2000) Gait & Posture, 12 (2), pp. 143-155; Suda, E., (2011) Análise Eletromiográfica Comparativa de Tornozelo Durante a Aterrissagem Em Jogadores de Vôlei Com Instabilidade Crônica; Sutherland, D., The evolution of clinical gait analysis: Part ii kinematics (2002) Gait & Posture, 16 (2), pp. 159-179; Welk, G., Differding, J., The utility of the digi-walker step counter ro assess daily physical activity patterns (2000) Medicine and Science in Sports and Exercise, 9 (32), pp. 481-488},
correspondence_address1={CEFITEC, Physics Department, FCT-UNL, 2829-516, Caparica, Portugal},
sponsors={Inst. Syst. Technol. Inf., Control Commun. (INSTICC)},
address={Barcelona},
isbn={9789898565365},
language={English},
abbrev_source_title={BIOSIGNALS - Proc. Int. Conf. Bio-Inspired Syst. Signal Process.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Putze2013347,
author={Putze, F. and Heger, D. and Müller, M. and Waldkirch, C. and Chassein, Y. and Kajic, I. and Schultz, T.},
title={Profiling arousal in response to complex stimuli using biosignals},
journal={BIOSIGNALS 2013 - Proceedings of the International Conference on Bio-Inspired Systems and Signal Processing},
year={2013},
pages={347-350},
note={cited By 0; Conference of International Conference on Bio-Inspired Systems and Signal Processing, BIOSIGNALS 2013 ; Conference Date: 11 February 2013 Through 14 February 2013;  Conference Code:97038},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877985343&partnerID=40&md5=67a6d85889d444c3147fa1d3890e5d15},
affiliation={Institute of Anthropomatics, Cognitive Systems Lab., Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany},
abstract={We investigate the use of biosignals (blood volume pressure and electrodermal activity) for person-independent profiling of arousal responses to complex, long-term stimuli. We report the design of a user study with 14 subjects to elicitate affective responses with films of different genres. We present a detailed analysis of the recorded signals and show that it is possible extract information on the differences between films and within each film from biosignals. We use this information to automatically discriminate four film classes in a person-independent fashion with an accuracy of up to 97.8%.},
author_keywords={Affective computing;  Biosignal-based arousal profiling;  Person-independent classification},
keywords={Affective Computing;  Affective response;  Biosignal-based arousal profiling;  Blood volume pressures;  Electrodermal activity;  Extract informations;  Person-independent;  Recorded signals, Electrical engineering, Signal processing},
references={Bradley, M., Lang, P., Measuring emotion: The self-assessment manikin and the semantic differential (1994) Journal of Behavior Therapy and Experimental Psychiatry, 25 (1), pp. 49-59; Ekman, P., Friesen, W., Simons, R., Is the startle reaction an emotion? (1985) Journal of Personality and Social Psychology, 49 (5), p. 1416; Lichtenstein, A., Oehme, A., Kupschick, S., Jrgensohn, T., Comparing two emotion models for deriving affective states from physiological data (2008) Affect and Emotion in Human-Computer Interaction, Volume 4868 of LNCS, pp. 35-50. , Peter, C. and Beale, R., editors Springer Berlin / Heidelberg; Picard, R., (2000) Affective Computing, , The MIT press; Picard, R., Vyzas, E., Healey, J., Toward machine emotional intelligence: Analysis of affective physiological state (2001) Transactions on Pattern Analysis and Machine Intelligence, pp. 1175-1191; Soleymani, M., Chanel, G., Kierkels, J.J.M., Pun, T., Affective characterization of movie scenes based on multimedia content analysis and user's physiological emotional responses (2008) Multimedia, pp. 228-235. , International Symposium on, 0},
correspondence_address1={Institute of Anthropomatics, Cognitive Systems Lab., Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany},
sponsors={Inst. Syst. Technol. Inf., Control Commun. (INSTICC)},
address={Barcelona},
isbn={9789898565365},
language={English},
abbrev_source_title={BIOSIGNALS - Proc. Int. Conf. Bio-Inspired Syst. Signal Process.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Putze2013129,
author={Putze, F. and Hild, J. and Kärgel, R. and Herff, C. and Redmann, A. and Beyerer, J. and Schultz, T.},
title={Locating user attention using eye tracking and EEG for spatio-temporal event selection},
journal={International Conference on Intelligent User Interfaces, Proceedings IUI},
year={2013},
pages={129-135},
doi={10.1145/2449396.2449415},
note={cited By 16; Conference of 18th International Conference on Intelligent User Interfaces, IUI 2013 ; Conference Date: 19 March 2013 Through 22 March 2013;  Conference Code:96418},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875837013&doi=10.1145%2f2449396.2449415&partnerID=40&md5=4369c37633d94e2f73dc903f81d9575e},
affiliation={Cognitive Systems Lab., Karlsruhe Institute of Technology, Germany; Vision and Fusion Laboratory, Karlsruhe Institute of Technology, Germany; Fraunhofer IOSB, Germany},
abstract={In expert video analysis, the selection of certain events in a continuous video stream is a frequently occurring operation, e.g., in surveillance applications. Due to the dynamic and rich visual input, the constantly high attention and the required hand-eye coordination for mouse interaction, this is a very demanding and exhausting task. Hence, relevant events might be missed. We propose to use eye tracking and electroencephalography (EEG) as additional input modalities for event selection. From eye tracking, we derive the spatial location of a perceived event and from patterns in the EEG signal we derive its temporal location within the video stream. This reduces the amount of the required active user input in the selection process, and thus has the potential to reduce the user's workload. In this paper, we describe the employed methods for the localization processes and introduce the developed scenario in which we investigate the feasibility of this approach. Finally, we present and discuss results on the accuracy and the speed of the method and investigate how the modalities interact. Copyright © 2013 ACM.},
author_keywords={EEG;  Event detection;  Expert video analysis;  Eye tracking},
keywords={Event detection;  Eye-tracking;  Hand eye coordination;  Input modalities;  Spatial location;  Spatio-temporal events;  Surveillance applications;  Video analysis, Electroencephalography;  Electrophysiology;  Video streaming, Security systems},
references={Bechara, A., Damasio, H., Damasio, A.R., Emotion, decision making and the orbitofrontal cortex (2000) Cerebral Cortex, 10 (3), pp. 295-307. , March 1; Hakenberg, J.P., (2011) Estimation of Gaze from EEG for Cursor Control, , http://hakenberg.de; Schlögl, A., Keinrath, C., Zimmermann, D., Scherer, R., Leeb, R., Pfurtscheller, G., A fully automated correction method of EOG artifacts in EEG recordings (2007) Clinical Neurophysiology, 118 (1), pp. 98-104. , January; Krusienski, D.J., Sellers, E.W., McFarland, D.J., Vaughan, T.M., Wolpaw, J.R., Toward enhanced P300 speller performance (2008) Journal of Neuroscience Methods, 167 (1), pp. 15-21. , January 15; Hsu, C.W., Chang, C.C., Lin, C.J.A., (2003) Practical Guide to Support Vector Classification; Shenoy, P., Desney, S.T., Human-aided computing: Utilizing implicit human processing to classify images Proceedings of the Twenty-sixth Annual SIGCHI Conference on Human Factors in Computing Systems, pp. 845-854. , CHI'08. New York, NY, USA; Gerson, A.D., Parra, L.C., Sajda, P., Cortically coupled computer vision for rapid image search (2006) IEEE Transactions on Neural Systems and Rehabilitation Engineering, 14 (2), pp. 174-179. , June; Yong, X., Fatourechi, M., Ward, R.K., Birch, G.E., The design of a point-and-click system by integrating a self-paced brain computer interface with an eye-tracker (2011) IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 1 (4), pp. 590-602. , December; Zander, T.O., Gaertner, M., Kothe, C., Vilimek, R., Combining eye gaze input with a brain-computer interface for touchless human-computer interaction (2010) International Journal of Human-Computer Interaction, 27 (1), pp. 38-51; Brunner, P., Joshi, S., Briskin, S., Wolpaw, J.R., Bischof, H., Schalk, G., Does the'P300' speller depend on eye gaze? (2010) Journal of Neural Engineering, 7 (5), p. 056013. , October 1; Jung, T.-P., Makeig, S., Humphries, C., Lee, T.-W., McKeown, M.J., Iragui, V., Sejnowski, T.J., Removing electroencephalographic artifacts by blind source separation (2000) Psychophysiology, 37 (2), pp. 163-178; Blankertz, B., Lemm, S., Treder, M., Haufe, S., Müller, K.-R., Single-trial analysis and classification of ERP components - A tutorial (2011) NeuroImage, 56 (2), pp. 814-825. , May 15; Chang, C., Lin, C., LIBSVM: A library for support vector machines (2011) ACM Trans. Intell. Syst. Technol, 2 (3), pp. 271-2727. , May; Venkataramanan, S., Prabhat, P., Choudhury, S.R., Nemade, H.B., Sahambi, J.S., Biomedical instrumentation based on electrooculogram (EOG) signal processing and application to a hospital alarm system (2005) Proc. ICISIP 2005, pp. 535-540. , IEEE Computer Society Washington, D.C., USA; Gezeck, S., Fischer, B., Timmer, J., Saccadic reaction times: A statistical analysis of multimodal distributions (1997) Vision Research, 37 (15), pp. 2119-2131; Holmqvist, K., Nyström, M., Andersson, R., Dewhurst, R., Jarodzka, H., Van De Weijer, J., (2011) Eye Tracking: A Comprehensive Guide to Methods and Measures, , OUP Oxford; Salvucci, D.D., Goldberg, J.H., Identifying fixations and saccades in eye-tracking protocols (2000) Proc. ETRA 2000, pp. 71-78. , ACM Press; Sibert, L.E., Jacob, R.J.K., Evaluation of eye gaze interaction (2000) Proc. of the SIGCHI Conference on Human Factors in Computing Systems, pp. 281-288. , ACM Press; Zhang, X., MacKenzie, I.S., Evaluating eye tracking with ISO 9241 - Part 9 (2007) Human-Computer Interaction. HCI Intelligent Multimodal Interaction Environments, pp. 779-788. , Springer; MacKenzie, I.S., An eye on user input: Research challenges in using the eye for computer input control (2010) Proc. ETRA 2010, pp. 11-12. , ACM Press; Ware, C., Mikaelian, H.H., An evaluation of an eye tracker as a device for computer input (1987) ACM SIGCHI Bulletin, 18 (4), pp. 183-188; Jacob, R.J.K., The use of eye movements in humancomputer interaction techniques: What you look at is what you get (1991) ACM Transactions on Information Systems (TOIS), 9 (2), pp. 152-169},
correspondence_address1={Putze, F.; Cognitive Systems Lab., Karlsruhe Institute of TechnologyGermany; email: felix.putze@kit.edu},
sponsors={ACM SIGART; ACM SIGCHI},
address={Santa Monica, CA},
isbn={9781450320559},
language={English},
abbrev_source_title={Int Conf Intell User Interfaces Proc IUI},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vu2013515,
author={Vu, N.T. and Schultz, T.},
title={Multilingual multilayer perceptron for rapid language adaptation between and across language families},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2013},
pages={515-519},
note={cited By 17; Conference of 14th Annual Conference of the International Speech Communication Association, INTERSPEECH 2013 ; Conference Date: 25 August 2013 Through 29 August 2013;  Conference Code:106915},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906275422&partnerID=40&md5=b9a873202754f417bd776068f1d6ae5a},
affiliation={Institute for Anthropomatics, United States},
abstract={In this paper, we present our latest investigations of multilingual Multilayer Perceptrons (MLPs) for rapid language adaptation between and across language families. We explore the impact of the amount of languages and data used for the multilingual MLP training process. We show that the overall system performance on the target language is significantly improved by initializing it with a multilingual MLP. Our experiments indicate that the more languages we use to train a multilingual MLP, the better is the initialization for MLP training. As a result, the ASR performance is improved, even if the target language and the source languages are not in the same language family. Our best results show an error rate improvement of up to 22.9% relative for different target languages (Czech, Hausa and Vietnamese) by using a multilingual MLP which has been trained with many different languages from the GlobalPhone corpus. In the case of very few training or adaptation data, an improvement of up to 24% relative in terms of error rate is observed. Copyright © 2013 ISCA.},
author_keywords={Multilingual bottle-neck feature;  Multilingual speech processing;  Rapid language adaptation},
keywords={Bottles;  Speech processing, Multi layer perceptron;  Multi-layer perceptrons (MLPs);  Multilingual bottle-neck feature;  Multilingual speech processing;  Rapid language adaptation;  Source language;  Target language;  Training process, Electric loads},
references={Hermansky, H., Wellis, D., Sharma, S., Tandem connectionist feature extraction for conventional HMMsystems (2000) Proc. ICASSP, Turkey; Grezl, F., Probabilistic and bottle-neck features for LVCSR of meetings (2007) Proc. ICASSP, , USA; Stolcke, A., Grezl, F., Hwang, M.-Y., Lei, X., Morgan, N., Vergyri, D., Cross-domain and cross-lingual portability of acoustic features estimated by multilayer perceptrons (2006) Proc. ICASSP; Toth, L., Frankel, J., Gosztolya, G., King, S., Cross-lingual portability of MLP-based tandem features - A case study for English and Hungarian (2008) Proc. Interspeech; Cetin, O., Magimai-Doss, M., Livescu, K., Kantor, A., King, S., Bartels, C., Frankel, J., Monolingual and crosslingual comparison of tandem features derived from articulatory and phone MLPs (2007) Proc. ASRU; Imseng, D., Bourlard, H., Magimai-Doss, M., Towards mixed language speech recognition systems (2010) Proc. Interspeech, , Japan; Plahl, C., Schlueter, R., Ney, H., Cross-lingual portability of Chinese and English neural network features for French and German LVCSR (2011) Proc. ASRU, , USA; Thomas, S., Ganapathy, S., Hermansky, H., Multilingual MLP features for low-resource LVCSR systems (2012) Proc. ICASSP, , Japan; Thomas, S., Ganapathy, S., Jansen, A., Hermansky, H., Datadriven posterior features for low resource speech recognition applications (2012) Proc. Interspeech, , USA; Vesely, K., Karafiat, M., Grezl, F., Janda, M., Egorova, E., The language-independent bottleneck features (2012) Proc. SLT, , USA; Vu, N.T., Metze, F., Schultz, T., Multilingual bottle-neck feature for under resourced languages (2012) Proc. SLTU, , South Africa; Vu, N.T., Breiter, W., Metze, F., Schultz, T., Initialization schemes for multilayer perceptron training and their effects on ASR using multilingual data (2012) Proc. Interspeech, , USA; Schultz, T., Vu, N.T., Schlippe, T., Globalphone: A multilingual text & speech database in 20 languages (2013) Proc. ICASSP, , Canada; Vu, N.T., Schlippe, T., Kraus, F., Schultz, T., Rapid bootstrapping of five eastern european languages using the rapid language adaptation toolkit (2010) Proc. Interspeech, , Japan; Schultz, T., Waibel, A., Language independent and language adaptive acoustic modeling for speech recognition (2001) Speech Communication, 35 (1-2), pp. 31-51; Vu, N.T., Schultz, T., Vietnamese large vocabulary continuous speech recognition (2009) Proc. ASRU, , Italy; Metze, F., Hsiao, R., Jin, Q., Nallasamy, U., Schultz, T., The 2010 CMU GALE speech-to-text system (2010) Proc. Interspeech, , Japan; http://www.icsi.berkeley.edu/Speech/qn.html},
sponsors={Amazon; et al.; European Language Resources Association (ELRA); Google; Microsoft; Sytral},
publisher={International Speech and Communication Association},
address={Lyon},
issn={2308457X},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Adel2013206,
author={Adel, H. and Vu, N.T. and Schultz, T.},
title={Combination of recurrent neural networks and factored language models for code-switching language modeling},
journal={ACL 2013 - 51st Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference},
year={2013},
volume={2},
pages={206-211},
note={cited By 34; Conference of 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013 ; Conference Date: 4 August 2013 Through 9 August 2013;  Conference Code:107371},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907341834&partnerID=40&md5=554adc0b71e0f2491d28927c69fbd71c},
affiliation={Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT), Germany},
abstract={In this paper, we investigate the application of recurrent neural network language models (RNNLM) and factored language models (FLM) to the task of language modeling for Code-Switching speech. We present a way to integrate part-of-speech tags (POS) and language information (LID) into these models which leads to significant improvements in terms of perplexity. Furthermore, a comparison between RNNLMs and FLMs and a detailed analysis of perplexities on the different backoff levels are performed. Finally, we show that recurrent neural networks and factored language models can be combined using linear interpolation to achieve the best performance. The final combined language model provides 37.8% relative improvement in terms of perplexity on the SEAME development set and a relative improvement of 32.7% on the evaluation set compared to the traditional n-gram language model. © 2013 Association for Computational Linguistics.},
author_keywords={Code switching;  Factored language models;  Language modeling;  Multilingual speech processing;  Recurrent neural networks},
keywords={Codes (symbols);  Recurrent neural networks;  Speech processing, Backoff;  Code-switching;  Language informations;  Language model;  Linear Interpolation;  Multilingual speech processing;  N-gram language models;  Part-of-speech tags, Computational linguistics},
references={Adel, H., Vu, N.T., Kraus, F., Schlippe, T., Schultz, T., Recurrent neural network language modeling for code switching conversational speech (2013) Proceedings of ICASSP 2013; Auer, P., (1999) Code-Switching in Conversation, , Routledge; Auer, P., From codeswitching via language mixing to fused lects toward a dynamic typology of bilingual speech (1999) International Journal of Bilingualism, 3 (4), pp. 309-332; Bilmes, J.A., Kirchhoff, K., Factored language models and generalized parallel backoff (2003) Proceedings of NAACL, 2003; Bokamba, E.G., Are there syntactic constraints on code-mixing? (1989) World Englishes, 8 (3), pp. 277-292; Chan, J.Y.C., Ching, P.C., Lee, T., Cao, H., Automatic speech recognition of cantonese-english code-mixing utterances (2006) Proceeding of Interspeech 2006; Duh, K., Kirchhoff, K., Automatic learning of language model structure (2004) Proceedings of the 20th International Conference on Computational Linguistics, p. 148; El-Desoky, A., Schlüter, R., Ney, H., A hybrid morphologically decomposed factored language models for arabic LVCSR (2010) NAACL 2010; Lyu, D.C., Tan, T.P., Cheng, E.S., Li, H., An analysis of mandarin-english code-switching speech corpus: SEAME (2011) Proceedings of Interspeech 2011; Marcus, M.P., Marcinkiewicz, M.A., Santorini, B., Building a large annotated corpus of english: The penn treebank (1993) Computational Linguistics, 19 (2), pp. 313-330; Mikolov, T., Karafiat, M., Burget, L., Jernocky, J., Khudanpur, S., Recurrent neural network based language model (2010) Proceedings of Interspeech 2010; Mikolov, T., Kombrink, S., Burget, L., Jernocky, J., Khudanpur, S., Extensions of recurrent neural network language model (2011) Proceedings of ICASSP 2011; Muysken, P., (2000) Bilingual Speech: A Typology of Code-mixing, 11. , Cambridge University Press; Oparin, I., Sundermeyer, M., Ney, H., Gauvain, J.-L., Performance analysis of neural networks in combination with n-gram language models (2012) ICASSP, 2012; Poplack, S., Syntactic structure and social function of code-switching (1978) Centro de Estudios Puertorriquenos, , City University of New York; Poplack, S., Sometimes ill start a sentence in spanish y termino en espanol: Toward a typology of code-switching (1980) Linguistics, 18 (7-8), pp. 581-618; Rosenfeld, R., Two decades of statistical language modeling: Where do we go from here? (2000) Proceedings of the IEEE, 88 (8), pp. 1270-1278. , (2000); Schultz, T., Fung, P., Burgmer, C., (2010) Detecting Code-switch Events Based on Textual Features; Shi, Y., Wiggers, P., Jonker, M., Towards recurrent neural network language model with linguistics and contextual features (2011) Proceedings of Interspeech 2011; Solorio, T., Liu, Y., Part-of-speech tagging for english-spanish code-switched text (2008) Proceedings of the Conference on Empirical Methods in Natural Language Processing, , Association for Computational Linguistics, 2008; Solorio, T., Liu, Y., Learning to predict codeswitching points (2008) Proceedings of the Conference on Empirical Methods in Natural Language Processing, , Association for Computational Linguistics, 2008; Toutanova, K., Manning, C.D., Enriching the knowledge sources used in a maximum entropy part-of-speech tagger (2000) Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora: Held in Conjunction with the 38th Annual Meeting of the Association for Computational Linguistics, 13; Toutanova, K., Klein, D., Manning, C.D., Singer, Y., Feature-rich part-of-speech tagging with a cyclic dependency network (2003) Proceedings of NAACL 2003; Vu, N.T., Lyu, D.C., Weiner, J., Telaar, D., Schlippe, T., Blaicher, F., Chng, E.S., Li, H., A first speech recognition system for mandarin-english code-switch conversational speech (2012) Proceedings of Interspeech 2012; Vu, N.T., Adel, H., Schultz, T., An investigation of code-switching attitude dependent language modeling (2013) Statistical Language and Speech Processing, First International Conference, 2013; Xue, N., Xia, F., Chiou, F.D., Palmer, M., The penn Chinese treebank: Phrase structure annotation of a large corpusk (2005) Natural Language Engineering, 11 (2), p. 207},
sponsors={Baidu; et al.; Google; Microsoft Research; ontotext; Qatar Computing Research Institute (QCRI)},
publisher={Association for Computational Linguistics (ACL)},
address={Sofia},
isbn={9781937284510},
language={English},
abbrev_source_title={ACL - Annu. Meet. Assoc. Comput. Linguist., Proc. Conf.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Premkumar20132202,
author={Premkumar, M.J.J. and Vu, N.T. and Schultz, T.},
title={Experiments towards a better LVCSR system for Tamil},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2013},
pages={2202-2206},
note={cited By 2; Conference of 14th Annual Conference of the International Speech Communication Association, INTERSPEECH 2013 ; Conference Date: 25 August 2013 Through 29 August 2013;  Conference Code:106915},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906247137&partnerID=40&md5=81e9a157d22465d0a5db1c30d76ed06d},
affiliation={Department of Computer Technology, Anna University, Chennai, India; Cognitive Systems Lab, Institute for Anthropomatics, United States},
abstract={This paper summarizes our latest efforts in the development of a Large Vocabulary Continuous Speech Recognition (LVCSR) system for Tamil at different levels: pronunciation dictionary, language modeling (LM) and front-end. Usually in Tamil there are not many word-pronunciation pairs to train data-driven grapheme-to-phoneme (G2P) converters. Therefore, we explore the correlation between the amount of training data and the performance of the grapheme-to-phoneme (G2P) conversion. To address the morphological complexity of Tamil, we investigate different levels of morphemes for language modeling including a comparison between our Dictionary Unit Merging Algorithm (DUMA) and Morfessor, followed by various experiments on hybrid systems using word and morpheme LMs. Finally, we integrate our multilingual bottle-neck features framework with Tamil LVCSR. The final best system produced 21.34% Syllable Error Rate (SyllER) on our Tamil test set. Copyright © 2013 ISCA.},
author_keywords={Hybrid system;  Language model;  Morpheme segmentation;  Multilingual bottle-neck features;  Tamil LVCSR},
keywords={Computational linguistics;  Continuous speech recognition;  Hybrid systems, Grapheme to phonemes;  Grapheme-to-phoneme conversion;  Language model;  Large vocabulary continuous speech recognition;  Morphological complexity;  Multilingual bottle-neck features;  Pronunciation dictionaries;  Tamil LVCSR, Experiments},
references={Jose, J.M., Vu, N.T., Schultz, T., Initial experiments with Tamil LVCSR (2012) Proceedings of IALP; Kumar, R., Kishore, S., Gopalakrishna, A., Chitturi, R., Joshi, S., Singh, S., Sitaram, R., Development of indian language speech databases for large vocabulary speech recognition systems (2005) Proceedings of SPECOM; Plauche, M., Udhyakummar, N., Wooters, C., Pal, J., Ramachadran, D., Speech recognition for illiterate access to information and technology (2006) Proceedings of First International Conference on ICT and Development; Lakshmi, A., Murthy, H.A., A syllable based continuous speech recognizer for Tamil (2006) Proceedings of Interspeech; Thangarajan, R., Natarajan, A.M., Selvam, M., Word and triphone based approaches in continuous speech recognition for Tamil language (2008) WSEAS Trans. Sig. Proc, 4 (3), pp. 76-85; Saraswathi, S., Geetha, T.V., Design of language models at various phases of Tamil speech recognition system (2010) International Journal of Engineering, Science and Technology, 2 (5), pp. 244-257; Saraswathi, S., Geetha, T.V., (2004) Building Language Models for Tamil Speech Recognition System, 3285, pp. 161-168. , LNCS; Saraswathi, S., Geetha, T.V., Morpheme based language model for Tamil speech recognition system (2007) The International Arab Journal of Information Technology, 4 (3). , July; Creutz, M., Hirsimaki, T., Kurimo, M., Puurula, A., Pylkkonen, J., Siivola, V., Varjokallio, M., Stolcke, A., Morph-based speech recognition and modeling of out-of-vocabulary words across languages (2007) ACM Transactions on Speech and Language Processing, 5 (1). , Dec; Vu, N.T., Metze, F., Schultz, T., Multilingual bottleneck features and its application for under-resourced languages (2012) Proceedings of SLTU; Vu, N.T., Breiter, W., Metze, F., Schultz, T., Initialization schemes for multilayer perceptron training and their impact on ASR performance using multilingual data (2012) Proceedings of Interspeech; Vu, N.T., Schlippe, T., Kraus, F., Schultz, T., Rapid bootstrapping of five Eastern European languages using the rapid language adaptation toolkit (2010) Proceedings of Interspeech; Kumar, C.S., Shunmugom, V., Nallsamy, U., Srinivasan, R., Rule-based automatic grapheme to phoneme conversion for Tamil (2004) Proceedings of ICSLT; Nallasamy, U., Kumar, C.S., Srinivasan, R., Swaminathan, R., Decision tree learning for automatic grapheme to phoneme conversion for tamil (2004) Proceedings of SPECOM; Hahn, S., Vozila, P., Bisani, M., Comparison of grapheme-to-phoneme methods on large pronunciation dictionaries and LVCSR tasks (2012) Proceedings of Interspeech; Bisani, M., Ney, H., Joint-sequence models for graphemeto- phoneme conversion (2008) Speech Communication, 50 (5), pp. 434-451; Novak, J., Yang, D., Minematsu, N., Hirose, K., Initial and Evaluations of An Open Source WFST-based Phoneticizer, , The University of Tokyo, Tokyo Institute of Technology; Yang, D., Rapid development of a G2P system based on WFST framework (2009) ASJ 2009 Autumn Session, pp. 111-112; Allauzen, C., Riley, M., Schalkwyk, J., Skut, W., Mohri, M., (2007) OpenFst: A General and Efficient Weighted Finitestate Transducer Library, pp. 11-23. , Prague, Czech Republic; Anandan, P., Saravanan, K., Parthasarathi, R., Geetha, T.V., Morphological analyzer for tamil (2002) Proceedings of ICON; Hacioglu, K., Pellom, B., On lexicon creation for Turkish LVCSR (2003) Proceedings of Eurospeech; El-Desoky, A., Gollan, C., Rybach, D., Schluter, R., Ney, H., Investigating the use of morphological decomposition and diacritization for improving Arabic LVCSR (2009) Proceedings of Interspeech; Byrne, W., Hajic, J., Ircing, P., Krbec, P., Psutka, J., Morpheme based language models for speech recognition of czech (2000) Text, Speech and Dialogue, 1902, pp. 139-162. , ser. LNCS; Adda-Decker, M., A corpus-based decompounding algorithm for German lexical modeling in LVCSR (2003) Proceedings of Eurospeech; Kneissler, J., Klakow, D., Speech recognition for huge vocabularies by using optimized sub-word units (2001) Proceedings of Eurospeech; Kiecza, D., Schultz, T., Waibel, A., Data-driven determination of appropriate dictionary units for Korean LVCSR (1999) Proceedings of ICASSP; Stolcke, A., SRILM - An extensible language modeling toolkit (2002) Proceedings of ICSLP; El-Desoky, A., Shaik, M.A.B., Schluter, R., Ney, H., Morpheme based factored language models for German LVCSR (2011) Proceedings of Interspeech; http://www.icsi.berkeley.edu/Speech/qn.html; Kirchhoff, K., Vergyri, D., Bilmes, J., Duh, K., Stolcke, A., Morphology-based language modeling for arabic speech recognition (2004) Proceedings of ICSLP},
sponsors={Amazon; et al.; European Language Resources Association (ELRA); Google; Microsoft; Sytral},
publisher={International Speech and Communication Association},
address={Lyon},
issn={2308457X},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schlippe20132698,
author={Schlippe, T. and Gren, L. and Vu, N.T. and Schultz, T.},
title={Unsupervised language model adaptation for automatic speech recognition of broadcast news using web 2.0},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2013},
pages={2698-2702},
note={cited By 12; Conference of 14th Annual Conference of the International Speech Communication Association, INTERSPEECH 2013 ; Conference Date: 25 August 2013 Through 29 August 2013;  Conference Code:106915},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906244264&partnerID=40&md5=33f9407bd3aceb358e191aabcf988bb2},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology (KIT), Germany},
abstract={We improve the automatic speech recognition of broadcast news using paradigms from Web 2.0 to obtain time- And topicrelevant text data for language modeling. We elaborate an unsupervised text collection and decoding strategy that includes crawling appropriate texts from RSS Feeds, complementing it with texts from Twitter, language model and vocabulary adaptation, as well as a 2-pass decoding. The word error rates of the tested French broadcast news shows from Europe 1 are reduced by almost 32% relative with an underlying language model from the GlobalPhone project [1] and by almost 4% with an underlying language model from the Quaero project. The tools that we use for the text normalization, the collection of RSS Feeds together with the text on the related websites, a TF-IDF-based topic words extraction, as well as the opportunity for language model interpolation are available in our Rapid Language Adaptation Toolkit [2] [3]. Copyright © 2013 ISCA.},
author_keywords={Automatic speech recognition;  Language modeling;  Text crawling;  Web 2.0},
keywords={Data mining;  Decoding;  Speech recognition;  World Wide Web, Automatic speech recognition;  Decoding strategy;  Language model;  Language model adaptation;  Text crawling;  Text normalizations;  Underlying language;  Web 2.0, Computational linguistics},
references={Schultz, T., Vu, N.T., Schlippe, T., Global phone: A multilingual text & speech database in 20 languages (2013) The 38th International Conference on Acoustics, Speech, and Signal Processing (ICASSP), , Vancouver, Canada; Black, A.W., Schultz, T., Rapid language adaptation tools and technologies for multilingual speech processing (2008) The International Conference on Acoustics, Speech, and Signal Processing (ICASSP); Vu, N.T., Schlippe, T., Kraus, F., Schultz, T., Rapid bootstrapping of five eastern european languages using the rapid language adaptation toolkit (2010) The 11th Annual Conference of the International Speech Communication Association (Interspeech), , Makuhari, Japan; O'Reilly, T., What is web 2.0: Design patterns and business models for the next generation of software (2007) Communications & Strategies, (1), p. 17; Bulyko, I., Ostendorf, M., Stolcke, A., Getting more mileage from web text sources for conversational speech language modeling using class-dependent mixtures (2003) The 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLTNAACL), , Association for Computational Linguistics; Rosenfeld, R., Optimizing lexical and N-gram coverage via judicious use of linguistic data (1995) The European Conference on Speech Technology (Eurospeech); Iyer, R., Ostendorf, M., Relevance weighting for combining multidomain data for N-gram language modeling Computer Speech & Language, 13 (3), pp. 267-282; Sarikaya, R., Gravano, A., Gao, Y., Rapid language model development using external resources for new spoken dialog domains The International Conference on Acoustics, Speech, and Signal Processing (ICASSP), , Philadelphia, Pennsylvania, USA; Sethy, A., Georgiou, P.G., Narayanan, S., Building topic specific language models from web data using competitive models (2005) The European Conference on Speech Technology (Eurospeech); Misu, T., Kawahara, T., A bootstrapping approach for developing language model of new spoken dialogue systems by selecting web texts (2006) The Annual Conference of the International Speech Communication Association (Interspeech), pp. 9-12; Lecorve, G., Gravier, G., Sebillot, P., On the use of web resources and natural language processing techniques to improve automatic speech recognition systems (2008) The Sixth International Conference on Language Resources and Evaluation (LREC'08); Lecorve, G., Gravier, G., Sebillot, P., An unsupervised web based topic language model adaptation method (2008) The International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 5081-5084. , IEEE; Kemp, T., (1999) Ein Automatisches Indexierungssystem Fur Fernsehnachrichtensendungen, , Ph.D. dissertation; Yu, H., Tomokiyo, T., Wang, Z., Waibel, A., New developments in automatic meeting transcription (2000) Proceedings of the International Conference on Spoken Language Processing (ICSLP), 4, pp. 310-313; Lecorve, G., Dines, J., Hain, T., Motlicek, P., Supervised and unsupervised web-based language model domain adaptation (2012) The 11th Annual Conference of the International Speech Communication Association (Interspeech); Auzanne, C., Garofolo, J.S., Fiscus, J.G., Fisher, W.M., Automatic language model adaptation for spoken document retrieval (2000) RIAO 2000 Conference on Content-Based Multimedia Information Access; Ohtsuki, K., Nguyen, L., Incremental language modeling for automatic transcription of broadcast news (2007) IEICE Transactions on Information and Systems, 90 (2), pp. 526-532; Khudanpur, S., Kim, W., Contemporaneous text as side information in statistical language modeling (2004) Computer Speech and Language, 18 (2), pp. 143-162; Kombrink, S., Mikolov, T., Karafiat, M., Burget, L., Improving language models for ASR using translated in-domain data (2012) The 37th International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2012), pp. 4405-4408. , Kyoto, Japan: IEEE; Vu, N.T., Lyu, D.-C., Weiner, J., Telaar, D., Schlippe, T., Blaicher, F., Chng, E.-S., Li, H., A first speech recognition system for mandarin-english code-switch conversational speech (2012) Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, pp. 4889-4892; Feng, J., Renger, B., Language modeling for voice-enabled social TV using tweets (2012) The 13th Annual Conference of the International Speech Communication Association (Interspeech 2012), , Portland, Oregon, USA; Adam, G., Bouras, C., Poulopoulos, V., Utilizing RSS feeds for crawling the web (2009) The Fourth International Conference on Internet and Web Applications and Services (ICIW 2009), pp. 211-216. , Venice/Mestre, Italy; Martins, C.A.D., (2008) Dynamic Language Modeling for European Portuguese, , dissertation, Universidade de Aveiro; Lamel, L., Courcinous, S., Despres, J., Gauvain, J.-L., Josse, Y., Kilgour, K., Kraft, F., Woehrling, C., Speech recognition for machine translation in quaero (2011) Proceedings of the International Workshop on Spoken Language Translation (IWSLT), , San Francisco, CA; Stolcke, A., SRILM - An extensible language modeling toolkit (2002) The International Conference on Spoken Language Processing, 2, pp. 901-904; Bisani, M., Ney, H., Joint-sequence models for grapheme-to- phoneme conversion (2008) Speech Communication, 50 (5), pp. 434-451},
sponsors={Amazon; et al.; European Language Resources Association (ELRA); Google; Microsoft; Sytral},
publisher={International Speech and Communication Association},
address={Lyon},
issn={2308457X},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Heger20124095,
author={Heger, D. and Jakel, R. and Putze, F. and Losch, M. and Schultz, T.},
title={Filling a glass of water: Continuously decoding the speed of 3D hand movements from EEG signals},
journal={Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
year={2012},
pages={4095-4098},
doi={10.1109/EMBC.2012.6346867},
art_number={6346867},
note={cited By 4; Conference of 34th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS 2012 ; Conference Date: 28 August 2012 Through 1 September 2012;  Conference Code:94236},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880948151&doi=10.1109%2fEMBC.2012.6346867&partnerID=40&md5=a44a2846bdfb1ac78ee17218a8ac970a},
affiliation={Cognitive Systems Lab, Institute for Anthropomatics, Karlsruhe Institute of Technology, Adenauerring 4, 76131 Karlsruhe, Germany; Humanoids and Intelligence Systems Lab, Institute for Anthropomatics, Karlsruhe Institute of Technology, Adenauerring 4, 76131 Karlsruhe, Germany},
abstract={We present a new system for the continuous decoding of hand movement speed in three-dimensional (3D) space from EEG signals. We recorded experimental data of five subjects during mimicking the natural task of filling a glass of water. The proposed system uses filter bank common spatial patterns and linear regression to estimate the speed of hand movements from artifact cleaned EEG signals. Average Pearson correlations between the speed trajectories predicted from EEG and the speed trajectories measured using a high-precision motion tracking system are r=0.41 for the x-axis, r=0.36 for the y-axis, r=0.48 for the z-axis, and r=0.17 for absolute speed in 3D space. © 2012 IEEE.},
keywords={3-D space;  Common spatial patterns;  EEG signals;  Hand movement;  High-precision motion;  Natural tasks;  Pearson correlation;  Speed trajectories;  System use;  Three-dimensional (3D) space, Decoding;  Filter banks;  Glass;  Three dimensional computer graphics, Speed, adult;  algorithm;  article;  brain computer interface;  brain mapping;  electroencephalography;  evoked muscle response;  hand;  human;  male;  methodology;  motor cortex;  movement (physiology);  physiology, Adult;  Algorithms;  Brain Mapping;  Brain-Computer Interfaces;  Electroencephalography;  Evoked Potentials, Motor;  Hand;  Humans;  Male;  Motor Cortex;  Movement},
references={Dillmann, R., Asfour, T., Do, M., Jäkel, R., Kasper, A., Azad, P., Ude, A., Lösch, M., Advances in robot programming by demonstration (2010) KI, Künstliche Intelligenz, 24, pp. 295-303; Schalk, G., Miller, K., Anderson, N., Wilson, J., Smyth, M., Ojemann, J., Moran, D., Leuthardt, E., Two-dimensional movement control using electrocorticographic signals in humans (2008) Journal of Neural Engineering, 5, p. 75; Hochberg, L., Bacher, D., Jarosiewicz, B., Masse, N., Simeral, J., Vogel, J., Haddadin, S., Van Der Smagt, P., Reach and grasp by people with tetraplegia using a neurally controlled robotic arm (2012) Nature, 485 (7398), pp. 372-375; Yuan, H., Perdoni, C., He, B., Relationship between speed and eeg activity during imagined and executed hand movements (2010) Journal of Neural Engineering, 7, p. 026001; Yuan, H., Perdoni, C., He, B., Decoding speed of imagined hand movement from eeg (2010) Engineering in Medicine and Biology Society (EMBC), pp. 142-145. , 2010 Annual International Conference of the IEEE. IEEE; Bradberry, T., Gentili, R., Contreras-Vidal, J., Reconstructing three-dimensional hand movements from noninvasive electroencephalographic signals (2010) The Journal of Neuroscience, 30, p. 3432; Lv, J., Li, Y., Gu, Z., Decoding hand movement velocity from electroencephalogram signals during a drawing task (2010) BioMedical Engineering OnLine, 9 (1), p. 64; Schlögl, A., Keinrath, C., Zimmermann, D., Scherer, R., Leeb, R., Pfurtscheller, G., A fully automated correction method of eog artifacts in eeg recordings (2007) Clinical Neurophysiology, 118 (1), pp. 98-104; Delorme, A., Makeig, S., Eeglab: An open source toolbox for analysis of single-trial eeg dynamics including independent component analysis (2004) Journal of Neuroscience Methods, 134, pp. 9-21; Croft, R., Barry, R., Removal of ocular artifact from the eeg: A review (2000) Neurophysiologie Clinique/Clinical Neurophysiology, 30 (1), pp. 5-19; Jung, T., Makeig, S., Humphries, C., Lee, T., Mckeown, M., Iragui, V., Sejnowski, T., Removing electroencephalographic artifacts by blind source separation (2000) Psychophysiology, 37, pp. 163-178; Goncharova, I., McFarland, D., Vaughan, T., Wolpaw, J., Emg contamination of eeg: Spectral and topographical characteristics (2003) Clinical Neurophysiology, 114 (9), pp. 1580-1593; Ang, K., Chin, Z., Zhang, H., Guan, C., Filter bank common spatial pattern (fbcsp) in brain-computer interface (2008) Neural Networks, 2008, pp. 2390-2397. , IJCNN 2008.(IEEE World Congress on Computational Intelligence). IEEE International Joint Conference on. IEEE; Chin, Z., Ang, K., Wang, C., Guan, C., Zhang, H., Multi-class filter bank common spatial pattern for four-class motor imagery bci (2009) Engineering in Medicine and Biology Society, pp. 571-574. , Annual International Conference of the IEEE. IEEE; Tangermann, M., Müller, K.-R., Aertsen, A., Birbaumer, N., Braun, C., Brunner, C., Leeb, R., Blankertz, B., Review of the bci competition iv (2012) Frontiers in Neuroscience, 6 (55); Ramoser, H., Muller-Gerking, J., Pfurtscheller, G., Optimal spatial filtering of single trial eeg during imagined hand movement (2000) Transactions on Rehabilitation Engineering, 8, pp. 441-446},
correspondence_address1={Heger, D.; Cognitive Systems Lab, Institute for Anthropomatics, Karlsruhe Institute of Technology, Adenauerring 4, 76131 Karlsruhe, Germany; email: dominic.heger@kit.edu},
sponsors={IEEE EMB; IEEE CAS; IEEE SMC; SONNET},
address={San Diego, CA},
issn={1557170X},
isbn={9781424441198},
pubmed_id={23366828},
language={English},
abbrev_source_title={Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Herff20121715,
author={Herff, C. and Putze, F. and Heger, D. and Guan, C. and Schultz, T.},
title={Speaking mode recognition from functional Near Infrared Spectroscopy},
journal={Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
year={2012},
pages={1715-1718},
doi={10.1109/EMBC.2012.6346279},
art_number={6346279},
note={cited By 12; Conference of 34th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS 2012 ; Conference Date: 28 August 2012 Through 1 September 2012;  Conference Code:94236},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880851792&doi=10.1109%2fEMBC.2012.6346279&partnerID=40&md5=71b583c6a446fb61fce6e357e97d7b14},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology, Adenauerring 4, 76131 Karlsruhe, Germany; Institute for Infocomm Research, Agency for Science, Technology and Research (ASTAR), 1 Fusionopolis Way, #21-01 Connexis, Singapore 138632, Singapore},
abstract={Speech is our most natural form of communication and even though functional Near Infrared Spectroscopy (fNIRS) is an increasingly popular modality for Brain Computer Interfaces (BCIs), there are, to the best of our knowledge, no previous studies on speech related tasks in fNIRS-based BCI. We conducted experiments on 5 subjects producing audible, silently uttered and imagined speech or do not produce any speech. For each of these speaking modes, we recorded fNIRS signals from the subjects performing these tasks and distinguish segments containing speech from those not containing speech, solely based on the fNIRS signals. Accuracies between 69% and 88% were achieved using support vector machines and a Mutual Information based Best Individual Feature approach. We are also able to discriminate the three speaking modes with 61% classification accuracy. We thereby demonstrate that speech is a very promising paradigm for fNIRS based BCI, as classification accuracies compare very favorably to those achieved in motor imagery BCIs with fNIRS. © 2012 IEEE.},
keywords={Classification accuracy;  Functional near infrared spectroscopy;  Mode recognition;  Motor imagery;  Mutual informations, Brain computer interface;  Near infrared spectroscopy;  Speech, Speech communication, adult;  article;  corpus callosum;  electrode;  hemodynamics;  human;  male;  methodology;  motor cortex;  near infrared spectroscopy;  physiology;  speech, Adult;  Corpus Callosum;  Electrodes;  Hemodynamics;  Humans;  Male;  Motor Cortex;  Spectroscopy, Near-Infrared;  Speech},
references={Coyle, S.M., Ward, T.E., Markham, C.M., Brain-computer interface using a simplified functional near-infrared spectroscopy system (2007) Journal of Neural Engineering, 4 (3), p. 219; Ang, K.K., Guan, C., Lee, K., Lee, J.Q., Nioka, S., Chance, B., A brain-computer interface for mental arithmetic task from SingleTrial near-infrared spectroscopy brain signals (2010) Int. Conference on Pattern Recognition, pp. 3764-3767; Binder, J.R., Swanson, S.J., Hammeke, T.A., Sabsevitz, D.S., A comparison of five fMRI protocols for mapping speech comprehension systems (2008) Epilepsia, 49, pp. 1980-1997. , Dec; Sato, H., Takeuchi, T., Sakai, K.L., Temporal cortex activation during speech recognition: An optical topography study (1999) Cognition, 73 (3), pp. B55-66. , Dec; Naito, M., Michioka, Y., Ozawa, K., Ito, Y., Kiguchi, M., Kanazawa, T., A communication means for totally locked-in als patients based on changes in cerebral blood volume measured with near-infrared light (2007) IEICE - Trans. Inf. Syst., E90-D (7), pp. 1028-1037. , July; Okada, E., Firbank, M., Schweiger, M., Arridge, S.R., Cope, M., Delpy, D.T., Theoretical and experimental investigation of near-infrared light propagation in a model of the adult head (1997) Appl. Opt., 36 (1), pp. 21-31. , Jan; Sassaroli, A., Fantini, S., Comment on the modified beerlambert law for scattering media (2004) Physics in Medicine and Biology, 49 (14), pp. N255; Knecht, S., Dräger, B., Deppe, M., Bobe, L., Lohmann, H., Flöel, A., Ringelstein, E.B., Henningsen, H., Handedness and hemispheric language dominance in healthy humans (2000) Brain, 123 (12), pp. 2512-2518; Ye, J.C., Tak, S., Jang, K.E., Jung, J., Jang, J., Nirs-spm: Statistical parametric mapping for near-infrared spectroscopy (2009) NeuroImage, 44, pp. 428-447; Oldfield, R.C., The assessment and analysis of handedness: The edinburgh inventory (1971) Neuropsychologia, 9, pp. 97-113; Leamy, D.J., Collins, R., Ward, T., Combining fNIRS and EEG to improve motor cortex activity classification during an imagined movement-based task (2011) HCI, (20), pp. 177-185; Ang, K.K., Yang Chin, Z., Zhang, H., Guan, C., Filter bank common spatial pattern (fbcsp) in brain-computer interface (2008) Neural Networks, 2008, pp. 2390-2397. , IJCNN 2008 June; Battiti, R., Using mutual information for selecting features in supervised neural net learning (1994) IEEE Transactions on Neural Networks / a Publication of the IEEE Neural Networks Council, 5 (4), pp. 537-550. , Jan},
correspondence_address1={Herff, C.; Cognitive Systems Lab, Karlsruhe Institute of Technology, Adenauerring 4, 76131 Karlsruhe, Germany; email: christian.herff@kit.edu},
sponsors={IEEE EMB; IEEE CAS; IEEE SMC; SONNET},
address={San Diego, CA},
issn={1557170X},
isbn={9781424441198},
pubmed_id={23366240},
language={English},
abbrev_source_title={Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schick2012217,
author={Schick, A. and Morlock, D. and Amma, C. and Schultz, T. and Stiefelhagen, R.},
title={Vision-based handwriting recognition for unrestricted text input in mid-air},
journal={ICMI'12 - Proceedings of the ACM International Conference on Multimodal Interaction},
year={2012},
pages={217-220},
doi={10.1145/2388676.2388719},
note={cited By 28; Conference of 14th ACM International Conference on Multimodal Interaction, ICMI 2012 ; Conference Date: 22 October 2012 Through 26 October 2012;  Conference Code:94095},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870194250&doi=10.1145%2f2388676.2388719&partnerID=40&md5=bb3f39f12aaf23e994cb791c38235f4d},
affiliation={Fraunhofer IOSB, Karlsruhe, Germany; Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={We propose a vision-based system that recognizes handwriting in mid-air. The system does not depend on sensors or markers attached to the users and allows unrestricted character and word input from any position. It is the result of combining handwriting recognition based on Hidden Markov Models with multi-camera 3D hand tracking. We evaluated the system for both quantitative and qualitative aspects. The system achieves recognition rates of 86:15% for character and 97:54% for small-vocabulary isolated word recognition. Limitations are due to slow and low-resolution cam- eras or physical strain. Overall, the proposed handwriting recognition system provides an easy-to-use and accurate text input modality without placing restrictions on the users. Copyright 2012 ACM.},
author_keywords={3D hand tracking;  Handwriting recognition},
keywords={3D hand tracking;  Handwriting recognition;  Isolated word recognition;  Multi-cameras;  Qualitative aspects;  Recognition rates;  Text input;  Unrestricted texts;  Vision-based handwriting;  Vision-based system, Hidden Markov models;  Interactive computer systems;  Three dimensional computer graphics, Character recognition},
references={Amma, C., Georgi, M., Schultz, T., Airwriting: Hands-free mobile text input by spotting and continuous recognition of 3D-space handwriting with inertial sensors (2012) Proc. International Symposium on Wearable Computers, pp. 52-59; Azarbayejani, A., Pentland, A., Real-time self-calibrating stereo person tracking using 3-D shape estimation from blob features (1996) Proc. International Conference on Pattern Recognition; Cooper, H., Holt, B., Bowden, R., Sign language recognition (2011) Visual Analysis of Humans, pp. 539-562; Kim, D., Choi, H., Kim, J., 3D space handwriting recognition with ligature model (2006) Proc. Ubiquitous Computing Systems; Liu, Y., Liu, X., Jia, Y., Hand-gesture based text input for wearable computers (2006) Proc. Computer Vision Systems; Mitra, S., Acharya, T., Gesture recognition: A survey (2007) IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews, 37 (3), pp. 311-324; Nickel, K., Stiefelhagen, R., Pointing gesture recognition based on 3D-tracking of face, hands and head orientation (2003) Proc. International Conference on Multimodal Interfaces; Plamondon, R., Srihari, S.N., On-line and off-line handwriting recognition: A comprehensive survey (2000) Transactions on Pattern Analysis, 22 (1), pp. 63-83; Rabiner, L., A tutorial on hidden Markov models and selected applications in speech recognition (1989) Proceedings of the IEEE, 77 (2), pp. 257-286; Schick, A., Camp, F.V.D., Ijsselmuiden, J., Stiefelhagen, R., Extending touch: Towards interaction with large-scale surfaces (2009) Proc. Interactive Tabletops and Surfaces, pp. 117-124; Shoemaker, G., Findlater, L., Dawson, J.Q., Booth, K.S., Mid-air text input techniques for very large wall displays (2009) Proc. Graphics Interface, pp. 231-238; Zafrulla, Z., Brashear, H., Starner, T., Hamilton, H., Presti, P., American sign language recognition with the kinect (2011) Proc. International Conference on Multimodal Interfaces, pp. 279-286; Zhou, S., Dong, Z., Li, W., Kwong, C.P., Hand-written character recognition using MEMS motion sensing technology (2008) Proc. Advanced Intelligent Mechatronics, pp. 1418-1423},
correspondence_address1={Schick, A.; Fraunhofer IOSB, Karlsruhe, Germany; email: alexander.schick@iosb.fraunhofer.de},
sponsors={ACM SIGCHI},
address={Santa Monica, CA},
isbn={9781450314671},
language={English},
abbrev_source_title={ICMI - Proc. ACM Int. Conf. Multimodal Interact.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Putze2012133,
author={Putze, F. and Schultz, T.},
title={Cognitive dialog systems for dynamic environments: Progress and challenges},
journal={Digital Signal Processing for In-Vehicle Systems and Safety},
year={2012},
pages={133-143},
doi={10.1007/978-1-4419-9607-7_8},
note={cited By 3; Conference of 4th Biennial Workshop on Digital Signal Processing for In-Vehicle Systems and Safety, DSP 2009 ; Conference Date: 25 June 2009 Through 27 June 2009;  Conference Code:103033},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897713381&doi=10.1007%2f978-1-4419-9607-7_8&partnerID=40&md5=fe2c754e011ec2b89d5366c1beeb0492},
affiliation={Cognitive Systems Lab., University of Karlsruhe, Karlsruhe, Germany},
abstract={In this chapter, we present our existing setup and ongoing research on the development of cognitive dialog systems for dynamic environments like cars, including the main components that we consider necessary to build dialog systems to estimate the user's mental processes (hence, cognitive) and adapt their behavior accordingly. In conducting realistic testing and recording environment to produce real-life data, a realistic driving simulator was used. We also needed to observe the user during these interactions in a multimodal way to estimate the current user state based on this data. This information is integrated with cognitive modeling components that enrich the observational data. We finally needed a dialog management system which is able to use this information for adapting its interaction behavior accordingly. In this chapter, we report our progress in building these components, give an overview over the challenges we identified during this work and the solutions we aim for. © Springer Science+Business Media, LLC 2012.},
author_keywords={Cognitive dialog system;  Cognitive model;  Human machine interaction;  User state detection},
keywords={Cognitive model;  Dialog systems;  Dynamic environments;  Human machine interaction;  Interaction behavior;  Observational data;  Recording environment;  State Detection, Cognitive systems;  Digital signal processing;  Human computer interaction, Information management},
references={Nass, C., Improving automotive safety by pairing driver emotion and car voice emotion (2005) Proceedings of CHI, , Oregon; Hassel, L., Hagen, E., Adaptation of an automotive dialogue system to users expertise and evaluation of the system (2006) Lang Res Eval, 40 (1), pp. 67-85; Gnjatović, M., Rösner, D., Emotion adaptive dialogue management in human-machine interaction: Adaptive dialogue management in the NIMITEK prototype system (2008) 19th European Meetings on Cybernetics and Systems Research, , University of Vienna, Vienna; Nasoz, F., Lisetti, C., Affective user modeling for adaptive intelligent user interfaces (2007) Proceedings of 12th HCI International Conference, , Beijing; Li, X., Ji, Q., Active affective state detection and user assistance with dynamic Bayesian networks (2005) IEEE Trans Syst Man Cybern, 35, p. 93; Conati, C., Probabilistic assessment of user's emotions during the interaction with educational games (2002) Appl Artif Intell, 16, pp. 555-575; Liang, Y., Real-time detection of driver cognitive distraction using support vector machines (2007) IEEE Trans Intell Transp Syst, 8 (2), pp. 340-350; Healey, J., Picard, R., Detecting stress during real-world driving tasks using physiological sensors (2005) IEEE Trans Intell Transp Syst, 6 (2), pp. 156-166; Larsson, S., Traum, D.R., Information state and dialogue management in the TRINDI dialogue move engine toolkit (2002) Nat Lang Eng, 6, pp. 3-4; Heger, D., Putze, F., Schultz, T., An adaptive information system for an empathic robot using EEG data (2010) 2nd International Conference on Social Robotics, , Singapore; Heger, D., Putze, F., Amma, C., Wielatt, T., Plotkin, I., Wand, M., Schultz, T., Biosignals studio: A flexible framework for biosignal capturing and processing (2010) 33 Rd Annual German Conference on Artificial Intelligence 2010, , Karlsruhe; Gao, H., (2009) Robust Face Alignment for Face Retrieval, , University of Karlsruhe (TH), Karlsruhe; Putze, F., Jarvis, J., Schultz, T., Multimodal recognition of cognitive workload for multitasking in the car (2010) 20th International Conference on Pattern Recognition, , Istanbul; Anderson, J., An integrated theory of the mind (2004) Psychol Rev, 111, pp. 1036-1060; Schultheis, H., LTM-C-an improved long-term emory for cognitive architectures (2006) Proceedings of the 7th International Conference on Cognitive Modeling, , Trieste; Bach, J., The micropsi agent architecture (2003) Proceedings of International Conference on Cognitive Modeling, , Bamberg; Putze, F., Schultz, T., Cognitive memory modeling for interactive systems in dynamic environments (2009) Proceedings of 1st International Workshop on Spoken Dialog Systems, , Kloster Irsee, 2009; Waibel, A., A one pass-decoder based on polymorphic linguistic context assignment (2001) Proceedings of ASRU, , Trento},
correspondence_address1={Putze, F.; Cognitive Systems Lab., University of Karlsruhe, Karlsruhe, Germany; email: felix.putze@kit.edu},
address={Dallas, TX},
isbn={9781441996060},
language={English},
abbrev_source_title={Digit. Signal Process. In-Veh. Syst. Saf.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nallasamy20121900,
author={Nallasamy, U. and Metze, F. and Schultz, T.},
title={Enhanced polyphone decision tree adaptation for Accented Speech Recognition},
journal={13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012},
year={2012},
volume={3},
pages={1900-1903},
note={cited By 11; Conference of 13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012 ; Conference Date: 9 September 2012 Through 13 September 2012;  Conference Code:97207},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878544116&partnerID=40&md5=9363456293684d1be86d2e91f650d2bf},
affiliation={Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Cognitive Systems Labs, Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={State-of-the-art Automatic Speech Recognition (ASR) systems struggle to handle accented speech, particularly if the target accent is under-represented in the training data. The acoustic variations presented by an unfamiliar accent render the ASR polyphone decision tree (PDT) and its associated Gaussian mixture models (GMM) misfit to the test data. In this paper, we improve on the previous work of adapting the polyphone decision tree, using a semi-continuous model based approach to address the problem of data sparsity. We extend the existing PDT to introduce additional states with shared parameters, corresponding to the new contextual variations identified in the adaptation data, while still robustly estimating the state-specific parameters on a relatively small dataset. We conduct ASR experiments on Arabic and English accents and show that our technique performs better than Maximum A-Posteriori (MAP) adaptation and a previous implementation of polyphone decision tree specialization (PDTS). Compared to MAP adapted system, we obtain 7% relative improvement in Word Error Rate (WER) for Arabic and 13.7% relative improvement for English accent adaptation.},
author_keywords={Accent adaptation;  Automatic speech recognition},
keywords={Accent adaptation;  Automatic speech recognition;  Automatic speech recognition system;  Gaussian Mixture Model;  Maximum a posteriori;  Model based approach;  Under-represented;  Word error rate, Data mining;  Decision trees, Speech recognition},
references={Humphries, J.J., Woodland, P.C., Using accent-specific pronunciation modelling for improved large vocabulary continuous speech recognition (1997) Proc. Eurospeech, , Rhodes; Schultz, T., Waibel, A., Polyphone decision tree specialization for language adaptation (2000) Proc. ICASSP, , Istanbul; Smit, P., Kurimo, M., Using stacked transformations for recognizing foreign accented speech (2011) Proc. ICASSP, , Prague; Singh, R., Raj, B., Stern, R.M., Domain adduced state tying for cross-domain acoustic modelling (1999) Proc. Eurospeech, , Budapest; Stuker, S., Modified polyphone decision tree specialization for porting multilingual grapheme based ASR systems to new languages (2008) Proc. ICASSP, , Las Vegas; Wang, Z., Schultz, T., Non-native spontaneous speech recognition through polyphone decisision tress specialization (2003) Proc. Interspeech, , Geneva; Riedhammer, K., Bocklet, T., Ghoshal, A., Povey, D., Revisting semi-continuous hidden Markov models (2012) Proc. ICASSP, , Tokyo; Soltau, H., Advances in Arabic speech transcription at IBM under the DARPA GALE program (2009) IEEE Trans on Audio, Speech and Language Proc., 17 (5); Metze, F., Hsiao, R., Jin, Q., Nallasamy, U., Schultz, T., The 2010 CMU GALE speech-to-text system (2010) Proc. Interspeech, , Makuhari; Soltau, H., Mangu, L., Biadsy, F., From modern standard Arabic to levantine ASR: Leveraging GALE for dialects (2011) Proc. ASRU, , Hawaii; Huang, X., Acero, A., Hon, H.W., Spoken language processing: A guide to theory (2001) Algorithm and System Development, , Prentice Hall; Clarke, C., Jurafsky, D., Limitations of MLLR adaptation with Spanish-accented English: An error analysis (2006) Proc. Interspeech, , Pittsburgh; Burget, L., Schwarz, P., Multilingual acoustic modeling for speech recognition based on subspace Gaussian mixture models (2010) Proc. ICASSP, , Dallas},
correspondence_address1={Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, United States},
address={Portland, OR},
isbn={9781622767595},
language={English},
abbrev_source_title={Annu. Conf. Int. Speech Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nallasamy2012360,
author={Nallasamy, U. and Metze, F. and Schultz, T.},
title={Active learning for accent adaptation in Automatic Speech Recognition},
journal={2012 IEEE Workshop on Spoken Language Technology, SLT 2012 - Proceedings},
year={2012},
pages={360-365},
doi={10.1109/SLT.2012.6424250},
art_number={6424250},
note={cited By 9; Conference of 2012 IEEE Workshop on Spoken Language Technology, SLT 2012 ; Conference Date: 2 December 2012 Through 5 December 2012;  Conference Code:95694},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874262789&doi=10.1109%2fSLT.2012.6424250&partnerID=40&md5=7845c02b1bb7230b603bd00826d3f2b6},
affiliation={Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Cognitive Systems Labs, Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={We experiment with active learning for speech recognition in the context of accent adaptation. We adapt a source recognizer on the target accent by selecting a relatively small, matched subset of utterances from a large, untranscribed and multi-accented corpus for human transcription. Traditionally, active learning in speech recognition has relied on uncertainty based sampling to choose the most informative data for manual labeling. Such an approach doesn't include explicit relevance criterion during data selection, which is crucial for choosing utterances to match the target accent, from datasets with wide-ranging speakers of different accents. We formulate a cross-entropy based relevance measure to complement uncertainty based sampling for active learning to aid accent adaptation. We evaluate the algorithm on two different setups for Arabic and English accents and show that our approach performs favorably to conventional data selection. We analyze the results to show the effectiveness of our approach in finding the most relevant subset of utterances for improving the speech recognizer on the target accent. © 2012 IEEE.},
author_keywords={Accent Adaptation;  Active Learning;  Automatic Speech Recognition},
keywords={Accent Adaptation;  Active Learning;  Automatic speech recognition;  Cross entropy;  Data Selection;  Manual labeling;  Relevance criteria;  Relevance measure;  Speech recognizer;  Uncertainty based samplings, Data reduction, Speech recognition},
references={Settles, B., Active learning literature survey (2009) Computer Sciences Technical Report 1648, , University of Wisconsin-Madison; Tomanek, K., Olsson, F., A web survey on the use of active learning to support annotation of text data (2009) Workshop on Active Learning for NLP, pp. 45-48. , Stroudsburg, PA, USA HLT '09; T̈ur, G̈., Hakkani-T̈ur, D.Z., Schapire, R.E., Combining active and semi-supervised learning for spoken language understanding (2005) Speech Communication, 45 (2), pp. 171-186; Riccardi, G., Hakkani-T̈ur, D., Active learning: Theory and applications to automatic speech recognition (2005) IEEE Transactions on Speech and Audio Processing, 13 (4), pp. 504-511; Yu, K., Gales, M.J.F., Wang, L., Woodland, P.C., Unsupervised training and directed manual transcription for LVCSR (2010) Speech Communication, 52 (7-8), pp. 652-663; Yu, D., Varadarajan, B., Deng, L., Acero, A., Active learning and semi-supervised learning for speech recognition: A unified framework using the global entropy reduction maximization criterion (2010) Computer Speech & Language, 24 (3); Itoh, N., Sainath, T.N., Jiang, D.N., Zhou, J., Ramabhadran, B., N-best entropy based data selection for acoustic modeling (2012) ICASSP, pp. 4133-4136; Hakkani-T̈ur, D.Z., Riccardi, G., Gorin, A.L., Active learning for automatic speech recognition (2002) ICASSP, pp. 3904-3907; Huang, S., Jin, R., Zhou, Z., Active learning by querying informative and representative examples (2010) NIPS, pp. 892-900; Mangu, L., Brill, E., Stolcke, A., Finding consensus in speech recognition: Word error minimization and other applications of confusion networks (2000) Computer Speech & Language, 14 (4); Cortes, C., Mohri, M., Riley, M., Rostamizadeh, A., Sample selection bias correction theory (2008) ALT, pp. 38-53; Blitzer, J., Dauḿe Iii., H., (2010) ICML Tutorial on Domain Adaptation, , http://adaptationtutorial.blitzer.com, June; Bickel, S., Br̈uckner, M., Scheffer, T., Discriminative learning under covariate shift (2009) Journal of Machine Learning Research, 10; Moore, R.C., Lewis, W., Intelligent selection of language model training data (2010) ACL (Short Papers), pp. 220-224; Nallasamy, U., Metze, F., Schultz, T., Enhanced polyphone decision tree adaptation for accented speech recognition (2012) Interspeech; Nallasamy, U., Metze, F., Schultz, T., Semi-supervised learning for speech recognition in the context of accent adaptation (2012) MLSLP Symposium; Metze, F., Hsiao, R., Jin, Q., Nallasamy, U., Schultz, T., The 2010 cmu gale speechto-text system (2010) Interspeech; Cui, X., Huang, J., Chien, J., Multi-view and multi-objective semi-supervised learning for hmm-based automatic speech recognition (2012) IEEE Transactions on Audio, Speech and Language Processing, 20 (7), pp. 1923-1935},
correspondence_address1={Nallasamy, U.; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, United States; email: funallasa@cs.cmu.edu},
sponsors={The Institute of Electrical and Electronics Engineers (IEEE); IEEE Signal Processing Society},
address={Miami, FL},
isbn={9781467351263},
language={English},
abbrev_source_title={IEEE Workshop Spoken Lang. Technol., SLT - Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vu20122585,
author={Vu, N.T. and Breiter, W. and Metze, F. and Schultz, T.},
title={An investigation on initialization schemes for multilayer perceptron training using multilingual data and their effect on ASR performance},
journal={13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012},
year={2012},
volume={3},
pages={2585-2588},
note={cited By 26; Conference of 13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012 ; Conference Date: 9 September 2012 Through 13 September 2012;  Conference Code:97207},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878559540&partnerID=40&md5=0374eaf853744c1aa82c3c450e327242},
affiliation={Cognitive Systems Lab, Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT), Germany; Language Technologies Institute, Carnegie Mellon University (CMU), Pittsburgh, PA, United States},
abstract={In this paper we present our latest investigation on initialization schemes for Multilayer Perceptron (MLP) training using multilingual data. We show that the overall performance of a MLP network improves significantly by initializing it with a multilingual MLP. We propose a new strategy called "open target language" MLP to train more flexible models for language adaptation, which is particularly suited for small amounts of training data. Furthermore, by applying Bottle-Neck feature (BN) initialized with multilingual MLP the ASR performance increases for both, the languages which were used for multilingual MLP training, and the new language. Our experiments show a word error rate improvements of up to 16.9% relative on a range of tasks for different target languages (Creole and Vietnamese) with manual and automatic transcribed training data.},
author_keywords={Bottle-Neck feature;  Language adaptation;  Multilingual multilayer perceptron},
keywords={Bottle-Neck feature;  Flexible model;  Language adaptation;  Multi layer perceptron;  Target language;  Training data;  Vietnamese;  Word error rate, Computer applications;  Computer simulation, Bottles},
references={Stolcke, A., Grzl, F., Hwang, M.-Y., Lei, X., Morgan, N., Vergyri, D., Cross-domain and cross-lingual portability of acoustic features estimated by multilayer perceptrons Proc. ICASSP 2006; Toth, L., Frankel, J., Gosztolya, G., King, S., Cross-lingual portability of MLP-based tandem features - A case study for English and Hungarian (2008) Interspeech; Cetin, O., Magimai-Doss, M., Livescu, K., Kantor, A., King, S., Bartels, C., Frankel, J., Monolingual and crosslingual comparison of tandem features derived from articulatory and phone MLPs (2007) Proc. ASRU; Imseng, D., Bourlard, H., Magimai.-Doss, M., Towards mixed language speech recognition systems (2010) Interspeech, , Japan; Plahl, C., Schlueter, R., Ney, H., Cross-lingual portability of chinese and english neural network features for French and German LVCSR (2011) Proc. ASRU, , USA; Metze, F., Hsiao, R., Jin, Q., Nallasamy, U., Schultz, T., The 2010 CMU GALE speech-to-text system (2010) Interspeech, , Japan; http://www.icsi.berkeley.edu/Speech/qn.html; Schultz, T., GlobalPhone: A multilingual speech and text database developed at Karlsruhe University (2002) Proc. ICSLP, , Denver, CO; http://www.speech.cs.cmu.edu/haitian; Schultz, T., Black, A., Rapid language adaptation tools and technologies for multilingual speech processing (2008) Proc. ICASSP, , USA; Schultz, T., Waibel, A., Language independent and language adaptive acoustic modeling for speech recognition (2001) Speech Communication, 35 (1-2), pp. 31-51. , August; Vu, N.T., Schultz, T., Vietnamese large vocabulary continuous speech recognition (2009) Proc. ASRU, , Italy; Lamel, L.F., Gauvain, J.L., Eskenazi, M., BREF a large vocabulary spoken corpus for French (1991) Proc. EuroSpeech, , Italy; Vu, N.T., Kraus, F., Schultz, T., Rapid building of an ASR system for under-resourced languages based on multilingual unsupervised training (2011) Interspeech 2011, , Italy},
correspondence_address1={Cognitive Systems Lab, Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT)Germany},
address={Portland, OR},
isbn={9781622767595},
language={English},
abbrev_source_title={Annu. Conf. Int. Speech Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schlippe20122295,
author={Schlippe, T. and Ochs, S. and Vu, N.T. and Schultz, T.},
title={Automatic error recovery for pronunciation dictionaries},
journal={13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012},
year={2012},
volume={3},
pages={2295-2298},
note={cited By 5; Conference of 13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012 ; Conference Date: 9 September 2012 Through 13 September 2012;  Conference Code:97207},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878526331&partnerID=40&md5=c473fcb5b0233b939b94791b69c26364},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology (KIT), Germany},
abstract={In this paper, we present our latest investigations on pronunciation modeling and its impact on ASR. We propose completely automatic methods to detect, remove, and substitute inconsistent or flawed entries in pronunciation dictionaries. The experiments were conducted on different tasks, namely (1) word-pronunciation pairs from the Czech, English, French, German, Polish, and Spanish Wiktionary [1], a multilingual wiki-based open content dictionary, (2) our GlobalPhone Hausa pronunciation dictionary [2], and (3) pronunciations to complement our Mandarin-English SEAME code-switch dictionary [3]. In the final results, we fairly observed on average an improvement of 2.0% relative in terms of word error rate and even 27.3% for the case of English Wiktionary word-pronunciation pairs.},
author_keywords={Automatic error recovery;  Multilingual speech recognition;  Pronunciation dictionaries},
keywords={Automatic method;  Multilingual speech recognition;  Open content;  Pronunciation dictionaries;  Pronunciation modeling;  Word error rate, Computer applications;  Computer simulation, Speech recognition},
references={Wiktionary - A Wiki-based Open Content Dictionary, , http://www.wiktionary.org, Website; Schlippe, T., Komgang Djomgang, E.G., Vu, N.T., Ochs, S., Schultz, T., Hausa large vocabulary continuous speech recognition (2012) SLTU; Vu, T., Lyu, D.-C., Weiner, J., Telaar, D., Schlippe, T., Blaicher, F., Chng, E.-S., Li, H., A first speech recognition system for Mandarin-English code-switch conversational speech (2012) ICASSP; Martirosian, O., Davel, M., Error analysis of a public domain pronunciation dictionary (2007) PRASA; Schlippe, T., Ochs, S., Schultz, T., Grapheme-to-phoneme model generation for Indo-European languages (2012) ICASSP; Besling, S., Heuristical and statistical methods for grapheme-to-phoneme conversion (1994) Konvens; Black, A.W., Lenzo, K., Pagel, V., Issues in building general letter to sound rules (1998) ESCA Workshop on Speech Synthesis; Maskey, S., Tomokiyo, L., Black, A.W., Bootstrapping phonetic lexicons for new languages (2004) Interspeech, , Jeju, Korea, October; Davel, M., Barnard, E., Bootstrapping for language resource generation (2003) PRASA; Mertens, P., Vercammen, F., Fonilex manual (1998) Tech. Rep., K. U. Leuven CCL; Kominek, J., Black, A.W., Learning pronunciation dictionaries: Language complexity and word selection strategies (2006) HLT; Davel, M., Barnard, E., The efficient generation of pronunciation dictionaries: Human factors during bootstrapping (2004) ICSLP; Ghoshal, A., Jansche, M., Khudanpur, S., Riley, M., Ulinski, M., Web-derived pronunciations (2009) ICASSP; Schlippe, T., Ochs, S., Schultz, T., Wiktionary as a source for automatic pronunciation extraction (2010) Interspeech; Davel, M., Martirosian, O., Pronunciation dictionary development in resource-scarce environments (2009) HLT; Vozila, P., Adams, J., Thomas, R., Grapheme to phoneme conversion and dictionary verification using graphonemes (2003) Eurospeech; Bisani, M., Ney, H., Joint sequence models for grapheme-to-phoneme conversion (2008) Speech Communication; Davel, M., De Wet, F., Verifying pronunciation dictionaries using conflict analysis (2010) Interspeech; Wolff, M., Eichner, M., Hoffmann, R., Measuring the quality of pronunciation dictionaries (2002) PMLA; Davel, M., Barnard, E., Developing consistent pronunciation variants (2006) Interspeech; Schultz, T., GlobalPhone: A multilingual speech and text database developed at Karlsruhe University (2002) ICSLP; Parihar, N., Picone, J., Pearce, D., Hirsch, H.G., Performance analysis of the Aurora large vocabulary baseline system (2004) European Signal Processing Conference; Au Yeung, S.-K., Siu, M.-H., Improved performance of Aurora-4 using HTK and unsupervised MLLR adaptation (2004) Int. Conference on Spoken Language Processing},
correspondence_address1={Cognitive Systems Lab, Karlsruhe Institute of Technology (KIT)Germany},
address={Portland, OR},
isbn={9781622767595},
language={English},
abbrev_source_title={Annu. Conf. Int. Speech Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Stahlberg201285,
author={Stahlberg, F. and Schlippe, T. and Vogel, S. and Schultz, T.},
title={Word segmentation through cross-lingual word-to-phoneme alignment},
journal={2012 IEEE Workshop on Spoken Language Technology, SLT 2012 - Proceedings},
year={2012},
pages={85-90},
doi={10.1109/SLT.2012.6424202},
art_number={6424202},
note={cited By 16; Conference of 2012 IEEE Workshop on Spoken Language Technology, SLT 2012 ; Conference Date: 2 December 2012 Through 5 December 2012;  Conference Code:95694},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874250689&doi=10.1109%2fSLT.2012.6424202&partnerID=40&md5=467406de9b4910e85b83d5b4e48a5d2a},
affiliation={Cognitive Systems Lab, Karlsruhe Institute of Technology (KIT), Germany; Qatar Computing Research Institute, Qatar Foundation, Qatar},
abstract={We present our new alignment model Model 3P for cross-lingual word-to-phoneme alignment, and show that unsupervised learning of word segmentation is more accurate when information of another language is used. Word segmentation with cross-lingual information is highly relevant to bootstrap pronunciation dictionaries from audio data for Automatic Speech Recognition, bypass the written form in Speech-to-Speech Translation or build the vocabulary of an unseen language, particularly in the context of under-resourced languages. Using Model 3P for the alignment between English words and Spanish phonemes outperforms a state-of-the-art monolingual word segmentation approach [1] on the BTEC corpus [2] by up to 42% absolute in F-Score on the phoneme level and a GIZA++ alignment based on IBM Model 3 by up to 17%. © 2012 IEEE.},
author_keywords={alignment model;  speech-to-speech translation;  under-resourced language;  word segmentation},
keywords={Audio data;  Automatic speech recognition;  Cross-lingual;  Cross-lingual information;  English word;  F-score;  IBM Models;  Pronunciation dictionaries;  Speech-to-speech translation;  Under-resourced languages;  Word segmentation, Computational linguistics;  Speech recognition;  Translation (languages), Alignment},
references={Johnson, M., Goldwater, S., Improving nonparameteric bayesian inference: Experiments on unsupervised word segmentation with adaptor Grammars (2009) ACLHLT, pp. 317-325; Kikui, G., Sumita, E., Takezawa, T., Yamamoto, S., Creating corpora for speech-to-speech translation (2003) Eurospeech; Gordon, R.G., Grimes, B.F., Ethnologue: Languages of the world (2005) SIL International, , 15th edition; Schultz, T., Kirchhoff, K., (2006) Multilingual Speech Processing, , Academic Press Amsterdam; Nettle, D., Romaine, S., (2000) Vanishing Voices: The Extinction of the World's Languages, , Oxford University Press; Besacier, L., Zhou, B., Gao, Y., Towards speech translation of non-written languages (2006) SLT, pp. 222-225; Kit, C.Y., (2000) Unsupervised Lexical Learning As Inductive Inference, , Ph.D. thesis University of Sheffield; Goldsmith, J., An algorithm for the unsupervised learning of morphology (2006) Natural Language Engineering, 12, pp. 353-371; Johnson, M., Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure (2008) ACL-HLT, pp. 398-406; Sẗuker, S., Waibel, A., Towards human translations guided language discovery for asr systems (2008) SLTU; Och, F.J., Ney, H., Improved statistical alignment models (2000) ACL, pp. 440-447; Vogel, S., Ney, H., Tillmann, C., HMM-based word alignment in statistical translation (1996) COLING, pp. 836-841; Brown, P., Della Pietra, S., Della Pietra, V., Mercer, R.L., The mathematics of statistical machine translation: Parameter estimation (1993) Computational Linguistics, 19, pp. 263-311; Och, F.J., Ney, H., A systematic comparison of various statistical alignment models (2003) Computational Linguistics, 29, pp. 19-51; Knight, K., A statistical MT tutorial workbook (1999) JHU Summer Workshop; Schultz, T., Globalphone: A multilingual speech and text database developed at karlsruhe university (2002) ICSLP; Fiscus, J., (2007) Speech Recognition Scoring Toolkit Ver. 2.3 (Sctk); Goldberg, D.E., Genetic algorithms in search (1989) Optimization, and Machine Learning, , Addison-Wesley; Kronfeld, M., Planatscher, H., Zell, A., The EvA2 optimization framework (2010) Learning and Intelligent Optimization, pp. 247-250; Van Rijsbergen, C.J., (1979) Information Retrieval, , Butterworth- Heinemann, Newton, MA, USA, 2nd edition},
correspondence_address1={Stahlberg, F.; Cognitive Systems Lab, Karlsruhe Institute of Technology (KIT)Germany},
sponsors={The Institute of Electrical and Electronics Engineers (IEEE); IEEE Signal Processing Society},
address={Miami, FL},
isbn={9781467351263},
language={English},
abbrev_source_title={IEEE Workshop Spoken Lang. Technol., SLT - Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Putze2012303,
author={Putze, F. and Meyer, J. and Borne, J. and Schultz, T. and Holt, D.V. and Funke, J.},
title={Combining cognitive modeling and EEG to predict user behavior in a search task},
journal={Proceedings of the 11th International Conference on Cognitive Modeling, ICCM 2012},
year={2012},
pages={303-304},
note={cited By 0; Conference of 11th International Conference on Cognitive Modeling, ICCM 2012 ; Conference Date: 13 April 2012 Through 15 April 2012;  Conference Code:96937},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877775837&partnerID=40&md5=c79617df31244eafcc3a033001f860da},
affiliation={Karlsruhe Institute of Technology (KIT), Germany; University of Heidelberg, Germany},
author_keywords={Cognitive modeling;  EEG;  HCI;  Search task;  Spectral analysis;  Temporal difference learning;  User modeling},
references={Anderson, J., Albert, M., Fincham, J., Tracing problem solving in real time: FMRI analysis of the subject-paced tower of hanoi (2005) Journal of Cognitive Neuroscience, 17, pp. 1261-1274; Anderson, J.R., Betts, S., Ferris, J.L., Fincham, J.M., Neural imaging to track mental states while using an intelligent tutoring system (2010) PNAS, 107, pp. 7018-7023; Fu, W., Anderson, J., From recurrent choice to skill learning: A reinforcement-learning model (2006) Journal of Experimental Psychology: General, 135 (2), pp. 183-206; Klimesch, W., EEC alpha and theta oscillations reflect cognitive and memory performance: A review and analysis (1998) Brain Research Reviews, 29, pp. 169-195; Onton, J., Delorme, A., Makeig, S., Frontal midline EEG dynamics during working memory (2005) Neurolmage, 27, pp. 341-356; Peebles, D.J., Cox, A.L., Modelling interactive behaviour with a rational cognitive architecture (2006) Human Computer Interaction Research in Web Design and Evaluation, , P. Zaphiris & S. Kurniawan (Eds.). London: Idea Group; Ritter, S., Anderson, J.R., Koedinger, K.R., Corbett, A., Cognitive tutor: Applied research in mathematics education (2007) Psychonomic Bulletin & Review, 14, pp. 249-255; Sutton, R.S., Barto, A.G., (1998) Reinforcement Learning: An Introduction, , Cambridge, MA: MIT Press},
correspondence_address1={Karlsruhe Institute of Technology (KIT)Germany},
sponsors={German Research Foundation (DFG); European Office of Aerospace Research and Development; German Cognitive Science Society; Gesellschaft von Freunden der TU Berlin e.V.; Cognitve Science Society},
address={Berlin},
isbn={9783798324084},
language={English},
abbrev_source_title={Proc. Int. Conf. Cognitive Model., ICCM},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Herff2012417,
author={Herff, C. and Heger, D. and Putze, F. and Guan, C. and Schultz, T.},
title={Cross-subject classification of speaking modes using fNIRS},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2012},
volume={7664 LNCS},
number={PART 2},
pages={417-424},
doi={10.1007/978-3-642-34481-7_51},
note={cited By 8; Conference of 19th International Conference on Neural Information Processing, ICONIP 2012 ; Conference Date: 12 November 2012 Through 15 November 2012;  Conference Code:93816},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869054577&doi=10.1007%2f978-3-642-34481-7_51&partnerID=40&md5=abf27e0e73b05245489d1c364da9998b},
affiliation={Cognitive Systems Lab. (CSL), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Institute for Infocomm Research, Agency for Science, Technology and Research (ASTAR), Singapore, Singapore},
abstract={In Brain-Computer Interface (BCI) research, subject and session specific training data is usually used to ensure satisfying classification results. In this paper, we show that neural responses to different speaking tasks recorded with functional Near Infrared spectroscopy (fNIRS) are consistent enough across speakers to robustly classify speaking modes with models trained exclusively on other subjects. Our study thereby suggests that future fNIRS-based BCIs can be designed without time-consuming training, which, besides being cumbersome, might be impossible for users with disabilities. Accuracies of 71% and 61% were achieved in distinguishing segments containing overt speech and silent speech from segments in which subjects were not speaking, without using any of the subject's data for training. To rule out artifact contamination, we filtered the data rigorously. To the best of our knowledge, there are no previous studies showing the zero training capability of fNIRS based BCIs. © 2012 Springer-Verlag.},
author_keywords={BCI;  cross-subject;  fNIRS;  session-transfer;  speech imagery},
keywords={BCI;  Brain-computer interfaces (BCI);  Classification results;  cross-subject;  fNIRS;  Functional near infrared spectroscopy;  Neural response;  session-transfer;  Training data;  Users with disabilities, Brain computer interface;  Classification (of information);  Data processing;  Near infrared spectroscopy, Functional neuroimaging},
references={Ang, K.K., Chin, Z.Y., Zhang, H., Guan, C., Filter bank common spatial pattern (FBCSP) in brain-computer interface (2008) IEEE International Joint Conference on Neural Networks, IJCNN, pp. 2390-2397. , IEEE; Ang, K.K., Guan, C., Lee, K., Lee, J.Q., Nioka, S., Chance, B., A Brain-Computer Interface for mental arithmetic task from single-trial near-infrared spectroscopy brain signals (2010) 20th International Conference on Pattern Recognition, pp. 3764-3767; Battiti, R., Using mutual information for selecting features in supervised neural net learning (1994) IEEE Transactions on Neural Networks, pp. 537-550; Coyle, S.M., Ward, T.E., Markham, C.M., Brain-computer interface using a simplified functional near-infrared spectroscopy system (2007) Journal of Neural Engineering, pp. 219-226; Cui, X., Bray, S., Reiss, A.L., Functional near infrared spectroscopy (NIRS) signal improvement based on negative correlation between oxygenated and deoxygenated hemoglobin dynamics (2010) NeuroImage, pp. 3039-3046; Herff, C., Putze, F., Heger, D., Guan, C., Schultz, T., Speaking mode recognition from functional near infrared spectroscopy (2012) International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), , to appear; Krauledat, M., Schröder, M., Blankertz, B., Müller, K.R., Reducing calibration time for brain-computer interfaces: A clustering approach (2007) Advances in Neural Information Processing Systems, pp. 753-760; Krauledat, M., Tangermann, M., Blankertz, B., Towards zero training for brain-computer interfacing (2008) PLoS One, pp. e2967; Leamy, D.J., Collins, R., Ward, T., Combining fNIRS and EEG to improve motor cortex activity classification during an imagined movement-based task (2011) HCI, (20), pp. 177-185; Lotte, F., Guan, C., Learning from other subjects helps reducing Brain-Computer Interface calibration time (2010) IEEE International Conference on Acoustics Speech and Signal Processing, pp. 614-617; Naito, M., Michioka, Y., Ozawa, K., Ito, Y., Kiguchi, M., Kanazawa, T., A communication means for totally locked-in als patients based on changes in cerebral blood volume measured with near-infrared light (2007) IEICE - Trans. Inf. Syst., pp. 1028-1037; Sassaroli, A., Fantini, S., Comment on the modified Beer-Lambert law for scattering media (2004) Physics in Medicine and Biology, pp. N255-N257; Ye, J.C., Tak, S., Jang, K.E., Jung, J., Jang, J., NIRS-SPM: Statistical parametric mapping for near-infrared spectroscopy (2009) NeuroImage, pp. 428-447},
correspondence_address1={Herff, C.; Cognitive Systems Lab. (CSL), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; email: christian.herff@kit.edu},
sponsors={United Development Company PSC (UDC); Qatar Petrochemical Company; ExxonMobil; Qatar Petroleum; Texas A and M University at Qatar; Asia Pacific Neural Network Assembly},
address={Doha},
issn={03029743},
isbn={9783642344800},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hsiao20124093,
author={Hsiao, R. and Schultz, T.},
title={Towards single pass discriminative training for speech recognition},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2012},
pages={4093-4096},
doi={10.1109/ICASSP.2012.6288818},
art_number={6288818},
note={cited By 0; Conference of 2012 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2012 ; Conference Date: 25 March 2012 Through 30 March 2012;  Conference Code:93091},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867590794&doi=10.1109%2fICASSP.2012.6288818&partnerID=40&md5=a1d79c5513ae0f99c0eb141a4020b566},
affiliation={InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={This paper describes how we can combine our previously proposed fast extended Baum-Welch algorithm and generalized discriminative feature transformation to achieve single pass discriminative training, which we only process the data once. Compared to the state of the art training procedure, which uses feature space maximum mutual information (fMMI) and boosted maximum mutual information (BMMI), our proposed training procedure can achieve around 80% of the improvement available from discriminative training. We also show that if we are allowed to process the data twice, it is possible to achieve almost all of the improvement. We evaluate different training procedures on various large scale tasks using Iraqi and modern standard Arabic speech recognition systems. © 2012 IEEE.},
author_keywords={discriminative training;  Speech recognition},
keywords={Arabic speech recognition;  Baum-Welch algorithms;  Discriminative features;  Discriminative training;  Feature space;  Maximum mutual information;  Modern standards;  Single pass;  State of the art;  Training procedures, Metadata;  Signal processing;  Stereophonic broadcasting, Speech recognition},
references={Hsiao, R., Schultz, T., Generalized Baum-Welch Algorithm and Its Implication to a New Extended Baum-Welch Algorithm Proceedings of the INTERSPEECH, 2011; Hsiao, R., Metze, F., Schultz, T., Improvements to Generalized Discriminative Feature Transformation for Speech Recognition Proceedings of the INTERSPEECH, 2010; Povey, D., Improvements to fMPE for Discriminative Training of Features Proceedings of the INTERSPEECH, 2005, pp. 2977-2980; Zhang, B., Matsoukas, S., Schwartz, R., Recent Progress on the Discriminative Region-dependent Transform for Speech Feature Extraction Proceedings of the INTERSPEECH, 2006, pp. 1573-1576; Gales, M.J.F., Maximum Likelihood Linear Transformations for HMM-based Speech Recognition (1998) Computer Speech and Language, 12, pp. 75-98; Kozat, S.S., Visweswariah, K., Gopinath, R.A., Feature Adaptation based on Gaussian Posteriors Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, 2006, pp. 221-224; Bach, N., Eck, M., Charoenpornsawat, P., Köhler, T., Stüker, S., Nguyen, T., Hsiao, R., Black, A.W., The CMU TransTac 2007 Eyes-free, and Handsfree Two-way Speech-to-speech Translation System Proceedings of the IWSLT, 2007; Metze, F., Hsiao, R., Jin, Q., Nallasamy, U., Schultz, T., The 2010 CMU GALE Speech-to-Text System Proceedings of the INTERSPEECH,Makuhari, Japan, 2010; Povey, D., Kanevsky, D., Kingsbury, B., Ramabhadran, B., Saon, G., Visweswariah, K., Boosted MMI for Model and Feature-space Discriminative Training Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, 2008, pp. 4057-4060; Cheng, C., Sha, F., Saul, L.K., A Fast Online Algorithm for Large Margin Training of Continous-density Hidden Markov Models Proceedings of the INTERSPEECH, 2009},
correspondence_address1={Hsiao, R.; InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: wrhsiao@cs.cmu.edu},
sponsors={Inst. Electr. Electron. Eng. Signal Process. Soc.},
address={Kyoto},
issn={15206149},
isbn={9781467300469},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schlippe20124801,
author={Schlippe, T. and Ochs, S. and Schultz, T.},
title={Grapheme-to-phoneme model generation for Indo-European languages},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2012},
pages={4801-4804},
doi={10.1109/ICASSP.2012.6288993},
art_number={6288993},
note={cited By 19; Conference of 2012 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2012 ; Conference Date: 25 March 2012 Through 30 March 2012;  Conference Code:93091},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867605828&doi=10.1109%2fICASSP.2012.6288993&partnerID=40&md5=2ad11a4833fe0b9490e24dd7508618b0},
affiliation={Cognitive Systems Lab., Karlsruhe Institute of Technology (KIT), Germany},
abstract={In this paper, we evaluate grapheme-to-phoneme (g2p) models among languages and of different quality. We created g2p models for Indo-European languages with word-pronunciation pairs from the GlobalPhone project and from Wiktionary [1]. Then we checked their quality in terms of consistency and complexity as well as their impact on Czech, English, French, Spanish, Polish, and German ASR. While the GlobalPhone dictionaries were manually cross-checked and have been used successfully in LVCSR, Wiktionary pronunciations have been provided by the Internet community and can be used to rapidely and economically create pronunciation dictionaries for new languages and domains. © 2012 IEEE.},
author_keywords={multilingual speech recognition;  pronunciation modeling;  web-derived pronunciations},
keywords={Internet communities;  Model generation;  Pronunciation dictionaries;  Pronunciation modeling;  web-derived pronunciations, Signal processing, Speech recognition},
references={Wiktionary - A Wiki-based Open Content Dictionary, , http://www.wiktionary.org, Website; Schlippe, T., Ochs, S., Schultz, T., Wiktionary as a Source for Automatic Pronunciation Extraction (2010) Interspeech; Ghoshal, A., Jansche, M., Khudanpurv, S., Riley, M., Ulinski, M., Web-derived Pronunciations (2009) ICASSP; (1999) Handbook of the International Phonetic Association: A Guide to the Use of the International Phonetic Alphabet, , International Phonetic Association Cambridge University Press; Schultz, T., GlobalPhone: A Multilingual Speech and Text Database Developed at Karlsruhe University (2002) ICSLP; Wolff, M., Eichner, M., Hoffmann, R., Measuring the Quality of Pronunciation Dictionaries (2002) PMLA; Davel, M., Barnard, E., Developing Consistent Pronunciation Models for Phonemic Variants (2006) Interspeech; Davel, M., Martirosian, O., Pronunciation Dictionary Development in Resource-Scarce Environments (2009) Interspeech; Kaplan, R.M., Kay, M., Regular Models of Phonological Rule Systems (1994) Computational Linguistics; Black, A.W., Lenzo, K., Pagel, V., Issues in Building General Letter to Sound Rules ESCA Workshop on Speech Synthesis, 1998; Besling, S., Heuristical and Statistical Methods for Grapheme-to-Phoneme Conversion (1994) Konvens; Bisani, M., Ney, H., Joint-Sequence Models for Grapheme-to-Phoneme Conversion (2008) Speech Communication; Kominek, J., (2009) TTS from Zero - Building Synthetic Voices for New Languages, , Doctoral Thesis},
correspondence_address1={Schlippe, T.; Cognitive Systems Lab., Karlsruhe Institute of Technology (KIT)Germany},
sponsors={Inst. Electr. Electron. Eng. Signal Process. Soc.},
address={Kyoto},
issn={15206149},
isbn={9781467300469},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vu20124345,
author={Vu, N.T. and Schultz, T. and Povey, D.},
title={Modeling gender dependency in the Subspace GMM framework},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2012},
pages={4345-4348},
doi={10.1109/ICASSP.2012.6288881},
art_number={6288881},
note={cited By 2; Conference of 2012 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2012 ; Conference Date: 25 March 2012 Through 30 March 2012;  Conference Code:93091},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867600063&doi=10.1109%2fICASSP.2012.6288881&partnerID=40&md5=51970445095a46736420677d6b270b5d},
affiliation={Karlsruhe Institute of Technology, Germany; Microsoft Research, United States},
abstract={The Subspace GMM acoustic model has both globally shared parameters and parameters specific to acoustic states, and this makes it possible to do various kinds of tying. In the past we have investigated sharing the global parameters among systems with distinct acoustic states; this can be useful in a multilingual setting. In the current paper we investigate the reverse idea: to have different global parameters for different acoustic conditions (gender, in this case) while sharing the acoustic-state-specific parameters. We experiment with modeling gender dependency in this way, and show Word Error Rate improvements on a range of tasks and comparable results to the Vocal Tract Length Normalization (VTLN)-like technique Exponential Transform (ET). © 2012 IEEE.},
author_keywords={gender dependency modeling;  Subspace Gaussian Mixture Models},
keywords={Acoustic conditions;  Acoustic model;  Exponential transforms;  Gaussian Mixture Model;  Global parameters;  Vocal tract length normalization;  Word error rate, Signal processing;  Speech recognition, Social sciences},
references={Povey, D., Burget, L., The Subspace Gaussian Mixture Model-A Structured Model for Speech Recognition (2011) Computer Speech & Language, 25 (2), pp. 404-439. , April; Burget, L., Schwarz, P., Multilingual acoustic modeling for speech recognition based on subspace Gaussian mixture models (2010) ICASSP; Woodland, P., Odell, J., Valtchev, V., Young, S., Large vocabulary continuous speech recognition using HTK Proc. ICASSP, 1994; Bahl, L., Performance of the IBM large vocabulary continuous speech recognition system on the ARPA Wall Street Journal task Proc. ICASSP, 1995; Povey, D., Zweig, G., Acero, A., Speaker Adaptation with an Exponential Transform (2011) ASRU; Povey, D., Ghoshal, A., The Kaldi Speech Recognition Toolkit (2011) ASRU; Paul, D., Baker, J., The design for the Wall Street Journal-based CSR corpus (1992) Proceedings of the Workshop on Speech and Natural Language, pp. 357-362. , Association for Computational Linguistics; Price, P., Fisher, W., Bernstein, J., Pallett, D., The DARPA 1000- word resource management database for continuous speech recognition (1988) ICASSP; Schultz, T., Globalphone: A multilingual speech and text database developed at Karlsruhe University Proc. ICSLP, 2002; Schultz, T., Black, A., Rapid Language Adaptation Tools and Technologies for Multilingual Speech Processing (2008) ICASSP; Vu, N.T., Schlippe, T., Kraus, F., Schultz, T., Rapid Bootstrapping of five Eastern European Languages using the Rapid Language Adaptation Toolkit (2010) Interspeech; Stolcke, A., SRILM-an extensible language modeling toolkit (2002) Proceedings of the International Conference on Spoken Language Processing, 2, pp. 901-904},
correspondence_address1={Vu, N.T.; Karlsruhe Institute of TechnologyGermany; email: thang.vu@kit.edu},
sponsors={Inst. Electr. Electron. Eng. Signal Process. Soc.},
address={Kyoto},
issn={15206149},
isbn={9781467300469},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Janke2012365,
author={Janke, M. and Wand, M. and Nakamura, K. and Schultz, T.},
title={Further investigations on EMG-to-speech conversion},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2012},
pages={365-368},
doi={10.1109/ICASSP.2012.6287892},
art_number={6287892},
note={cited By 10; Conference of 2012 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2012 ; Conference Date: 25 March 2012 Through 30 March 2012;  Conference Code:93091},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867607895&doi=10.1109%2fICASSP.2012.6287892&partnerID=40&md5=08888c595dd3e3e43167cfcb45d9130f},
affiliation={Cognitive Systems Lab., Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany},
abstract={Our study deals with a Silent Speech Interface based on mapping surface electromyographic (EMG) signals to speech waveforms. Electromyographic signals recorded from the facial muscles capture the activity of the human articulatory apparatus and therefore allow to retrace speech, even when no audible signal is produced. The mapping of EMG signals to speech is done via a Gaussian mixture model (GMM)-based conversion technique. In this paper, we follow the lead of EMG-based speech-to-text systems and apply two major recent technological advances to our system, namely, we consider session-independent systems, which are robust against electrode repositioning, and we show that mapping the EMG signal to whispered speech creates a better speech signal than a mapping to normally spoken speech. We objectively evaluate the performance of our systems using a spectral distortion measure. © 2012 IEEE.},
author_keywords={Electromyography;  Silent Speech;  Speech Synthesis;  Voice Conversion},
keywords={Audible signals;  Electromyographic signal;  EMG signal;  Facial muscles;  Gaussian Mixture Model;  Spectral distortions;  Speech interface;  Speech signals;  Speech waveforms;  Speech-to-text system;  Technological advances;  Voice conversion;  Whispered speech, Electromyography;  Mapping;  Signal processing;  Speech processing;  Speech synthesis, Speech communication},
references={Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Silent Speech Interfaces (2010) Speech Communication, 52 (4), pp. 270-287; Chan, A.D.C., Englehart, K., Hudgins, B., Lovely, D.F., HiddenMarkov Model Classification ofMyolectric Signals in Speech (2002) IEEE Engineering in Medicine and Biology Society, 21 (5), pp. 143-146. , D.F; Jou, S.C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards Continuous Speech Recognition using Surface Electromyography (2006) Proceedings of Interspeech 2006, pp. 573-576; Toth, A.R., Wand, M., Schultz, T., Synthesizing Speech from Electromyography using Voice Transformation Techniques (2009) Proceedings of Interspeech 2009, pp. 652-655; Wand, M., Toth, A., Jou, S.C., Schultz, T., Impact of Different Speaking Modes on EMG-based Speech Recognition (2009) Proceedings of Interspeech 2009, pp. 648-651; Nakamura, K., Janke, M., Wand, M., Schultz, T., Estimation of Fundamental Frequency from Surface Electromyographic Data: EMG-to-F0 (2011) Proceedings of International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 573-576; Stylianou, Y., Cappe, O., Moulines, E., Continuous Probabilistic Transform for Voice Conversion (1998) IEEE Transaction on Speech and Audio Processing (SAP), 6 (2), pp. 131-142; Kain, A., Macon, M.W., Spectral Voice Conversion for Text-to-speech Synthesis (1998) Proceedings of International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 285-288; Janke, M., Wand, M., Schultz, T., Spectral EnergyMapping for EMG-based Recognition of Silent Speech Proceedings of First International Workshop on Bio-inspired Human-Machine Interfaces and Healthcare Applications, 2010; Tran, V.A., Bailly, A.G., Loevenbruck, H., Toda, T., Multimodal HMM-based NAM-to-speech conversion (2009) Proceedings of Interspeech 2009, pp. 656-659; Toda, T., Shikano, K., NAM-to-Speech Conversion with Gaussian Mixture Models (2005) Proceedings of Interspeech 2005, pp. 1957-1960},
correspondence_address1={Janke, M.; Cognitive Systems Lab., Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; email: matthias.janke@kit.edu},
sponsors={Inst. Electr. Electron. Eng. Signal Process. Soc.},
address={Kyoto},
issn={15206149},
isbn={9781467300469},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vu20124889,
author={Vu, N.T. and Lyu, D.-C. and Weiner, J. and Telaar, D. and Schlippe, T. and Blaicher, F. and Chng, E.-S. and Schultz, T. and Li, H.},
title={A first speech recognition system for Mandarin-English code-switch conversational speech},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2012},
pages={4889-4892},
doi={10.1109/ICASSP.2012.6289015},
art_number={6289015},
note={cited By 80; Conference of 2012 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2012 ; Conference Date: 25 March 2012 Through 30 March 2012;  Conference Code:93091},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867602646&doi=10.1109%2fICASSP.2012.6289015&partnerID=40&md5=fa4f7d7f904bf906713abe2f28be006e},
affiliation={Cognitive Systems Lab., Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT), Germany; School of Computer Engineering, Nanyang Technological University, Singapore, Singapore},
abstract={This paper presents first steps toward a large vocabulary continuous speech recognition system (LVCSR) for conversational Mandarin-English code-switching (CS) speech. We applied state-of-the-art techniques such as speaker adaptive and discriminative training to build the first baseline system on the SEAME corpus [1] (South East Asia Mandarin-English). For acoustic modeling, we applied different phone merging approaches based on the International Phonetic Alphabet (IPA) and Bhattacharyya distance in combination with discriminative training to improve accuracy. On language model level, we investigated statistical machine translation (SMT) - based text generation approaches for building code-switching language models. Furthermore, we integrated the provided information from a language identification system (LID) into the decoding process by using a multi-stream approach. Our best 2-pass system achieves a Mixed Error Rate (MER) of 36.6% on the SEAME development set. © 2012 IEEE.},
author_keywords={code-switching;  multilingual speech recognition},
keywords={Acoustic modeling;  Baseline systems;  Bhattacharyya distance;  code-switching;  Decoding process;  Discriminative training;  International Phonetic Alphabet;  Language identification system;  Language model;  Large vocabulary continuous speech recognition;  Mixed errors;  Multi-stream;  Southeast Asia;  Speech recognition systems;  Statistical machine translation;  Text generations, Building codes;  Continuous speech recognition;  Signal processing, Computational linguistics},
references={Lyu, D., Tan, T., Chng, E., Li, H., An Analysis of a Mandarin-English Code-switching Speech Corpus: SEAME Interspeech, Japan, 2010; Auer, P., (1998) Code-Switching in Conversation: Language, Interaction and Identity, , London: Routledge; Yeh, C., Huang, C., Sun, L., Lee, L., An integrated Framework for Transcribing Mandarin-English Code-mixed Lectures with Improved Acoustic and Language Modeling ISCSLP, Taiwan, 2010; Tsai, T., Chiang, C.Y., Yu, H., Lo, L., Wang, Y.R., Chen, S.H., A Study on Hakka and Mixed Hakka-Mandarin Speech Recognition ISCSLP, Taiwan, 2010; Yu, S., Zhang, S., Xu, B., Chinese-English Bilingual Phone Modeling for Cross-Language Speech Recognition ICASSP, Canada, 2004; Cao, H., Lee, T., Ching, P.C., Development of the Cantonese-English code-mixing speech corpora (2010) Computer Processing of Asian Spoken Languages, pp. 204-207. , Shuichi Itahashi and Chiu-yu Tseng et al., eds., Japan: Consideration Books, March; Bhuvanagiri, K., Kopparapu, S., An Approach to Mixed Language Automatic Speech Recognition Oriental COCOSDA, Nepal, 2010; Lyu, D.C., Lyu, R.Y., Chiang, Y., Hsu, C.N., Recognition on Code- Switching among the Chinese Dialects ICASSP, France, 2006; http://www.speech.cs.cmu.edu/cgi-bin/cmudict; Hsiao, R., Fuhs, M., Tam, Y., Jin, Q., Schultz, T., The CMU-InterACT 2008 Mandarin Transcription System Interspeech, Australia, 2008; Chen, W., Tan, Y., Chng, E., Li, H., The development of a Singapore English call resource Oriental COCOSDA, Nepal, 2010; Finke, M., Geutner, P., Hild, H., Kemp, T., Ries, K., Westphal, M., The Karlsruhe Verbmobil Speech Recognition Engine ICASSP, Germany, 1997; Gales, M., Semi-tied covariance matrices for hidden Markov models (1999) IEEE Transactions Speech and Audio Processing, 7, pp. 272-281; Povey, D., (2003) Discriminative Training for Large Vocabulary Speech Recognition, , Ph.D. dissertation, Cambridge University Engineering Dept; (1999) Handbook, IPA: Handbook of the International Phonetic Association; Mak, B., Barnard, E., Phone Clustering Using the Bhattacharyya Distance (1996) Proceedings of ISCSLP, Philadelphia, PA, USA, pp. 2005-2008; Stolcke, A., SRILM an Extensible Language Modeling Toolkit ISCSLP, 2002; Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B., Herbst, E., Moses: Open Source Toolkit for Statistical Machine Translation Annual Meeting of ACL, Demonstration Session, Czech Republic, 2007},
correspondence_address1={Vu, N.T.; Cognitive Systems Lab., Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT)Germany; email: thang.vu@kit.edu},
sponsors={Inst. Electr. Electron. Eng. Signal Process. Soc.},
address={Kyoto},
issn={15206149},
isbn={9781467300469},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Amma201252,
author={Amma, C. and Georgi, M. and Schultz, T.},
title={Airwriting: Hands-free mobile text input by spotting and continuous recognition of 3d-space handwriting with inertial sensors},
journal={Proceedings - International Symposium on Wearable Computers, ISWC},
year={2012},
pages={52-59},
doi={10.1109/ISWC.2012.21},
art_number={6246142},
note={cited By 61; Conference of 16th International Symposium on Wearable Computers, ISWC 2012 ; Conference Date: 18 June 2012 Through 22 June 2012;  Conference Code:92808},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866538938&doi=10.1109%2fISWC.2012.21&partnerID=40&md5=99291be2e60fc1691f098ca06307533e},
affiliation={Cognitive Systems Lab., Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany},
abstract={We present an input method which enables complex hands-free interaction through 3d handwriting recognition. Users can write text in the air as if they were using an imaginary blackboard. Motion sensing is done wirelessly by accelerometers and gyroscopes which are attached to the back of the hand. We propose a two-stage approach for spotting and recognition of handwriting gestures. The spotting stage uses a Support Vector Machine to identify data segments which contain handwriting. The recognition stage uses Hidden Markov Models (HMM) to generate the text representation from the motion sensor data. Individual characters are modeled by HMMs and concatenated to word models. Our system can continuously recognize arbitrary sentences, based on a freely definable vocabulary with over 8000 words. A statistical language model is used to enhance recognition performance and restrict the search space. We report the results from a nine-user experiment on sentence recognition for person dependent and person independent setups on 3d-space handwriting data. For the person independent setup, a word error rate of 11% is achieved, for the person dependent setup 3% are achieved. We evaluate the spotting algorithm in a second experiment on a realistic dataset including everyday activities and achieve a sample based recall of 99% and a precision of 25%. We show that additional filtering in the recognition stage can detect up to 99% of the false positive segments. © 2012 IEEE.},
author_keywords={Accelerometers;  Handwriting recognition;  User interfaces;  Wearable computers},
keywords={Data segment;  Data sets;  False positive;  Hands-free;  Hands-free interactions;  Handwriting data;  Handwriting recognition;  Inertial sensor;  Input methods;  Motion sensing;  Motion sensors;  Person-dependent;  Person-independent;  Recognition of handwriting;  Recognition performance;  Search spaces;  Statistical language models;  Text input;  Text representation;  Word error rate;  Word models, Accelerometers;  Experiments;  Hidden Markov models;  Natural language processing systems;  Speech recognition;  Three dimensional computer graphics;  User interfaces;  Wearable computers, Character recognition},
references={Mitra, S., Acharya, T., Gesture Recognition: A Survey (2007) IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews, 37, pp. 311-324; Lyons, K., Starner, T., Plaisted, D., Fusia, J., Lyons, A., Drew, A., Looney, E.W., Twiddler typing: One-handed chording text entry for mobile phones Proc. of the SIGCHI Conference on Human Factors in Computing Systems (CHI'04), 2004; MacKenzie, I.S., Soukoreff, R.W., Helga, J., 1 thumb, 4 buttons, 20 words per minute: Design and evaluation of h4-writer Proc. of the 24th Annual ACM Symposium on User Interface Software and Technology (UIST'11), 2011; Mistry, P., Maes, P., Chang, L., Wuw - Wear ur world: A wearable gestural interface Proc. of the 27th International Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA '09), 2009; Tamaki, E., Miyaki, T., Rekimoto, J., Brainy hand: An ear-worn hand gesture interaction device Proc. of the 27th International Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA '09), 2009; Gustafson, S., Bierwirth, D., Baudisch, P., Imaginary interfaces: Spatial interaction with empty hands and without visual feedback Proc. of the 23nd Annual ACM Symposium on User Interface Software and Technology (UIST'10), 2010; Junker, H., Amft, O., Lukowicz, P., Tröster, G., Gesture spotting with body-worn inertial sensors to detect user activities (2008) Pattern Recognition, 41 (6), pp. 2010-2024; Raffa, G., Lee, J., Nachman, L., Song, J., Don't slow me down: Bringing energy efficiency to continuous gesture recognition Proc. International Symposium on Wearable Computers (ISWC'10), 2010; Lee, H.-K., Kim, J., An hmm-based threshold model approach for gesture recognition (1999) IEEE Transactions on Pattern Analysis and Machine Intelligence, 21 (10), pp. 961-973. , oct; Stiefmeier, T., Roggen, D., Tröster, G., Ogris, G., Lukowicz, P., Wearable activity tracking in car manufacturing (2008) IEEE Pervasive Computing, 7 (2), p. 42; Kallio, S., Kela, J., Mantyjarvi, J., Online gesture recognition system for mobile interaction Proc. IEEE International Conference on Systems, Man and Cybernetics (ICSMC'03), 2003; Hein, A., Hoffmeyer, A., Kirste, T., Utilizing an accelerometric bracelet for ubiquitous gesture-based interaction (2009) Lecture Notes in Computer Science, 5615, pp. 519-527. , Universal Access in Human-Computer Interaction. Intelligent and Ubiquitous Interaction Environments, ser. Springer Berlin / Heidelberg; Amft, O., Amstutz, R., Smailagic, A., Siewiorek, D., Trster, G., Gesture-controlled user input to complete questionnaires on wrist-worn watches (2009) Lecture Notes in Computer Science, 5611, pp. 131-140. , Human-Computer Interaction. Novel Interaction Methods and Techniques, ser. Springer Berlin / Heidelberg; Kim, D., Choi, H., Kim, J., 3d space handwriting recognition with ligature model (2006) Lecture Notes in Computer Science, 4239, pp. 41-56. , Ubiquitous Computing Systems, ser. Springer Berlin / Heidelberg; Bang, W.-C., Chang, W., Kang, K.-H., Choi, E.-S., Potanin, A., Kim, D.-Y., Self-contained spatial input device for wearable computers Proc. IEEE International Symposium on Wearable Computers (ISWC'03), 2003; Plamondon, R., Srihari, S., Online and off-line handwriting recognition: A comprehensive survey (2000) IEEE Transactions on Pattern Analysis and Machine Intelligence, 22 (1), pp. 63-84; McGuire, R., Hernandez-Rebollar, J., Starner, T., Henderson, V., Brashear, H., Ross, D., Towards a one-way american sign language translator Proc. Sixth IEEE International Conference on Automatic Face and Gesture Recognition (FGR'04), 2004; Amma, C., Gehrig, D., Schultz, T., Airwriting recognition using wearable motion sensors Proceedings of the 1st Augmented Human International Conference (AH'10), 2010; Amma, C., Schultz, T., Airwriting: Demonstrating Mobile Text Input by 3d-Space Handwriting Proc. of the ACM International Conference on Intelligent User Interfaces (IUI'12), 2012; Woodman, O.J., (2007) An Introduction to Inertial Navigation, , University of Cambridge, Tech. Rep; Rabiner, L., A tutorial on hidden markov models and selected applications in speech recognition (1989) Proceedings of the IEEE, 77 (2), pp. 257-286; Huang, X., Acero, A., Hon, H., (2001) Spoken Language Processing, , Prentice Hall; Soltau, H., Metze, F., Fügen, C., Waibel, A., A one-pass decoder based on polymorphic linguistic context assignment IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU '01), 2001; Schultz, T., GlobalPhone: A multilingual speech and text database developed at Karlsruhe University Proc. of the International Conference on Spoken Language Processing, (ICSLP'02), 2002},
correspondence_address1={Amma, C.; Cognitive Systems Lab., Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; email: christoph.amma@kit.edu},
sponsors={Google; Nokia; Interaction-design.org},
address={Newcastle},
issn={15504816},
isbn={9780769546971},
language={English},
abbrev_source_title={Proc. Int. Symp. Wearable Comput. ISWC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kuehne2012634,
author={Kuehne, H. and Gehrig, D. and Schultz, T. and Stiefelhagen, R.},
title={On-line action recognition from sparse feature flow},
journal={VISAPP 2012 - Proceedings of the International Conference on Computer Vision Theory and Applications},
year={2012},
volume={1},
pages={634-639},
note={cited By 7; Conference of International Conference on Computer Vision Theory and Applications, VISAPP 2012 ; Conference Date: 24 February 2012 Through 26 February 2012;  Conference Code:90194},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862136321&partnerID=40&md5=90ad27bc50124b8757e39ad4784c061f},
affiliation={Computer Vision for Human-Computer Interaction Lab., Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Cognitive Systems Lab., Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany},
abstract={The fast and robust recognition of human actions is an important aspect for many video-based applications in the field of human computer interaction and surveillance. Although current recognition algorithms provide more and more advanced results, their usability for on-line applications is still limited. To bridge this gap a online video-based action recognition system is presented that combines histograms of sparse feature point flow with an HMM-based action recognition. The usage of feature point motion is computational more efficient than the more common histograms of optical flow (HoF) by reaching a similar recognition accuracy. For recognition we use low-level action units that are modeled by Hidden-Markov-Models (HMM). They are assembled by a context free grammar to recognize complex activities. The concatenation of small action units to higher level tasks allows the robust recognition of action sequences as well as a continuous on-line evaluation of the ongoing activity. The average runtime is around 34 ms for processing one frame and around 20 ms for calculating one hypothesis for the current action. Assuming that one hypothesis per second is needed, the system can provide a mean capacity of 25 fps. The systems accuracy is compared with state of the art recognition results on a common benchmark dataset as well as with a marker-based recognition system, showing similar results for the given evaluation scenario. The presented approach can be seen as a step towards the on-line evaluation and recognition of human motion directly from video data.},
author_keywords={Action recognition;  Human computer interaction;  Motion analysis;  Sequence analysis},
keywords={Action recognition;  Action recognition systems;  Action sequences;  Action Unit;  Benchmark datasets;  Complex activity;  Human actions;  Human motions;  Motion analysis;  On-line applications;  On-line evaluation;  Point flows;  Recognition accuracy;  Recognition algorithm;  Recognition systems;  Robust recognition;  Runtimes;  Sequence analysis;  State of the art;  Video data, Computer vision;  Context free grammars;  Graphic methods;  Hidden Markov models;  Human computer interaction;  Image recognition;  Security systems, Motion estimation},
references={Danafar, S., Gheissari, N., Action recognition for surveillance applications using optic flow and svm (2007) ACCV, 2, pp. 457-466; Efros, A.A., Berg, A.C., Mori, G., Malik, J., Recognizing action at a distance (2003) IEEE International Conference on Computer Vision, pp. 726-733. , Nice, France; Finke, M., Geutner, P., Hild, H., Kemp, T., Ries, K., Westphal, M., The karlsruhe-verbmobil speech recognition engine (1997) ICASSP-97, 1, pp. 83-86; Gehrig, D., Khne, H., Wrner, A., Schultz, T., Hmm-based human motion recognition with optical flow data (2009) 9th IEEE-RAS International Conference on Humanoid Robots, Humanoids 2009, , Paris, France; Ivanov, Y.A., Bobick, A.F., Recognition of visual activities and interactions by stochastic parsing (2000) IEEE Transactions on Pattern Analysis and Machine Intelligence, 22, pp. 852-872; Koehler, H., Woerner, A., Motion-based feature tracking for articulated motion analysis (2008) Workshop on Multimodal Interactions Analysis of Users A Controlled Environment, IEEE Int. Conf. on Multimodal Interfaces (ICMI 2008), , Chania, Greece; Lucas, B.D., Kanade, T., (1981) An Iterative Image Registration Technique with An Application to Stereo Vision; Lucena, M.J., De La Blanca, N.P., Fuertes, J.M., Marín- Jiménez, M.J., Human action recognition using optical flow accumulated local histograms (2009) Iberian Conf. on Pattern Recognition and Image Analysis, IbPRIA, pp. 32-39; Marszalek, M., Laptev, I., Schmid, C., Actions in context (2009) Computer Vision and Pattern Recognition, IEEE Computer Society Conference on, pp. 2929-2936; Martinetz, T., Schulten, K., A .neural-gas. network learns topologies (1991) Artificial Neural Networks, 1, pp. 397-402; Mendoza, M.A., De La Blanca, N.P., Marín-Jiménez, M.J., Fitting product of hmm to human motions (2009) Proc. of the 13th Int. Conf. on Computer Analysis of Images and Patterns, CAIP, pp. 824-831. , Berlin, Heidelberg. Springer-Verlag; Messing, R., Pal, C., Kautz, H., Activity recognition using the velocity histories of tracked keypoints (2009) ICCV, , Washington, DC, USA. IEEE Computer Society; Shi, J., Tomasi, C., Good features to track (1994) Proceedings of the Conference on Computer Vision and Pattern Recognition, pp. 593-600; Soltau, H., Metze, F., Fugen, C., Waibel, A., A one-pass decoder based on polymorphic linguistic context assignment (2001) ASRU, pp. 214-217; Tomasi, C., Kanade, T., Detection and tracking of point features. Technical report (1991) International Journal of Computer Vision},
correspondence_address1={Kuehne, H.; Computer Vision for Human-Computer Interaction Lab., Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; email: hildegard.kuehne@kit.edu},
sponsors={Inst. Syst. Technol. Inf., Control Commun. (INSTICC)},
address={Rome},
isbn={9789898565037},
language={English},
abbrev_source_title={VISAPP - Proc. Int. Conf. Comput. Vis. Theory Appl.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wand2012101,
author={Wand, M. and Janke, M. and Schultz, T.},
title={Decision-tree based analysis of speaking mode discrepancies in EMG-based speech recognition},
journal={BIOSIGNALS 2012 - Proceedings of the International Conference on Bio-Inspired Systems and Signal Processing},
year={2012},
pages={101-109},
note={cited By 5; Conference of International Conference on Bio-inspired Systems and Signal Processing, BIOSIGNALS 2012 ; Conference Date: 1 February 2012 Through 4 February 2012;  Conference Code:90161},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861983633&partnerID=40&md5=091ed0a1eebc75a42229b0da6e0eb0e6},
affiliation={Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={This study is concerned with the impact of speaking mode variabilities on speech recognition by surface electromyography (EMG). In EMG-based speech recognition, we capture the electric potentials of the human articulatory muscles by surface electrodes, so that the resulting signal can be used for speech processing. This enables the user to communicate silently, without uttering any sound. Previous studies have shown that the processing of silent speech creates a new challenge, namely that EMG signals of audible and silent speech are quite distinct. In this study we consider EMG signals of three speaking modes: audibly spoken speech, whispered speech, and silently mouthed speech. We present an approach to quantify the differences between these speaking modes by means of phonetic decision trees and show that this measure correlates highly with differences in the performance of a recognizer on the different speaking modes. We furthermore reinvestigate the spectral mapping algorithm, which reduces the discrepancy between different speaking modes, and give an evaluation of its effectiveness.},
author_keywords={EMG;  EMG-based speech recognition;  Phonetic decision tree;  Silent speech interfaces},
keywords={EMG;  EMG signal;  Phonetic decision tree;  Spectral mappings;  Speech interface;  Surface electrode;  Surface electromyography;  Whispered speech, Conformal mapping;  Decision trees;  Electric potential;  Electromyography;  Forestry;  Signal processing;  Speech processing, Speech recognition, Decision Theory;  Electric Potential;  Evaluation;  Forestry;  Trees},
references={Bahl, L.R., De Souza, P.V., Gopalakrishnan, P.S., Nahmoo, D., Picheny, M.A., Decision trees for phonological rules in continuous speech (1991) Proc. of the IEEE International Conference of Acoustics, Speech, and Signal Processing (ICASSP), pp. 185-188. , Toronto, Ontario, Canada; Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Silent speech interfaces (2010) Speech Communication, 52 (4), pp. 270-287; Finke, M., Rogina, I., Wide context acoustic modeling in read vs. Spontaneous speech (1997) Proc. ICASSP, 3, pp. 1743-1746; Janke, M., Wand, M., Schultz, T., A spectral mapping method for EMG-based recognition of silent speech (2010) Proc. B-interface; Janke, M., Wand, M., Schultz, T., Impact of lack of acoustic feedback in EMG-based silent speech recognition (2010) Proc. Interspeech; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proc. Inter-speech, pp. 573-576. , Pittsburgh, PA; Kirchhoff, K., (1999) Robust Speech Recognition Using Articulatory Information, , PhD thesis, University of Bielefeld; Metze, F., Waibel, A., A flexible stream architecture for ASR using articulatory features (2002) Proc. of the International Conference on Spoken Language Processing (ICSLP), pp. 2133-2136. , Denver, Colorado, USA; Schultz, T., Waibel, A., Language independent and language adaptive acoustic modeling for speech recognition (2001) Speech Communication, 35, pp. 31-51; Schultz, T., Wand, M., Modeling coarticulation in large vocabulary EMG-based speech recognition (2010) Speech Communication, 52, pp. 341-353; Schünke, M., Schulte, E., Schumacher, U., (2006) Prometheus - Lernatlas der Anatomie, Volume [3]: Kopf und Neuroanatomie, , Thieme Verlag, Stuttgart, New York; Wand, M., Janke, M., Schultz, T., Investigations on speaking mode discrepancies in EMG-based speech recognition (2011) Proc. Interspeech; Wand, M., Jou, S.-C.S., Toth, A.R., Schultz, T., Impact of different speaking modes on EMG-based speech recognition (2009) Proc. Interspeech; Wand, M., Schultz, T., Session-independent EMG-based speech recognition (2011) Proc. Biosignals; Welch, P., The use of fast fourier transform for the estimation of power spectra: A method based on time averaging over short, modified periodograms (1967) Audio and Electroacoustics, 15 (2), pp. 70-73. , IEEE Transactions on},
correspondence_address1={Wand, M.; Karlsruhe Institute of Technology, Karlsruhe, Germany; email: michael.wand@kit.edu},
sponsors={Inst. Syst. Technol. Inf., Control Commun. (INSTICC)},
address={Vilamoura, Algarve},
isbn={9789898425898},
language={English},
abbrev_source_title={BIOSIGNALS - Proc. Int. Conf. Bio-Inspired Syst. Signal Process.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Amma2012319,
author={Amma, C. and Schultz, T.},
title={Airwriting: Demonstrating mobile text input by 3d-space handwriting recognition},
journal={International Conference on Intelligent User Interfaces, Proceedings IUI},
year={2012},
pages={319-320},
doi={10.1145/2166966.2167032},
note={cited By 5; Conference of 2012 17th ACM International Conference on Intelligent User Interfaces, IUI'12 ; Conference Date: 14 February 2012 Through 17 February 2012;  Conference Code:89402},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860000285&doi=10.1145%2f2166966.2167032&partnerID=40&md5=55f6864f3bcb4124c26b5e552c1288b6},
affiliation={Karlsruhe Institute of Technology, 76131 Karlsruhe, Germany},
abstract={We demonstrate our airwriting interface for mobile handsfree text entry. The interface enables a user to input text into a computer by writing in the air like on an imaginary blackboard. Hand motion is measured by an accelerometer and a gyroscope attached to the back of the hand and data is sent wirelessly to the processing computer. The system can continuously recognize arbitrary sentences based on a predefined vocabulary in real-time. The recognizer uses Hidden Markov Models (HMM) together with a statistical language model. We achieve a user-independent word error rate of 11% for a 8K vocabulary based on an experiment with nine users.},
author_keywords={Accelerometers;  Gesture recognition;  Handwriting recognition;  Human computer interaction;  Wearable computing},
keywords={Hand motion;  Hands-free;  Handwriting recognition;  Statistical language models;  Text entry;  Text input;  Wearable computing;  Word error rate, Gesture recognition;  Hidden Markov models;  Human computer interaction;  Wearable computers, Accelerometers},
references={Agrawal, S., Constandache, I., Gaonkar, S., Roy Choudhury, R., Caves, K., Deruyter, F., Using mobile phones to write in air (2011) Proc. of MobySys '11, pp. 15-28. , ACM; Amma, C., Gehrig, D., Schultz, T., Airwriting recognition using wearable motion sensors (2010) Proc. Augmented Human Conf, , ACM Press; Gustafson, S., Bierwirth, D., Baudisch, P., Imaginary interfaces: Spatial interaction with empty hands and without visual feedback (2010) Proc. of UIST '10, pp. 3-12. , ACM; Kim, D., Choi, H., Kim, J., 3d space handwriting recognition with ligature model (2006) Ubiquitous Computing Systems, pp. 41-56. , vol. 4239/2006 of LNCS, Springer Berlin / Heidelberg; Mistry, P., Maes, P., Sixthsense: A wearable gestural interface (2009) ACM SIGGRAPH ASIA Sketches, p. 11. , ACM},
correspondence_address1={Amma, C.; Karlsruhe Institute of Technology, 76131 Karlsruhe, Germany; email: christoph.amma@kit.edu},
sponsors={ACM Special Interest Group on Artificial Intelligence (SIGART); ACM Special Interest Group on Artificial Intelligence (SIGCHI)},
address={Lisbon},
isbn={9781450310482},
language={English},
abbrev_source_title={Int Conf Intell User Interfaces Proc IUI},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{MelvinJose201281,
author={Melvin Jose, J. and Vu, N.T. and Schultz, T.},
title={Initial experiments with Tamil LVCSR},
journal={Proceedings - 2012 International Conference on Asian Language Processing, IALP 2012},
year={2012},
pages={81-84},
doi={10.1109/IALP.2012.46},
art_number={6473701},
note={cited By 1; Conference of 2012 International Conference on Asian Language Processing, IALP 2012 ; Conference Date: 13 November 2012 Through 15 November 2012;  Conference Code:105167},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900868667&doi=10.1109%2fIALP.2012.46&partnerID=40&md5=9dc5a64e6ea62ecac6974986f1fe1050},
affiliation={Department of Computer Technology, MIT Campus, Anna University, Chennai, India; Cognitive Systems Lab, Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany},
abstract={In this paper we present our recent efforts towards building a large vocabulary continuous speech recognizer for Tamil. We describe the text and speech corpus collected to realize this task. The data was complemented by a large amount of text data crawled from various Tamil news websites. The Tamil speech recognition system was bootstrapped using the Rapid Language Adaptation scheme which employs a multilingual phone inventory. After initialization, we built a word-based and syllable-based system with a Syllable Error Rate (SyllER) of 29.30% and 34.16%, respectively. We propose a data-driven approach to obtain better dictionary units to overcome the challenge of the agglutinative nature of Tamil. The approach produced a significant improvement of 27.20% and 15.12% relative SyllER on the test set over the syllable- and word-based systems, respectively. Our current best system has a SyllER of 17.44% on read newspaper speech. © 2012 IEEE.},
author_keywords={Agglutinative language;  Dictionary units;  LVCSR System;  Morphological complexity;  Multilingual bootstrap},
keywords={Speech recognition, Agglutinative language;  Continuous speech;  Data-driven approach;  LVCSR System;  Morphological complexity;  Multilingual bootstrap;  Speech recognition systems;  Word-based systems, Natural language processing systems},
references={Schultz, T., Black, A.W., Badaskar, S., Hornyak, M., Kominek, J., SPICE: Web-based tools for rapid language adaptation in speech processing systems (2007) Proceedings of Interspeech, , August; Kumar, R., Kishore, S., Gopalakrishna, A., Chitturi, R., Joshi, S., Singh, S., Sitaram, R., Development of Indian language speech databases for large vocabulary speech recognition systems (2005) SPECOM Proceedings; Plauche, M., Udhyakummar, N., Wooters, C., Pal, J., Ramachadran, D., Speech Recognition for illiterate access to information and technology (2006) Proceedings of First International Conference on ICT and Development; Lakshmi, A., Murthy, H.A., A syllable based continuous speech recognizer for Tamil (2006) Interspeech; Saraswathi, S., Geetha, T.V., Design of language models at various phases of Tamil speech recognition system (2010) International Journal of Engineering, Science and Technology, 2 (5), pp. 244-257; Thangarajan, R., Natarajan, A.M., Selvam, M., Word and triphone based approaches in continuous speech recognition for Tamil language (2008) WSEAS Trans. Sig. Proc, 4 (3), pp. 76-85; Schultz, T., Kirchhoff, K., (2006) Multilingual Speech Processing, p. 88. , Elsevier, Academic Press; Bisani, M., Ney, H., Joint-sequence models for grapheme-tophoneme conversion (2008) Speech Communication, 50 (5), pp. 434-451; Stolcke, A., SRILM-An extensible language modeling toolkit (2002) Intl. Conf. Spoken Language Processing, pp. 901-904. , September; Kiecza, D., Schultz, T., Waibel, A., Data-Driven determination of appropriate dictionary units for Korean LVCSR (1999) Proceedings of ICASSP, pp. 323-327},
sponsors={},
publisher={IEEE Computer Society},
address={Hanoi},
language={English},
abbrev_source_title={Proc. - Int. Conf. Asian Lang. Process., IALP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gehrig20114819,
author={Gehrig, D. and Krauthausen, P. and Rybok, L. and Kuehne, H. and Hanebeck, U.D. and Schultz, T. and Stiefelhagen, R.},
title={Combined intention, activity, and motion recognition for a humanoid household robot},
journal={IEEE International Conference on Intelligent Robots and Systems},
year={2011},
pages={4819-4825},
doi={10.1109/IROS.2011.6048716},
art_number={6048716},
note={cited By 25; Conference of 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems: Celebrating 50 Years of Robotics, IROS'11 ; Conference Date: 25 September 2011 Through 30 September 2011;  Conference Code:87712},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84455172977&doi=10.1109%2fIROS.2011.6048716&partnerID=40&md5=63b6d5aa4cec9601b3ad9920811ed0ba},
affiliation={Cognitive Systems Lab. (CSL), Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Intelligent Sensor-Actuator-Systems Laboratory (ISAS), Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Computer Vision for Human-Computer Interaction Lab. (CV:HCI), Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Fraunhofer IOSB, Karlsruhe, Germany},
abstract={In this paper, a multi-level approach to intention, activity, and motion recognition for a humanoid robot is proposed. Our system processes images from a monocular camera and combines this information with domain knowledge. The recognition works on-line and in real-time, it is independent of the test person, but limited to predefined view-points. Main contributions of this paper are the extensible, multi-level modeling of the robot's vision system, the efficient activity and motion recognition, and the asynchronous information fusion based on generic processing of mid-level recognition results. The complementarity of the activity and motion recognition renders the approach robust against misclassifications. Experimental results on a real-world data set of complex kitchen tasks, e.g., Prepare Cereals or Lay Table, prove the performance and robustness of the multi-level recognition approach. © 2011 IEEE.},
keywords={Domain knowledge;  Household robots;  Humanoid robot;  Misclassifications;  Monocular cameras;  Motion recognition;  Multi-level;  Multilevel modeling;  Real world data;  System process;  Vision systems, Anthropomorphic robots;  Computer vision;  Robotics;  Virtual reality, Intelligent robots},
references={Schrempf, O.C., Hanselmann, A., Hanebeck, U.D., Efficient Representation and Fusion of Hybrid Joint Densities for Clusters in Nonlinear Hybrid Bayesian Networks (2006) Fusion; Schölkopf, B., Smola, A., (2002) Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond; Rabiner, L., A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition Proceedings of the IEEE, 1989; Schrempf, O.C., Hanebeck, U.D., A Generic Model for Estimating User Intentions in Human-Robot Cooperation (2005) ICINCO; Carberry, S., Techniques for Plan Recognition (2001) User Modeling and User-Adapted Interaction; Bui, H.H., A General Model for Online Probabilistic Plan Recognition (2003) IJCAI; Tahboub, K.A., Intelligent Human-Machine Interaction Based on Dynamic Bayesian Networks Probabilistic Intention Recognition (2006) Intelligent and Robotic Systems; Turaga, P., Chellappa, R., Subrahmanian, V.S., Udrea, O., Machine Recognition of Human Activities: A Survey (2008) IEEE Transactions on Circuits and Systems; Poppe, R., A Survey on Vision-Based Human Action Recognition (2010) Image and Vision Computing; Park, S., Aggarwal, J.K., A Hierarchical Bayesian Network for Event Recognition of Human Actions and Interactions (2004) Multimedia Systems; Ivanov, Y.A., Bobick, A.F., Recognition of Visual Activities and Interactions by Stochastic Parsing (2000) PAMI; Tran, S.D., Davis, L.S., Event modeling and recognition using markov logic networks (2008) ECCV; Wang, H., Ullah, M.M., Kläser, A., Laptev, I., Schmid, C., Evaluation of Local Spatio-Temporal Features for Action Recognition (2009) BMVC; Yang, J., Xu, Y., Chen, C.S., Human Action Learning via Hidden Markov Model (2002) IEEE Trans. on Systems, Man, and Cybernetics; Brand, M., Oliver, N., Pentland, A., Coupled Hidden Markov Models for Complex Action Recognition (1997) CVPR; Gehrig, D., Stein, T., Fischer, A., Schwameder, H., Schultz, T., Towards semantic segmentation of human motion sequences Proceedings of the 33rd Annual German Conference on Advances in Artificial Intelligence, 2010; Casile, A., Giese, M.A., Critical Features for the Recognition of Biological Motion (2005) Journal of Vision; Soltau, H., Metze, F., Fügen, C., Waibel, A., A One-Pass Decoder based on Polymorphic Linguistic Context Assignment (2001) ASRU; Finke, M., Geutner, P., Hild, H., Kemp, T., Ries, K., Westphal, M., The Karlsruhe-Verbmobil Sspeech Recognition Engine (1997) ICASSP; Nevill-Manning, C., Witten, I., Identifying Hierarchical Structure in Sequences: A linear-time algorithm (1997) Journal of Artificial Intelligence Research; Martinetz, T.M., Berkovich, S.G., Schulten, K.J., Neural-Gas'Network for Vector Quantization and its Application to Time-Series Prediction (1993) IEEE Transactions on Neural Networks; Csurka, G., Dance, C., Fan, L., Willamowski, J., Bray, C., Visual Categorization with Bags of Keypoints ECCV Workshop on Statistical Learning in Computer Vision, 2004; Laptev, I., Marszalek, M., Schmid, C., Rozenfeld, B., Learning Realistic Human Actions from Movies (2008) CVPR; Schindler, K., Van Gool, L., Action Snippets: How many frames does human action recognition require? (2008) CVPR; Bay, H., Tuytelaars, T., Van Gool, L., SURF: Speeded up Robust Features (2006) ECCV; Poppe, R., Poel, M., Comparison of Silhouette Shape Descriptors for Example-based Human Pose Recovery Intl. Conf. on Automatic Face and Gesture Recognition, 2006; Chang, C.-C., Lin, C.-J., (2001) LIBSVM: A Library for Support Vector Machines; Lin, H.-T., Lin, C.-J., Weng, R.C., A note on Platt's Probabilistic Outputs for Support Vector Machines (2007) Machine Learning; Wu, T.-F., Lin, C.-J., Weng, R.C., Probability Estimates for Multi-class Classification by Pairwise Coupling (2004) Journal of Machine Learning Research; Murphy, K., (2002) Dynamic Bayesian Network: Representation, Inference and Learning, , Ph.D. dissertation, UC Berkeley; Asfour, T., Regenstein, K., Azad, P., Schröder, J., Bierbaum, A., Vahrenkamp, N., Dillmann, R., ARMAR-III: An integrated humanoid platform for sensory-motor control (2006) Humanoids},
correspondence_address1={Gehrig, D.; Cognitive Systems Lab. (CSL), Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; email: dirk.gehrig@kit.edu},
sponsors={IEEE Robotics and Automation Society (RAS); IEEE Industrial Electronics Society (IES); Robotics Society of Japan (RSJ); Society of Instrument and Control Engineers (SICE); New Technology Foundation (NTF)},
address={San Francisco, CA},
isbn={9781612844541},
coden={85RBA},
language={English},
abbrev_source_title={IEEE Int Conf Intell Rob Syst},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jarvis2011205,
author={Jarvis, J. and Putze, F. and Heger, D. and Schultz, T.},
title={Multimodal person independent recognition of workload related biosignal patterns},
journal={ICMI'11 - Proceedings of the 2011 ACM International Conference on Multimodal Interaction},
year={2011},
pages={205-208},
doi={10.1145/2070481.2070516},
note={cited By 12; Conference of 2011 ACM International Conference on Multimodal Interaction, ICMI'11 ; Conference Date: 14 November 2011 Through 18 November 2011;  Conference Code:87685},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-83455198241&doi=10.1145%2f2070481.2070516&partnerID=40&md5=18ed01afc5b1087bf6cb134cadd563ac},
affiliation={Cognitive Systems Lab., Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={This paper presents an online multimodal person independent workload classification system using blood volume pressure, respiration measures, electrodermal activity and electroencephalography. For each modality a classifier based on linear discriminant analysis is trained. The classification results obtained on short data frames are fused using weighted majority voting. The system was trained and evaluated on a large training corpus of 152 participants, exposed to controlled and uncontrolled scenarios for inducing workload, including a driving task conducted in a realistic driving simulator. Using person dependent feature space normalization, we achieve a classification accuracy of up to 94% for discrimination of relaxed state vs. high workload. © 2011 ACM.},
author_keywords={biosignal;  digital signal processing;  mental workload;  multimodal;  pattern recognition},
keywords={Biosignals;  Blood volume pressures;  Classification accuracy;  Classification results;  Classification system;  Data frames;  Driving simulator;  Driving tasks;  Electrodermal activity;  Feature space;  Linear discriminant analysis;  Majority voting;  Mental workload;  Multi-modal;  Person-dependent;  Person-independent;  Relaxed state;  Training corpus, Automobile simulators;  Digital signal processing;  Discriminant analysis;  Electrophysiology;  Feature extraction;  Interactive computer systems;  Pattern recognition;  Signal processing, Pattern recognition systems},
references={Adeli, H., Zhou, Z., Dadmehr, N., Analysis of EEG records in an epileptic patient using wavelet transform (2003) Journal of Neuroscience Methods, 123 (1), pp. 69-87. , DOI 10.1016/S0165-0270(02)00340-0, PII S0165027002003400; Bergasa, L., Nuevo, J., Sotelo, M., Barea, R., Lopez, M., Real-time system for monitoring driver vigilance (2006) Intelligent Transportation Systems, IEEE Transactions on, 7 (1), pp. 63-77; Healey, J.A., Picard, R.W., Detecting stress during real-world driving tasks using physiological sensors (2005) IEEE Transactions on Intelligent Transportation Systems, 6 (2), pp. 156-166. , DOI 10.1109/TITS.2005.848368; Heger, D., Putze, F., Amma, C., Wand, M., Plotkin, I., Wielatt, T., Schultz, T., Biosignalsstudio: A flexible framework for biosignal capturing and processing (2010) KI 2010: Advances in Artificial Intelligence; Heger, D., Putze, F., Schultz, T., Online workload recognition from eeg data during cognitive tests and human-machine interaction (2010) KI 2010: Advances in Artificial Intelligence, pp. 410-417; Hyvarinen, A., Oja, E., Independent component analysis: Algorithms and applications (2000) Neural Networks, 13 (4-5), pp. 411-430. , DOI 10.1016/S0893-6080(00)00026-5, PII S0893608000000265; Jung, T.-P., Makeig, S., Humphries, C., Lee, T.-W., Mckeown, M.J., Iragui, V., Sejnowski, T.J., Removing electroencephalographic artifacts by blind source separation (2000) Psychophysiology, 37 (2), pp. 163-178. , DOI 10.1017/S0048577200980259; Kuhn, F., Methode zur Bewertung der Fahrerablenkung durch Fahrerinformations- Systeme (2005) World Usability Day; Lansdown, T.C., Brook-Carter, N., Kersloot, T., Distraction from multiple in-vehicle secondary tasks: Vehicle performance and mental workload implications (2004) Ergonomics, 47 (1), pp. 91-104. , DOI 10.1080/00140130310001629775; Lin, Y., Leng, H., Yang, G., Cai, H., An intelligent noninvasive sensor for driver pulse wave measurement (2007) IEEE Sensors Journal, 7 (5), pp. 790-799. , DOI 10.1109/JSEN.2007.894923; Putze, F., Jarvis, J.-P., Schultz, T., Multimodal recognition of cognitive workload for multitasking in the car (2010) Proc. 20th Int Pattern Recognition (ICPR) Conf, pp. 3748-3751},
correspondence_address1={Jarvis, J.; Cognitive Systems Lab., Karlsruhe Institute of Technology, Karlsruhe, Germany; email: jan.jarvis@student.kit.edu},
sponsors={Special Interest Group on Computer-Human Interaction (ACM SIGCHI)},
address={Alicante},
isbn={9781450306416},
language={English},
abbrev_source_title={ICMI - Proc. ACM Int. Conf. Multimodal Interact.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hsiao2011773,
author={Hsiao, R. and Schultz, T.},
title={Generalized Baum-Welch algorithm and its implication to a new extended Baum-Welch algorithm},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2011},
pages={773-776},
note={cited By 8; Conference of 12th Annual Conference of the International Speech Communication Association, INTERSPEECH 2011 ; Conference Date: 27 August 2011 Through 31 August 2011;  Conference Code:92405},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865747510&partnerID=40&md5=6502158f3ad78bb371103a4302715e35},
affiliation={InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={This paper describes how we can use the generalized Baum- Welch (GBW) algorithm to develop better extended Baum- Welch (EBW) algorithms. Based on GBW, we show that the backoff term in the EBWalgorithm comes from KL-divergence which is used as a regularization function. This finding allows us to develop a fast EBW algorithm, which can reduce the time of model space discriminative training by half, without incurring any degradation on recognition accuracy. We compare the performance of the new EBW algorithm with the original one on various large scale systems including Farsi, Iraqi and modern standard Arabic ASR systems. Copyright © 2011 ISCA.},
author_keywords={Discriminative training;  Speech recognition},
keywords={Baum-Welch algorithms;  Discriminative training;  KL-divergence;  Model spaces;  Modern standards;  Recognition accuracy;  Regularization function, Degradation;  Speech recognition, Algorithms},
references={Normandin, Y., Morgera, S.D., An improved MMIE training algorithm for speaker-independent, small vocabulary, continuous speech recognition (1991) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing; Povey, D., Kanevsky, D., Kingsbury, B., Ramabhadran, B., Saon, G., Visweswariah, K., Boosted MMI for model and feature-space discriminative training (2008) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, pp. 4057-4060; Hsiao, R., Tam, Y.C., Schultz, T., Generalized baum-welch algorithm for discriminative training on large vocabulary continuous speech recognition system (2009) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing; Ahmed, N.A., Gokhale, D.V., Entropy expressions and their estimators for multivariate distributions (1989) IEEE Transactions on Information Theory, 35, pp. 688-692; Bach, N., Eck, M., Charoenpornsawat, P., Köhler, T., Stüker, S., Nguyen, T., Hsiao, R., Black, A.W., The CMU TransTac 2007 eyes-free, and hands-free two-way speech-to-speech translation system (2007) Proceedings of the IWSLT; Metze, F., Hsiao, R., Jin, Q., Nallasamy, U., Schultz, T., The 2010 CMU GALE speech-to-text system (2010) Proceedings of the INTERSPEECH, , Makuhari, Japan},
correspondence_address1={Hsiao, R.; InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: wrhsiao@cs.cmu.edu},
sponsors={Loquendo S.p.A.; Nuance Communications; AT and T Labs - Research; Google Research; Interactive Media S.p.A.},
address={Florence},
issn={19909772},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wand2011601,
author={Wand, M. and Janke, M. and Schultz, T.},
title={Investigations on speaking mode discrepancies in EMG-based speech recognition},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2011},
pages={601-604},
note={cited By 9; Conference of 12th Annual Conference of the International Speech Communication Association, INTERSPEECH 2011 ; Conference Date: 27 August 2011 Through 31 August 2011;  Conference Code:92405},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865780252&partnerID=40&md5=a86c6e2f09bfd840888e5bc84da17df4},
affiliation={Cognitive Systems Lab., Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={In this paper we present our recent study on the impact of speaking mode variabilities on speech recognition by surface electromyography (EMG). Surface electromyography captures the electric potentials of the human articulatory muscles, which enables a user to communicate naturally without making any audible sound. Our previous experiments have shown that the EMG signal varies greatly between different speaking modes, like audibly uttered speech and silently articulated speech. In this study we extend our previous research and quantify the impact of different speaking modes by investigating the amount of mode-specific leaves in phonetic decision trees. We show that this measure correlates highly with discrepancies in the spectral energy of the EMG signal, as well as with differences in the performance of a recognizer on different speaking modes. We furthermore present how EMG signal adaptation by spectral mapping decreases the effect of the speaking mode. Copyright © 2011 ISCA.},
author_keywords={EMG;  EMG-based speech recognition;  Phonetic decision tree;  Silent Speech Interfaces},
keywords={Audible sound;  EMG;  EMG signal;  Phonetic decision tree;  Spectral energy;  Spectral mappings;  Speech interface;  Surface electromyography, Decision trees;  Electric potential;  Electromyography;  Forestry;  Photomapping, Speech recognition, Algorithms;  Biometrics;  Decision Making;  Forestry;  Pattern Recognition},
references={Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Silent speech interfaces (2010) Speech Communication, 52 (4), pp. 270-287; Schultz, T., Wand, M., Modeling coarticulation in large vocabulary EMG-based speech recognition (2010) Speech Communication, 52, pp. 341-353; Wand, M., Jou, S.-C.S., Toth, A.R., Schultz, T., Impact of different speaking modes on EMG-based speech recognition (2009) Proc. Interspeech; Janke, M., Wand, M., Schulttz, T., A spectral mapping method for EMG-based recognition of silent speech (2010) Proc. B-INTERFACE; Janke, M., Wand, M., Schultz, T., Impact of lack of acoustic feedback in EMG-based silent speech recognition (2010) Proc. Interspeech; Eide, E., Gish, H., Jeanrenaud, P., Mielke, A., Understanding and improving speech recognition performance through the use of diagnostic tools (1995) Proc. ICASSP; Schaaf, T., Metze, F., Analysis of gender normalization using MLP and VTLN features (2010) Proc. Interspeech; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) IEEE Workshop on Automatic Speech Recognition and Understanding, , San Juan, Puerto Rico; Schünke, M., Schulte, E., Schumacher, U., (2006) Prometheus - Lernatlas der Anatomie, 3. , Stuttgart New York: Thieme Verlag, Kopf und Neuroanatomie; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proc. Interspeech, , Pittsburgh, PA, Sep; Metze, F., Waibel, A., A flexible stream architecture for ASR using articulatory features (2002) Proc. of the International Conference on Spoken Language Processing (ICSLP), , Denver, Colorado, USA, Sep; Finke, M., Rogina, I., Wide Context Acoustic Modeling in Read vs. Spontaneous Speech (1997) Proc. ICASSP, 3},
correspondence_address1={Wand, M.; Cognitive Systems Lab., Karlsruhe Institute of Technology, Karlsruhe, Germany; email: michael.wand@kit.edu},
sponsors={Loquendo S.p.A.; Nuance Communications; AT and T Labs - Research; Google Research; Interactive Media S.p.A.},
address={Florence},
issn={19909772},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vu20113145,
author={Vu, N.T. and Kraus, F. and Schultz, T.},
title={Rapid building of an ASR system for under-resourced languages based on multilingual unsupervised training},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2011},
pages={3145-3148},
note={cited By 23; Conference of 12th Annual Conference of the International Speech Communication Association, INTERSPEECH 2011 ; Conference Date: 27 August 2011 Through 31 August 2011;  Conference Code:92405},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865764419&partnerID=40&md5=ab43374282e1dca0d988213fc336a9c5},
affiliation={Cognitive Systems Lab., Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT), Germany},
abstract={This paper presents our work on rapid language adaptation of acoustic models based on multilingual cross-language bootstrapping and unsupervised training. We used Automatic Speech Recognition (ASR) systems in the six source languages English, French, German, Spanish, Bulgarian and Polish to build from scratch an ASR system for Vietnamese, an underresourced language. System building was performed without using any transcribed audio data by applying three consecutive steps, i.e. cross-language transfer, unsupervised training based on the "multilingual A-stabil" confidence score [1], and bootstrapping. We investigated the correlation between performance of "multilingual A-stabil" and the number of source languages and improved the performance of "multilingual A-stabil" by applying it at the syllable level. Furthermore, we showed that increasing the amount of source language ASR systems for the multilingual framework results in better performance of the final ASR system in the target language Vietnamese. The final Vietnamese recognition system has a Syllable Error Rate (SyllER) of 16.8% on the development set and 16.1% on the evaluation set. Copyright © 2011 ISCA.},
author_keywords={Multilingual A-Stabil;  Rapid language adaptation of ASR;  Unsupervised training},
keywords={Acoustic model;  Audio data;  Automatic speech recognition system;  Confidence score;  Error rate;  Multilingual A-Stabil;  Rapid language adaptation of ASR;  Recognition systems;  Source language;  System building;  Target language;  Unsupervised training, Linguistics, Natural language processing systems},
references={Vu, N.T., Kraus, F., Schultz, T., Multilingual A-stabil: A new confidence score for multilingual unsupervised training (2010) IEEE Workshop on Spoken Language Technology, SLT 2010, , Berkeley, California, USA; Vu, N.T., Kraus, F., Schultz, T., Cross-language bootstrapping based on completely unsupervised training using multilingual Astabil (2011) International Conference on Acoustics, Speech and Signal Processing, ICASSP 2011, , Prague, Czech Republic, 22-27 Mai; Zavaliagkos, G., Colthurst, T., Utilizing untranscribed training data to improve performance (1998) DARPA Broadcast News Transcription and Understanding Workshop, , Landsdowne, VA, USA, Feb; Kemp, T., Schaaf, T., Estimating confidence using word lattices (1997) Proc. of Eurospeech, pp. 827-830; Wessel, F., MacHerey, K., Ney, H., A comparison of wordgraph and N-best list based confidence measures (1999) Proc. of Eurospeech, , Budapest, Hungary; Evermann, G., Woodland, P., Large vocabulary decoding and confidence estimation using word posterior probabilities (2000) Proc. ICASSP, , Istanbul, Turkey; Zhang, R., Rudnicky, A.I., A new data selection approach for semi-supervised acoustic modeling (2006) Proc. of ICASSP, , Toulouse, France; Schultz, T., Waibel, A., Experiments on cross-language acoustic modeling (2001) Proc. Eurospeech, , Aalborg, Denmark; Lööf, J., Gollan, C., Ney, H., Cross-language bootstrapping for unsupervised acoustic model training: Rapid development of a polish speech recognition system (2009) Interspeech, pp. 88-91. , Brighton, U.K; Lamel, L., Gauvain, J., Adda, G., Unsupervised acoustic modelling (2002) Proc. ICASSP, , Orlando, USA; Schultz, T., Globalphone: A multilingual speech and text database developed at karlsruhe university (2002) Proc. ICSLP, , Denver, CO; Schultz, T., Black, A., Rapid language adaptation tools and technologies for multilingual speech processing (2008) Proc. ICASSP, , Las Vegas, USA; Vu, N.T., Schlippe, T., Kraus, F., Schultz, T., Rapid bootstrapping of five eastern european languages using the rapid language adaptation toolkit (2010) Interspeech, , Makuhari, Japan; Schultz, T., Waibel, A., Language independent and language adaptive acoustic modeling for speech recognition (2001) Speech Communication August, 35 (1-2), pp. 31-51; Vu, N.T., Schultz, T., Vietnamese large vocabulary continuous speech recognition Automatic Speech Recognition and Understanding, ASRU 2009, , Merano, Italy, 13.12.2009; (1999) Handbook IPA: Handbook of the International Phonetic Association; Mangu, L., Brill, E., Stolcke, A., Finding consensus among- words: Lattice-based word error minimization Proc. of EUROSPEECH99, , Budapest, Hungary; Gales, M., Semi-tied covariance matrices for hidden Markov models (1999) IEEE Transactions Speech and Audio Processing, 7, pp. 272-281},
correspondence_address1={Vu, N.T.; Cognitive Systems Lab., Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT)Germany; email: thang.vu@kit.edu},
sponsors={Loquendo S.p.A.; Nuance Communications; AT and T Labs - Research; Google Research; Interactive Media S.p.A.},
address={Florence},
issn={19909772},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yang20112925,
author={Yang, Q. and Jin, Q. and Schultz, T.},
title={Investigation of cross-show speaker diarization},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2011},
pages={2925-2928},
note={cited By 18; Conference of 12th Annual Conference of the International Speech Communication Association, INTERSPEECH 2011 ; Conference Date: 27 August 2011 Through 31 August 2011;  Conference Code:92405},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865734172&partnerID=40&md5=af15531573847238fa8b6f41229b9cf9},
affiliation={Cognitive Systems Lab., Karlsruhe Institute of Technology, Karlsruhe, Germany; Language Technologies Institute, Carnegie Mellon University, United States},
abstract={The goal of cross-show diarization is to index speech segments of speakers from a set of shows, with the particular challenge that reappearing speakers across shows have to be labeled with the same speaker identity. In this paper, we introduce three cross-show diarization systems namely Global-BIC-Seg, Global-BIC-Cluster, and Incremental. We compared the three systems on a set of 46 English scientific podcast shows. Among the three systems, the Global-BIC-Cluster achieves the best performance with 15.53% and 13.21% cross-show diarization error rate (DER) on the dev and test set, respectively. However, an incremental approach is more practical since data and shows are typically collected over time. By applying T-Norm on our incremental system, we obtain 13.18% and 10.97% relative improvements in terms of cross-show DER on dev and test set. We also investigate the impact of the show processing order on cross-show diarization for the incremental system. Copyright © 2011 ISCA.},
author_keywords={Conversational podcast shows;  Cross-show diarization;  Speaker diarization},
keywords={Conversational podcast shows;  Cross-show diarization;  Error rate;  Incremental approach;  Speaker diarization;  Speech segments;  Test sets;  Three systems, Computer simulation, Computer applications},
references={Zhu, X., Barras, C., Meignier, S., Gauvain, J.-L., Combining speaker identification and BIC for speaker diarization (2005) Proc. of Interspeech, , Lisbon, Portugal; Anguera, X., Wooters, C., Peskin, B., Aguiló, M., Robust speaker segmentation for meetings: The ICSI-SRI spring 2005 diarization system (2005) Proc. of MLMI/NIST Workshop, , Edinburgh, UK, July; Reynolds, D.A., Torres-Carrasquillo, P., The MIT lincoln laboratory RT-04F diarization systems: Applications to broadcast audio and telephone conversations (2004) Proc. Fall 2004 Rich Transcription Workshop (RT-04), , Palisades, NY, November; Meignier, S., Moraru, D., Fredouille, C., Bonastre, J.-F., Besacier, L., Step-by-step and integrated approaches in broadcast news speaker diarization (2005) Computer Speech and Language; Friedland, A.G., Vinyals, B.O., Yan Huang, C., Muller, D.C., Fusing short term and long term features for improved speaker diarization (2009) IEEE International Conference on Acoustics Speech and Signal Processing, pp. 4077-4080; Tran, V.-A., Le, V.B., Barras, C., Lamel, L., Comparing multistage approaches for cross-show speaker diarization (2011) Interspeech 2011, , submitted to, Florence, Italy; Žibert, J., Vesnicer, B., Mihelič, F., A system for speaker detection and tracking in audio broadcast news (2008) Informatica(Slovenia), pp. 51-61. , August; Canseco-Rodriguez, L., Lamel, L., Gauvain, J.-L., Speaker diarization from speech transcripts (2004) Proc. of International Conference on Spoken Language Processing (ICSLP), , Jeju Island, Korear, October; Markov, K., Nakamura, S., Never-ending learning system for on-line speaker diarization (2007) Automatic Speech Recognition & Understanding, 2007. ASRU. IEEE Workshop on, , Kyoto, Japan, December; Jin, Q., Laskowski, K., Schultz, T., Waibel, A., Speaker segmentation and clustering in meetings (2004) ICSLP, , Jeju, South-Korea; Li, R., Schultz, T., Jin, Q., Improving speaker segmentation via speaker identification and text segmentation Proc. of Interspeech, , Brighton, UK; Reynolds, D.A., Quatieri, T.F., Dunn, R.B., Speaker verification using adapted Gaussian mixture models (2000) Digital Signal Processing, p. 2000; The Naked Scientists Online, , http://www.thenakedscientists.com/; (2001) The 2001 NIST Speaker Recognition Evaluation, , http://www.nist.gov/speech/tests/spk/2001; Pelecanos, J., Sridharan, S., Feauture warping for robust speaker verification (2001) Proc. Speaker Odyssey 2001 Conference, , June; Galliano, S., Gravier, G., Chaubard, L., The ESTER 2 evaluation campaign for the rich transcription of French radio broadcasts (2009) Proc. of Interspeech, , Brighton, UK; Auckenthaler, R., Careya, M., Lloyd-Thomas, H., Score normalization for text-independent speaker verification systems (2000) Digital Signal Processing, pp. 42-54},
correspondence_address1={Yang, Q.; Cognitive Systems Lab., Karlsruhe Institute of Technology, Karlsruhe, Germany; email: qian.yang@kit.edu},
sponsors={Loquendo S.p.A.; Nuance Communications; AT and T Labs - Research; Google Research; Interactive Media S.p.A.},
address={Florence},
issn={19909772},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Herff20112213,
author={Herff, C. and Janke, M. and Wand, M. and Schultz, T.},
title={Impact of different feedback mechanisms in EMG-based speech recognition},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2011},
pages={2213-2216},
note={cited By 7; Conference of 12th Annual Conference of the International Speech Communication Association, INTERSPEECH 2011 ; Conference Date: 27 August 2011 Through 31 August 2011;  Conference Code:92405},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865760831&partnerID=40&md5=7f329ffef8016b01fa2de96447577ab5},
affiliation={Cognitive Systems Lab., Karlsruhe Institute of Technology, Germany},
abstract={This paper reports on our recent research in the feedback effects of Silent Speech. Our technology is based on surface electromyography (EMG) which captures the electrical potentials of the human articulatory muscles rather than the acoustic speech signal. While recognition results are good for loudly articulated speech and when experienced users speak silently, novice users usually achieve far worse results when speaking silently. Since there is no acoustic feedback when speaking silently, we investigate different kinds of feedback modes: no additional feedback except the natural somatosensory feedback (like the touching of the lips), visual feedback using a mirror and indirect acoustic feedback by speaking simultaneously to a previously recorded audio signal. In addition we examine recorded EMG data when the subject speaks audibly and silently in a loud environment to see if the Lombard effect can be observed in Silent Speech, too. Copyright © 2011 ISCA.},
author_keywords={Elecromyography;  EMG-based speech recognition;  Lack of acoustic feedback;  Lombard effect;  Silent speech},
keywords={Acoustic feedback;  Acoustic speech;  Audio signal;  Elecromyography;  Electrical potential;  Feedback effects;  Feedback mechanisms;  Feedback mode;  Lombard effect;  Novice user;  Somatosensory feedbacks;  Surface electromyography;  Visual feedback, Visual communication, Speech recognition},
references={Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Silent speech interfaces (2010) Speech Communication, 52; Janke, M., Wand, M., Schultz, T., Impact of lack of acoustic feedback in EMG-based silent speech recognition (2010) Proc. Interspeech; Lombard, E., Le signe de l'elevation de la voix (1911) Ann. Mal. Oreille Larynx; Chan, A., Englehart, K., Hudgins, B., Lovely, D., Myoelectric signals to augment speech recognition (2001) Medical and Biological Engineering and Computing, 39, pp. 500-506; Jorgensen, C., Lee, D., Agabon, S., Sub auditory speech recognition based on EMG/EPG signals (2003) Proceedings of International Joint Conference on Neural Networks (IJCNN), , Portland, Oregon; Jou, S.-C.S., Schultz, T., Waibel, A., Continuous electromyographic speech recognition with a multi-stream decoding architecture (2007) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), , Honolulu, Hawaii; Janke, M., Wand, M., Schultz, T., A spectral mapping method for EMG-based recognition of silent speech (2010) Proc. B-INTERFACE; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 331-336. , San Juan, Puerto Rico; Van Summers, W., Pisoni, D., Bernacki, R., Pedlow, R., Stokes, M., Effects of noise on speech production: Acoustic and perceptual analyses (1988) Journal of the Acoustical Society of America, 84 (3), pp. 917-928; Vatikiotis-Bateson, E., Chung, V., Lutz, K., Mirante, N., Otten, J., Tan, J., Auditory, but perhaps not visual, processing of lombard speech (2006) J. Acoust. Soc. Am., 119; Welch, P., The use of fast fourier transform for the estimation of power spectra: A method based on time averaging over short, modified periodograms (1967) Audio and Electroacoustics, IEEE Transactions on, 15 (2). , Jun; Schultz, T., Wand, M., Modeling coarticulation in large vocabulary EMG-based speech recognition (2010) Speech Communication, 52, pp. 341-353},
correspondence_address1={Herff, C.; Cognitive Systems Lab., Karlsruhe Institute of TechnologyGermany; email: Christian.Herff@student.kit.edu},
sponsors={Loquendo S.p.A.; Nuance Communications; AT and T Labs - Research; Google Research; Interactive Media S.p.A.},
address={Florence},
issn={19909772},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Reich20112641,
author={Reich, D. and Putze, F. and Heger, D. and Ijsselmuiden, J. and Stiefelhagen, R. and Schultz, T.},
title={A real-time Speech Command Detector for a Smart Control Room},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2011},
pages={2641-2644},
note={cited By 11; Conference of 12th Annual Conference of the International Speech Communication Association, INTERSPEECH 2011 ; Conference Date: 27 August 2011 Through 31 August 2011;  Conference Code:92405},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865792713&partnerID=40&md5=8287437682f0cd274ee0ca88a462a083},
affiliation={Cognitive Systems Laboratory, Karlsruhe Institute of Technology, Germany; Fraunhofer Institute of Optronics, System Technologies and Image Exploitation, Germany},
abstract={In this work we present an always-on speech recognition system that discriminates spoken commands directed to the system from other spoken input. For discrimination we integrated various features ranging from prosodic cues and decoding features to linguistic information. The resulting "Speech Command Detector" provides intuitive hands-free user interaction in a Smart Control Room environment where voice commands are directed toward a large interactive display. Based on a recognition vocabulary of 259 words with more than 10k possible commands, the Speech Command Detector detected 88.3% of the commands correctly maintaining a very low False Positive Rate of 1.5%. In a cross-domain setup the system was evaluated on a Star Trek episode. With only minor adjustments, our system achieved very promising results with 91.2% command detection rate at a False Positive Rate of 1.8%. Copyright © 2011 ISCA.},
author_keywords={Always-on spoken command detection;  Prosodic and confidence-based features;  Smart environment},
keywords={Cross-domain;  Detection rates;  False positive rates;  Hands-free;  Large interactive displays;  Linguistic information;  Prosodic and confidence-based features;  Smart control room;  Smart environment;  Speech commands;  Speech recognition systems;  Star treks;  User interaction;  Voice command, Detectors, Speech recognition},
references={Kawahara, T., Ishizuka, K., Doshita, S., Lee, C., Speaking-style dependent lexicalized filler model for key-phrase detection and verification (1998) Fifth International Conference on Spoken Language Processing; Obuchi, Y., Togami, M., Sumiyoshi, T., Intentional voice command detection for completely hands-free speech interface in home environments (2008) Ninth Annual Conference of the International Speech Communication Association; Yamagata, T., Sako, A., Takiguchi, T., Ariki, Y., System request detection in conversation based on acoustic and speaker alternation features (2007) Eighth Annual Conference of the International Speech Communication Association; Yamagata, T., Takiguchi, T., Ariki, Y., System request detection in human conversation based on multi-resolution gabor wavelet features (2009) Tenth Annual Conference of the International Speech Communication Association; Katzenmaier, M., Stiefelhagen, R., Schultz, T., Identifying the addressee in human-human-robot interactions based on head pose and speech (2004) Proceedings of the 6th International Conference on Multimodal Interfaces, pp. 144-151. , ACM; Shriberg, E., Stolcke, A., Jurafsky, D., Coccaro, N., Meteer, M., Bates, R., Taylor, P., Van Ess-Dykema, C., Can prosody aid the automatic classification of dialog acts in conversational speech? (1998) Language and Speech, 41 (3-4), p. 443; Amano-Kusumoto, A., Hosom, J., Shafran, I., Classifying clear and conversational speech based on acoustic features (2009) Tenth Annual Conference of the International Speech Communication Association; Ijsselmuiden, J., Stiefelhagen, R., Towards high-level human activity recognition through computer vision and temporal logic (2010) KI 2010: Advances in Artificial Intelligence, pp. 426-435; Stiefelhagen, R., Fugen, C., Gieselmann, R., Holzapfel, H., Nickel, K., Waibel, A., Natural human-robot interaction using speech, head pose and gestures (2004) Intelligent Robots and Systems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ International Conference on, 3, pp. 2422-2427. , IEEE; Soltau, H., Metze, F., Fugen, C., Waibel, A., A one-pass decoder based on polymorphic linguistic context assignment (2001) Automatic Speech Recognition and Understanding, 2001. ASRU'01. IEEE Workshop on, pp. 214-217. , IEEE; Schaaf, T., Detection of OOV words using generalized word models and a semantic class language model (2001) Proc. Eurospeech, pp. 2581-2584. , Citeseer; Laskowski, K., Jin, Q., Modeling instantaneous intonation for speaker identification using the fundamental frequency variation spectrum (2009) Proc. of ICASSP, pp. 4541-4544},
correspondence_address1={Reich, D.; Cognitive Systems Laboratory, Karlsruhe Institute of TechnologyGermany; email: daniel.reich@student.kit.edu},
sponsors={Loquendo S.p.A.; Nuance Communications; AT and T Labs - Research; Google Research; Interactive Media S.p.A.},
address={Florence},
issn={19909772},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nallasamy20111721,
author={Nallasamy, U. and Garbus, M. and Metze, F. and Jin, Q. and Schaaf, T. and Schultz, T.},
title={Analysis of dialectal influence in pan-Arabic ASR},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2011},
pages={1721-1724},
note={cited By 1; Conference of 12th Annual Conference of the International Speech Communication Association, INTERSPEECH 2011 ; Conference Date: 27 August 2011 Through 31 August 2011;  Conference Code:92405},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865707910&partnerID=40&md5=37bfbac009639a9d88e973e4cb26a445},
affiliation={Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, United States; M Modal Technologies, Pittsburgh, PA, United States},
abstract={In this paper, we analyze the impact of five Arabic dialects on the front-end and pronunciation dictionary components of an Automatic Speech Recognition (ASR) system. We use ASR's phonetic decision tree as a diagnostic tool to compare the robustness of MFCC and MLP front-ends to dialectal variations in the speech data and found that MLP Bottle-Neck features are less robust to such variations. We also perform a rule-based analysis of the pronunciation dictionary, which enables us to identify dialectal words in the vocabulary and automatically generate pronunciations for unseen words. We show that our technique produces pronunciations with an average phone error rate 9.2%. Copyright © 2011 ISCA.},
author_keywords={Automatic speech recognition;  Dialect analysis;  Front-end evaluation},
keywords={Automatic speech recognition;  Automatic speech recognition system;  Diagnostic tools;  Dialect analysis;  Dialectal variation;  Front-end evaluation;  Phone error rate;  Phonetic decision tree;  Pronunciation dictionaries;  Rule based;  Speech data, Computer applications;  Computer simulation, Speech recognition},
references={Kirchhoff, K., Vergyri, D., Cross-dialectal data sharing for acoustic modeling in Arabic speech recognition (2005) Speech Communication, 46, pp. 37-51; Kirchhoff, K., Novel approaches to Arabic speech recognition - Final report from the JHU summer workshop 2002 (2002) Tech. Rep., John-Hopkins University; Lamel, L., Messaoudi, A., Gauvain, J.-L., Automatic speech-totext transcription in Arabic (2009) ACM Transactions on Asian Language Information Processing, 8 (4); Schaaf, T., Metze, F., Analysis of gender normalization using MLP and VTLN features (2010) Proc. Interspeech; SAMPA for Arabic - phon.ucl.ac.uk/home/sampa/arabic.htm; Nallasamy, U., Noamany, M., Schaaf, T., Fuhs, M., Schultz, T., CMU/InterACT Arabic speech recognition system for GALE (2010) GALE Book, , Chap, in J. Olive (ed.); Hermansky, H., Ellis, D., Sharma, S., Tandem connectionist feature extraction for conventional HMM systems (2000) Proc. ICASSP; Grézl, F., Fousek, P., Optimizing bottle-neck features for LVCSR (2008) Proc. ICASSP; Metze, F., Hsiao, R., Jin, Q., Nallasamy, U., Schultz, T., The 2010 CMU GALE speech-to-text system Proc. Interspeech 2010; www.icsi.berkeley.edu/Speech/qn.html; Grézl, F., Karafiát, M., Burget, L., Investigation into bottleneck features for meeting speech recognition (2009) Proc. Interspeech; Finke, M., Rogina, I., Wide context acoustic modeling in read vs. spontaneous speech (1997) Proc. ICASSP; Biadsy, F., Habash, N., Hirschberg, J., Improving the Arabic Pronunciation Dictionary for phone and word recognition with linguistically-based pronunciation rules (2009) Proc. NAACL; Biadsy, F., Soltau, H., Mangu, L., Navratil, J., Hirschberg, J., Discriminative phonotactics for dialect recognition using context-dependent phone classifiers Proc. Odyssey 2010; Black, A., Lenzo, K., Pagel, V., Issues in building general letter to sound rules (1998) Proc. ESCA WSS; Charoenpornsawat, P., Schultz, T., Example based graphemeto-phoneme conversion for Thai Proc. Interspeech 2006; Nelder, J.A., Mead, R., A simplex method for function minimization (1965) Computer Journal, 7, pp. 308-313},
correspondence_address1={Nallasamy, U.; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, United States; email: unallasa@cs.cmu.edu},
sponsors={Loquendo S.p.A.; Nuance Communications; AT and T Labs - Research; Google Research; Interactive Media S.p.A.},
address={Florence},
issn={19909772},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Heger2011415,
author={Heger, D. and Putze, F. and Schultz, T.},
title={An EEG adaptive information system for an empathic robot},
journal={International Journal of Social Robotics},
year={2011},
volume={3},
number={4},
pages={415-425},
doi={10.1007/s12369-011-0107-x},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857521874&doi=10.1007%2fs12369-011-0107-x&partnerID=40&md5=6860ed5ba77f3bc7d65b743dbfe94e20},
affiliation={Institute for Anthropomatics, Cognitive Systems Lab (CSL), Karlsruhe Institute of Technology, Adenauerring 4, 76131 Karlsruhe, Germany},
abstract={This article introduces a speech-driven information system for a humanoid robot that is able to adapt its information presentation strategy according to brain patterns of its user. Brain patterns are classified from electroen-cephalographic (EEG) signals and correspond to situations of low and high mental workload. The robot dynamically selects the information presentation style that best matches the detected patterns. The resulting end-to-end system consisting of recognition and adaptation components is tested in an evaluation study with 20 participants. We achieve a mean recognition rate of 83.5% for discrimination between low and high mental workload. Furthermore, we compare the dynamic adaptation strategy with two static presentation strategies. The evaluation results show that the adaptation of the presentation strategy according to workload improves over the static presentation strategy in both, information correctness and completeness. In addition, the adaptive strategy is favored over the static strategy as user satisfaction improves significantly. This paper presents the first systematic analysis of a real-time EEG-adaptive end-to-end information system for a humanoid robot. The achieved evaluation results indicate its great potential for empathic human-robot interaction. © Springer Science & Business Media BV 2011.},
author_keywords={Adaptive interaction strategies;  Electroencephalography;  Human evaluation study;  Human-robot spoken interaction;  Mental workload recognition},
keywords={Adaptive information systems;  Adaptive interaction;  Adaptive strategy;  Best match;  Dynamic adaptations;  End-to-end systems;  Evaluation results;  Evaluation study;  Human evaluation study;  Humanoid robot;  Information presentation;  Mental workload;  Recognition rates;  Spoken interaction;  Systematic analysis;  User satisfaction, Anthropomorphic robots;  Electroencephalography;  Electrophysiology;  Human computer interaction;  Information systems, Neuroimaging},
funding_details={Deutsche ForschungsgemeinschaftDeutsche Forschungsgemeinschaft, DFG},
funding_text 1={This work has been supported in part by the Deutsche Forschungsgemeinschaft (DFG) within the Collaborative Research Center 588 “Humanoid Robots—Learning and Cooperating Multimodal Robots”.},
references={Heger, D., Putze, F., Schultz, T., An adaptive information system for an empathic robot using EEG data (2010) International Conference On Social Robotics, pp. 151-160. , Springer, Berlin; Breazeal, C., (2004) Designing Sociable Robots, , MIT Press, Cambridge; Torrey, C., Powers, A., Marge, M., Fussell, S., Kiesler, S., Effects of adaptive robot dialogue on information exchange and social relations (2006) 1st ACM SIGCHI/SIGART Conference On Human-robot Interaction, pp. 126-133; Liu, C., Rani, P., Sarkar, N., Human-robot interaction using affective cues (2006) 15th IEEE International Symposium On Robot and Human Interactive Communication (ROMAN), pp. 285-290; Bonarini, A., Mainardi, L., Matteucci, M., Tognetti, S., Colombo, R., Stress recognition in a robotic rehabilitation task (2008) Robotic Helpers: User Interaction, Interfaces and Companions In As-sistive and Therapy Robotics, a Workshop At ACM/IEEE HRI; Wolpaw, J., Birbaumer, N., McFarland, D., Pfurtscheller, G., Vaughan, T., Brain-computer interfaces for communication and control (2002) Clin Neurophysiol, 113 (6), p. 767; Mason, S., Bashashati, A., Fatourechi, M., Navarro, K., Birch, G., A comprehensive survey of brain interface technology designs (2007) Ann Biomed Eng, 35 (2), p. 137; Bell, C., Shenoy, P., Chalodhorn, R., Rao, R., Control of a hu-manoid robot by a noninvasive brain-computer interface in humans (2008) J Neural Eng, 5, p. 214; Millan, J., Renkens, F., Mouriño, J., Gerstner, W., Noninvasive brain-actuated control of a mobile robot by human EEG (2004) IEEE Trans Biomed Eng, 51 (6), p. 1026; McFarland, D., Wolpaw, J., Brain-computer interface operation of robotic and prosthetic devices (2008) Computer, 41 (10), p. 52; McFarland, D., Sarnacki, W., Wolpaw, J., Electroencephalo-graphic (EEG) control of three-dimensional movement (2010) J Neural Eng, 7, p. 036007; Berka, C., Levendowski, D., Cvetinovic, M., Petrovic, M., Davis, G., Lumicao, M., Zivkovic, V., Olmstead, R., Realtime analysis of EEG indexes of alertness, cognition, and memory acquired with a wireless EEG headset (2004) Int J Hum-Comput Interact, 17 (2), p. 151; Wilson, G., Russell, C., Performance enhancement in an uninhabited air vehicle task using psychophysiologically determined adaptive aiding (2007) Hum Factors, 49 (6), p. 1005; Chen, D., Vertegaal, R., Using mental load for managing interruptions in physiologically attentive user interfaces (2004) CHI'04 Extended Abstracts On Human Factors In Computing Systems, pp. 1513-1516; Kohlmorgen, J., Dornhege, G., Braun, M., Blankertz, B., Muller, K., Curio, G., Hagemann, K., Kincses, W., (2007) Toward Brain-computer Interfacing, pp. 409-422. , MIT Press, Cambridge; Gevins, A., Smith, M., Neurophysiological measures of cognitive workload during human-computer interaction (2003) Theor Issues Ergon Sci, 4 (1), p. 113; Honal, M., Schultz, T., (2008) Determine Task Demand From Brain Activity; Larsson, S., Traum, D., Information state and dialogue management in the TRINDI dialogue move engine toolkit (2000) Nat Lang Eng, 6 (3-4), p. 323; Schröder, M., Trouvain, J., The German text-to-speech synthesis system Mary: A tool for research, development and teaching (2003) Int J Speech Technol, 6 (4), p. 365; Heger, D., Putze, F., Schultz, T., Online workload recognition from EEG data during cognitive tests and human-machine interaction (2010) 33rd Annual German Conference On Artificial Intelligence (KI2010), pp. 410-417; Jasper, H., The 10-20 electrode system of the international federation (1958) Electroencephalogr Clin Neurophysiol, 10, p. 371; Heger, D., Putze, F., Amma, C., Wand, M., Plotkin, I., Wielatt, T., Schultz, T., BiosignalsStudio: A flexible framework for biosignal capturing and processing (2010) 33rd Annual German Conference On Artificial Intelligence (KI2010), pp. 33-39; Makeig, S., Bell, A., Jung, T., Sejnowski, T., Independent component analysis of electroencephalographic data (1996) Advances In Neural Information Processing Systems, pp. 145-151; de Clercq, W., Vergult, A., Vanrumste, B., van Paesschen, W., van Huffel, S., Canonical correlation analysis applied to remove muscle artifacts from the electroencephalogram (2006) IEEE Trans Biomed Eng, 53 (12), p. 2583; Chang, C., Lin, C., (2001) LIBSVM: A Library For Support Vector Machines; Asfour, T., Welke, K., Azad, P., Ude, A., Dillmann, R., The Karlsruhe humanoid head (2008) 8th IEEE-RAS International Conference On Humanoid Robots (humanoids), pp. 447-453; Eriksen, C., Schultz, D., Information processing in visual search: A continuous flow conception and experimental results (1979) At-ten Percept Psychophys, 25 (4), p. 249; Hart, S., Staveland, L., Human mental workload (1988) Development of NASA-TLX (task Load Index), pp. 139-183},
correspondence_address1={Heger, D.; Institute for Anthropomatics, Cognitive Systems Lab (CSL), Karlsruhe Institute of Technology, Adenauerring 4, 76131 Karlsruhe, Germany; email: dominic.heger@kit.edu},
issn={18754791},
language={English},
abbrev_source_title={Int. J. Soc. Rob.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Heger2011436,
author={Heger, D. and Putze, F. and Schultz, T.},
title={Online recognition of facial actions for natural EEG-based BCI applications},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2011},
volume={6975 LNCS},
number={PART 2},
pages={436-446},
doi={10.1007/978-3-642-24571-8_56},
note={cited By 9; Conference of 4th International Conference on Affective Computing and Intelligent Interaction, ACII 2011 ; Conference Date: 9 October 2011 Through 12 October 2011;  Conference Code:87046},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054836104&doi=10.1007%2f978-3-642-24571-8_56&partnerID=40&md5=e1857e4f8fe3359a3b64362007f5261a},
affiliation={Cognitive Systems Lab. (CSL), Karlsruhe Institute of Technology (KIT), Germany},
abstract={We present a system for classification of nine voluntary facial actions, i.e. Neutral, Smile, Sad, Surprise, Angry, Speak, Blink, Left, and Right. The data is assessed by an Emotiv EPOC wireless EEG head-set. We derive spectral features and step function features that represent the main signal characteristics of the recorded data in a straightforward manner. With a two stage classification setup using support vector machines we achieve an overall recognition accuracy of 81.8%. Furthermore, we show a qualitative evaluation of an online system for facial action recognition using the EPOC device. © 2011 Springer-Verlag.},
keywords={Facial action;  On-line recognition;  Qualitative evaluations;  Recognition accuracy;  Signal characteristic;  Spectral feature;  Step functions;  Two stage;  Facial action;  Facial action recognition;  On-line recognition;  Qualitative evaluations;  Recognition accuracy;  Signal characteristic;  Spectral feature;  Step functions, Face recognition;  Intelligent computing, Intelligent computing;  Face recognition},
references={Emotiv Software Development Kit User Manual for Release 1.0.0.4; Blair, R., Facial expressions, their communicatory functions and neuro-cognitive substrates (2003) Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, 358 (1431), p. 561; Boot, L., (2009) Facial Expressions in EEG/EMG Recordings, , Master's thesis, University of Twente; Chin, Z., Ang, K., Guan, C., Multiclass voluntary facial expression classification based on filter bank common spatial pattern (2008) 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS 2008, pp. 1005-1008. , IEEE, Los Alamitos; Croft, R., Barry, R., Removal of ocular artifact from the eeg: A review (2000) Neurophysiologie Clinique/Clinical Neurophysiology, 30 (1), pp. 5-19; Ekman, P., Friesen, W., (1977) Facial Action Coding System, , Consulting Psychologists Press, Stanford University, Palo Alto; Fatourechi, M., Bashashati, A., Ward, R., Birch, G., Emg and eog artifacts in brain computer interface systems: A survey (2007) Clinical Neurophysiology, 118 (3), pp. 480-494; Goncharova, I., McFarland, D., Vaughan, T., Wolpaw, J., Emg contamination of eeg: Spectral and topographical characteristics (2003) Clinical Neurophysiology, 114 (9), pp. 1580-1593; Heger, D., Putze, F., Amma, C., Wand, M., Plotkin, I., Wielatt, T., Schultz, T., BiosignalsStudio: A flexible framework for biosignal capturing and processing (2010) LNCS, 6359, pp. 33-39. , Dillmann, R., Beyerer, J., Hanebeck, U.D., Schultz, T. (eds.) KI 2010. Springer, Heidelberg; Heger, D., Putze, F., Schultz, T., An adaptive information system for an empathic robot using EEG data (2010) LNCS, 6414, pp. 151-160. , Ge, S.S., Li, H., Cabibihan, J.-J., Tan, Y.K. (eds.) ICSR 2010. Springer, Heidelberg; Korb, S., Grandjean, D., Scherer, K., Investigating the production of emotional facial expressions: A combined electroencephalographic (eeg) and electromyographic (emg) approach (2008) 8th IEEE International Conference on Automatic Face Gesture Recognition, FG 2008, pp. 1-6. , September; Vanhatalo, S., Voipio, J., Dewaraja, A., Holmes, M., Miller, J., Topography and elimination of slow eeg responses related to tongue movements (2003) Neuroimage, 20 (2), pp. 1419-1423; Wolpaw, J., Birbaumer, N., McFarland, D., Pfurtscheller, G., Vaughan, T., Brain-computer interfaces for communication and control (2002) Clinical Neurophysiology, 113 (6), pp. 767-791; Zander, T., Kothe, C., Jatzev, S., Gaertner, M., Enhancing human-computer interaction with input from active and passive brain-computer interfaces (2010) Brain-Computer Interfaces, pp. 181-199},
correspondence_address1={Heger, D.; Cognitive Systems Lab. (CSL), Karlsruhe Institute of Technology (KIT)Germany; email: dominic.heger@kit.edu},
sponsors={University of Memphis; HUMAINE Association; FedEx Institute of Technology; Institute for Intelligent Systems; Aldebaran Robotics},
address={Memphis, TN},
issn={03029743},
isbn={9783642245701},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wand2011757,
author={Wand, M. and Schultz, T.},
title={Analysis of phone confusion in EMG-based speech recognition},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2011},
pages={757-760},
doi={10.1109/ICASSP.2011.5946514},
art_number={5946514},
note={cited By 9; Conference of 36th IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2011 ; Conference Date: 22 May 2011 Through 27 May 2011;  Conference Code:85875},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051603386&doi=10.1109%2fICASSP.2011.5946514&partnerID=40&md5=cca9deaf0dcd6fe45d5ba012cc265086},
affiliation={Cognitive Systems Lab., Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={In this paper we present a study on phone confusabilities based on phone recognition experiments from facial surface electromyographic (EMG) signals. In our study EMG captures the electrical potentials of the human articulatory muscles. This technology can be used to create Silent Speech Interfaces, where a user can communicate naturally without uttering any sound. This paper investigates to which extent different phone properties can be recognized from an EMG signal, shows which weaknesses have yet to be overcome, and compares the results to acoustic-based recognition of phones. © 2011 IEEE.},
author_keywords={Electromyography;  Phone Recognition;  Phonetic Features;  Phonetics;  Speech Recognition},
keywords={Electrical potential;  Electromyographic signal;  EMG signal;  Facial surfaces;  Phone recognition;  Phonetic Features;  Speech interface, Linguistics;  Signal processing;  Speech communication;  Telephone sets, Speech recognition},
references={Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Silent Speech Interfaces (2010) Speech Communication, 52 (4), pp. 270-287; Jorgensen, C., Lee, D., Agabon, S., Sub Auditory Speech Recognition Based on EMG/EPG Signals Proceedings of International Joint Conference on Neural Networks (IJCNN), Portland, Oregon, 2003, pp. 3128-3133; Jou, S.-C.S., Schultz, T., Waibel, A., Continuous Electromyographic Speech Recognition with a Multi-Stream Decoding Architecture Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Honolulu, Hawaii, 2007, pp. 401-404; Janke, M., Wand, M., Schultz, T., A Spectral Mapping Method for EMG-based Recognition of Silent Speech Proc. B-INTERFACE, 2010, pp. 22-31; Schünke, M., Schulte, E., Schumacher, U., Kopf und Neuroanatomie (2006) Prometheus - Lernatlas der Anatomie, 3. , Thieme Verlag, Stuttgart, New York; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session Independent Non-Audible Speech Recognition Using Surface Electromyography IEEE Workshop on Automatic Speech Recognition and Understanding, San Juan, Puerto Rico, 2005, pp. 331-336; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards Continuous Speech Recognition using Surface Electromyography Proc. Interspeech, Pittsburgh, PA, Sep 2006, pp. 573-576; Ueda, N., Nakano, R., Ghahramani, Z., Hinton, G.E., Split and Merge EM Algorithm for Improving Gaussian Mixture Density Estimates (2000) Journal of VLSI Signal Processing, 26, pp. 133-140},
correspondence_address1={Wand, M.; Cognitive Systems Lab., Karlsruhe Institute of Technology, Karlsruhe, Germany},
sponsors={Inst. Electr. Electron. Eng. Signal Process. Soc.},
address={Prague},
issn={15206149},
isbn={9781457705397},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vu20115000,
author={Vu, N.T. and Kraus, F. and Schultz, T.},
title={Cross-language bootstrapping based on completely unsupervised training using multilingual A-stabil},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2011},
pages={5000-5003},
doi={10.1109/ICASSP.2011.5947479},
art_number={5947479},
note={cited By 30; Conference of 36th IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2011 ; Conference Date: 22 May 2011 Through 27 May 2011;  Conference Code:85875},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051617867&doi=10.1109%2fICASSP.2011.5947479&partnerID=40&md5=a85a950d5d2edc7eb8d3edf599d83f89},
affiliation={Cognitive Systems Lab. (CSL), Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT), Germany},
abstract={This paper presents our work on rapid language adaptation of acoustic models based on multilingual cross-language bootstrapping and unsupervised training. We used Automatic Speech Recognition (ASR) systems in English, French, German, and Spanish to build a Czech ASR system from scratch. System building was performed without using any transcribed audio data by applying three consecutive steps, i.e. cross-language transfer, unsupervised training based on the "multilingual A-stabil" confidence score [1], and bootstrapping. Based on the confidence score we selected 72% (16.6 hours) of the available audio data with a transcription WER of less than 14.5%. The cross-language bootstrap achieves a word error rate of 23.3% on the Czech development set and 22.4% on the evaluation set. These results are very promising as the performance compares favorably to the Czech ASR system which was trained on 23 hours of manually transcribed data (21.8% on the development set and 21.3% on the evaluation set). © 2011 IEEE.},
author_keywords={multilingual A-Stabil;  rapid language adaptation of ASR;  unsupervised training},
keywords={Acoustic model;  Audio data;  Automatic speech recognition system;  Confidence score;  multilingual A-Stabil;  rapid language adaptation of ASR;  System building;  Unsupervised training;  Word error rate, Linguistics;  Signal processing;  Speech communication;  Transcription, Speech recognition},
references={Vu, N.T., Kraus, F., Schultz, T., Multilingual A-stabil: A new confidence score for multilingual unsupervised training (2010) IEEE Workshop on Spoken Language Technology, SLT 2010, Berkeley, California, USA; Zavaliagkos, G., Colthurst, T., Utilizing untranscribed training data to improve performance DARPA Broadcast News Transcription and Understanding Workshop, Landsdowne, VA, USA, Feb. 1998; Kemp, T., Schaaf, T., Estimating confidence using word lattices (1997) Proc. of Eurospeech, pp. 827-830; Wessel, F., Macherey, K., Ney, H., A comparison of wordgraph and N-best list based confidence measures Proc. of Eurospeech, Budapest, Hungary, 1999; Evermann, G., Woodland, P., Large vocabulary decoding and confidence estimation using word posterior probabilities Proc. ICASSP, Istanbul, Turkey, 2000; Zhang, R., Rudnicky, A.I., A New Data Selection Approach for Semi-Supervised Acoustic Modeling Proc. of ICASSP, Toulouse, France, 2006; Schultz, T., Waibel, A., Experiments on cross-language acoustic modeling Proc. Eurospeech, Aalborg, Denmark, 2001; Lööf, J., Gollan, C., Ney, H., Cross-language Bootstrapping for Unsupervised Acoustic Model Training: Rapid Development of a Polish Speech Recognition System (2009) Interspeech, pp. 88-91. , Brighton, U.K; Lamel, L., Gauvain, J., Adda, G., Unsupervised acoustic modelling Proc. ICASSP Orlando, USA, 2002; Schultz, T., GlobalPhone: A Multilingual Speech and Text Database developed at Karlsruhe University Proc. ICSLP Denver, CO, 2002; Schultz, T., Black, A., Rapid Language Adaptation Tools and Technologies for Multilingual Speech Processing Proc. ICASSP Las Vegas, USA 2008; Vu, N.T., Schlippe, T., Kraus, F., Schultz, T., Rapid Bootstrapping of five Eastern European Languages using the Rapid Language Adaptation Toolkit Interspeech, Makuhari, Japan, 2010; Schultz, T., Waibel, A., Language Independent and Language Adaptive Acoustic Modeling for Speech Recognition (2001) Speech Communication, 35 (1-2), pp. 31-51. , August},
correspondence_address1={Vu, N.T.; Cognitive Systems Lab. (CSL), Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT)Germany; email: thang.vu@kit.edu},
sponsors={Inst. Electr. Electron. Eng. Signal Process. Soc.},
address={Prague},
issn={15206149},
isbn={9781457705397},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nakamura2011573,
author={Nakamura, K. and Janke, M. and Wand, M. and Schultz, T.},
title={Estimation of fundamental frequency from surface electromyographic data: EMG-to-F0},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2011},
pages={573-576},
doi={10.1109/ICASSP.2011.5946468},
art_number={5946468},
note={cited By 16; Conference of 36th IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2011 ; Conference Date: 22 May 2011 Through 27 May 2011;  Conference Code:85875},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051646302&doi=10.1109%2fICASSP.2011.5946468&partnerID=40&md5=f5550818fa2a4c6ca17203daedfc0ac7},
affiliation={Cognitive Systems Lab., Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany},
abstract={In this paper, we present our recent studies of F0 estimation from the surface electromyographic (EMG) data using a Gaussian mixture model (GMM)-based voice conversion (VC) technique, referred to as EMG-to-F 0. In our approach, a support vector machine recognizes individual frames as unvoiced and voiced (U/V), and voiced F0 contours are discriminated by the trained GMM based on the manner of minimum mean-square error. EMG-to-F0 is experimentally evaluated using three data sets of different speakers. Each data set includes almost 500 utterances. Objective experiments demonstrate that we achieve a correlation coefficient of up to 0.49 between estimated and target F0 contours with more than 84% U/V decision accuracy, although the results have large variations. © 2011 IEEE.},
author_keywords={Electromyography;  Feature estimation;  Fundamental frequency;  Voice conversion},
keywords={Correlation coefficient;  Data sets;  Electromyographic;  Feature estimation;  Fundamental frequencies;  Gaussian Mixture Model;  Minimum mean-square error;  Voice conversion, Estimation;  Natural frequencies;  Signal processing;  Speech communication;  Speech processing, Frequency estimation},
references={Nakajima, Y., Kashioka, H., Campbell, N., Shikano, K., Non-Audible Murmur (NAM) Recognition (2006) IEICE Transactions on Information and Systems, E89-D (1), pp. 1-8; Chan, A.D.C., Englehart, K., Hudgins, B., Lovely, D.F., Hidden Markov Model Classification of Myolectric Signals in Speech (2002) IEEE Engineering in Medicine and Biology Society, 21 (5), pp. 143-146. , D.F; Schultz, T., Wand, M., Modeling Coarticulation in EMG-based Continuous Speech Recognition (2010) Speech Communication Journal, 52 (4), pp. 341-353; Jou, S.C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards Continuous Speech Recognition using Surface Electromyography (2006) Proceedings of Interspeech 2006, pp. 573-576; Toth, A.R., Wand, M., Schultz, T., Synthesizing Speech from Electromyography using Voice Transformation Techniques (2009) Proceedings of Interspeech 2009, pp. 652-655; Vapnik, V.N., (1995) The Nature of Statistical Learning Theory, , Springer; Stylianou, Y., Cappe, O., Moulines, E., Continuous Probabilistic Transform for Voice Conversion (1998) IEEE Transaction on Speech and Audio Processing (SAP), 6 (2), pp. 131-142; Kain, A., Macon, M.W., Spectral Voice Conversion for Text-to-speech Synthesis (1998) Proceedings of International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 285-288; Toda, T., Shikano, K., NAM-to-Speech Conversion with Gaussian Mixture Models (2005) Proceedings of Interspeech 2005, pp. 1957-1960; Nakamura, K., Toda, T., Saruwatari, H., Shikano, K., Electrolaryngeal Speech Enhancement Based on Statistical Voice Conversion (2009) Proceedings of Interspeech 2009 - Eurospeech, pp. 1431-1434; Kawahara, H., Katayose, H., De Cheveigné, A., Patterson, R.D., Fixed Point Analysis of Frequency to Instantaneous Frequency Mapping for Accurate Estimation of F0 and Periodicity (1999) Proceedings of EUROSPEECH, pp. 2781-2784; (2011) SVM-Light Support Vector Machine, , http://www.cs.cornell.edu/People/tj/svm_light/, confirmed on 17.02},
correspondence_address1={Nakamura, K.; Cognitive Systems Lab., Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; email: kei-naka@is.naist.jp},
sponsors={Inst. Electr. Electron. Eng. Signal Process. Soc.},
address={Prague},
issn={15206149},
isbn={9781457705397},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wand2011295,
author={Wand, M. and Schultz, T.},
title={Session-independent EMG-based speech recognition},
journal={BIOSIGNALS 2011 - Proceedings of the International Conference on Bio-Inspired Systems and Signal Processing},
year={2011},
pages={295-300},
note={cited By 43; Conference of International Conference on Bio-Inspired Systems and Signal Processing, BIOSIGNALS 2011 ; Conference Date: 26 January 2011 Through 29 January 2011;  Conference Code:85510},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960267600&partnerID=40&md5=7912524a5b8edf7683fc5c7b5b902847},
affiliation={Cognitive Systems Lab., Karlsruhe Institute of Technology, Adenauerring 4, 76131 Karlsruhe, Germany},
abstract={This paper reports on our recent research in speech recognition by surface electromyography (EMG), which is the technology of recording the electric activation potentials of the human articulatory muscles by surface electrodes in order to recognize speech. This method can be used to create Silent Speech Interfaces, since the EMG signal is available even when no audible signal is transmitted or captured. Several past studies have shown that EMG signals may vary greatly between different recording sessions, even of one and the same speaker. This paper shows that session-independent training methods may be used to obtain robust EMG-based speech recognizers which cope well with unseen recording sessions as well as with speaking mode variations. Our best session-independent recognition system, trained on 280 utterances of 7 different sessions, achieves an average 21.93% Word Error Rate (WER) on a testing vocabulary of 108 words. The overall best session-adaptive recognition system, based on a session-independent system and adapted towards the test session with 40 adaptation sentences, achieves an average WER of 15.66%, which is a relative improvement of 21% compared to the baseline average WER of 19.96% of a session-dependent recognition system trained only on a single session of 40 sentences.},
author_keywords={Electromyography;  EMG-based speech recognition;  Silent speech interfaces},
keywords={Audible signals;  Electric activation;  EMG signal;  EMG-based speech recognition;  Recognition systems;  Silent speech interfaces;  Speech interface;  Speech recognizer;  Surface electrode;  Surface electromyography;  Training methods;  Word error rate, Signal processing, Speech recognition},
references={Chan, A., Englehart, K., Hudgins, B., Lovely, D., Myoelectric Signals to Augment Speech Recognition (2001) Medical and Biological Engineering and Computing, 39, pp. 500-506; Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Silent Speech Interfaces (2010) Speech Communication, 52; Janke, M., Wand, M., Schultz, T., A Spectral Mapping Method for EMG-based Recognition of Silent Speech (2010) Proc. B-INTERFACE; Janke, M., Wand, M., Schultz, T., Impact of Lack of Acoustic Feedback in EMG-based Silent Speech Recognition (2010) Proc. Interspeech; Jorgensen, C., Lee, D., Agabon, S., Sub Auditory Speech Recognition Based on EMG/EPG Signals (2003) Proceedings of International Joint Conference on Neural Networks (IJCNN), pp. 3128-3133. , Portland, Oregon; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards Continuous Speech Recognition using Surface Electromyography (2006) Proc. Interspeech, pp. 573-576. , Pittsburgh, PA; Jou, S.-C.S., Schultz, T., Waibel, A., Continuous Electromyographic Speech Recognition with a Multi-Stream Decoding Architecture (2007) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 401-404. , Honolulu, Hawaii; Leggetter, C.J., Woodland, P.C., Maximum Likelihood Linear Regression for Speaker Adaptation of Continuous Density Hidden Markov Models (1995) Computer Speech and Language, 9, pp. 171-185; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session Independent Non-Audible Speech Recognition Using Surface Electromyography (2005) IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 331-336. , San Juan, Puerto Rico; Schultz, T., Wand, M., Modeling Coarticulation in Large Vocabulary EMG-based Speech Recognition (2010) Speech Communication, 52, pp. 341-353; Schünke, M., Schulte, E., Schumacher, U., Kopf und Neuroanatomie (2006) Prometheus - Lernatlas der Anatomie, 3. , Thieme Verlag, Stuttgart, New York; Wand, M., Schultz, T., Towards Speaker-Adaptive Speech Recognition Based on Surface Electromyography (2009) Proc. Biosignals, pp. 155-162. , Porto, Portugal},
correspondence_address1={Wand, M.; Cognitive Systems Lab., Karlsruhe Institute of Technology, Adenauerring 4, 76131 Karlsruhe, Germany; email: michael.wand@kit.edu},
sponsors={Inst. Syst. Technol. Inf., Control Commun. (INSTICC)},
address={Rome},
isbn={9789898425355},
language={English},
abbrev_source_title={BIOSIGNALS - Proc. Int. Conf. Bio-Inspired Syst. Signal Process.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Heger2010151,
author={Heger, D. and Putze, F. and Schultz, T.},
title={An adaptive information system for an empathic robot using EEG data},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6414 LNAI},
pages={151-160},
doi={10.1007/978-3-642-17248-9_16},
note={cited By 4; Conference of 2nd International Conference on Social Robotics, ICSR 2010 ; Conference Date: 23 November 2010 Through 24 November 2010;  Conference Code:82714},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649971728&doi=10.1007%2f978-3-642-17248-9_16&partnerID=40&md5=8255521a2be2ac2ea4a3b51e3c9c64f6},
affiliation={Cognitive Systems Lab. (CSL), Karlsruhe Institute of Technology (KIT), Germany},
abstract={In this paper we introduce a speech-based information system for a humanoid robot that is able to adapt its information presentation strategy to different brain patterns of its user. Brain patterns are classified from electroencephalographic (EEG) signals and correspond to situations of low and high mental workload. The robot selects an information presentation style that best matches the detected patterns. The complete system of recognition and adaptation is tested in an evaluation study with ten participants. We achieve a mean recognition rate of 80% and show that an adaptive information presentation strategy improves user satisfaction in comparison to static strategies. © 2010 Springer-Verlag.},
keywords={Adaptive information presentation;  Adaptive information systems;  Best match;  Complete system;  Electroencephalographic signals;  Evaluation study;  Humanoid robot;  Information presentation;  Mental workload;  Recognition rates;  User satisfaction, Anthropomorphic robots;  Information systems, Robotics},
references={Asfour, T., Welke, K., Azad, P., Ude, A., Dillmann, R., The Karlsruhe Humanoid Head (2008) 8th IEEE-RAS International Conference on Humanoid Robots (Humanoids), pp. 447-453; Bonarini, A., Mainardi, L., Matteucci, M., Tognetti, S., Colombo, R., Stress recognition in a robotic rehabilitation task Robotic Helpers: User Interaction, Interfaces and Companions in Assistive and Therapy Robotics, a Workshop at ACM/IEEE HRI (2008); Breazeal, C., (2004) Designing Sociable Robots, , The MIT Press, Cambridge; Chen, D., Vertegaal, R., Using mental load for managing interruptions in physiologically attentive user interfaces (2004) CHI 2004 Extended Abstracts on Human Factors in Computing Systems, pp. 1513-1516; Hart, S., Staveland, L., Development of NASA-TLX (Task Load Index) (1988) Human Mental Workload, pp. 139-183; Heger, D., Putze, F., Amma, C., Wand, M., Plotkin, I., Wielatt, T., Schultz, T., BiosignalsStudio: A flexible Framework for Biosignal Capturing and Processing 33rd Annual German Conference on Artificial Intelligence, KI 2010 (2010); Heger, D., Putze, F., Schultz, T., Online Workload Recognition from EEG data during Cognitive Tests and Human-Machine Interaction 33rd Annual German Conference on Artificial Intelligence, KI 2010 (2010); Kohlmorgen, J., Dornhege, G., Braun, M., Blankertz, B., Muller, K., Curio, G., Hagemann, K., Kincses, W., Improving human performance in a real operating environment through real-time mental workload detection (2007) Toward Brain-Computer Interfacing, pp. 409-422. , The MIT Press, Cambridge; Liu, C., Rani, P., Sarkar, N., Human-Robot interaction using affective cues (2006) 15th IEEE International Symposium on Robot and Human Interactive Communication (ROMAN), pp. 285-290; Torrey, C., Powers, A., Marge, M., Fussell, S., Kiesler, S., Effects of adaptive robot dialogue on information exchange and social relations (2006) 1st ACM SIGCHI/SIGART Conference on Human-Robot Interaction, pp. 126-133},
correspondence_address1={Heger, D.; Cognitive Systems Lab. (CSL), Karlsruhe Institute of Technology (KIT)Germany; email: dominic.heger@kit.edu},
sponsors={Servo Dynamics Private Limited; Aldebaran Robotics},
address={Singapore},
issn={03029743},
isbn={3642172474; 9783642172472},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Putze20103014,
author={Putze, F. and Schultz, T.},
title={Utterance selection for speech acts in a cognitive tourguide scenario},
journal={Proceedings of the 11th Annual Conference of the International Speech Communication Association, INTERSPEECH 2010},
year={2010},
pages={3014-3017},
note={cited By 1; Conference of 11th Annual Conference of the International Speech Communication Association: Spoken Language Processing for All, INTERSPEECH 2010 ; Conference Date: 26 September 2010 Through 30 September 2010;  Conference Code:85334},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959845007&partnerID=40&md5=c299bf445265b9dad041b1384a04b43e},
affiliation={Cognitive Systems Lab., Karlsruhe Institute of Technology, Germany},
abstract={This paper describes the integration of a cognitive memory model into a spoken dialog system for an in-car tourguide application. This memory model enhances the capabilities of the system and of the simulated user by estimating if and which information is relevant and useful in a given situation. An evaluation study with 15 human judges is performed to demonstrate the feasibility of the described approach. The results show that the proposed utterance selection strategy and the memory model significantly improve the human-like interaction behavior of the spoken dialog system in terms of the amount and quality of given information, relevance, manner, and naturalness of the spoken interaction. © 2010 ISCA.},
author_keywords={Cognition;  Memory model;  Spoken interaction;  Utterance selection;  Workload},
keywords={Cognition;  Memory model;  Spoken interaction;  Utterance selection;  Workload, Computer simulation;  Mathematical models, Speech communication},
references={Baddeley, A., Hitch, G., Working memory (1974) Psychology of Learning and Motivation, pp. 47-89; Cowan, N., (1998) Attention and Memory: An Integrated Framework, , Oxford University Press, USA; Anderson, J.R., Bothell, D., Byrne, M.D., Douglass, S., Lebiere, C., Qin, Y., An integrated theory of the mind (2004) Psychological Review, 111 (4), pp. 1036-1060. , DOI 10.1037/0033-295X.111.4.1036; Grosz, B., The representation and use of focus in a system for understanding dialogs (1977) Proceedings of the Fifth International Joint Conference on Artificial Intelligence, pp. 67-76; Schatzmann, J., Weilhammer, K., Stuttle, M., Young, S., A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies (2006) The Knowledge Engineering Review, 21; Jameson, A., Schäfer, R., Weis, T., Berthold, A., Weyrath, T., Making systems sensitive to the user's time and working memory constraints (1998) Proceedings of the 4th International Conference on Intelligent User Interfaces. ACM, pp. 79-86; Janarthanam, S., Lemon, O., Learning lexical alignment policies for generating referring expressions in spoken dialogue systems (2009) Proceedings of the 12th European Workshop on Natural Language Generation, pp. 74-81; Schultheis, H., Barkowsky, T., Bertel, S., Ltm-c - An improved long-term memory for cognitive architectures (2006) Proceedings of the 7th International Conference on Cognitive Modeling; Putze, F., Schultz, T., Cognitive memory modeling for interactive systems in dynamic environments (2009) International Workshop Series on Spoken Dialogue Systems Technology; Ai, H., Litman, D.J., Assessing dialog system user simulation evaluation measures using human judges (2008) Proceedings of ACL-08: HLT, pp. 622-629. , Columbus, Ohio: Association for Computational Linguistics, June},
correspondence_address1={Putze, F.; Cognitive Systems Lab., Karlsruhe Institute of TechnologyGermany; email: felix.putze@kit.edu},
sponsors={Renesas Electronics Corporation; Google; Microsoft Corporation; Nuance Communications, Inc.; Appen Pty Ltd},
address={Makuhari, Chiba},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schlippe20102290,
author={Schlippe, T. and Ochs, S. and Schultz, T.},
title={Wiktionary as a source for automatic pronunciation extraction},
journal={Proceedings of the 11th Annual Conference of the International Speech Communication Association, INTERSPEECH 2010},
year={2010},
pages={2290-2293},
note={cited By 16; Conference of 11th Annual Conference of the International Speech Communication Association: Spoken Language Processing for All, INTERSPEECH 2010 ; Conference Date: 26 September 2010 Through 30 September 2010;  Conference Code:85334},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959851710&partnerID=40&md5=df0118b255896491d500151a2b9fc38a},
affiliation={Cognitive Systems Lab., Karlsruhe Institute of Technology (KIT), Germany},
abstract={In this paper, we analyze whether dictionaries from the World Wide Web which contain phonetic notations, may support the rapid creation of pronunciation dictionaries within the speech recognition and speech synthesis system building process. As a representative dictionary, we selected Wiktionary [1] since it is at hand in multiple languages and, in addition to the definitions of the words, many phonetic notations in terms of the International Phonetic Alphabet (IPA) are available. Given word lists in four languages English, French, German, and Spanish, we calculated the percentage of words with phonetic notations in Wiktionary. Furthermore, two quality checks were performed: First, we compared pronunciations from Wiktionary to pronunciations from dictionaries based on the GlobalPhone project, which had been created in a rule-based fashion and were manually cross-checked [2]. Second, we analyzed the impact of Wiktionary pronunciations on automatic speech recognition (ASR) systems. French Wiktionary achieved the best pronunciation coverage, containing 92.58% phonetic notations for the French GlobalPhone word list as well as 76.12% and 30.16% for country and international city names. In our ASR systems evaluation, the Spanish system gained the most improvement from Wiktionary pronunciations with 7.22% relative word error rate reduction. © 2010 ISCA.},
author_keywords={Automatic speech recognition;  Crowdsourcing;  Pronunciation dictionary;  Rapid language adaptation},
keywords={Automatic speech recognition;  Automatic speech recognition system;  Crowdsourcing;  International Phonetic Alphabet;  Multiple languages;  Pronunciation dictionaries;  Quality checks;  Rapid language adaptation;  Rule based;  Speech synthesis system;  Word error rate reductions;  Word lists, Linguistics;  Speech communication;  Speech synthesis;  User interfaces;  World Wide Web, Speech recognition},
references={Wiktionary - A Wiki-based Open Content Dictionary, , http://www.wiktionary.org, [Online]; Schultz, T., GlobalPhone: A multilingual speech and text database developed at karlsruhe university (2002) Proceedings of the ICSLP, pp. 345-348; Schultz, T., Black, A.W., Badaskar, S., Hornyak, M., Kominek, J., SPICE: Web-based tools for rapid language adaptation in speech processing systems (2007) Proceedings of Interspeech, , Antwerp, Belgium, August; Black, A.W., Schultz, T., Rapid language adaptation tools and technologies for multilingual speech processing (2008) Proceedings of the ICASSP, , Las Vegas, USA; (1999) Handbook of the International Phonetic Association: A Guide to the use of the International Phonetic Alphabet, , I. P. Association, Cambridge University Press; Schultz, T., Waibel, A., Polyphone decision tree specialization for language adaptation (2000) Proceedings of the ICASSP, , Instanbul; Zhu, X., Rosenfeld, R., Improving trigram language modeling with the world wide web (2001) Proceedings of International Conference on Acoustics, , Speech, and Signal Processing; Besling, S., Heuristical and statistical methods for grapheme-to-phoneme conversion (1994) Konvens, , Vienna, Austria; Black, A.W., Lenzo, K., Pagel, V., Issues in building general letter to sound rules (1998) Proceedings of ESCA Workshop on Speech Synthesis, pp. 77-80. , Australia; Kominek, J., Black, A.W., Learning pronunciation dictionaries: Language complexity and word selection strategies (2006) Proceedings of the HLT Conference of the NAACL, pp. 232-239; Davel, M., Barnard, E., The efficient generation of pronunciation dictionaries: Human factors during bootstrapping (2004) Proceedings of the 8th ICSLP, , Korea; Ghoshal, A., Jansche, M., Khudanpur, S., Riley, M., Ulinski, M., Web-derived pronunciations (2009) Proceedings of the 2009 ICASSP. Washington, pp. 4289-4292. , DC, USA: IEEE Computer Society; Llitjós, A.F., Black, A.W., Evaluation and collection of proper name pronunciations online (2002) Proceedings of LREC2002, , Las Palmas, Canary Islands; List of Wiktionary Editions, Ranked by Article Count., , http://meta.wikimedia.org/wiki/List_of_Wiktionaries, [Online]; Wölfel, M., Channel selection by class separability measures for automatic transcriptions on distant microphones (2007) Proceedings of Interspeech},
correspondence_address1={Schlippe, T.; Cognitive Systems Lab., Karlsruhe Institute of Technology (KIT)Germany; email: tim.schlippe@kit.edu},
sponsors={Renesas Electronics Corporation; Google; Microsoft Corporation; Nuance Communications, Inc.; Appen Pty Ltd},
address={Makuhari, Chiba},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Janke20102686,
author={Janke, M. and Wand, M. and Schultz, T.},
title={Impact of lack of acoustic feedback in EMG-based silent speech recognition},
journal={Proceedings of the 11th Annual Conference of the International Speech Communication Association, INTERSPEECH 2010},
year={2010},
pages={2686-2689},
note={cited By 18; Conference of 11th Annual Conference of the International Speech Communication Association: Spoken Language Processing for All, INTERSPEECH 2010 ; Conference Date: 26 September 2010 Through 30 September 2010;  Conference Code:85334},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959839217&partnerID=40&md5=519a160b4a8182ceca14cd05e29b6b86},
affiliation={Cognitive Systems Lab., Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={This paper presents our recent advances in speech recognition based on surface electromyography (EMG). This technology allows for Silent Speech Interfaces since EMG captures the electrical potentials of the human articulatory muscles rather than the acoustic speech signal. Our earlier experiments have shown that the EMG signal is greatly impacted by the mode of speaking. In this study we extend this line of research by comparing EMG signals from audible, whispered, and silent speaking mode. We distinguish between phonetic features like consonants and vowels and show that the lack of acoustic feedback in silent speech implies an increased focus on somatosensoric feedback, which is visible in the EMG signal. Based on this analysis we develop a spectral mapping method to compensate for these differences. Finally, we apply the spectral mapping to the front-end of our speech recognition system and show that recognition rates on silent speech improve by up to 11.59% relative. © 2010 ISCA.},
author_keywords={EMG;  EMG-based speech recognition;  Silent Speech Interfaces;  Somatosensoric feedback},
keywords={Acoustic feedback;  Acoustic speech;  Electrical potential;  EMG;  EMG signal;  EMG-based speech recognition;  Recognition rates;  Silent Speech Interfaces;  Somatosensoric feedback;  Spectral mappings;  Spectral-mapping method;  Speech interface;  Speech recognition systems;  Surface electromyography, Linguistics;  Photomapping;  Speech communication, Speech recognition},
references={Schultz, T., Wand, M., Modeling coarticulation in large vocabulary EMG-based speech recognition (2010) Speech Communication, 52 (4), pp. 341-353; Guenther, F.H., Ghosh, S.S., Tourville, J.A., Neural modeling and imaging of the cortical interactions underlying syllable production (2006) Brain and Language, 96 (3), pp. 280-301. , DOI 10.1016/j.bandl.2005.06.001, PII S0093934X0500115X; Guenther, F.H., Hampson, M., Johnson, D., A Theoretical Investigation of Reference Frames for the Planning of Speech Movements (1998) Psychological Review, 105 (4), pp. 611-633; Janke, M., Wand, M., Schultz, T., A spectral mapping method for EMG-based recognition of silent speech (2010) First International Workshop on Bio-inspired Human-machine Interfaces and Healthcare Applications, , B-INTERFACE; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 331-336. , San Juan, Puerto Rico; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proc. Interspeech, pp. 573-576. , Pittsburgh, PA; Wand, M., Jou, S.-C.S., Toth, A.R., Schultz, T., Impact of different speaking modes on EMG-based speech recognition (2009) Proc. Interspeech; Welch, P., The use of fast fourier transform for the estimation of power spectra: A method based on time averaging over short, modified periodograms (1967) IEEE Transactions on Audio and Electroacoustics, 15 (2), pp. 70-73},
correspondence_address1={Janke, M.; Cognitive Systems Lab., Karlsruhe Institute of Technology, Karlsruhe, Germany; email: matthias.janke@student.kit.edu},
sponsors={Renesas Electronics Corporation; Google; Microsoft Corporation; Nuance Communications, Inc.; Appen Pty Ltd},
address={Makuhari, Chiba},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hsiao20101361,
author={Hsiao, R. and Metze, F. and Schultz, T.},
title={Improvements to generalized discriminative feature transformation for speech recognition},
journal={Proceedings of the 11th Annual Conference of the International Speech Communication Association, INTERSPEECH 2010},
year={2010},
pages={1361-1364},
note={cited By 0; Conference of 11th Annual Conference of the International Speech Communication Association: Spoken Language Processing for All, INTERSPEECH 2010 ; Conference Date: 26 September 2010 Through 30 September 2010;  Conference Code:85334},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959837276&partnerID=40&md5=22e07e6008d22f56cb8197c42a359520},
affiliation={InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={Generalized Discriminative Feature Transformation (GDFT) is a feature space discriminative training algorithm for automatic speech recognition (ASR). GDFT uses Lagrange relaxation to transform the constrained maximum likelihood linear regression (CMLLR) algorithm for feature space discriminative training. This paper presents recent improvements on GDFT, which are achieved by regularization to the optimization problem. The resulting algorithm is called regularized GDFT (rGDFT) and we show that many regularization and smoothing techniques developed for model space discriminative training are also applicable to feature space training. We evaluated rGDFT on a real-time Iraqi ASR system and also on a large scale Arabic ASR task. © 2010 ISCA.},
author_keywords={Discriminative training;  Speech recognition},
keywords={Automatic speech recognition;  Discriminative features;  Discriminative training;  Feature space;  Lagrange relaxation;  Maximum likelihood linear regression;  Model spaces;  Optimization problems;  Smoothing techniques, Algorithms;  Feature extraction;  Maximum likelihood;  Speech communication, Speech recognition},
references={Hsiao, R., Schultz, T., Generalized discriminative feature transformation for speech recognition (2009) Proceedings of the INTERSPEECH, pp. 664-667; Povey, D., Improvements to fMPE for discriminative training of features (2005) Proceedings of the INTERSPEECH, pp. 2977-2980; Zhang, B., Matsoukas, S., Schwartz, R., Recent progress on the discriminative region-dependent transform for speech feature extraction (2006) Proceedings of the INTERSPEECH, pp. 1573-1576; Saon, G., Povey, D., Soltau, H., Large margin semitied covariance transforms for discriminative training (2009) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, pp. 3753-3756; Gales, M.J.F., Maximum likelihood linear transformations for HMM-based speech recognition (1998) Computer Speech and Language, 12 (2), pp. 75-98; Kozat, S.S., Visweswariah, K., Gopinath, R.A., Feature adaptation based on Gaussian posteriors (2006) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, pp. 221-224; Bach, N., Eck, M., Charoenpornsawat, P., Köhler, T., Stüker, S., Nguyen, T., Hsiao, R., Black, A.W., The CMU TransTac 2007 eyes-free, and hands-free two-way speech-to-speech translation system (2007) Proceedings of the IWSLT; Povey, D., Kanevsky, D., Kingsbury, B., Ramabhadran, B., Saon, G., Visweswariah, K., Boosted MMI for model and feature-space discriminative training (2008) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, pp. 4057-4060; Noamany, M., Schaaf, T., Schultz, T., Advances in the CMU/InterACT arabic GALE transcription system (2007) Proceedings of HLT/NAACL, pp. 129-132; Grézl, F., Fousek, P., Optimizing bottle-neck features for LVCSR (2008) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, pp. 4729-4732},
correspondence_address1={Hsiao, R.; InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: wrhsiao@cs.cmu.edu},
sponsors={Renesas Electronics Corporation; Google; Microsoft Corporation; Nuance Communications, Inc.; Appen Pty Ltd},
address={Makuhari, Chiba},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vu2010183,
author={Vu, N.T. and Kraus, F. and Schultz, T.},
title={Multilingual A-stabil: A new confidence score for multilingual unsupervised training},
journal={2010 IEEE Workshop on Spoken Language Technology, SLT 2010 - Proceedings},
year={2010},
pages={183-188},
doi={10.1109/SLT.2010.5700848},
art_number={5700848},
note={cited By 21; Conference of 2010 IEEE Workshop on Spoken Language Technology, SLT 2010 ; Conference Date: 12 December 2010 Through 15 December 2010;  Conference Code:83877},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951796711&doi=10.1109%2fSLT.2010.5700848&partnerID=40&md5=d4ed248282edbf877bc3f25a37956f68},
affiliation={Cognitive Systems Lab., Karlsruhe Institute of Technology (KIT), Germany},
abstract={This paper presents our work in Automatic Speech Recognition (ASR) in the context of multilingual unsupervised training with application to Czech. Starting without any transcribed acoustic training data we built a Czech ASR by combining cross-language bootstrapping and confidence based unsupervised training. We present our new method called "multilingual A-stabil" to compute confidence scores and explore the relative effectiveness of acoustic models from more than one language such as Russian, Bulgarian, Polish and Croatian for unsupervised training. While conventional confidence measures such as gamma and A-stabil [1] [2] work well with well-trained acoustic models but have problems with poorly estimated acoustic models, our new method works well in both cases. We describe our multilingual unsupervised training framework which gives very promising results in our experiments. We were able to select 80.5% of the audio training data (18.5 hours) with a transcription WER of 14.5% when using a small amount of untranscribed data (only about 23 hours). The final best WER on Czech is 23.6% on the development set and 22.9% on the evaluation set by using cross-lingual boostrapping, which is very close to the performance of the Czech ASR trained with 23 hours audio data with manual transcriptions (23.1% on the development set and 22.3% on the evaluation set). ©2010 IEEE.},
author_keywords={Confidence score;  Multilingual ASR;  Unsupervised training},
keywords={Acoustic model;  Audio data;  Automatic speech recognition;  Confidence Measure;  Confidence score;  Croatians;  Cross-lingual;  Multilingual ASR;  Training data;  Unsupervised training, Transcription, Speech recognition},
references={Kemp, T., Schaaf, T., Estimating confidence using word lattices (1997) Proc. of European Conference on Speech Communication Technology, pp. 827-830; Schaaf, T., Kemp, T., Confidence Measures for Spontaneous Speech Recognition (1997) Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, 2; Zavaliagkos, G., Colthurst, T., Utilizing untranscribed training data to improve performance DARPA Broadcast News Transcription and Unterstanding Workshop, Landsdowne, VA, USA, Feb. 1998; Wessel, F., Macherey, K., Ney, H., A comparision of wordgraph and N-best list based confidence measures European Conference in Speech Communication and Technology, Budapest, Hungary, 1999; Evermann, G., Woodland, P., Large vocabulary decoding and confi- Dence estimation using word posterior probabilities Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing, Instanbul, Turkey, 2000; Lööf, J., Gollan, C., Ney, H., Cross-language Bootstrapping for Unsupervised Acoustic Model Training: Rapid Development of a Polish Speech Recognition System (2009) Interspeech, pp. 88-91. , Brighton, U.K; Lamel, L., Gauvain, J.-L., Adda, G., Unsupervised acoustic modelling Proc. ICASSP Orlando, USA, 2002; Schultz, T., Waibel, A., Experiments on cross-language acoustic modeling Proc. European Conf. on Speech Communication and Technology, Aalborg, Denmark, 2001; Schultz, T., Black, A., Rapid Language Adaptation Tools and Technologies for Multilingual Speech Processing Proc. ICASSP Las Vegas, USA 2008; Schultz, T., GlobalPhone: A Multilingual Speech and Text Database developed at Karlsruhe University Proc. ICSLP Denver, CO, 2002; Schultz, T., Waibel, A., Language Independent and Language Adaptive Acoustic Modeling for Speech Recognition (2001) Speech Communication, 35 (1-2), pp. 31-51. , August; Stolcke, A., SRILM - An extensible language modeling toolkit Proc. Int. Conf. on Spoken Language Processing, Denver, Colorado, 2002; Vu, N.T., Schlippe, T., Kraus, F., Schultz, T., Rapid Bootstrapping of five Eastern European Languages using the Rapid Language Adaptation Toolkit Interspeech, Makuhari, Japan, 2010},
correspondence_address1={Vu, N. T.; Cognitive Systems Lab., Karlsruhe Institute of Technology (KIT)Germany; email: thang.vu@kit.edu},
sponsors={The Institute of Electrical and Electronics Engineers (IEEE); IEEE Signal Processing Society},
address={Berkeley, CA},
isbn={9781424479030},
language={English},
abbrev_source_title={IEEE Workshop Spoken Lang. Technol., SLT - Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vu2010865,
author={Vu, N.T. and Schlippe, T. and Kraus, F. and Schultz, T.},
title={Rapid bootstrapping of five Eastern European languages using the Rapid Language Adaptation Toolkit},
journal={Proceedings of the 11th Annual Conference of the International Speech Communication Association, INTERSPEECH 2010},
year={2010},
pages={865-868},
note={cited By 26; Conference of 11th Annual Conference of the International Speech Communication Association: Spoken Language Processing for All, INTERSPEECH 2010 ; Conference Date: 26 September 2010 Through 30 September 2010;  Conference Code:85334},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959858216&partnerID=40&md5=9b0d6525777b0fcabbe96b7e58f824c5},
affiliation={Cognitive Systems Lab., Karlsruhe Institute of Technology (KIT), Germany},
abstract={This paper presents our latest efforts toward LVCSR systems for five Eastern European languages such as Bulgarian, Croatian, Czech, Polish, and Russian using our Rapid Language Adaptation Toolkit (RLAT) [1]. We investigated the possibility of crawling large quantities of text material from the Internet, which is very cheap but also requires text post-processing steps due to the varying text quality. The goal of this study is to determine the best strategy for language model optimization on the given domain in a short time period with minimal human effort. Our results show that we can build an initial ASR system for these five languages in only twenty days using RLAT. On the multilingual GlobalPhone speech corpus [2], we achieved a word error rate (WER) of 16.9% for Bulgarian, 32.8% for Croatian, 23.5% for Czech, 20.4% for Polish, and 36.2% for Russian. © 2010 ISCA.},
author_keywords={Automatic speech recognition;  Eastern European languages;  Rapid language adaptation;  RLAT},
keywords={Automatic speech recognition;  Best strategy;  Croatians;  European languages;  Language model;  Post processing;  Rapid language adaptation;  RLAT;  Speech corpora;  Text materials;  Text qualities;  Time-periods;  Word error rate, Computational linguistics, Speech recognition},
references={Schultz, T., Black, A., Rapid language adaptation tools and technologies for multilingual speech processing (2008) Proc. ICASSP Las Vegas, , NV; Schultz, T., GlobalPhone: A multilingual speech and text database developed at karlsruhe university (2002) Proc. ICSLP Denver, , CO; Schultz, T., Waibel, A., Language-independent and language-adaptive acoustic modeling for speech recognition (2001) Speech Communication, 35 (1-2), pp. 31-51. , DOI 10.1016/S0167-6393(00)00094-7, PII S0167639300000947; Stolcke, A., SRILM - An extensible language modeling toolkit (2002) Proceedings of ICSLP; Mircheva, A., (2006) Bulgarian Speech Recognition and Multilingual Language Modeling, , Study thesis, Uni Karlsruhe, March; Scheytt, P., Geutner, P., Waibel, A., Serbo-Croatian LVCSR on the dictation and broadcast news domain (1998) International Conference on Acoustics, Speech, and Signal Processing 1998, , ICASSP 1998, Seattle, USA, 01. May; Byrne, W., Hajič, J., Ircing, P., Jelinek, F., Khudanpur, S., Krbec, P., Psutka, J., On large vocabulary continuous speech recognition of highly inflectional language - Czech Eurospeech 2001; Icring, P., Psutka, J., (2001) Two-pass Recognition of Czech Speech Using Adaptive Vocabulary, , TSD, Železnaá Ruda, Czech Republic; Lööf, J., Gollan, C., Ney, H., Cross-language bootstrapping for unsupervised acoustic model training: Rapid development of a polish speech recognition system (2009) Interspeech 2009, pp. 88-91. , Brighton, U.K., September; Stüker, S., Schultz, T., A grapheme based speech recognition system for Russian (2004) 9th International Conference Speech and Computer 2004, , SPECOM 2004, St. Petersburg, Russia; http://www-i6.informatik.rwth-aachen.de/web/Software/g2p.html},
correspondence_address1={Vu, N.T.; Cognitive Systems Lab., Karlsruhe Institute of Technology (KIT)Germany; email: thang.vu@kit.edu},
sponsors={Renesas Electronics Corporation; Google; Microsoft Corporation; Nuance Communications, Inc.; Appen Pty Ltd},
address={Makuhari, Chiba},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schlippe20101816,
author={Schlippe, T. and Zhu, C. and Jan, G. and Schultz, T.},
title={Text normalization based on statistical machine translation and internet user support},
journal={Proceedings of the 11th Annual Conference of the International Speech Communication Association, INTERSPEECH 2010},
year={2010},
pages={1816-1819},
note={cited By 5; Conference of 11th Annual Conference of the International Speech Communication Association: Spoken Language Processing for All, INTERSPEECH 2010 ; Conference Date: 26 September 2010 Through 30 September 2010;  Conference Code:85334},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959845545&partnerID=40&md5=75a78a2e2591be430bfb806abdc69145},
affiliation={Cognitive Systems Lab., Karlsruhe Institute of Technology (KIT), Germany},
abstract={In this paper, we describe and compare systems for text normalization based on statistical machine translation (SMT) methods which are constructed with the support of internet users. Internet users normalize text displayed in a web interface, thereby providing a parallel corpus of normalized and non-normalized text. With this corpus, SMT models are generated to translate non-normalized into normalized text. To build traditional language-specific text normalization systems, knowledge of linguistics as well as established computer skills to implement text normalization rules are required. Our systems are built without profound computer knowledge due to the simple self-explanatory user interface and the automatic generation of the SMT models. Additionally, no inhouse knowledge of the language to normalize is required due to the multilingual expertise of the internet community. All techniques are applied on French texts, crawled with our Rapid Language Adaptation Toolkit [1] and compared through Levenshtein edit distance [2], BLEU score [3], and perplexity. © 2010 ISCA.},
author_keywords={Automatic speech recognition;  Crowdsourcing;  Rapid language adaptation;  Statistical machine translation;  Text normalization},
keywords={Automatic speech recognition;  Crowdsourcing;  Rapid language adaptation;  Statistical machine translation;  Text normalization, Character recognition;  Information theory;  Linguistics;  Speech communication;  Speech recognition;  Telecommunication networks;  Translation (languages);  User interfaces;  World Wide Web, Speech transmission},
references={Schultz, T., Black, A.W., Badaskar, S., Hornyak, M., Kominek, J., Spice: Web-based tools for rapid language adaptation in speech processing systems (2007) Antwerp, Belgium: Proceedings of Interspeech, , August; Levenshtein, V.I., Binary codes capable of correcting deletions, insertions, and reversals (1966) Soviet Physics-doklady, 10, pp. 707-710; Papineni, K., Roukos, S., Ward, T., Zhu, W.-J., BLEU: A method for automatic evaluation of machine translation (2002) Proceedings of the 40th ACL, , Philadelphia; Schlippe, T., Nguyen, T., Vogel, S., Diacritization as a translation problem and as a sequence labeling problem (2008) The Eighth Conference of the Association for Machine Translation in the Americas (AMTA 2008), , Waikiki, Hawai'i, 21-25 October; Adda, G., Adda-Decker, M., Gauvain, J.-L., Lamel, L., Text normalization and speech recognition in french (1997) Proc. ESCA Eurospeech'97, pp. 2711-2714; Gralinski, F., Jassem, K., Wagner, A., Wypych, M., Text normalization as a special case of machine translation (2006) Proceedings of International Multiconference on Computer Science and Information Technology, , Wisla, Poland November; Henriquez, C.A., Hernandez, A., A N-gram-based statistical machine translation approach for text normalization on chat-speak style communications (2009) CAW2 (Content Analysis in Web 2.0), , April; Aw, A., Zhang, M., Xiao, J., Su, J., A phrase-based statistical model for SMS text normalization (2006) Proceedings of the COLING/ACL, pp. 33-40. , Sydney; Schultz, T., Waibel, A., Experiments on cross-language acoustic modeling (2001) Proceedings of Eurospeech, pp. 2721-2724. , Alborg; Koehn, P., Hoang, H., An Chris Callison-Burch, A.B., Federico, M., Bertoldi, N., Cowan, B., Shen, W., Herbst, E., Moses: Open source toolkit for statistical machine translation (2007) Annual Meeting of ACL, Demonstration Session, , Prag, Czech Republic, June; Och, F.J., Ney, H., A systematic comparison of various statistical alignment models (2003) Computational Linguistics, 29 (1), pp. 19-51. , DOI 10.1162/089120103321337421; Stolcke, A., SRILM - An extensible language modeling toolkit (2002) International Conference on Spoken Language Processing, , Denver, USA; Simard, M., Ueffing, N., Isabelle, P., Kuhn, R., Rule-based translation with statistical phrase-based post-editing (2007) Proceedings of the Second Workshop on Statistical Machine Translation, , Prague, Czech Republic, June; Schlippe, T., Ochs, S., Schultz, T., Wiktionary as a source for automatic pronunciation extraction (2010) 11th Annual Conference of the International Speech Communication Association (Interspeech 2010), , Makuhari, Japan, 26-30 September},
correspondence_address1={Schlippe, T.; Cognitive Systems Lab., Karlsruhe Institute of Technology (KIT)Germany; email: tim.schlippe@kit.edu},
sponsors={Renesas Electronics Corporation; Google; Microsoft Corporation; Nuance Communications, Inc.; Appen Pty Ltd},
address={Makuhari, Chiba},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Metze20101501,
author={Metze, F. and Hsiao, R. and Jin, Q. and Nallasamy, U. and Schultz, T.},
title={The 2010 CMU GALE Speech-to-Text system},
journal={Proceedings of the 11th Annual Conference of the International Speech Communication Association, INTERSPEECH 2010},
year={2010},
pages={1501-1504},
note={cited By 11; Conference of 11th Annual Conference of the International Speech Communication Association: Spoken Language Processing for All, INTERSPEECH 2010 ; Conference Date: 26 September 2010 Through 30 September 2010;  Conference Code:85334},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959828316&partnerID=40&md5=229f2e6d4cfeccaef5087d1c3e685eaa},
affiliation={Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, United States},
abstract={This paper describes the latest Speech-to-Text system developed for the Global Autonomous Language Exploitation ("GALE") domain by Carnegie Mellon University (CMU). This systems uses discriminative training, bottle-neck features and other techniques that were not used in previous versions of our system, and is trained on 1150 hours of data from a variety of Arabic speech sources. In this paper, we show how different lexica, pre-processing, and system combination techniques can be used to improve the final output, and provide analysis of the improvements achieved by the individual techniques. © 2010 ISCA.},
author_keywords={Bottle-neck features;  Discriminative training;  Speech recognition},
keywords={Arabic speech;  Bottle-neck features;  Carnegie Mellon University;  Discriminative training;  Pre-processing;  Speech-to-text system;  System combination, Speech communication, Speech recognition},
references={Grézl, F., Fousek, P., Optimizing bottle-neck features for LVCSR (2008) Proc. ICASSP, , Las Vegas, NV; USA: IEEE. Apr; Hsiao, R., Schultz, T., Generalized discriminative feature transformation for speech recognition (2009) Proc. INTERSPEECH, , Brighton; UK: ISCA. Sep; Saon, G., Soltau, H., Chaudhari, U., Chu, S., Kingsbury, B., Kuo, H.-K., Mangu, L., Povey, D., The IBM 2008 GALE arabic speech transcription system (2010) Proc. ICASSP, , Dallas, TX; USA: IEEE. Apr; Tomalin, M., Diehl, F., Gales, M., Park, J., Woodland, P., Recent improvements to the cambridge arabic speech-to-text systems (2010) Proc. ICASSP, , Dallas, TX; USA: IEEE. Apr; Fousek, P., Lamel, L., Gauvain, J.-L., Transcribing broadcast data using MLP features (2008) Proc. InterSpeech 2008, , Brisbane; Australia: ISCA. Sep; Vergyri, D., Mandal, A., Wang, W., Stolcke, A., Zheng, J., Graciarena, M., Rybach, D., Morgan, N., Development of the SRI/ nightingale arabic ASR system (2008) Proc. INTERSPEECH, , Brisbane, Australia: ISCA. Sep; Nguyen, L., Ng, T., Nguyen, K., Zbib, R., Makhoul, J., Lexical and phonetic modeling for arabic automatic speech recognition (2009) Proc. INTERSPEECH, , Brighton, UK: ISCA. Sep; Noamany, M., Schaaf, T., Schultz, T., Advances in the CMU/InterACT arabic GALE transcription system (2007) Proc. NAACL/ HLT 2007, pp. 129-132. , Companion Volume, Short Papers. Rochester, NY; USA: ACL. April; Soltau, H., Metze, F., Fügen, C., Waibel, A., A one-pass decoder based on polymorphic linguistic context assignment (2001) Proc. ASRU 2001, , Madonna di Campiglio, Italy: IEEE. Dec; Gales, M.J.F., Semi-tied covariance matrices for hidden Markov models (1999) IEEE Transactions on Speech and Audio Processing, 2. , May; Jin, Q., Schultz, T., Speaker segmentation and clustering in meetings (2004) Proc. ICSLP, , Jeju Island; Korea: ISCA. Oct; Stolcke, A., SRILM - An extensible language modeling toolkit (2002) Proc. Intl. Conf. on Spoken Language Processing, , Denver, CO: ISCA. Sep; Buckwalter, T., Issues in arabic orthography and morphology analysis (2004) Proc. COLING, , Geneva; Switzerland; Hermansky, H., Ellis, D.P., Sharma, S., Tandem connectionist feature extraction for conventional HMM systems (2000) Proc. ICASSP, 3. , Istanbul; Turkey: IEEE. Apr; Park, J., Diehl, F., Gales, M.J.F., Tomalin, M., Woodland, P.C., Training and adapting MLP features for arabic speech recognition (2009) Proc. ICASSP 2009, , Taipei; Taiwan: IEEE. Apr; Zhan, P., Westphal, M., Speaker normalization based on frequency warping (1997) Proc. ICASSP 1997, , München; Bavaria: IEEE. Apr; Povey, D., Kanevsky, D., Kingsbury, B., Ramabhadran, B., Saon, G., Visweswariah, K., Boosted MMI for model and featurespace discriminative training (2008) Proc. ICASSP, , Las Vegas, NV; USA: IEEE. Apr; Tam, Y.-C., Schultz, T., Correlated bigram LSA for unsuper-vised LM adaptation (2008) Proc. Neural Information Processing Systems, , NIPS, Vancouver, BC; Canada. Dec; Ma, C., Kuo, H.-K.J., Soltau, H., Cui, X., Chaudhari, U., Mangu, L., Lee, C.-H., A comparative study on system combination schemes for LVCSR (2010) Proc ICASSP, , Dallas, TX; USA: IEEE. Mar},
correspondence_address1={Metze, F.; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, United States; email: fmetze@cs.cmu.edu},
sponsors={Renesas Electronics Corporation; Google; Microsoft Corporation; Nuance Communications, Inc.; Appen Pty Ltd},
address={Makuhari, Chiba},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Heger2010410,
author={Heger, D. and Putze, F. and Schultz, T.},
title={Online workload recognition from EEG data during cognitive tests and human-machine interaction},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6359 LNAI},
pages={410-417},
doi={10.1007/978-3-642-16111-7_47},
note={cited By 29; Conference of 33rd Annual German Conference on Artificial Intelligence, KI 2010 ; Conference Date: 21 September 2010 Through 24 September 2010;  Conference Code:82410},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78349282259&doi=10.1007%2f978-3-642-16111-7_47&partnerID=40&md5=964cfbcc21ec53b678962f9f849962d6},
affiliation={Cognitive Systems Lab (CSL), Karlsruhe Institute of Technology (KIT), Germany},
abstract={This paper presents a system for live recognition of mental workload using spectral features from EEG data classified by Support Vector Machines. Recognition rates of more than 90% could be reached for five subjects performing two different cognitive tasks according to the flanker and the switching paradigms. Furthermore, we show results of the system in application on realistic data of computer work, indicating that the system can provide valuable information for the adaptation of a variety of intelligent systems in human-machine interaction. © 2010 Springer-Verlag Berlin Heidelberg.},
keywords={Cognitive task;  Cognitive tests;  Computer work;  Human machine interaction;  Mental workload;  Realistic data;  Recognition rates;  Spectral feature;  Cognitive task;  Cognitive tests;  Computer work;  Eeg datum;  Human machine interaction;  Mental workload;  Spectral feature, Feature extraction;  Intelligent systems;  Man machine systems;  Potassium iodide;  Artificial intelligence;  Intelligent systems;  Man machine systems, Human computer interaction;  Human computer interaction},
funding_details={Deutsche ForschungsgemeinschaftDeutsche Forschungsgemeinschaft, DFG},
funding_text 1={This work has been supported by the Deutsche Forschungsgemeinschaft (DFG) within Collaborative Research Center 588 “Humanoid Robots - Learning and Cooperating Multimodal Robots” [11].},
references={Gevins, A., Smith, M., Neurophysiological measures of cognitive workload during human-computer interaction (2003) Theoretical Issues in Ergonomics Science, 4 (1), pp. 113-131; Berka, C., Levendowski, D., Cvetinovic, M., Petrovic, M., Davis, G., Lumicao, M., Zivkovic, V., Olmstead, R., Real-time analysis of EEG indexes of alertness, cognition and memory acquired with a wireless EEG headset (2004) International Journal of Human-Computer Interaction, 17 (2), pp. 151-170; Kohlmorgen, J., Dornhege, G., Braun, M., Blankertz, B., Müller, K., Curio, G., Hagemann, K., Kincses, W., Improving human performance in a real operating environment through real-time mental workload detection Toward Brain-Computer Interfacing, pp. 409-422; Honal, M., Schultz, T., Determine task demand from brain activity (2008) International Conference on Bio-Inspired Systems and Signal Processing; Putze, F., Jarvis, J., Schultz, T., Multimodal recognition of cognitive workload for multitasking in the car (2010) Accepted for International Conference on Pattern Recognition 2010; Jasper, H., The 10-20 electrode system of the international federation (1958) Electroen-Cephalography and Clinical Neurophysiology, 10, pp. 371-375; Heger, D., Putze, F., Amma, C., Wand, M., Plotkin, I., Wielatt, T., Schultz, T., BiosignalsStudio: A flexible framework for biosignal capturing and processing (2010) KI 2010, 6359, pp. 33-39. , Dillmann, R., et al. (eds.) LNCS (LNAI) Springer, Heidelberg; Chang, C., Lin, C., (2001) LIBSVM: A Library for Support Vector Machines; Zschocke, S., (2002) Klinische Elektroenzephalographie, , Springer, Heidelberg; Koles, Z., Flor-Henry, P., Mental activity and the EEG: Task and workload related effects (1981) Medical and Biological Engineering and Computing, 19 (2), pp. 185-194; Collaborative Research Center 588 Humanoid Robots - Learning and Cooperating Multimodal Robots, , http://www.sfb588.uni-karlsruhe.de/},
correspondence_address1={Heger, D.; Cognitive Systems Lab (CSL), Karlsruhe Institute of Technology (KIT)Germany; email: dominic.heger@kit.edu},
sponsors={ontoprise GmbH; PTV Planung Transport Verkehr AG; Springer Verlag},
address={Karlsruhe},
issn={03029743},
isbn={3642161103; 9783642161100},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dillmann2010,
author={Dillmann, R. and Beyerer, J. and Hanebeck, U.D. and Schultz, T.},
title={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics): Preface},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6359 LNAI},
pages={VI},
note={cited By 0; Conference of 33rd Annual German Conference on Artificial Intelligence, KI 2010 ; Conference Date: 21 September 2010 Through 24 September 2010;  Conference Code:82410},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78349265329&partnerID=40&md5=41a08a0ada3de051f3f5abd8462a5926},
correspondence_address1={Dillmann, R.},
sponsors={ontoprise GmbH; PTV Planung Transport Verkehr AG; Springer Verlag},
address={Karlsruhe},
issn={03029743},
isbn={3642161103; 9783642161100},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Editorial},
source={Scopus},
}

@ARTICLE{Gehrig2010436,
author={Gehrig, D. and Stein, T. and Fischer, A. and Schwameder, H. and Schultz, T.},
title={Towards semantic segmentation of human motion sequences},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6359 LNAI},
pages={436-443},
doi={10.1007/978-3-642-16111-7_50},
note={cited By 3; Conference of 33rd Annual German Conference on Artificial Intelligence, KI 2010 ; Conference Date: 21 September 2010 Through 24 September 2010;  Conference Code:82410},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78349273984&doi=10.1007%2f978-3-642-16111-7_50&partnerID=40&md5=8c255151325a4c6b6d9ea3ca115746dd},
affiliation={Institute for Anthropomatics, Germany; Institute for Sport and Sport Science, Karlsruhe Institute of Technology, Germany},
abstract={In robotics research is an increasing need for knowledge about human motions. However humans tend to perceive motion in terms of discrete motion primitives. Most systems use data-driven motion segmentation to retrieve motion primitives. Besides that the actual intention and context of the motion is not taken into account. In our work we propose a procedure for segmenting motions according to their functional goals, which allows a structuring and modeling of functional motion primitives. The manual procedure is the first step towards an automatic functional motion representation. This procedure is useful for applications such as imitation learning and human motion recognition. We applied the proposed procedure on several motion sequences and built a motion recognition system based on manually segmented motion capture data. We got a motion primitive error rate of 0.9 % for the marker-based recognition. Consequently the proposed procedure yields motion primitives that are suitable for human motion recognition. © 2010 Springer-Verlag Berlin Heidelberg.},
keywords={Data-driven;  Discrete motion;  Error rate;  Functional motions;  Human motion recognition;  Human motions;  Imitation learning;  Motion capture data;  Motion primitives;  Motion recognition;  Motion segmentation;  Motion sequences;  Robotics research;  Semantic segmentation;  Functional motions;  Human motion recognition;  Imitation learning;  Motion capture data;  Motion primitives;  Motion recognition;  Motion segmentation;  Semantic segmentation, Artificial intelligence;  Image segmentation;  Potassium iodide;  Artificial intelligence;  Image segmentation;  Motion analysis;  Semantics, Motion estimation;  Motion estimation},
funding_details={Deutsche ForschungsgemeinschaftDeutsche Forschungsgemeinschaft, DFG},
funding_details={Deutsche ForschungsgemeinschaftDeutsche Forschungsgemeinschaft, DFG},
funding_text 1={1This work has been supported by the Deutsche Forschungsgemeinschaft (DFG) within Collaborative Research Center 588 “Humanoid Robots - Learning and Coop-erating Multimodal Robots”},
references={Schaal, S., The new robotics - Towards human-centered machines (2007) HFSP J., 1 (2), pp. 115-126; Kahol, K., Gesture segmentation in complex motion sequences (2003) Proceedings IEEE International Conference on Image Processing, pp. 105-108; Giese, M., Poggio, T., Neural mechanisms for the recognition of biological movements (2003) Nature Reviews, 4, pp. 179-192; Rizzolatti, G., Fogassi, L., Gallese, V., Neurophysiological mechanisms underlying the understanding and imitation of action (2001) Nature Reviews, 2, pp. 661-670; Pastor, P., Hoffmann, H., Asfour, T., Schaal, S., Learning and generalization of motor skills by learning from demonstration (2009) ICRA 2009: Proceedings of the 2009 IEEE International Conference on Robotics and Automation, pp. 1293-1298; Gehrig, D., Kuehne, H., Woerner, A., Schultz, T., HMM-based human motion recognition with optical flow data (2009) 9th IEEE-RAS International Conference on Humanoid Robots, Humanoids; Krueger, N., Piater, J., Woergoetter, F., Geib, C., Petrick, R., Steedman, M., Ude, A., Dillmann, R., (2009) A Formal Definition of Object Action Complexes and Examples at Different Levels of the Process Hierarchy; Barbic, J., Safonova, A., Pan, J.-Y., Faloutsos, C., Hodgins, J.K., Pollard, N.S., Segmenting motion capture data into distinct behaviors (2004) Graphics Interface, pp. 185-194; Reng, L., Moeslund, T.B., Granum, E., Finding motion primitives in human body gestures (2006) GW 2005, 3881, pp. 133-144. , Gibet, S., Courty, N., Kamp, J.-F. (eds.) LNCS (LNAI) Springer, Heidelberg; Aksoy, E.E., Abramov, A., Woergoetter, F., Dellen, B., Categorizing object-action relations from semantic scene graphs (2010) IEEE International Conference on Robotics and Automation, pp. 398-405; Sridhar, M., Cohn, G.A., Hogg, D., Learning functional object categories from a relational spatio-temporal representation (2008) 18th European Conference on Artificial Intelligence; Guerra-Filho, G., Aloimonos, Y., Towards a sensorimotor WordNet SM: Closing the semantic gap (2006) Proc. of the International WordNet Conference, , GWC; Ivanov, Y.A., Bobick, A.F., Recognition of visual activities and interactions by stochastic parsing (2000) IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 852-872; Goehner, U., (1992) Einfuehrung in die Bewegungslehre des Sports, Teil 1: Die Sportlichen Bewegungen (Introduction to Human Movement Science, Part 1: Sports Movements), , Hofmann, Schorndorf; Kelso, J.A.S., Fuchs, A., Lancaster, R., Holroyd, T., Cheyne, D., Weinberg, H., Dynamic cortical activity in the human brain reveals motor equivalence (1998) Nature, 392, pp. 814-818; Simonidis, C., Seemann, W., MkdTools - Human models with matlab (2008) The 10th International Symposium on 3D Analysis of Human Movement - Fusion Works, , Wassink, R. (ed.); Soltau, H., Metze, F., Fügen, C., Waibel, A., A one-pass decoder based on polymorphic linguistic context assignment (2001) ASRU, pp. 214-217; Finke, M., Geutner, P., Hild, H., Kemp, T., Ries, K., Westphal, M., The karlsruhe-verbmobil speech recognition engine (1997) ICASSP, 1, pp. 83-86},
correspondence_address1={Gehrig, D.; Institute for AnthropomaticsGermany; email: dirk.gehrig@kit.edu},
sponsors={ontoprise GmbH; PTV Planung Transport Verkehr AG; Springer Verlag},
address={Karlsruhe},
issn={03029743},
isbn={3642161103; 9783642161100},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Heger201033,
author={Heger, D. and Putze, F. and Amma, C. and Wand, M. and Plotkin, I. and Wielatt, T. and Schultz, T.},
title={BiosignalsStudio: A flexible framework for biosignal capturing and processing},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6359 LNAI},
pages={33-39},
doi={10.1007/978-3-642-16111-7_3},
note={cited By 7; Conference of 33rd Annual German Conference on Artificial Intelligence, KI 2010 ; Conference Date: 21 September 2010 Through 24 September 2010;  Conference Code:82410},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78349250552&doi=10.1007%2f978-3-642-16111-7_3&partnerID=40&md5=f452491e807930d20f929ed428fd0f9e},
affiliation={Karlsruhe Institute of Technology (KIT), Cognitive Systems Lab (CSL), Germany},
abstract={In this paper we introduce BiosignalsStudio (BSS), a framework for multimodal sensor data acquisition. Due to its flexible architecture it can be used for large scale multimodal data collections as well as a multimodal input layer for intelligent systems. The paper describes the software framework and its contributions to our research work and systems. © 2010 Springer-Verlag Berlin Heidelberg.},
keywords={Biosignals;  Flexible architectures;  Flexible framework;  Input layers;  Multi-modal;  Multi-modal data;  Multimodal sensor;  Software frameworks;  Biosignals;  Flexible architectures;  Flexible framework;  Multi-modal data;  Multimodal inputs;  Multimodal sensor;  Software frameworks, Intelligent systems;  Artificial intelligence;  Computer programming;  Intelligent systems, Potassium iodide;  Data acquisition},
references={G. Recorder Biosignal Recording Software, , http://www.gtec.at, Herbersteinstrasse 60, 8020 Graz, Austria; Unit 13, 22 Lexington Drive Bella Vista, , http://www.adinstruments.com, NSW, Australia: LabChart (2153); 11500 N Mopac Expwy, , http://www.ni.com/labview, ustin, TX, USA: Lab View; Mader, S., Peter, C., Göcke, R., Schultz, R., Voskamp, J., Urban, B., A freely configurable, multi-modal sensor sytem for affective computing (2004) Proceedings of the Affective Dialogue Systems Workshop, , Kloster Irrsee, Germany; PROATECH LLC, , http://www.bioera.net; Veigl, C., (2006) An Open-Source System for Biosignal- and Camera-Mouse Applications Submission for the Young Researchers Consortium of the ICCHP 2006, , http://www.shifz.org/brainbay; Putze, F., Schultz, T., Cognitive dialog systems for dynamic environments: Progress and challenges (2009) Proceedings of the 4th Biennial Workshop on DSP for In-Vehicle Systems and Safety; Putze, F., Jarvis, J.-P., Schultz, T., Multimodal recognition of cognitive workload for multitasking in the car (2010) Proceedings of the 20th International Conference on Pattern Recognition, , Accepted for Publication in the; Heger, D., Putze, F., Schultz, T., Online workload recognition during cogitive tests and human-computer interaction (2010) KI 2010, 6359, pp. 402-409. , Dillmann, R., et al. (eds.) LNCS (LNAI) Springer, Heidelberg; Amma, C., Gehrig, D., Schultz, T., Airwriting recognition using wearable motion sensors (2010) Proceedings of the First Augmented Human International Conference},
correspondence_address1={Heger, D.; Karlsruhe Institute of Technology (KIT), Cognitive Systems Lab (CSL)Germany; email: dominic.heger@kit.edu},
sponsors={ontoprise GmbH; PTV Planung Transport Verkehr AG; Springer Verlag},
address={Karlsruhe},
issn={03029743},
isbn={3642161103; 9783642161100},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Putze20103748,
author={Putze, F. and Jarvis, J.-P. and Schultz, T.},
title={Multimodal recognition of cognitive workload for multitasking in the car},
journal={Proceedings - International Conference on Pattern Recognition},
year={2010},
pages={3748-3751},
doi={10.1109/ICPR.2010.913},
art_number={5597574},
note={cited By 35; Conference of 2010 20th International Conference on Pattern Recognition, ICPR 2010 ; Conference Date: 23 August 2010 Through 26 August 2010;  Conference Code:82392},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149473665&doi=10.1109%2fICPR.2010.913&partnerID=40&md5=f0d2e928a5080aefa3daa5016f605271},
affiliation={Karlsruhe Institute of Technology (KIT), Cognitive Systems Lab. (CSL), Germany},
abstract={This work describes the development and evaluation of a recognizer for different levels of cognitive workload in the car. We collected multiple biosignal streams (skin conductance, pulse, respiration, EEG) during an experiment in a driving simulator in which the drivers performed a primary driving task and several secondary tasks of varying difficulty. From this data, an SVM based workload classifier was trained and evaluated. © 2010 IEEE.},
keywords={Biosignals;  Cognitive workloads;  Driving simulator;  Multimodal recognition;  Primary driving tasks;  Skin conductance, Automobile simulators, Pattern recognition},
references={Hart, L., Staveland, S., (1988) Human Mental Workload, pp. 139-183. , chapter Development of NASA-TLX (Task Load Index), Amsterdam: North Holland B.V; Healey, J., Picard, R., Detecting stress during real-world driving tasks using physiological sensors (2005) IEEE Transactions on Intelligent Transportation Systems, 6 (2), pp. 156-166; Honal, M., Schultz, T., Determine task demand from brain activity Proceedings of the 3rd International Conference on Bio-inspired Systems and Signal Processing, 2008; Hsu, C., Chang, C., Lin, C., (2003) A Practical Guide to Support Vector Classification; Kuhn, F., (2005) Methode Zur Bewertung der Fahrerablenkung Durch Fahrerinformations- Systeme, , DaimlerChrysler AG Research & Technology; Liang, Y., Reyes, M.L., Lee, J.D., Real-time detection of driver cognitive distraction using support vector machines (2007) IEEE Transactions on Intelligent Transportation Systems, 8 (2), pp. 340-350. , DOI 10.1109/TITS.2007.895298; Putze, F., Schultz, T., Cognitive dialog systems for dynamic environments: Progress and challenges Proceedings of the 4th Biennial Workshop on DSP for In-Vehicle Systems and Safety, 2009},
correspondence_address1={Putze, F.; Karlsruhe Institute of Technology (KIT), Cognitive Systems Lab. (CSL)Germany; email: felix.putze@kit.edu},
address={Istanbul},
issn={10514651},
isbn={9780769541099},
coden={PICRE},
language={English},
abbrev_source_title={Proc. Int. Conf. Pattern Recognit.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jin20104518,
author={Jin, Q. and Li, R. and Yang, Q. and Laskowski, K. and Schultz, T.},
title={Speaker identification with distant microphone speech},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2010},
pages={4518-4521},
doi={10.1109/ICASSP.2010.5495590},
art_number={5495590},
note={cited By 10; Conference of 2010 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2010 ; Conference Date: 14 March 2010 Through 19 March 2010;  Conference Code:81981},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049405703&doi=10.1109%2fICASSP.2010.5495590&partnerID=40&md5=1087b4aefa3ba04a40cfc927f5423f3e},
affiliation={Language Technologies Institute, Carnegie Mellon University, United States; Fakultät für Informatik, Universität Karlsruhe (TH), Karlsruhe, Germany},
abstract={The field of speaker identification has recently seen significant advancement, but improvements have tended to be benchmarked on near-field speech, ignoring the more realistic setting of far-field-instrumented speakers. In this work we present several findings on far-field speech from the MIXER5 Corpus, in the areas of feature extraction, speaker modeling, and multichannel score combination. First, we observe that minimum-variance distortionless response (MVDR) features outperform Mel-frequency cepstral coefficient (MFCC) features, and that fundamental frequency variation (FFV) features offer complimentary information to both MFCC and MVDR features. Second, we present evidence that factor analysis significantly improves system performance, compared to the more traditional GMM/UBM strategy. Third, we find that frame-based score competition significantly improves performance under mismatched conditions with multiple channels available. ©2010 IEEE.},
author_keywords={Distant speech;  Factor analysis;  Far-field speech;  Front-end features;  Speaker identification},
keywords={Distant speech;  Factor analysis;  Far-field;  Front-end features;  Speaker identification, Loudspeakers;  Signal processing;  Speech recognition, Feature extraction},
references={Doddington, G., Speaker recognition - Identifying people by their voices (1985) Proceedings of the IEEE, 73 (11), pp. 1651-1664; Kanak, A., Erzin, E., Yemez, Y., Tekalp, A., Joint audio-video processing for biometric speaker identification (2003) Proceedings of Multimedia and Expo. (ICME), pp. 561-564; Tranter, S.E., Reynolds, D.A., An overview of automatic speaker diarization systems (2006) IEEE Trans. on Audio, Speech, and Language Processing, 14 (5), pp. 1557-1565; Lee, C.H., Soong, F.K., Paliwal, K.K., (1996) Automatic Speech and Speaker Recognition: Advanced Topics, , Springer, ISBN:0792397061; Furui, S., Towards Robust Speech Recognition under Adverse Conditions (1992) ESCA Workshop on Speech Processing in Adverse Conditions, pp. 31-42; Bimbot, F., Bonastre, J.-F., Fredouille, C., Gravier, G., Magrin-Chagnolleau, I., Meignier, S., Merlin, T., Reynolds, D.A., A Tutorial on Text-Independent Speaker Verification (2004) Journal on Applied Signal Processing, 4, pp. 430-451; Brandschain, L., Cieri, C., Graff, D., Neely, A., Walker, K., Speaker Recognition: Building the Mixer 4 and 5 Corpora Proceedings of the LREC, 2008; Woefel, M., McDonough, J.W., Minimum variance distortionless response spectral estimation, review and refinements (2005) IEEE Signal Processing Magazine, 22 (5), pp. 117-126. , Sept; Wolfel, M., Yang, Q., Jin, Q., Schultz, T., Speaker Identification usingWarped MVDR Cepstral Features (2009) Nterspeech; Laskowski, K., Edlund, J., Heldner, M., An instantaneous vector representation of delta pitch for speaker-change prediction in conversational dialogue systems Proc. ICASSP, 2008; Laskowski, K., Jin, Q., Modeling Instantaneous Intonation for Speaker Identification Using the Fundamental Frequency Variation Spectrum (2009) Proceedings of ICASSP; Kenny, P., Boulianne, G., Ouellet, P., Dumouchel, P., Factor analysis simplified (2005) ICASSP; Bonastre, J.-F., Scheffer, N., Matrouf, D., Fredouille, C., Larcher, A., Preti, A., Pouchoulin, G., Mason, J., ALIZE/SpkDet: A state-of-the-art open source software for speaker recognition Proc. Odyssey, 2008; Jin, Q., Schultz, T., Waibel, A., Far-field Speaker Recognition (2007) IEEE Transactions on Audio, Speech, and Language Processing (TASL), 15 (7). , September; Reynolds, D., Quatieri, T., Dunn, R., Speaker verification using adapted Gaussian mixture models (2000) Digital Signal Processing, 10, pp. 19-41},
correspondence_address1={Jin, Q.; Language Technologies Institute, Carnegie Mellon UniversityUnited States},
sponsors={The Institute of Electrical and Electronics Engineers; Signal Processing Society},
address={Dallas, TX},
issn={15206149},
isbn={9781424442966},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Janke201022,
author={Janke, M. and Wand, M. and Schultz, T.},
title={A spectral mapping method for EMG-based recognition of silent speech},
journal={Proceedings of B-INTERFACE 2010 - 1st International Wokshop on Bio-inspired Human-Machine Interfaces and Healthcare Applications, in Conjunction with BIOSTEC 2010},
year={2010},
pages={22-31},
note={cited By 15; Conference of 1st International Wokshop on Bio-inspired Human-Machine Interfaces and Healthcare Applications, B-INTERFACE 2010, in Conjunction with BIOSTEC 2010 ; Conference Date: 21 January 2010 Through 22 January 2010;  Conference Code:81596},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956304341&partnerID=40&md5=4c83017bee64a001729ad8a386f8d946},
affiliation={Cognitive Systems Lab., Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={This paper reports on our latest study on speech recognition based on surface electromyography (EMG). This technology allows for Silent Speech Interfaces since EMG captures the electrical potentials of the human articulatory muscles rather than the acoustic speech signal. Therefore, our technology enables speech recognition to be applied to silently mouthed speech. Earlier experiments indicate that the EMG signal is greatly impacted by the mode of speaking. In this study we analyze and compare EMG signals from audible, whispered, and silent speech. We quantify the differences and develop a spectral mapping method to compensate for these differences. Finally, we apply the spectral mapping to the front-end of our speech recognition system and show that recognition rates on silent speech improve by up to 12.3% relative.},
keywords={Acoustic speech;  Electrical potential;  EMG signal;  Recognition rates;  Spectral mappings;  Spectral-mapping method;  Speech interface;  Speech recognition systems;  Surface electromyography, Man machine systems;  Photomapping, Speech recognition},
references={Sugie, N., Tsunoda, K., A speech prosthesis employing a speech synthesizer - Vowel discrimination from perioral muscle activities and vowel production (1985) IEEE Trans. Biomed. Eng., 32, pp. 485-490; Chan, A., Englehart, K., Hudgins, B., Lovely, D., Myoelectric signals to augment speech recognition (2001) Medical and Biological Engineering and Computing, 39, pp. 500-506; Jorgensen, C., Lee, D., Agabon, S., Sub auditory speech recognition based on EMG/EPG signals (2003) Proceedings of International Joint Conference on Neural Networks (IJCNN), pp. 3128-3133. , Portland, Oregon; Jou, S.C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proc. Interspeech, pp. 573-576. , Pittsburgh, PA; Schultz, T., Wand, M., Modeling coarticulation in large vocabulary EMG-based speech recognition (2009) Speech Communication Journal, , to appear; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 331-336. , San Juan, Puerto Rico; Wand, M., Jou, S.C.S., Toth, A.R., Schultz, T., Impact of different speaking modes on EMG-based speech recognition (2009) Proc. Interspeech.; Welch, P., The use of fast fourier transform for the estimation of power spectra: A method based on time averaging over short, modified periodograms (1967) Audio and Electroacoustics, IEEE Transactions on, 15, pp. 70-73},
correspondence_address1={Janke, M.; Cognitive Systems Lab., Karlsruhe Institute of Technology, Karlsruhe, Germany; email: matthias.janke@student.kit.edu},
address={Valencia},
isbn={9789896740207},
language={English},
abbrev_source_title={Proc. B-INTERFACE - Int. Wokshop Bio-inspired Hum.-Mach. Interfaces Healthc. Appl., Conjunction BIOSTEC},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Schultz2010595,
author={Schultz, T.},
title={ICCHP keynote: Recognizing silent and weak speech based on electromyography},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6179 LNCS},
number={PART 1},
pages={595-604},
doi={10.1007/978-3-642-14097-6_96},
note={cited By 2; Conference of 12th International Conference on Computers Helping People with Special Needs, ICCHP 2010 ; Conference Date: 14 July 2010 Through 16 July 2010;  Conference Code:81189},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954917016&doi=10.1007%2f978-3-642-14097-6_96&partnerID=40&md5=6e05cfbd9954a6d3439cddd62c2e1a87},
affiliation={Cognitive Systems Lab., Karlsruhe Institute of Technology, Karlsruhe, Germany},
abstract={In the past decade, the performance of automatic speech processing systems, including speech recognition, spoken language translation, and speech synthesis, has improved dramatically. This has resulted in an increasingly widespread use of speech and language technologies in a large variety of applications. However, speech-driven interfaces based on conventional acoustic speech signals still suffer from several limitations. © 2010 Springer-Verlag Berlin Heidelberg.},
keywords={Acoustic speech;  Automatic speech processing systems;  Language technology;  Spoken language translation, Computational linguistics;  Information theory;  Speech processing;  Speech synthesis;  Telecommunication equipment, Speech recognition},
references={Bartels, J.L., Andreasen, D., Ehirim, P., Mao, H., Seibert, S., Wright, E.J., Kennedy, P.R., Neurotrophic electrode: Method of assembly and implantation into human motor speech cortex (2008) Journal of Neuroscience Methods, 174 (2), pp. 168-176; Birbaumer, N., The thought translation device (ttd) for completely paralyzed patients (2000) IEEE Transactions on Rehabilitation Engineering, 8 (2), pp. 190-193; Blankertz, B., Dornhege, G., Krauledat, M., Müller, K.-R., Kunzmann, V., Losch, F., Curio, G., The berlin brain-computer interface: Eeg-based communication without subject training (2006) IEEE Transactions on Neural Systems and Rehabilitation Engineering, 14 (2), pp. 147-152; Brumberg, J.S., Nieto-Castanon, A., Kennedy, P.R., Guenther, F.H., Brain-Computer Interfaces for Speech Communication, , Speech Communication, Special Issue on Silent Speech Interfaces (April 2010) (in press); (2008), http://www.articulograph.de, Carstens: Carstens Medizinelektronik, November 6, 2008; Chan, A.D.C., (2003) Multi-expert automatic speech recognition system using myoelectric signals, , Ph.D. Dissertation, Department of Electrical and Computer Engineering, University of New Brunswick, Canada; DaSalla, C., Kambara, H., Sato, M., Koike, Y., (2008), Personal communication on EEG classification of vowel speech imagery using common spatial patterns; Denby, B., Stone, M., Speech synthesis from real time ultrasound images of the tongue (2004) Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2004), 1, pp. 1685-1688. , Montreal, Canada, May 17-21; Denby, B., Oussar, Y., Dreyfus, G., Stone, M., Prospects for a silent speech interface using ultrasound imaging (2006) IEEE ICASSP, pp. 1365-1368. , Toulouse, France; Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., (2010) Silent Speech Interfaces, , Speech Communication, Special Issue on Silent Speech Interfaces April, in press; (2007), Dornhege, G., del Millan, J.R., Hinterberger, T., McFarland, D., Muller, K.-R. (eds.): Towards brain-computer interfacing. MIT Press, Cambridge; Fagan, M.J., Ell, S.R., Gilbert, J.M., Sarrazin, E., Chapman, P.M., Development of a (silent) speech recognition system for patients following laryngectomy (2008) Medical Engineering and Physics, 30 (4), pp. 419-125; Guenther, F.H., Ghosh, S.S., Tourville, J.A., Neural modeling and imaging of the cortical interactions underlying syllable production (2007) Brain and Language, 96, pp. 280-301; Heracleous, P., Kaino, T., Saruwatari, H., Shikano, K., Unvoiced speech recognition using tissue-conductive acoustic sensor (2007) EURASIP Journal on Advances in Signal Processing, 2007 (1), pp. 1-11; Hochberg, L.R., Simeral, J.D., Kim, S., Stein, J., Friehs, G.M., Black, M.J., Donoghue, J.P., More than two years of intracortically-based cursor control via a neural interface system (2008) Neurosicence Meeting Planner 2008, , Program No. 673.15, Washington, DC; Hueber, T., Aversano, G., Chollet, G., Denby, B., Dreyfus, G., Oussar, Y., Roussel, P., Stone, M., Eigentongue feature extraction for an ultrasound-based silent speech interface (2007) IEEE ICASSP, 1, pp. 1245-1248. , Honolulu; Hueber, T., Benaroya, E.-L., Chollet, G., Denby, B., Dreyfus, G., Stone, M., Development of a silent speech interface driven by ultrasound and optical images of the tongue and lips (2010) Speech Communication, Special Issue on Silent Speech Interfaces, , April, in press; Hummel, J., Figl, M., Birkfellner, W., Bax, M.R., Shahidi, R., Maurer, C.R., Bergmann, H., Evaluation of a new electromagnetic tracking system using a standardized assessment protocol (2006) Physics in Medicine and Biology, 51, pp. N205-N210; Izzetoglu, K., Bunce, S., Onaral, B., Pourrezaei, K., Chance, B., Functional optical brain imaging using near-infrared during cognitive tasks (2004) International Journal of HCI, 17 (2), pp. 211-227; Jorgensen, C., Lee, D.D., Agabon, S., Sub auditory speech recognition based on emg signals (2003) Proceedings of the International Joint Conference on Neural Networks (IJCNN), pp. 3128-3133; Jorgensen, C., Binsted, K., Web browser control using emg based sub vocal speech recognition (2005) Proceedings of the 38th Annual Hawaii International Conference on System Sciences, pp. 294c1-294c8. , 8. IEEE, Los Alamitos; Jorgensen, C., Dusan, S., (2010) Speech interfaces based upon surface electromyography, , Speech Communication, Special Issue on Silent Speech Interfaces April, in press; Jou, S., Schultz, T., Walliczek, M., Kraft, F., Towards continuous speech recognition using surface electromyography (2006) INTERSPEECH 2006 and 9th International Conference on Spoken Language Processing, 2, pp. 573-576; Jou, S., Schultz, T., Waibel, A., Multi-stream articulatory feature classifiers for surface electromyographic continuous speech recognition (2007) Proceedings of International Conference on Acoustics, Speech, and Signal Processing, , IEEE, Honolulu; Kennedy, P.R., Bakay, R.A.E., Moore, M.M., Adams, K., Goldwaithe, J., Direct control of a computer from the human central nervous system (2000) IEEE Transactions on Rehabilitation Engineering, 8 (2), pp. 198-202; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 331-336. , San Juan, Puerto Rico; Manabe, H., Hiraiwa, A., Sugimura, T., Unvoiced speech recognition using emg-mime speech recognition (2003) Proceedings of CHI, Human Factors in Computing Systems, pp. 794-795. , Ft. Lauderdale, Florida; Manabe, H., Zhang, Z., Multi-stream hmm for emg-based speech recognition (2004) Proceedings of 26th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, 2, pp. 4389-4392. , San Francisco, California, September 1-5; Nakajima, Y., Kashioka, H., Shikano, K., Campbell, N., Non-audible murmur recognition input interface using stethoscopic microphone attached to the skin (2003) Proceedings of IEEE ICASSP, pp. 708-711; Nakajima, Y., Kashioka, H., Campbell, N., Shikano, K., Non-audible murmur (nam) recognition (2006) IEICE Transactions on Information and Systems, E89-D (1), pp. 1-8; Ng, L., Burnett, G., Holzrichter, J., Gable, T., Denoising of human speech using combined acoustic and em sensor signal processing (2000) Proc. Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 1, pp. 229-232. , Istanbul, Turkey, June 5-9; Porbadnigk, A., Wester, M., Calliess, J., Schultz, T., Eeg-based speech recognition - Impact of temporal effects (2009) Biosignals 2009, pp. 376-381. , Porto, Portugal, January; Quatieri, T.F., Messing, D., Brady, K., Campbell, W.B., Campbell, J.P., Brandstein, M., Weinstein, C.J., Gatewood, P.D., Exploiting nonacoustic sensors for speech enhancement (2006) IEEE Transactions on Audio, Speech, and Language Processing, 14 (2), pp. 533-544; Schönle, P.W., Gräbe, K., Wenig, P., Höhne, J., Schrader, J., Conrad, B., Electromagnetic articulography: Use of alternating magnetic fields for tracking movements of multiple points inside and outside the vocal tract (1987) Brain and Language, 31, pp. 26-35; Schultz, T., Wand, M., (2010) Modeling coarticulation in large vocabulary EMG-based speech recognition, , Speech Communication. Special Issue on Silent Speech Interfaces April, in press; Suppes, P., Lu, Z.-L., Han, B., Brain wave recognition of words (1997) Proceedings of the National Academy of Scientists of the USA, 94, pp. 14965-14969; (2004) MIT Lincoln Labs report ESC-TR-2004-084, , Tardelli, J.D. (ed.), Pilot Corpus for Multisensor Speech Processing; Titze, I.R., Story, B.H., Burnett, G.C., Holzrichter, J.F., Ng, L.C., Lea, W.A., Comparison between electroglottography and electromagnetic glottography (2000) Journal of the Acoustical Society of America, 107 (1), pp. 581-588; Tran, V.-A., Bailly, G., Loevenbruck, H., Toda, T., (2010) Improvement to a NAM-captured whisper-to-speech system, , Speech Communication, Special Issue on Silent Speech Interfaces April, in press; Walliczek, M., Kraft, F., Jou, S.-C., Schultz, T., Waibel, A., Sub-word unit based nonaudible speech recognition using surface electromyography (2006) Proceedings of Interspeech, pp. 1487-1490. , Pittsburgh, USA; Wand, M., Schultz, T., Towards speaker-adaptive speech recognition based on surface electromyography (2009) Proceedings of Biosignals, , Porto, Portugal, in press; Wester, M., Schultz, T., (2006) Unspoken speech - speech recognition based on electroencephalography, , Master's thesis, Universität Karlsruhe (TH), Karlsruhe, Germany; Wolpaw, J.R., Birbaumer, N., McFarland, D., Pfurtscheller, G., Vaughan, T., Braincomputer interfaces for communication and control (2002) Clinical Neurophysiology, 113 (6), pp. 767-791},
correspondence_address1={Schultz, T.; Cognitive Systems Lab., Karlsruhe Institute of Technology, Karlsruhe, Germany; email: tanja@ira.uka.de},
address={Vienna},
issn={03029743},
isbn={3642140963; 9783642140969},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Amma2010,
author={Amma, C. and Gehrig, D. and Schultz, T.},
title={Airwriting recognition using wearable motion sensors},
journal={ACM International Conference Proceeding Series},
year={2010},
doi={10.1145/1785455.1785465},
art_number={1785465},
note={cited By 11; Conference of 1st Augmented Human International Conference, AH'10 ; Conference Date: 2 April 2010 Through 3 April 2010;  Conference Code:80967},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954493926&doi=10.1145%2f1785455.1785465&partnerID=40&md5=9de036b171f93d510502d63ecfa764e9},
affiliation={Cognitive Systems Lab. (CSL), Karlsruhe Institute of Technology, Germany},
abstract={In this work we present a wearable input device which enables the user to input text into a computer. The text is written into the air via character gestures, like using an imaginary blackboard. To allow hands-free operation, we designed and implemented a data glove, equipped with three gyroscopes and three accelerometers to measure hand motion. Data is sent wirelessly to the computer via Bluetooth. We use HMMs for character recognition and concatenated character models for word recognition. As features we apply normalized raw sensor signals. Experiments on single character and word recognition are performed to evaluate the end-to-end system. On a character database with 10 writers, we achieve an average writer-dependent character recognition rate of 94.8% and a writer-independent character recognition rate of 81.9%. Based on a small vocabulary of 652 words, we achieve a single-writer word recognition rate of 97.5%, a performance we deem is advisable for many applications. The final system is integrated into an online word recognition demonstration system to showcase its applicability. © 2010 ACM.},
keywords={Character database;  Character models;  Data glove;  End-to-end systems;  Hand motion;  Hands-free;  Independent character;  Motion sensors;  Raw sensor;  Recognition rates;  Wearable input devices;  Word recognition, Character recognition;  Sensors, Vocabulary control},
references={Amft, O., Amstutz, R., Smailagic, A., Siewiorek, D., Tröster, G., Gesture-controlled user input to complete questionnaires on wrist-worn watches (2009) Lecture Notes in Computer Science, 5611, pp. 131-140. , Human-Computer Interaction. Novel Interaction Methods and Techniques, Springer Berlin / Heidelberg; Bang, W.-C., Chang, W., Kang, K.-H., Choi, E.-S., Potanin, A., Kim, D.-Y., Self-contained spatial input device for wearable computers (2003) Seventh IEEE International Symposium on Wearable Computers, 2003. Proceedings, pp. 26-34; Brashear, H., Starner, T., Lukowicz, P., Junker, H., Using multiple sensors for mobile sign language recognition (2003) Seventh IEEE International Symposium on Wearable Computers, 2003. Proceedings, pp. 45-52; Cho, S.-J., Kim, J., Bayesian network modeling of strokes and their relationships for on-line handwriting recognition (2004) Pattern Recognition, 37 (2), pp. 253-264; Hein, A., Hoffmeyer, A., Kirste, T., Utilizing an accelerometric bracelet for ubiquitous gesture-based interaction (2009) Lecture Notes in Computer Science, 5615, pp. 519-527. , Universal Access in Human-Computer Interaction. Intelligent and Ubiquitous Interaction Environments, Springer Berlin / Heidelberg; Hofmann, F., Heyer, P., Hommel, G., Velocity profile based recognition of dynamic gestures with discrete hidden markov models (1998) Lecture Notes in Computer Science, 1371, pp. 81-95. , Gesture and Sign Language in Human-Computer Interaction, Springer Berlin / Heidelberg; Kim, D., Choi, H., Kim, J., 3d space handwriting recognition with ligature model (2006) Lecture Notes in Computer Science, 4239, pp. 41-56. , Ubiquitous Computing Systems, Springer Berlin / Heidelberg, 2006; Kim, I.-C., Chien, S.-I., Analysis of 3d hand trajectory gestures using stroke-based composite hidden markov models (2001) Applied Intelligence, 15 (2), pp. 131-143; Liang, R.-H., Ouhyoung, M., A real-time continuous gesture recognition system for sign language (1998) Third IEEE International Conference on Automatic Face and Gesture Recognition, 1998. Proceedings, pp. 558-567; McGuire, R., Hernandez-Rebollar, J., Starner, T., Henderson, V., Brashear, H., Ross, D., Towards a one-way american sign language translator (2004) Sixth IEEE International Conference on Automatic Face and Gesture Recognition, 2004. Proceedings, pp. 620-625; Oh, J., Cho, S.-J., Bang, W.-C., Chang, W., Choi, E., Yang, J., Cho, J., Kim, D., Inertial sensor based recognition of 3-d character gestures with an ensemble classifiers (2004) Ninth International Workshop on Frontiers in Handwriting Recognition, 2004. IWFHR-9 2004, pp. 112-117; Plamondon, R., Srihari, S., Online and off-line handwriting recognition: A comprehensive survey (2000) IEEE Transactions on Pattern Analysis and Machine Intelligence, 22 (1), pp. 63-84; Rabiner, L., A tutorial on hidden markov models and selected applications in speech recognition (1989) Proceedings of the IEEE, pp. 257-286; Rehm, M., Bee, N., André, E., Wave like an egyptian: Accelerometer based gesture recognition for culture specific interactions (2008) BCS-HCI '08: Proceedings of the 22nd British HCI Group Annual Conference on HCI 2008, pp. 13-22. , Swinton, UK, UK, British Computer Society; Wu, J., Pan, G., Zhang, D., Qi, G., Li, S., Gesture recognition with a 3-d accelerometer (2009) Lecture Notes in Computer Science, 5585, pp. 25-38. , Ubiquitous Intelligence and Computing, Springer Berlin / Heidelberg},
correspondence_address1={Amma, C.; Cognitive Systems Lab. (CSL), Karlsruhe Institute of TechnologyGermany; email: christoph.amma@kit.edu},
sponsors={Sporaltec},
address={Megeve},
isbn={9781605588254},
language={English},
abbrev_source_title={ACM Int. Conf. Proc. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wand2010271,
author={Wand, M. and Schultz, T.},
title={Speaker-adaptive speech recognition based on surface electromyography},
journal={Communications in Computer and Information Science},
year={2010},
volume={52},
pages={271-285},
doi={10.1007/978-3-642-11721-3_21},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950365973&doi=10.1007%2f978-3-642-11721-3_21&partnerID=40&md5=4eeeeabba54907fee5979734c8180cf1},
affiliation={Universität Karlsruhe (TH), Germany},
abstract={We present our recent advances in silent speech interfaces using electromyographic signals that capture the movements of the human articulatory muscles at the skin surface for recognizing continuously spoken speech. Previous systems were limited to speaker- and session-dependent recognition tasks on small amounts of training and test data. In this article we present speaker-independent and speaker-adaptive training methods which allow us to use a large corpus of data from many speakers to train acoustic models more reliably. We use the speaker-dependent system as baseline, carefully tuning the data preprocessing and acoustic modeling. Then on our corpus we compare the performance of speaker-dependent and speaker-independent acoustic models and carry out model adaptation experiments. © 2010 Springer-Verlag Berlin Heidelberg.},
keywords={Acoustic model;  Acoustic modeling;  Adaptive training;  Data preprocessing;  Electromyographic signal;  Model Adaptation;  Skin surfaces;  Speech interface;  Surface electromyography;  Test data, Biomedical engineering;  Feature extraction, Speech recognition},
references={Jou, S.-C., Schultz, T., Waibel, A., Whispery speech recognition using adapted articulatory features (2005) Proc. ICASSP; Nakajima, Y., Kashioka, H., Shikano, K., Campbell, N., Non-audible murmur recognition (2003) Proc. Eurospeech; Hueber, T., Chollet, G., Denby, B., Dreyfus, G., Stonem, Continuous-speech phone recognition from ultrasound and optical images of the tongue and lips (2007) Proc. Interspeech, pp. 658-661; Jorgensen, C., Binsted, K., Web browser control using emg based sub vocal speech recognition (2005) Proceedings of the 38th Hawaii International Conference on System Sciences; Chan, A., Englehart, K., Hudgins, B., Lovely, D., Hidden markov model classification of myolectric signals in speech (2002) IEEE Engineering in Medicine and Biology Magazine, 21 (9), pp. 143-146; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proc. Interspeech, , Pittsburgh PA September; Wand, M., Stan Jou, S.-C., Schultz, T., Wavelet-based front-end for electromyographic speech recognition (2007) Proc. Interspeech; Jou, S.-C., Maier-Hein, L., Schultz, T., Waibel, A., Articulatory feature classification using surface electromyography (2006) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2006), pp. 15-19. , Toulouse, France, May; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) Proc. ASRU; Dietrich, M., (2008) The Effects of Stress Reactivity on Extralaryngeal Muscle Tension in Vocally Normal Participants As A Function of Personality, , PhD thesis, University of Pittsburgh; Yu, H., Waibel, A., Streamlining the front end of a speech recognizer (2000) Proc. ICSLP; Walliczek, M., Kraft, F., Jou, S.-C., Schultz, T., Waibel, A., Sub-word unit based non- audible speech recognition using surface electromyography (2006) Proc. Interspeech, , Pittsburgh PA September; Kirchhoff, K., (1999) Robust Speech Recognition Using Articulatory Information, , PhD thesis, University of Bielefeld; Metze, F., (2005) Articulatory Features for Conversational Speech Recognition, , PhD thesis, University of Karlsruhe; Metze, F., Waibel, A., A flexible stream architecture for ASR using articulatory features (2002) Proc. ICSLP, , September; Stan Jou, S.-C., Schultz, T., Automatic speech recognition based on electromyographic biosignals, page accepted for publication (2009) Communications in Computer and Information Science (CCIS), BIOSTEC - BIOSIGNALS 2008 Best Papers, pp. 305-320. , Springer, Heidelberg; Jou, S.-C.S., Schultz, T., Waibel, A., Continuous electromyographic speech recognition with a multi-stream decoding architecture (2007) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2007), , Honolulu, Hawaii, US, April 15-20; Frankel, J., Wester, M., King, S., Articulatory feature recognition using dynamic bayesian networks (2004) Proc. ICSLP; Schultz, T., Wand, M., Modeling coarticulation in large vocabulary EMG-based speech recognition (2009) Speech Communication Journal, , to appear; Bahl, L.R., De Souza, P.V., Gopalakrishnan, P.S., Nahmoo, D., Picheny, M.A., Decision trees for phonological rules in continuous speech (1991) Proc. ICASSP; Leggetter, C.J., Woodland, P.C., Maximum likelihood linear regression for speaker adaptation of continuous density hidden markov models (1995) Computer Speech and Language, 9, pp. 171-185},
correspondence_address1={Wand, M.; Universität Karlsruhe (TH)Germany; email: mwand@ira.uka.de},
editor={Fred A, Gamboa H, Filipe J},
issn={18650929},
isbn={9783642117206},
language={English},
abbrev_source_title={Commun. Comput. Info. Sci.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Schultz2010341,
author={Schultz, T. and Wand, M.},
title={Modeling coarticulation in EMG-based continuous speech recognition},
journal={Speech Communication},
year={2010},
volume={52},
number={4},
pages={341-353},
doi={10.1016/j.specom.2009.12.002},
note={cited By 104},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-76849099234&doi=10.1016%2fj.specom.2009.12.002&partnerID=40&md5=b98d451f34f369e6dba9b1c1bb24a905},
affiliation={Karlsruhe Institute of Technology, Cognitive Systems Laboratory, Adenauerring 4, 76131 Karlsruhe, Germany},
abstract={This paper discusses the use of surface electromyography for automatic speech recognition. Electromyographic signals captured at the facial muscles record the activity of the human articulatory apparatus and thus allow to trace back a speech signal even if it is spoken silently. Since speech is captured before it gets airborne, the resulting signal is not masked by ambient noise. The resulting Silent Speech Interface has the potential to overcome major limitations of conventional speech-driven interfaces: it is not prone to any environmental noise, allows to silently transmit confidential information, and does not disturb bystanders. We describe our new approach of phonetic feature bundling for modeling coarticulation in EMG-based speech recognition and report results on the EMG-PIT corpus, a multiple speaker large vocabulary database of silent and audible EMG speech recordings, which we recently collected. Our results on speaker-dependent and speaker-independent setups show that modeling the interdependence of phonetic features reduces the word error rate of the baseline system by over 33% relative. Our final system achieves 10% word error rate for the best-recognized speaker on a 101-word vocabulary task, bringing EMG-based speech recognition within a useful range for the application of Silent Speech Interfaces. © 2009 Elsevier B.V. All rights reserved.},
author_keywords={EMG-based speech recognition;  Phonetic features;  Silent Speech Interfaces},
keywords={Ambient noise;  Automatic speech recognition;  Baseline systems;  Co-articulation;  Confidential information;  Electromyographic signal;  Environmental noise;  Facial muscles;  Large vocabulary;  New approaches;  Phonetic features;  Speech interface;  Speech recording;  Speech signals;  Surface electromyography;  Word error rate, Continuous speech recognition;  Feature extraction;  Speech processing, Linguistics},
funding_details={University of PittsburghUniversity of Pittsburgh, Pitt},
funding_text 1={The authors would like to thank Szu-Chen (Stan) Jou for his in-depth support with the initial recognition system, his help with the EMG-PIT collection and the data scripts. We also thank Maria Dietrich for recruiting all subjects and carrying out major parts of the database collection. Her study was supported in part through funding received from the SHRS Research Development Fund, School of Health and Rehabilitation Sciences, University of Pittsburgh.},
references={Bahl, L.R., de Souza, P.V., Gopalakrishnan, P.S., Nahamoo, D., Picheny, M.A., Decision trees for phonological rules in continuous speech (1991) Proc. IEEE Internat. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), pp. 185-188. , Toronto, Ontario, Canada, pp; Beyerlein, P., (2000) Diskriminative Modellkombination in Spracherkennungssystemen mit großem Wortschatz, , Ph.D. Thesis. RWTH Aachen; Chan, A., Englehart, K., Hudgins, B., Lovely, D., Myoelectric signals to augment speech recognition (2001) Med. Biological Eng. Comput., 39, pp. 500-506; Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J., Silent speech interfaces (2010) Speech Comm., 52 (4), pp. 270-287; Dietrich, M., (2008) The Effects of Stress Reactivity on Extralaryngeal Muscle Tension in Vocally Normal Participants as a Function of Personality, , Ph.D. Thesis. University of Pittsburgh; Frankel, J., Wester, M., King, S., Articulatory feature recognition using dynamic Bayesian networks (2004) Proc. Internat. Conf. on Spoken Language Processing (ICSLP), pp. 1202-1205. , Jeju Island, Korea, pp; International Phonetic Association, (1999) Handbook of the International Phonetic Association, , Cambridge University Press; Jorgensen, C., Lee, D., Agabon, S., Sub auditory speech recognition based on EMG/EPG signals (2003) Proc. Internat. Joint Conf. on Neural Networks (IJCNN), pp. 3128-3133. , Portland, Oregon, pp; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proc. Interspeech, pp. 573-576. , Pittsburgh, PA, pp; Jou, S.-C.S., Schultz, T., Waibel, A., Continuous electromyographic speech recognition with a multi-stream decoding architecture (2007) Proc. IEEE Internat. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), pp. 401-404. , Honolulu, Hawaii, pp; Kirchhoff, K., (1999) Robust Speech Recognition using Articulatory Information, , Ph.D. Thesis. University of Bielefeld; Leveau, B., Andersson, G., Output forms: Data analysis and applications (1992) Selected Topics in Surface Electromyography for Use in the Occupational Setting: Expert Perspective; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 331-336. , San Juan, Puerto Rico, pp; Metze, F., (2005) Articulatory Features for Conversational Speech Recognition, , Ph.D. Thesis. University of Karlsruhe; Metze, F., Waibel, A., A flexible stream architecture for ASR using articulatory features (2002) Proc. Internat. Conf. on Spoken Language Processing (ICSLP), pp. 2133-2136. , Denver, Colorado, USA, pp; Morse, M.S., Day, S.H., Trull, B., Morse, H., Use of myoelectric signals to recognize speech (1989) Proc. 11th Annual Conf. of the IEEE, pp. 1793-1794. , Engineering in Medicine and Biology Society, pp; Morse, M.S., Gopalan, Y.N., Wright, M., Speech recognition using myoelectric signals with neural networks (1991) Proc. 13th Annual Conf. of the IEEE, pp. 1877-1878. , Engineering in Medicine and Biology Society, pp; Morse, M.S., O'Brien, E.M., Research summary of a scheme to ascertain the availability of speech information in the myoelectric signals of neck and head muscles using surface electrodes (1986) Comput. Biological Med., 16 (6), pp. 399-410; Schünke, M., Schulte, E., Schumacher, U., Prometheus - Lernatlas der Anatomie (2006) Kopf und Neuroanatomie, 3. , Thieme Verlag, Stuttgart, New York; Sugie, N., Tsunoda, K., A speech prosthesis employing a speech synthesizer - vowel discrimination from perioral muscle activities and vowel production (1985) IEEE Trans. Biomed. Eng., 32 (7), pp. 485-490; Ueda, N., Nakano, R., Ghahramani, Z., Hinton, G.E., Split and merge EM algorithm for improving gaussian mixture density estimates (2000) J. VLSI Signal Process., 26, pp. 133-140; Walliczek, M., Kraft, F., Jou, S.-C., Schultz, T., Waibel, A., Sub-word unit based non-audible speech recognition using surface electromyography (2006) Proc. Interspeech, pp. 1487-1490. , Pittsburgh, PA, pp; Wand, M., Schultz, T., in press. Speaker-adaptive speech recognition based on surface electromyography. In: BIOSTEC - BIOSIGNALS 2009 Best Papers. Communications in Computer and Information Science (CCIS) Series. Springer, Heidelberg; Wand, M., Schultz, T., Towards speaker-adaptive speech recognition based on surface electromyography (2009) Proc. Biosignals, pp. 155-162. , Porto, Portugal, pp; Wand, M., Toth, A., Jou, S.-C., Schultz, T., Impact of different speaking modes on EMG-based speech recognition (2009) Proc. Interspeech, , Brighton, United Kingdom; Yu, H., Schultz, T., Enhanced tree clustering with single pronunciation dictionary for conversational speech recognition (2003) Proc. Eurospeech, pp. 1869-1872. , Geneva, Switzerland, pp; Yu, H., Waibel, A., Streamlining the front end of a speech recognizer (2000) Proc. Internat. Conf. on Spoken Language Processing (ICSLP), pp. 353-356. , Beijing, China, pp},
correspondence_address1={Schultz, T.; Karlsruhe Institute of Technology, Cognitive Systems Laboratory, Adenauerring 4, 76131 Karlsruhe, Germany; email: tanja.schultz@kit.edu},
issn={01676393},
coden={SCOMD},
language={English},
abbrev_source_title={Speech Commun},
document_type={Article},
source={Scopus},
}

@ARTICLE{Denby2010270,
author={Denby, B. and Schultz, T. and Honda, K. and Hueber, T. and Gilbert, J.M. and Brumberg, J.S.},
title={Silent speech interfaces},
journal={Speech Communication},
year={2010},
volume={52},
number={4},
pages={270-287},
doi={10.1016/j.specom.2009.08.002},
note={cited By 234},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-76849116340&doi=10.1016%2fj.specom.2009.08.002&partnerID=40&md5=7c9c9b2e1e40b01add18e37ad397c3d1},
affiliation={Université Pierre et Marie Curie - Paris VI, 4 place Jussieu, 75005 Paris, France; Cognitive Systems Laboratory, Universität Karlsruhe, Am Fasanengarten 5, 76131 Karlsruhe, Germany; ATR Cognitive Information Science Laboratories, 2-2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0288, Japan; Laboratoire d'Electronique de l'ESPCI-ParisTech, 10 rue Vauquelin, 75005 Paris, France; Department of Engineering, University of Hull, Hull, HU6 7RX, United Kingdom; Department of Cognitive and Neural Systems, Boston University, 677 Beacon Street, Boston, MA 02215, United States},
abstract={The possibility of speech processing in the absence of an intelligible acoustic signal has given rise to the idea of a 'silent speech' interface, to be used as an aid for the speech-handicapped, or as part of a communications system operating in silence-required or high-background-noise environments. The article first outlines the emergence of the silent speech interface from the fields of speech production, automatic speech processing, speech pathology research, and telecommunications privacy issues, and then follows with a presentation of demonstrator systems based on seven different types of technologies. A concluding section underlining some of the common challenges faced by silent speech interface researchers, and ideas for possible future directions, is also provided. © 2009 Elsevier B.V. All rights reserved.},
author_keywords={Cellular telephones;  Silent speech;  Speech pathologies;  Speech recognition;  Speech synthesis},
keywords={Acoustic signals;  Automatic speech processing;  Cellular telephones;  Communications systems;  Demonstrator systems;  Noise environments;  Possible futures;  Privacy issue;  Speech interface;  Speech pathologies;  Speech pathology;  Speech production, Cellular telephone systems;  Industrial research;  Pathology;  Speech processing;  Speech synthesis;  Telephone;  Telephone sets, Speech recognition},
funding_details={Agence Nationale de la RechercheAgence Nationale de la Recherche, ANR, ANR-06-BLAN-0166},
funding_details={National Science FoundationNational Science Foundation, NSF, SBE-0354378},
funding_details={DÃ©lÃ©gation GÃ©nÃ©rale pour l'ArmementDÃ©lÃ©gation GÃ©nÃ©rale pour l'Armement, DGA},
funding_details={Centre de CompÃ©tences Nanosciences Ile-de-FranceCentre de CompÃ©tences Nanosciences Ile-de-France},
funding_details={National Institute on Deafness and Other Communication DisordersNational Institute on Deafness and Other Communication Disorders, NIDCD, R44 DC007050-02, R01 DC07683},
funding_text 1={The authors acknowledge support from the French Department of Defense (DGA) ; the “ Centre de Microélectronique de Paris Ile-de-France ” (CEMIP); the French National Research Agency (ANR) under the contract number ANR-06-BLAN-0166 ; the ENT Consultants’ Fund, Hull and East Yorkshire Hospitals NHS Trust ; the National Institute on Deafness and other Communication Disorders ( R01 DC07683 ; R44 DC007050-02 ); and the National Science Foundation ( SBE-0354378 ). They also wish to thank Gérard Chollet; Maureen Stone; Laurent Benaroya; Gérard Dreyfus; Pierre Roussel; and Szu-Chen (Stan) Jou for their help in preparing this article.},
references={Arnal, A., Badin, P., Brock, G., Connan, P.-Y., Florig, E., Perez, N., Perrier, P., Zerling, J.-P., (2000) Une base de données cinéradiographiques du français, XXIIIèmes Journées d'Etude sur la Parole, pp. 425-428. , Aussois, France, pp; Baken, R.J., Robbins, J., Fisher, H., Blom, E., Singer, M., A comparative acoustic study of normal, esophageal and tracheoesophageal speech production (1984) J. Speech Hearing Disorders, 49, pp. 202-210; Bartels, J.L., Andreasen, D., Ehirim, P., Mao, H., Seibert, S., Wright, E.J., Kennedy, P.R., Neurotrophic electrode: method of assembly and implantation into human motor speech cortex (2008) J. Neurosci. Methods, 174 (2), pp. 168-176; Betts, B.J., Binsted, K., Jorgensen, C., Small-vocabulary speech recognition using surface electromyography (2006) Interact. Comput.: Interdisciplinary J. Human-Comput. Interact., 18, pp. 1242-1259; Birbaumer, N., The thought translation device (TTD) for completely paralyzed patients (2000) IEEE Trans. Rehabil. Eng., 8 (2), pp. 190-193; Blankertz, B., Dornhege, G., Krauledat, M., Müller, K.-R., Kunzmann, V., Losch, F., Curio, G., The Berlin brain-computer interface: EEG-based communication without subject training (2006) IEEE Trans. Neural Systems Rehabil. Eng., 14 (2), pp. 147-152; Blom, E.D., Singer, M.I., Surgical prosthetic approaches for postlaryngectomy voice restoration (1979) Laryngectomy Rehabilitation, pp. 251-276. , Keith R.L., and Darley F.C. (Eds), Texas College Hill Press, Houston; Bos, J.C., Tack, D.W., 2005. Speech Input Hardware Investigation for Future Dismounted Soldier Computer Systems, DRCD Toronto CR 2005-064; Brown, D.R., Ludwig, R., Pelteku, A., Bogdanov, G., Keenaghan, K., A novel non-acoustic voiced speech sensor (2004) Meas. Sci. Technol., 15, pp. 1291-1302; Brown, D.R., Keenaghan, K., Desimini, S., Measuring glottal activity during voiced speech using a tuned electromagnetic resonating collar sensor (2005) Meas. Sci. Technol., 16, pp. 2381-2390; Brumberg, J.S., Andreasen, D.S., Bartels, J.L., Guenther, F.H., Kennedy, P.R., Siebert, S.A., Schwartz, A.B., Wright, E.J., Human speech cortex long-term recordings: Formant frequency analyses (2007) Neuroscience Meeting Planner, , Program No. 517.17, San Diego, USA; Brumberg, J.S., Nieto-Castanon, A., Guenther, F.H., Bartels, J.L., Wright, E.J., Siebert, S.A., Andreasen, D.S., Kennedy, P.R., Methods for construction of a long-term human brain machine interface with the Neurotrophic Electrode (2007) Neuroscience Meeting Planner, , Program No. 779.5, Washington, DC; Brumberg, J.S., Nieto-Castanon, A., Kennedy, P.R., Guenther, F.H., Brain-computer interfaces for speech communication (2010) Speech Comm., 52 (4), pp. 367-379; Burnett, G.C., Gable, T.G., Holzrichter, J.F., Ng, L.C., Voiced excitation functions calculated from micro-power impulse radar information (1997) J. Acoust. Soc. Amer., 102, pp. 3168A; Carstens Medizinelektronik, 2008. %3chttp://www.articulograph.de/%3e; Chan, A.D.C., Englehart, K., Hudgins, B., Lovely, D.F., Myo-electric signals to augment speech recognition (2001) Med. Biological Eng. Comput., 39, pp. 500-504; Chan, A.D.C., (2003) Multi-expert automatic speech recognition system using myoelectric signals, , Ph.D. Dissertation, Department of Electrical and Computer Engineering, University of New Brunswick, Canada; Crevier-Buchman, L., Laryngectomy patients and the psychological aspects of their tracheostomy (2002) Rev. Laryngol. Otolaryngol. Rhinol., 123, pp. 137-139; Davidson, L., Comparing tongue shapes from ultrasound imaging using smoothing spline analysis of variance (2005) J. Acoust. Soc. Amer., 120 (1), pp. 407-415; DaSalla, C.S., Kambara, H., Sato, M., Koike, Y., Spatial filtering and single-trial classification of EEG during vowel speech imagery (2009) Proceedings of the 3rd International Convention on Rehabilitation Engineering & Assistive Technology, Singapore. Article, 27. , ACM, New York, NY, USA. ISBN:978-1-60558-792-9; Dekens, T., Patsis, Y., Verhelst, W., Beaugendre, F., Capman, F., A multi-sensor speech database with applications towards robust speech processing in hostile environments (2008) Proc. 6th Internat. Language Resources and Evaluation (LREC'08), , European Language Resources Association ELRA, Marrakech, Morocco, 28-30 May; Denby, B., Stone, M., Speech synthesis from real time ultrasound images of the tongue (2004) Proc. IEEE Internat. Conf. on Acoustics, Speech, and Signal Processing, (ICASSP'04), 1, pp. I685-I688. , Montréal, Canada, 17-21 May; Denby, B., Oussar, Y., Dreyfus, G., Stone, M., (2006) Prospects for a Silent Speech Interface Using Ultrasound Imaging, , IEEE ICASSP, Toulouse, France pp. I365-I368; (2007) Towards Brain-Computer Interfacing, , Dornhege G., del R. Millan J., Hinterberger T., McFarland D., and Müller K.-R. (Eds), MIT Press; Drummond, S., Dancer, J., Krueger, K., Spring, G., Perceptual and acoustical analysis of alaryngeal speech: determinants of intelligibility (1996) Percept. Motor Skills, 83, pp. 801-802; Dupont, S., Ris, C., Combined use of close-talk and throat microphones for improved speech recognition under non-stationary background noise (2004) Proc. Robust 2004, Workshop (ITRW) on Robustness Issues in Conversational Interaction, , Norwich, UK, August; Epstein, C.M., (1983) Introduction to EEG and evoked potentials, , J.B. Lippincot Co; Epstein, M.A., Ultrasound and the IRB (2005) Clin. Linguist. Phonet., 16 (6), pp. 567-572; Fagan, M.J., Ell, S.R., Gilbert, J.M., Sarrazin, E., Chapman, P.M., Development of a (silent) speech recognition system for patients following laryngectomy (2008) Med. Eng. Phys., 30 (4), pp. 419-425; Fitzpatrick, M., 2002. Lip-reading cellphone silences loudmouths, New Scientist, edition of 03 April 2002; Furui, S., (2001) Digital Speech Processing Synthesis and Recognition. second ed., , Marcel Dekker; Georgopoulos, A.P., Kalaska, J.F., Caminiti, R., Massey, J.T., On the relations between the direction of two-dimensional arm movements and cell discharge in primate motor cortex (1982) J. Neurosci., 2 (11), pp. 1527-1537; Gibbon, F., Bibliography of Electropalatographic (EPG) Studies in English (1957-2005), Queen Margaret University, Edinburgh, UK (2005) September 2005. %3chttp, , www.qmu.ac.uk/ssrc/cleftnet/EPG_biblio_2005_september.pdf%3e; Gracco, V.L., Tremblay, P., Pike, B., Imaging speech production using fMRI (2005) NeuroImage, 26 (1), pp. 294-301. , 15 May; Guenther, F.H., Brumberg, J.S., Nieto-Castanon, A., Bartels, J.L., Siebert, S.A., Wright, E.J., Tourville, J.A., Kennedy, P.R., A brain-computer interface for real-time speech synthesis by a locked-in individual implanted with a Neurotrophic Electrode (2008) Neuroscience Meeting Planner, , Program No. 712.1, Washington, DC; Hasegawa, T., Ohtani, K., Oral image to voice converter, image input microphone (1992) Proc. IEEE ICCS/ISITA, 20 (1), pp. 617-620. , Singapore; Hasegawa-Johnson, M., (2008), Private communication; Heracleous, P., Kaino, T., Saruwatari, H., Shikano, K., Unvoiced speech recognition using tissue-conductive acoustic sensor (2007) EURASIP J. Adv. Signal Process., 2007 (1), pp. 1-11; Hirahara, T., Otani, M., Shimizu, S., Toda, M., Nakamura, K., Nakajima, Y., Shikano, K., Silent-speech enhancement system utilizing body-conducted vocal-tract resonance signals (2010) Speech Comm., 52 (4), pp. 301-313; Hochberg, L.R., Serruya, M.D., Friehs, G.M., Mukand, J.A., Saleh, M., Caplan, A.H., Branner, A., Donoghue, J.P., Neuronal ensemble control of prosthetic devices by a human with tetraplegia (2006) Nature, 442 (7099), pp. 164-171; Hochberg, L.R., Simeral, J.D., Kim, S., Stein, J., Friehs, G.M., Black, M.J., Donoghue, J.P., More than two years of intracortically-based cursor control via a neural interface system (2008) Neurosicence Meeting Planner, , Program No. 673.15, Washington, DC; Holmes, J., Holmes, W., (2001) Speech Synthesis and Recognition, , Taylor and Francis; Hoole, P., Nguyen, N., Electromagnetic articulography in coarticulation research (1999) Coarticulation: Theory, Data and Techniques, pp. 260-269. , Hardcastle W.H., and Hewlitt N. (Eds), Cambridge University Press; House, D., Granström, B., Multimodal speech synthesis: Improving information flow in dialogue systems using 3D talking heads (2002) Lecture Notes in Computer Science, 2443, pp. 65-84. , Artificial Intelligence: Methodology, Systems, and Applications., Springer, Berlin/Heidelberg, pp; Hueber, T., Aversano, G., Chollet, G., Denby, B., Dreyfus, G., Oussar, Y., Roussel, P., Stone, M., Eigentongue feature extraction for an ultrasound-based silent speech interface (2007) IEEE Internat. Conf. on Acoustic, Speech, and Signal Processing, ICASSP07, 1, pp. 1245-1248. , Honolulu; Hueber, T., Chollet, G., Denby, B., Stone, M., Zouari, L., Ouisper: Corpus based synthesis driven by articulatory data (2007) Internat. Congress of Phonetic Sciences, pp. 2193-2196. , Saarbrücken, Germany, pp; Hueber, T., Chollet, G., Denby, B., Dreyfus, G., Stone, M., Continuous-speech Phone Recognition from Ultrasound and Optical Images of the Tongue and Lips (2007) Interspeech, pp. 658-661. , Antwerp, Belgium, pp; Hueber, T., Chollet, G., Denby, B., Dreyfus, G., Stone, M., Phone Recognition from Ultrasound and Optical Video Sequences for a Silent Speech Interface (2008) Interspeech, pp. 2032-2035. , Brisbane, Australia, pp; Hueber, T., Chollet, G., Denby, B., Stone, M., 2008b. Acquisition of ultrasound, video and acoustic speech data for a silent-speech interface application. In: Internat. Seminar on Speech Production, Strasbourg, France, pp. 365-369; Hueber, T., Benaroya, E.-L., Chollet, G., Denby, B., Dreyfus, G., Stone, M., Development of a silent speech interface driven by ultrasound and optical images of the tongue and lips (2010) Speech Comm., 52 (4), pp. 288-300; Hummel, J., Figl, M., Birkfellner, W., Bax, M.R., Shahidi, R., Maurer, C.R., Bergmann, H., Evaluation of a new electromagnetic tracking system using a standardized assessment protocol (2006) Phys. Med. Biol., 51, pp. N205-N210; Brain Computer Interfaces (2008) IEEE Computer, 41 (10). , IEEE, October; Jorgensen, C., Lee, D.D., Agabon, S., Sub auditory speech recognition based on EMG signals (2003) Proc. Internat. Joint Conf. on Neural Networks (IJCNN), pp. 3128-3133; Jorgensen, C., Binsted, K., 2005. Web browser control using EMG based sub vocal speech recognition. In: Proc. 38th Annual Hawaii Internat. Conf. on System Sciences. IEEE, pp. 294c.1-294c.8; Jorgensen, C., Dusan, S., Speech interfaces based upon surface electromyography (2010) Speech Comm., 52 (4), pp. 354-366; Jou, S., Schultz, T., Walliczek, M., Kraft, F., Towards continuous speech recognition using surface electromyography (2006) INTERSPEECH 2006 and 9th Internat. Conf. on Spoken Language Processing, 2, pp. 573-576; Jou, S., Schultz, T., Waibel, A., Multi-stream articulatory feature classifiers for surface electromyographic continuous speech recognition (2007) Internat. Conf. on Acoustics, Speech, and Signal Processing. IEEE, , Honolulu, Hawaii; Kennedy, P.R., The cone electrode: a long-term electrode that records from neurites grown onto its recording surface (1989) J. Neurosci. Methods, 29, pp. 181-193; Kennedy, P.R., Comparing electrodes for use as cortical control signals: Tiny tines, tiny wires or tiny cones on wires: which is best? (2006) The Electrical Engineering Handbook Series, 1. , The Biomedical Engineering Handbook, third ed, CRS/Taylor and Francis, Boca Raton; Kennedy, P.R., Bakay, R.A.E., Moore, M.M., Adams, K., Goldwaithe, J., Direct control of a computer from the human central nervous system (2000) IEEE Trans. Rehabil. Eng., 8 (2), pp. 198-202; Kennedy, P.R., Bakay, R.A.E., Restoration of neural output from a paralyzed patient by direct brain connection (1998) NeuroReport, 9, pp. 1707-1711; Kim, S., Simeral, J.D., Hochberg, L.R., Donoghue, J.P., Friehs, G.M., Black, M.J., Multi-state decoding of point-and-click control signals from motor cortical activity in a human with tetraplegia (2007) Neural Engineering, 2007, CNE'07 3rd Internat. IEEE/EMBS Conf, pp. 486-489; Levinson, S.E., (2005) Mathematical Models for Speech Technology, , John Wiley; Lotte, F., Congedo, M., Lecuyer, A., Lamarche, F., Arnaldi, B., A review of classification algorithms for EEG-based brain computer interfaces (2007) J. Neural Eng., 4, pp. R1-R13; Maeda, S., Compensatory articulation during speech: Evidence from the analysis and synthesis of vocal-tract shapes using an articulatory model (1990) Speech Production and Speech Modelling, pp. 131-149. , Hardcastle W., and Marchal A. (Eds), Kluwer Academic Publisher, Amsterdam; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 331-336. , San Juan, Puerto Rico, pp; Manabe, H., Hiraiwa, A., Sugimura, T., Unvoiced speech recognition using EMG-mime speech recognition (2003) Proc. CHI, Human Factors in Computing Systems, pp. 794-795. , Ft. Lauderdale, Florida, pp; Manabe, H., Zhang, Z., Multi-stream HMM for EMG-based speech recognition (2004) Proc. 26th Annual International Conf. of the IEEE Engineering in Medicine and Biology Society, 2, pp. 4389-4392. , 1-5 September, San Francisco, California; Marchal, A., Hardcastle, W.J., Instrumentation and database for the cross-language study of coarticulation (1993) Lang. Speech, 36 (1), pp. 3-20; Maynard, E.M., Nordhausen, C.T., Normann, R.A., The Utah intracortical electrode array: a recording structure for potential brain-computer interfaces (1997) Electroencephalogr. Clin. Neurophysiol., 102 (3), pp. 228-239; Miller, L.E., Andreasen, D.S., Bartels, J.L., Kennedy, P.R., Robesco, J., Siebert, S.A., Wright, E.J., Human speech cortex long-term recordings: Bayesian analyses (2007) Neuroscience Meeting Planner, , Program No. 517.20, San Diego, USA; Moeller, 2008. %3chttp://innovationdays.alcatel-lucent.com/2008/documents/Talking.Beyond.Hearing.pdf%3e; Morse, M.S., O'Brien, E.M., Research summary of a scheme to ascertain the availability of speech information in the myoelectric signals of neck and head muscles using surface electrodes (1986) Comput. Biol. Med., 16 (6), pp. 399-410; Morse, M.S., Day, S.H., Trull, B., Morse, H., 1989. Use of myoelectric signals to recognize speech. In: Images of the Twenty-First Century - Proc. 11th Annual Internat. Conf. of the IEEE Engineering in Medicine and Biology Society, Part 2, 11. Alliance for Engineering in Medicine and Biology, pp. 1793-1794; Morse, M.S., Gopalan, Y.N., Wright, M., Speech recognition using myoelectric signals with neural networks (1991) Proc. 13th Annual Internat. Conf. of the IEEE Engineering in Medicine and Biology Society, 13 (4), pp. 1877-1878. , Piscataway, NJ, United States. IEEE, pp; Munhall, K.G., Vatikiotis-Bateson, E., Tohkura, Y., X-ray film database for speech research (1995) J. Acoust. Soc. Amer., 98, pp. 1222-1224; Nakajima, Y., Development and evaluation of soft silicone NAM microphone (2005) Technical Report IEICE, pp. 7-12. , SP2005-7, pp, in Japanese; Nakajima, Y., Kashioka, H., Shikano, K., Campbell, N., Non-audible murmur recognition (2003) Proc. Eurospeech 2003, pp. 2601-2604; Nakajima, Y., Kashioka, H., Shikano, K., Campbell, N., Non-audible murmur recognition input interface using stethoscopic microphone attached to the skin (2003) Proc. IEEE ICASSP, pp. 708-711; Nakajima, Y., Kashioka, H., Campbell, N., Shikano, K., Non-audible murmur (NAM) recognition (2006) IEICE Trans. Inform. Systems, E89-D (1), pp. 1-8; Nakamura, H., Method of recognizing speech using a lip image (1988), United States Patent 4769845, September 06; NessAiver, M.S., Stone, M., Parthasarathy, V., Kahana, Y., Paritsky, A., Recording high quality speech during tagged cine-MRI studies using a fiber optic microphone (2006) J. Magnet. Reson. Imag., 23, pp. 92-97; Neuper, C., Müller, G.R., Kübler, A., Birbaumer, N., Pfurtscheller, G., Clinical application of an EEG-based brain computer interface: a case study in a patient with severe motor impairment (2003) Clin. Neurophysiol., 114, pp. 399-409; Ng, L., Burnett, G., Holzrichter, J., Gable, T., Denoising of human speech using combined acoustic and EM sensor signal processing (2000) Internat. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 1, pp. 229-232. , Istanbul, Turkey, 5-9 June; Nguyen, N., Marchal, A., Content, A., Modeling tongue-palate contact patterns in the production of speech (1996) J. Phonetics, 1996, pp. 77-98; Nijholt, A., Tan, D., Pfurtscheller, G., Brunner, C., Millán, J.d.R., Allison, B., Graimann, B., Müller, K.R., Brain-Computer Interfacing for Intelligent Systems (2008) Intell. Systems, 23 (3), pp. 72-79; Otani, M., Shimizu, S., Hirahara, T., Vocal tract shapes of non-audible murmur production (2008) Acoust. Sci. Technol., 29, pp. 195-198; Ouisper, 2006. Oral Ultrasound synthetIc SpEech SouRce, Projet Blanc, National Research Agency (ANR), France, Contract No. ANR-06-BLAN-0166, 2006-2009; Patil, S.A., Hansen, J.H.L., A competitive alternative for speaker assessment: physiological Microphone (PMIC) (2010) Speech Comm., 52 (4), pp. 327-340; Perkell, J., Cohen, M., Svirsky, M., Matthies, M., Garabieta, I., Jackson, M., Electro-magnetic midsagittal articulometer (EMMA) systems for transducing speech articulatory movements (1992) J. Acoust. Soc. Amer., 92, pp. 3078-3096; Petajan, E.D., Automatic lipreading to enhance speech recognition (1984) IEEE Communications Society Global Telecommunications Conf, , Atlanta, USA; Porbadnigk, A., Wester, M., Calliess, J., Schultz, T., EEG-based speech recognition - impact of temporal effects (2009) Biosignals 2009, pp. 376-381. , Porto, Portugal, January; Preuss, R.D., Fabbri, D.R., Cruthirds, D.R., Noise robust vocoding at 2400 bps (2006) 8th Internat. Conf. on Signal Processing, ICSP 2006, Guilin, China, November 16-20, 1, pp. 16-20; Quatieri, T.F., Messing, D., Brady, K., Campbell, W.B., Campbell, J.P., Brandstein, M., Weinstein, C.J., Gatewood, P.D., Exploiting nonacoustic sensors for speech enhancement (2006) IEEE Trans. Audio Speech Lang. Process., 14 (2), pp. 533-544; Rothenberg, M., A multichannel electroglottograph (1992) J. Voice, 6 (1), pp. 36-43; Rubin, P., Vatikiotis-Bateson, E., Talking heads (1998) Proc. Audio Visual Speech Process., 1998, pp. 233-238; Brain Computer Interfaces (2008) IEEE Signal Process. Mag. (special issue), , Sajda, P, Mueller, K.-R, Shenoy, K.V, Eds; Schönle, P.W., Gräbe, K., Wenig, P., Höhne, J., Schrader, J., Conrad, B., Electromagnetic articulography: Use of alternating magnetic fields for tracking movements of multiple points inside and outside the vocal tract (1987) Brain Lang., 31, pp. 26-35; Schroeter, J., Ostermann, J., Graf, H.P., Beutnagel, M., Cosatto, E., Syrdal, A., Conkie, A., Stylianou, Y., Multimodal speech synthesis (2000) IEEE Internat. Conf. on Multimedia and Expo, pp. 571-574; Schultz, T., Wand, M., Modeling coarticulation in large vocabulary EMG-based speech recognition (2010) Speech Comm., 52 (4), pp. 341-353; Stone, M., Sonies, B., Shawker, T., Weiss, G., Nadel, L., Analysis of real-time ultrasound images of tongue configuration using a grid-digitizing system (1983) J. Phonetics, 11, pp. 207-218; Stone, M., Shawker, T., An ultrasound examination of tongue movement during swallowing (1986) Dysphagia, 1, pp. 78-83; Stone, M., Davis, E., A head and transducer support (HATS) system for use in ultrasound imaging of the tongue during speech (1995) J. Acoust. Soc. Amer., 98, pp. 3107-3112; Stone, M., A guide to analyzing tongue motion from ultrasound images (2005) Clin. Linguist. Phonet., 19 (6-7), pp. 455-502; Sugie, N., Tsunoda, K., A speech prosthesis employing a speech synthesizer-vowel discrimination from perioral muscle activities and vowel production (1985) IEEE Trans. Biomed. Eng., BME-32 (7), pp. 485-490; Suppes, P., Lu, Z.-L., Han, B., Brain wave recognition of words (1997) Proc. Nat. Acad. Sci. USA, 94, pp. 14965-14969; (2003) MIT Lincoln Labs Report, , Tardelli, J.D, Ed, ESC-TR-2004-084. Pilot Corpus for Multisensor Speech Processing; Tatham, M., The place of electromyography in speech research (1971) Behav. Technol., 6; TERC, 2009. %3chttp://spinlab.wpi.edu/projects/terc/terc.html%3e; Titze, I.R., Story, B.H., Burnett, G.C., Holzrichter, J.F.N.G.L.C., Lea, W.A., Comparison between electroglottography and electromagnetic glottography (2000) J. Acoust. Soc. Amer., 107 (1), pp. 581-588. , January; Tran, V.-A., Bailly, G., Loevenbruck, H., Jutten, C., Improvement to a NAM captured whisper-to-speech system (2008) Interspeech, pp. 1465-1498. , Brisbane, Australia, pp; Tran, V.-A., Bailly, G., Loevenbruck, H., Toda, T., Predicting F0 and voicing from NAM-captured whispered speech (2008) Proc. Speech Prosody, , Campinas, Brazil; Tran, V.-A., Bailly, G., Loevenbruck, H., Toda, T., Improvement to a NAM-captured whisper-to-speech system (2010) Speech Comm., 52 (4), pp. 314-326; Truccolo, W., Friehs, G.M., Donoghue, J.P., Hochberg, L.R., Primary motor cortex tuning to intended movement kinematics in humans with tetraplegia (2008) J. Neurosci., 28 (5), pp. 1163-1178; Walliczek, M., Kraft, F., Jou, S.-C., Schultz, T., Waibel, A., Sub-word unit based non-audible speech recognition using surface electromyography (2006) Proc. Interspeech, pp. 1487-1490. , Pittsburgh, USA, pp; Wand, M., Schultz, T., Speaker-adaptive speech recognition based on surface electromyography (2009) BIOSTEC - BIOSIGNALS 2009 best papers, , Communications in Computer and Information Science (CCIS) series. Springer, Heidelberg, in press; Wester, M., Schultz, T., (2006) Unspoken speech - speech recognition based on electroencephalography, , Master's Thesis, Universität Karlsruhe TH, Karlsruhe, Germany; Wolpaw, J.R., Birbaumer, N., McFarland, D., Pfurtscheller, G., Vaughan, T., Brain-computer interfaces for communication and control (2002) Clin. Neurophysiol., 113 (6), pp. 767-791; Wrench, A.A., Scobbie, J.M., Categorising vocalisation of English / l / using EPG, EMA and ultrasound (2003) 6th Internat. Seminar on Speech Production, Manly, pp. 314-319. , Sydney, Australia, 7-10 December; Wrench, A., Scobbie, J., Linden, M., Evaluation of a helmet to hold an ultrasound probe (2007) Ultrafest IV, , New York, USA; Wright, E.J., Andreasen, D.S., Bartels, J.L., Brumberg, J.S., Guenther, F.H., Kennedy, P.R., Miller, L.E., Velliste, M., Human speech cortex long-term recordings: Neural net analyses (2007) Neuroscience Meeting Planner, , Program No. 517.18, San Diego, USA},
correspondence_address1={Denby, B.; Université Pierre et Marie Curie - Paris VI, 4 place Jussieu, 75005 Paris, France; email: denby@ieee.org},
issn={01676393},
coden={SCOMD},
language={English},
abbrev_source_title={Speech Commun},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dillmann2010VI,
author={Dillmann, R. and Beyerer, J. and Hanebeck, U.D. and Schultz, T.},
title={Preface},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6359 LNAI},
pages={VI},
doi={10.1515/9783110537949-005},
note={cited By 0; Conference of 33rd Annual German Conference on Artificial Intelligence, KI 2010 ; Conference Date: 21 September 2010 Through 24 September 2010;  Conference Code:206069},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038217577&doi=10.1515%2f9783110537949-005&partnerID=40&md5=5288c8f1c32fe2d7158a916deb875a63},
affiliation={KIT, Institute for Anthropomatics, Karlsruhe, Germany; KIT, Lehrstuhl für Interaktive Echtzeitsysteme (IES), Karlsruhe, Germany; KIT, Lehrstuhl für Intelligente Sensor-Aktor-Systeme (ISAS), Karlsruhe, Germany; KIT, Cognitive Systems Lab (CSL), Karlsruhe, Germany},
editor={Hanebeck U.D., Beyerer J., Dillmann R., Schultz T.},
sponsors={ontoprise GmbH, Karlsruhe; PTV Planung Transport Verkehr AG, Karlsruhe; Springer Verlag, Heidelberg},
publisher={Springer Verlag},
issn={03029743},
isbn={9783642161100},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Editorial},
source={Scopus},
}

@CONFERENCE{Tam20091633,
author={Tam, Y.-C. and Schultz, T.},
title={Correlated bigram LSA for unsupervised language model adaptation},
journal={Advances in Neural Information Processing Systems 21 - Proceedings of the 2008 Conference},
year={2009},
pages={1633-1640},
note={cited By 8; Conference of 22nd Annual Conference on Neural Information Processing Systems, NIPS 2008 ; Conference Date: 8 December 2008 Through 11 December 2008;  Conference Code:89076},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858770011&partnerID=40&md5=b5faacc6f79a77a57c75e0a4befd0a00},
affiliation={InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={We present a correlated bigram LSA approach for unsupervised LMadaptation for automatic speech recognition. The model is trained using efficient variational EM and smoothed using the proposed fractional Kneser-Ney smoothing which handles fractional counts. We address the scalability issue to large training corpora via bootstrapping of bigram LSA from unigram LSA. For LM adaptation, unigram and bigram LSA are integrated into the background N-gram LM via marginal adaptation and linear interpolation respectively. Experimental results on the Mandarin RT04 test set show that applying unigram and bigram LSA together yields 6%-8% relative perplexity reduction and 2.5% relative character error rate reduction which is statistically significant compared to applying only unigram LSA. On the large-scale evaluation on Arabic, 3% relative word error rate reduction is achieved which is also statistically significant.},
keywords={Automatic speech recognition;  Character error rates;  Language model adaptation;  Linear Interpolation;  Marginal adaptation;  Scalability issue;  Test sets;  Training corpus;  Variational EM;  Word error rate reductions, Computational linguistics},
references={Bellegarda, J.R., Large vocabulary speech recognition with multispan statistical language models (2000) IEEE Transactions on Speech and Audio Processing, 8 (1), pp. 76-84. , DOI 10.1109/89.817455; Blei, D., Ng, A., Jordan, M., Latent dirichlet allocation (2003) Journal of Machine Learning Research, pp. 1107-1135; Tam, Y.C., Schultz, T., Language model adaptation using variational Bayes inference (2005) Proceedings of Interspeech; Mrva, D., Woodland, P.C., Unsupervised language model adaptation for mandarin broadcast conversation transcription (2006) Proceedings of Interspeech; Griffiths, T., Steyvers, M., Blei, D., Tenenbaum, J., Integrating topics and syntax (2004) Advances in Neural Information Processing Systems; Hsu, B.J., Glass, J., Style and topic language model adaptation using HMM-LDA (2006) Proceedings of Empirical Methods on Natural Language Processing (EMNLP); Wallach, H.M., Topic modeling: Beyond bag-of-words (2006) International Conference on Machine Learning; Xu, P., Emami, A., Jelinek, F., Training connectionist models for the structured language model (2003) Proceedings of Empirical Methods on Natural Language Processing (EMNLP); Kneser, R., Ney, H., Improved backing-off forM-gram language modeling (1995) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 1, pp. 181-184; Kneser, R., Peters, J., Klakow, D., Language model adaptation using dynamic marginals (1997) Proceedings of European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1971-1974; Iyer, R., Ostendorf, M., Modeling long distance dependence in language: Topic mixtures versus dynamic cache models (1999) IEEE Transactions on Speech and Audio Processing, 7 (1), pp. 30-39. , Jan; Wang, X., McCallum, A., Wei, X., Topical N-grams: Phrase and topic discovery, with an application to information retrieval (2007) IEEE International Conference on Data Mining; Minka, T., (1999) The Dirichlet-tree Distribution; Tam, Y.C., Schultz, T., Correlated latent semantic model for unsupervised language model adaptation (2007) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP); Stolcke, A., SRILM - An extensible language modeling toolkit (2002) Proceedings of International Conference on Spoken Language Processing (ICSLP)},
correspondence_address1={Tam, Y.-C.; InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: yct@cs.cmu.edu},
sponsors={Microsoft; Google; PASCAL Network; Yahoo Research; Willow Garage},
address={Vancouver, BC},
isbn={9781605609492},
language={English},
abbrev_source_title={Adv. Neural Inf. Process. Syst. - Proc. Conf.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vu2009333,
author={Vu, N.T. and Schultz, T.},
title={Vietnamese large vocabulary continuous speech recognition},
journal={Proceedings of the 2009 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2009},
year={2009},
pages={333-338},
doi={10.1109/ASRU.2009.5373424},
art_number={5373424},
note={cited By 22; Conference of 2009 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2009 ; Conference Date: 13 December 2009 Through 17 December 2009;  Conference Code:79490},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949417963&doi=10.1109%2fASRU.2009.5373424&partnerID=40&md5=8ee4b7423e02f1f2cd0a11ed2457b06b},
affiliation={Cognitive Systems Lab (CSL), Institute for Anthropomatics, University of Karlsruhe, Germany},
abstract={We report on our recent efforts toward a large vocabulary Vietnamese speech recognition system. In particular, we describe the Vietnamese text and speech database recently collected as part of our GlobalPhone corpus. The data was complemented by a large collection of text data crawled from various Vietnamese websites. To bootstrap the Vietnamese speech recognition system we used our Rapid Language Adaptation scheme applying a multilingual phone inventory. After initialization we investigated the peculiarities of the Vietnamese language and achieved significant improvements by implementing different tone modeling schemes, extended by pitch extraction, handling multiwords to address the monosyllable structure of Vietnamese, and featuring language modeling based on 5-grams. Furthermore, we addressed the issue of dialectal variations between South and North Vietnam by creating dialect dependent pronunciations and including dialect in the context decision tree of the recognizer. Our currently best recognition system achieves a word error rate of 11.7% on read newspaper speech. © 2009 IEEE.},
keywords={Adaptation scheme;  Dialectal variation;  Language modeling;  Large vocabulary;  Large vocabulary continuous speech recognition;  Pitch extraction;  Recognition systems;  Speech database;  Text data;  Tone modeling;  Viet Nam;  Vietnamese speech;  Word error rate, Computational linguistics;  Continuous speech recognition;  Decision trees;  Vocabulary control, Telephone systems},
references={Schultz, T., Waibel, A., Language Independent and Language Adaptive Acoustic Modeling for Speech Recognition (2001) In Speech Communication, 35 (1-2), pp. 31-51. , August; Suebvisai, S., Charoenpornsawat, P., Black, A.W., Woszczyna, M., Schultz, T., Thai Automatic Speech Recogntion (2005) Proc ICASSP 2005, Philadelphia, PA; Yu, H., Waibel, A., Streamlining the front end of speech recognizer (2000) Proc ICSLP; Rogina, I., (1997) Optimization of Parameters for Dictation with unlimited Vocabulary, , Dissertation, University Karlsruhe, Germany; Leipzig Vietnamese Pronunciation Dictionary. In: http://www.informatik. uni-leipzig-de/~duc/Dict/install.html; Kjell Schubert. Pitch tracking and his application on speech recognition. In: Diploma Thesis at University of Kalsruhe(TH), Germany, 1998; Nguyen Hong Quang, Pascal Nocera, Eric Castelli, Trinh Van Loan. A Novel Approach in Continuous Speech Recognition for Vietnamese, an isolating tonal language. In: SLTU, Hanoi, Vietnam, 2008; Schultz, T., GlobalPhone: A Multilingual Speech and Text Database developed at Karlsruhe University (2002) Proc. ICSLP, , Denver, CO; Schultz, T., Black, A., Rapid Language Adaptation Tools and Technologies for Multilingual Speech Processing (2008) Proc ICASSP, , Las Vegas, NV; A M Noll. Short-Time Spectrum and Cepstrum Techniques for Vocal-Pitch Detection. In: J. Acoust. Soc. Am. 36, Issue 2, pp. 296-302, 1964; Tat Vu, T., Dung, T., Chi Luong, M., Paul Hosom, J., Vietnamese Large Vocabulary Continuous Speech Recognition (2005) Interspeech, , Lisbon Portugal},
correspondence_address1={Vu, N. T.; Cognitive Systems Lab (CSL), Institute for Anthropomatics, University of KarlsruheGermany; email: Thang.Vu@student.kit.edu},
address={Merano},
isbn={9781424454792},
language={English},
abbrev_source_title={Proc. IEEE Workshop Autom. Speech Recognit. Underst., ASRU},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Stoimenov2009293,
author={Stoimenov, E. and Schultz, T.},
title={A multiplatform speech recognition decoder based on weighted finite-state transducers},
journal={Proceedings of the 2009 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2009},
year={2009},
pages={293-298},
doi={10.1109/ASRU.2009.5373404},
art_number={5373404},
note={cited By 1; Conference of 2009 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2009 ; Conference Date: 13 December 2009 Through 17 December 2009;  Conference Code:79490},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949411292&doi=10.1109%2fASRU.2009.5373404&partnerID=40&md5=332d66f059e9a385fcc7ab5600dfcc69},
affiliation={Cognitive Systems Labs., Institute for Anthropomatics, University of Karlsruhe, Am Fasanengarten 5, D-76131 Karlsruhe, Germany},
abstract={Speech recognition decoders based on static graphs have recently proven to significantly outperform the traditional approach of prefix tree expansion in terms of decoding speed [1], [2]. The reduced search effort makes static graph decoders an attractive alternative for tasks concerned with limited processing power or memory footprint on devices such as PDAs, internet tablets, and smart phones. In this paper we explore the benefits of decoding with an optimized speech recognition network over the fully task-optimized prefix-tree based decoder IBIS [3]. We designed and implemented a new decoder called SWIFT (Speedy WeIgthed Finite-state Transducer) based on WFSTs with its application to embedded platforms in mind. After describing the design, the network construction and storage process, we present evaluation results on a small task suitable for embedded applications, and on a large task, namely the European Parliament Plenary Sessions (EPPS) task from the TC-STAR project [20]. The SWIFT Decoder is up to 50% faster than IBIS on both tasks. In addition, SWIFT achieves significant memory consumption reductions obtained by our innovative network specific storage layout optimization. © 2009 IEEE.},
keywords={Decoding speed;  Embedded application;  Embedded platforms;  European Parliament;  Evaluation results;  Finite state transducers;  Memory consumption;  Memory footprint;  Multi-platform;  Network construction;  Plenary sessions;  Prefix trees;  Processing power;  Smart phones;  Storage layouts;  Tree-based;  Weighted finite-state transducers, Decoding;  Optimization;  Transducers, Speech recognition},
references={Kanthak, S., Ney, H., Riley, M., Mohri, M., A comparison of two LVR search optimizations techniques (2002) ICSLP'02, , Denver, Colorado, USA; Dolfing, H., A comparison of prefix tree and finite-state transducer search space modelings for large-vocabulary speech recognition (2002) ICSLP'02, , Denver, Colorado, USA; Soltau, H., Metze, F., Fügen, C., Waibel, A., A One Pass-Decoder Based on Polymorphic Linguistic Context Assignment (2001) ASRU, , Trento, Italy; Mohri, M., Pereira, F., Riley, M., Weighted finite-state transducers in speech recognition (2002) Computer Speech and Language, 16, pp. 69-88; Goffin, V., Allauzen, C., Bocchieri, E., Hakkani-Tür, D., Ljolje, A., Parthasarathy, S., Rahim, M., Saraclar, M., The AT&T Watson Speech Recognizer (2005) Interspeech '05, , Lisbon, Portugal; Saon, G., Povey, D., Zweig, G., Anatomy of an extremely fast LVCSR decoder (2005) Interspeech '05, , Lisbon, Portugal; Caseiro, D., Trancoso, I., Using Dynamic WFST Composition for Recognizing Broadcast News (2002) ICSLP'02, , Denver, Colorado, USA; Hori, T., Nakamura, A., Generalized Fast On-the-fly Composition Algorithm for WFST-Based Speech Recognition (2005) Interspeech '05, , Lisbon, Portugal; Cheng, O., Dines, J., Doss, M.M., A Generalized Dynamic Composition Algorithm of Weighted Finite-State Transducers for Large Vocabulary Speech Recognition (2007) ICASSP '07, , Honolulu, Hawaii, USA; McDonough, J., Stoimenov, E., Klakow, D., An Algorithm for Fast Composition of Weighted Finite-State Transducers (2007) ASRU '07, , Kyoto, Japan; Olsen, J., Gao, Y., Ding, G., Yang, X., A Decoder for Large Vocabulary Continuous Short Message Dictation on Embedded Devices (2008) ICASSP '08, , Las Vegas, Nevada, USA; Bocchieri, E., Blewett, D., A Decoder for LVCSR Based on Fixed-Point Arithmetic (2006) ICASSP, , Toulouse, France; Moore, D., Dines, J., Doss, M.M., Vepa, J., Cheng, O., Hain, T., Juicer: A Weighted Finite-State Transducer Speech Decoder (2006) MLMI '06, , Washington DC, USA; Allauzen, C., Mohri, M., Roark, B., Riley, M., A Generalized Construction of Integrated Speech Recognition Transducers (2004) ICASSP '04, , Montréal, Canada; Stoimenov, E., McDonough, J., Memory Efficient Modeling of Polyphone Context with Weighted Finite-State Transducers (2007) Interspeech '07, , Antwerp, Belgium; Young, S.J., Russell, N.H., Thornton, J.H.S., Token Passing: A Simple Conceptual Model for Connected Speech Recognition Systems (1989), Technical Report, University of Cambridge; Köhler, T., Fügen, C., Stüker, S., Waibel, A., Rapid Porting of ASR Systems to Mobile Devices (2005) Interspeech '05, , Lisboa, Portugal; Allauzen, C., Riley, M., Schalkwyk, J., Skut, W., Mohri, M., OpenFst: A General and Efficient Weighted Finite-State Transducer Library (2007) CIAA '07, , Prague, Czech Republic; Kikui, G., Sumita, E., Takezawa, T., Yamamoto, S., Creating corpora for speech-to-speech translation (2003) Interspeech '03, pp. 381-384. , Geneva, Switzerland; Technology and Corpora for Speech to Speech Translation (TC-STAR). Integrated Project funded by the European Commission, Project No. FP6-506738, 2004-2007. http://www.tc-star.org; Stolke, A., SRILM - An Extensible Language Modeling Toolkit (2002) ICSLP '02, , Denver, Colorado, USA; Mohri, M., Riley, M., Network optimizations for large vocabulary speech recognition (2002) Computer Speech and Language, 16, pp. 69-88; Schuster, M., Hori, T., Efficient Generation of High-order Context-Dependent Weighted Finite-State Transducers for Speech Recognition (2005) ICASSP '05, , Philadelphia, PA, USA},
correspondence_address1={Stoimenov, E.; Cognitive Systems Labs., Institute for Anthropomatics, University of Karlsruhe, Am Fasanengarten 5, D-76131 Karlsruhe, Germany; email: emilian@ira.uka.de},
address={Merano},
isbn={9781424454792},
language={English},
abbrev_source_title={Proc. IEEE Workshop Autom. Speech Recognit. Underst., ASRU},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schaaff2009,
author={Schaaff, K. and Schultz, T.},
title={Towards emotion recognition from electroencephalographic signals},
journal={Proceedings - 2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops, ACII 2009},
year={2009},
doi={10.1109/ACII.2009.5349316},
art_number={5349316},
note={cited By 63; Conference of 2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops, ACII 2009 ; Conference Date: 10 September 2009 Through 12 September 2009;  Conference Code:79475},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949407065&doi=10.1109%2fACII.2009.5349316&partnerID=40&md5=cdb30760b590782a044b4ec113f1bbf3},
affiliation={University of Karlsruhe (TH), Karlsruhe, Germany},
abstract={During the last decades, information about the emotional state of users has become more and more important in human-computer interaction. Automatic emotion recognition enables the computer to recognize a user's emotional state and thus allows for appropriate reaction, which may pave the way for computers to act emotionally in the future. In the current study, we investigate different feature sets to build an emotion recognition system from electroencephalographic signals. We used pictures from the International Affective Picture System to induce three emotional states: pleasant, neutral, and unpleasant. We designed a headband with four build-in electrodes at the forehead, which was used to record data from five subjects. Compared to standard EEG-caps, the headband is comfortable to wear and easy to attach, which makes it more suitable for everyday life conditions. To solve the recognition task we developed a system based on support vector machines. With this system we were able to achieve an average recognition rate up to 66.7% on subject dependent recognition, solely based on EEG signals. © 2009 IEEE.},
keywords={EEG signals;  Electroencephalographic signals;  Emotion recognition;  Emotional state;  Feature sets;  Picture system;  Recognition rates;  System-based, Electroencephalography;  Intelligent computing;  Knowledge management, Human computer interaction},
references={Aftanas, L.I., Reva, N.V., Varlamov, A.A., Pavlov, S.V., Makhnev, V.P., Analysis of evoked EEG synchronization and desynchronization in conditions of motional activation in humans: Temporal and topographic characteristics (2004) Neuroscience and behavioral physiology, 34 (8), pp. 859-867. , October; Becker, K., (2003) Varioport™, , http://www.becker-meditec.de; Bradley, M., Lang, P.J., The international affective picture system (iaps) in the study of emotion and attention (2007) Handbook of Emotion Elicitation and Assessment, pp. 29-46. , J. A. Coan and J. J. B. Allen, editors, chapter 2, New York: Oxford University Press; Bradley, M.M., Codispoti, M., Cuthbert, B.N., Lang, P.J., Emotion and motivation I: Defensive and appetitive reactions in picture processing (2001) Emotion, 1 (3), pp. 276-298. , September; Bradley, M.M., Lang, P.J., Measuring emotion: The self-assessment manikin and the semantic differential (1994) Journal of Behavior Therapy and Experimental Psychiatry, 25, pp. 49-59; Chanel, G., Kronegg, J., Grandjean, D., Pun, T., Emotion assessment: Arousal evaluation using EEG's and peripheral physiological signals (2006) Lecture Notes in Computer Science, 4105, pp. 530-537. , B. Gunsel, A. K. Jain, A. M. Tekalp, and B. Sankur, editors, Proc. Int. Workshop Multimedia Content Representation, Classification and Security MRCS, Istanbul, Turkey, Springer; Chang, C.-C., Lin, C.-J., (2008) LIBSVM: A Library for Support Vector Machines, 2008, , Last updated: May 13; Cuthbert, B.N., Schupp, H.T., Bradley, M.M., Birbaumer, N., Lang, P.J., Brain potentials in affective picture processing: Covariation withautonomic arousal and affective report (2000) Biol Psychol, 52 (2), pp. 95-111. , Mar; Davidson, R.J., Anterior cerebral asymmetry and the nature of emotion (1992) Brain and Cognition, 20 (1), pp. 125-151. , Sep; Davidson, R.J., Ekman, P., Sarona, C.D., Senulis, J.A., Friesen, W.V., Approach / withdrawal and cerebral asymmetry: Emotional expression and brain physiology (1990) Journal of Personality and Social Psychology, 58 (2), pp. 330-341. , Feb; Haag, A., Goronzy, S., Schaich, P., Williams, J., Emotion recognition using bio-sensors: First steps towards an automatic system (2004) Lecture Notes in Computer Science, 3068, pp. 33-48; Honal, M., Identifying user state using electroencephalographic data (2005) Proceedings of the International Conference on Multimodal Input (ICMI); Honal, M., Schultz, T., Determine task demand from brain activity (2008) Biosignals, p. 2008; C. W. Hsu, C. C. Chang, and C. J. Lin. A practical guide to support vector classification. Technical report, Department of Computer Science, Taipei, 2003. Last updated: May 21, 2008; Jasper, H.H., The ten-twenty electrode system of the international federation in electroencephalography and clinical neurophysiology (1958) EEG Journal, 10, pp. 371-375; Keil, A., Bradley, M.M., Hauk, O., Rockstroh, B., Elbert, T., Lang, P.J., Large-scale neural correlates of affective picture processing (2002) Psychophysiology, 39 (5), pp. 641-649. , Clinical Trial, Sept; Kim, K.H., Bang, S.W., Kim, S.R., Emotion recognition system using short-term monitoring of physiological signals (2004) Medical and Biological Engineering and Computing, 42, pp. 419-427; Kostyunina, M., Kulikov, M., Frequency characteristics of eeg spectra in the emotions (1996) Neuroscience and Behavioral Physiology, 26 (4), pp. 340-343. , July; Lang, P., Bradley, M., Cuthbert, B., International affective picture system (iaps): Affective ratings of pictures and instruction manual (2005), Technical Report A-6, University of Florida, Gainesville, FL; C. Mayer. UKA {EMG\EEG} Studio v2.0, 2005; Musha, T., Terasaki, Y., Haque, H.A., Ivamitsky, G.A., Feature extraction from EEGs associated with emotions (1997) Artificial Life and Robotics, 1 (1), pp. 15-19. , March; Picard, R.W., Healey, J., Affective wearables (1997) ISWC, pp. 90-97; Picard, R.W., Vyzas, E., Healey, J., Toward machine emotional intelligence: Analysis of affective physiological state (2001) IEEE Transactions on Pattern Analysis and Machine Intelligence, 23, pp. 1175-1191. , Oct; Reeves, B., Nass, C., (1995) The Media Equation: How People Treat Computers, Televisions, and New Media as Real People and Places, , Cambridge University Press; Scherer, K.R., (1981) Speech and Emotional States, pp. 189-220. , Speech Evaluation in Psychiatry. Grune & Stratton, New York, j. darby edition; K. Takahashi. Remarks on svm-based emotion recognition from multi-modal bio-potential signals. Robot and Human Interactive Communication, 2004. ROMAN 2004. 13th IEEE International Workshop on Robot and Human Interactive Communication, pages 95-100, 2004},
correspondence_address1={Schaaff, K.; University of Karlsruhe (TH), Karlsruhe, Germany; email: schaaff@fzi.de},
sponsors={humaine; University of Twente; GaTE - Game research for training and entertainment; PHILIPS; IOP - Mens-Machine Interactie},
address={Amsterdam},
isbn={9781424447992},
language={English},
abbrev_source_title={Proc. - Int. Conf. Affective Comput. Intelligent Interact. Workshops, ACII},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schaaff2009792,
author={Schaaff, K. and Schultz, T.},
title={Towards an EEG-based emotion recognizer for humanoid robots},
journal={Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
year={2009},
pages={792-796},
doi={10.1109/ROMAN.2009.5326306},
art_number={5326306},
note={cited By 56; Conference of 18th IEEE International Symposium on Robot and Human Interactive, RO-MAN 2009 ; Conference Date: 27 September 2009 Through 2 October 2009;  Conference Code:78777},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-72849135549&doi=10.1109%2fROMAN.2009.5326306&partnerID=40&md5=efc892aeceb2afb6511a731fbc1a2864},
affiliation={University of Karlsruhe (TH), Karlsruhe, Germany},
abstract={In the field of interaction between humans and robots emotions have been disregarded for a long time. During the last few years interest in emotion research in this area has been constantly increasing as giving a robot the ability to react to the emotional state of the user can help to make the interaction more human-like and enhance the acceptance of the robots. In this paper we investigate a method to facilitate emotion recognition from electroencephalographic signals. For this purpose we developed a headband to measure electroen-cephalographic signals on the forehead. Using this headband we collected data from five subjects. To induce emotions we used 90 pictures from the International Affective Picture System (IAPS) belonging to the three categories pleasant, neutral, and unpleasant. For emotion recognition we developed a system based on support vector machines (SVMs). With this system an average recognition rate of 47.11% could be achieved on subject dependent recognition. © 2009 IEEE.},
keywords={Electroencephalographic signals;  Emotion recognition;  Emotional state;  Humanoid robot;  Picture system;  Recognition rates;  System-based, Anthropomorphic robots;  Face recognition;  Support vector machines, Psychology computing},
references={Reeves, B., Nass, C., (1995) The Media Equation: How People Treat Computers, Televisions, and New Media as Real People and Places, , Cambridge University Press; Honal, M., Determining user state and mental task demand from electroencephalographic data, (2005), Master's thesis, Universität Karlsruhe TH, Karlsruhe, Germany; Honal, M., Schultz, T., Determine task demand from brain activity (2008) Biosignals, p. 2008; Scherer, K.R., Speech and Emotional States (1981) ser. Speech Evaluation in Psychiatry, pp. 189-220. , j. darby ed, New York: Grune & Stratton; Picard, R.W., Vyzas, E., Healey, J., Toward machine emotional intelligence: Analysis of affective physiological state (2001) IEEE Transactions on Pattern Analysis and Machine Intelligence, 23, pp. 1175-1191. , Oct; Kim, K.H., Bang, S.W., Kim, S.R., Emotion recognition system using short-term monitoring of physiological signals (2004) Medical and Biological Engineering and Computing, 42, pp. 419-427; Haag, A., Goronzy, S., Schaich, P., Williams, J., Emotion recognition using bio-sensors: First steps towards an automatic system (2004) Lecture Notes in Computer Science, 3068, pp. 33-48; K. Takahashi, Remarks on svm-based emotion recognition from multi-modal bio-potential signals, Robot and Human Interactive Communication, 2004. ROMAN 2004. 13th IEEE International Workshop on Robot and Human Interactive Communication, pp. 95-100, 2004; Davidson, R.J., Ekman, P., Sarona, C.D., Senulis, J.A., Friesen, W.V., Approach / withdrawal and cerebral asymmetry: Emotional expression and brain physiology (1990) Journal of Personality and Social Psychology, 58 (2), pp. 330-341. , Feb; Kostyunina, M., Kulikov, M., Frequency characteristics of eeg spectra in the emotions (1996) Neuroscience and Behavioral Physiology, 26 (4), pp. 340-343. , July; Aftanas, L.I., Reva, N.V., Varlamov, A.A., Pavlov, S.V., Makhnev, V.P., Analysis of evoked EEG synchronization and desynchronization in conditions of motional activation in humans: Temporal and topographic characteristics (2004) Neuroscience and behavioral physiology, 34 (8), pp. 859-867. , October; Chanel, G., Kronegg, J., Grandjean, D., Pun, T., Emotion assessment: Arousal evaluation using EEG's and peripheral physiological signals (2006) Lecture Notes in Computer Science, 4105, pp. 530-537. , Proc. Int. Workshop Multimedia Content Representation, Classification and Security MRCS, B. Gunsel, A. K. Jain, A. M. Tekalp, and B. Sankur, Eds, Istanbul, Turkey:, Springer; Jasper, H.H., The ten-twenty electrode system of the international federation in electroencephalography and clinical neurophysiology (1958) EEG Journal, 10, pp. 371-375; Becker, K., (2003) Varioport™, , http://www.becker-meditec.de; C. Mayer, UKA {EMG|EEG} Studio v2.0, 2005; Wester, M., Unspoken speech - speech recognition based on electroencephalography, (2006), Master's thesis, Universität Karlsruhe TH, Karlsruhe, Germany; Wand, M., Schultz, T., Towards speaker-adaptive speech recognition based on surface electromyography (2009) Biosignals, p. 2009; Chang, C.-C., Lin, C.-J., LIBSVM: A Library for Support Vector Machines, 2008, , http://www.csie.ntu.edu.tw/cjlin/papers/libsvm.pdf, last updated: May 13, 2008, Online, Available; Lang, P., Bradley, M., Cuthbert, B., (2005) International affective picture system (iaps): Affective ratings of pictures and instruction manual, , University of Florida, Gainesville, FL, Tech. Rep. A-6; Bradley, M., Lang, P.J., The international affective picture system (iaps) in the study of emotion and attention (2007) Handbook of Emotion Elicitation and Assessment, pp. 29-46. , J. A. Coan and J. J. B. Allen, Eds. New York: Oxford University Press, ch. 2, pp; Davidson, R.J., Anterior cerebral asymmetry and the nature of emotion (1992) Brain and Cognition, 20 (1), pp. 125-151. , Sep; Musha, T., Terasaki, Y., Haque, H.A., Ivamitsky, G.A., Feature extraction from EEGs associated with emotions (1997) Artificial Life and Robotics, 1 (1), pp. 15-19. , March; Hsu, C.W., Chang, C.C., Lin, C.J., (2003) A practical guide to support vector classification, , Department of Computer Science, Taipei, Tech. Rep, last updated: May 21},
correspondence_address1={Schaaff, K.; University of Karlsruhe (TH), Karlsruhe, Germany; email: schaaff@fzi.de},
address={Toyama},
isbn={9781424450817},
language={English},
abbrev_source_title={Proc. IEEE Int. Workshop Robot Human Interact. Commun.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gibert2009421,
author={Gibert, G. and Pruzinec, M. and Schultz, T. and Stevens, C.},
title={Enhancement of human computer interaction with facial electromyographic sensors},
journal={Proceedings of the 21st Annual Conference of the Australian Computer-Human Interaction Special Interest Group - Design: Open 24/7, OZCHI '09},
year={2009},
volume={411},
pages={421-424},
doi={10.1145/1738826.1738914},
note={cited By 18; Conference of 21st Annual Conference of the Australian Computer-Human Interaction Special Interest Group - Design: Open 24/7, OZCHI '09 ; Conference Date: 23 November 2009 Through 27 November 2009;  Conference Code:80455},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953003827&doi=10.1145%2f1738826.1738914&partnerID=40&md5=61d19059bd5c39b635906aa9c4991e40},
affiliation={MARCS Auditory Laboratories, University of Western Sydney, Locked Bag 1797, Penrith South DC, NSW 1797, Australia; Cognitive Systems Lab., University of Karlsruhe (TH), Adenauerring 4, 76131 Karlsruhe, Germany},
abstract={In this paper we describe a way to enhance human computer interaction using facial Electromyographic (EMG) sensors. Indeed, to know the emotional state of the user enables adaptable interaction specific to the mood of the user. This way, Human Computer Interaction (HCI) will gain in ergonomics and ecological validity. While expressions recognition systems based on video need exaggerated facial expressions to reach high recognition rates, the technique we developed using electrophysiological data enables faster detection of facial expressions and even in the presence of subtle movements. Features from 8 EMG sensors located around the face were extracted. Gaussian models for six basic facial expressions - anger, surprise, disgust, happiness, sadness and neutral - were learnt from these features and provide a mean recognition rate of 92%. Finally, a prototype of one possible application of this system was developed wherein the output of the recognizer was sent to the expressions module of a 3D avatar that then mimicked the expression. © ACM 2009.},
author_keywords={EMG;  Facial expressions;  Gaussian models},
keywords={3D Avatars;  Ecological validity;  Electromyographic;  Emotional state;  Facial Expressions;  Gaussian model;  Recognition rates;  Recognition systems, Ergonomics;  Interactive computer systems;  Sensors;  Software prototyping;  Three dimensional computer graphics, Human computer interaction},
references={Ang, L.B.P., Belen, E.F., Facial expression recognition through pattern analysis of facial muscle movements utilizing electromyogram sensors (2004) TENCON 2004. 2004 IEEE Region 10 Conference; Bartlett, M., Littlewort, G., Automatic recognition of facial actions in spontaneous expressions (2006) Journal of Multimedia, 1 (6), pp. 22-35; Becker, K., (2003) VarioportTM, , http://www.beckermeditec.de; Burnham, D., Dale, R., (2006) From Talking Heads to Thinking Heads: A Research Platform for Human Communication Science, , http://thinkinghead.uws.edu.au/index.html, from; Busso, C., Deng, Z., Analysis of emotion recognition using facial expressions, speech and multimodal information (2004) Sixth International Conference on Multimodal Interfaces ICMI, State College, PA; Chin, Z.Y., Ang, K.K., Multiclass voluntary facial expression classification based on Filter Bank Common Spatial Pattern (2008) Engineering in Medicine and Biology Society, EMBS 2008. 30th Annual International Conference of the IEEE; Dimberg, U., Thunberg, M., Rapid facial reactions to emotional facial expressions (1998) Scandinavian Journal of Psychology, 39 (1), pp. 39-45; Ekman, P., Friesen, W.V., Constants across Cultures in the Face ans Emotion (1971) Journal of Personality and Social Psychology, 17 (2), pp. 124-129; Fridlund, A.J., Cacioppo, J.T., Guidelines for Human Electromyographic Research (1986) Psychophysiology, 23 (5), pp. 567-589; Krell, G., Niese, R., Facial Expression Recognition with Multi-channel Deconvolution (2009) Advances in Pattern Recognition, 2009. ICAPR '09. Seventh International Conference on; Lucero, J.C., Munhall, K.G., A model of facial biomechanics for speech production (1999) Journal of the Acoustical Society of America, 106 (5), pp. 2834-2842; Minoru, H., Chisaki, Y., Development and Control of a Face Robot Imitating Human Muscular Structures (2006) Intelligent Robots and Systems, 2006 IEEE/RSJ International Conference on; Morishima, S., Face analysis and synthesis (2001) Signal Processing Magazine, IEEE, 18 (3), pp. 26-34; Reaz, M.B.I., Hussain, M.S., Techniques of EMG signal analysis: Detection, processing, classification and applications (2006) Biological Procedures Online, pp. 11-35; Tian, Y.I., Kanade, T., Recognizing action units for facial expression analysis (2001) Pattern Analysis and Machine Intelligence, IEEE Transactions on, 23 (2), pp. 97-115; Tingfan, W., Butko, N.J., Learning to Make Facial Expressions (2009) Development and Learning, 2009. ICDL 2009. IEEE 8th International Conference on; Toth, A., Wand, M., Synthesizing Speech from Electromyography using Voice Transformation Techniques (2009) Interspeech; Wand, M., Schultz, T., Towards Speaker-Adaptive Speech Recognition Based on Surface Electromyography (2009) Biosignals},
correspondence_address1={Gibert, G.; MARCS Auditory Laboratories, University of Western Sydney, Locked Bag 1797, Penrith South DC, NSW 1797, Australia; email: g.gibert@uws.edu.au},
address={Melbourne, VIC},
isbn={9781605588544},
language={English},
abbrev_source_title={Proc. Annu. Conf. Aust. Comput.-Hum. Interact. Spec. Interest Group - Des.: Open 24/7, OZCHI},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gehrig2009425,
author={Gehrig, D. and Kuehne, H. and Woerner, A. and Schultz, T.},
title={HMM-based human motion recognition with optical flow data},
journal={9th IEEE-RAS International Conference on Humanoid Robots, HUMANOIDS09},
year={2009},
pages={425-430},
doi={10.1109/ICHR.2009.5379546},
art_number={5379546},
note={cited By 26; Conference of 9th IEEE-RAS International Conference on Humanoid Robots, HUMANOIDS09 ; Conference Date: 7 December 2009 Through 10 December 2009;  Conference Code:79719},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950582190&doi=10.1109%2fICHR.2009.5379546&partnerID=40&md5=3e70b201082aa24b31e4530e5a008b36},
affiliation={Institute for Anthropomatics, Universitaet Karlsruhe (TH), 76131 Karlsruhe, Germany},
abstract={Human motion recognition is traditionally approached by either recognizing basic motions from features derived from video input or by interpreting complex motions by applying a high-level hierarchy of motion primitives. The former method is usually limited to rather simple motions while the latter requires human expert knowledge to build up a suitable hierarchy. In this paper we propose a new approach that uses the strength of both methods while overcoming their respective limitations. Our approach is able to recognize the motion units within complex motion sequences. The recognition process applies Hidden Markov Models (HMM) based on features consisting of optical flow gradient histograms. For each primitive motion unit we train one HMM and then concatenate these primitive motion units to form complex motion sequences. Modeling sequences with HMMs allows for a very flexible combination of motion units into motion sequences. They can either be combined in a restrictive rule-based formulation using predefined grammars or be more flexibly combined using a statistical model of sequence probabilities. In this paper we are mainly interested in the comparison of the optical flow features with marker-based features, therefore we do not use a motion grammar. We apply our approach to 24 motion units forming five complex motion sequences as they appear in a real-world kitchen tasks. The results show that the proposed approach allows for a very fast low-level recognition of human motion units without the need for any complex reconstruction, post processing or pose estimation. Straight-forward characteristic flow fields in combination with HMM sequence modeling are sufficient to reliably recognize complex motions even with an unrestricted search. Our results show that this search already achieves 13.1 % recognition error rate. We compare HMM models based on the optical flow features to those derived from a marker-based system. Our recognition results indicate that optical flow features achieve a competitive performance. ©2009 IEEE.},
keywords={Complex motion;  HMM models;  Human expert knowledge;  Human motion recognition;  Human motions;  Motion primitives;  Motion sequences;  New approaches;  Pose estimation;  Post processing;  Real-world;  Recognition error;  Recognition process;  Rule based;  Sequence modeling;  Statistical models, Anthropomorphic robots;  Hidden Markov models;  Motion estimation;  Object recognition;  Speech recognition, Optical flows},
references={Mori, G., Efros, A.A., Berg, A.C., Malik, J., Recognizing action at a distance (2003) IEEE International Conference on Computer Vision, pp. 726-733. , Nice, France; Bouguet, J.Y., (2002) Pyramidal Implementation of the Lucas Kanade Feature Tracker: Description of the Algorithm; Bradski, G.R., Davis, J., Motion segmentation and pose recognition with motion history gradients (2000) Machine Vision and Applications, pp. 238-244; Yilmaz, A., Rao, C., Shah, M., View-invariant representation and recognition of actions (2002) International Journal of Computer Vision, 50 (2), pp. 203-226; Danafar, S., Gheissari, N., Action recognition for surveillance applications using optic flow and svm (2007) ACCV, 2, pp. 457-466; Finke, M., Geumer, P., Hild, H., Kemp, T., Ries, K., Westphal, M., The karlsruhe-verbmobil speech recognition engine ICASSP-97, 1, pp. 83-86. , vol. 1; Lokman, J., Imai, J.I., Kaneko, M., Understanding human action in daily life scene based on action decomposition using dictionary terms and bayesian network Second International Symposium on Universal Communication, 2008. ISUC '08, 2008, pp. 67-74. , Dec; Laptev, I., Junejo, I.N., Dexter, E., Perez, P., Cross-view action recognition from temporal self-similarities (2008) ECCV, 2, pp. 293-306; Krüger, V., Kragic, D., Ude, A., Geib, C., The meaning of action: A review on action recognition and mapping (2007) Advanced Robotics, 21 (13), pp. 1473-1501; Lucas, B.D., Kanade, T., (1981) An Iterative Image Registration Technique with An Application to Stereo Vision; Moeslund, T.B., Hilton, A., Kruger, V., A survey of advances in vision-based human motion capture and analysis (2006) Computer Vision and Image Understanding, 104 (2-3), pp. 90-126; Ryoo, M.S., Aggarwal, J.K., Hierarchical recognition of human activities interacting with objects (2007) Computer Vision and Pattern Recognition, 2007. CVPR '07. IEEE Conference on, pp. 1-8. , June; Soltau, H., Metze, F., Fugen, C., Waibel, A., A one-pass decoder based on polymorphic linguistic context assignment (2001) ASRU, pp. 214-217; Vogler, C., Metaxas, D., Parallel hidden markov models for american sign language recognition (1999) ICCV, pp. 116-122; Wilson, A.D., Bobick, A.F., Parametric hidden markov models for gesture recognition (1999) IEEE Transactions on Pattern Analysis and Machine Intelligence, 21, pp. 884-900},
correspondence_address1={Gehrig, D.; Institute for Anthropomatics, Universitaet Karlsruhe (TH), 76131 Karlsruhe, Germany; email: dgehrig@ira.uka.de},
sponsors={IEEE Robotics and Automation Society; Aldebaran Robotics; French National Center for Scientific Research; College de France; Pierre and Marie Curie University},
address={Paris},
isbn={9781424445882},
language={English},
abbrev_source_title={IEEE-RAS Int. Conf. Humanoid Robots, HUMANOIDS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jin2009529,
author={Jin, Q. and Toth, A.R. and Schultz, T. and Black, A.W.},
title={Speaker de-identification via voice transformation},
journal={Proceedings of the 2009 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2009},
year={2009},
pages={529-533},
doi={10.1109/ASRU.2009.5373356},
art_number={5373356},
note={cited By 19; Conference of 2009 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2009 ; Conference Date: 13 December 2009 Through 17 December 2009;  Conference Code:79490},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949378080&doi=10.1109%2fASRU.2009.5373356&partnerID=40&md5=27bc6a98487250f656c3894440288f79},
affiliation={Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={It is a common feature of modern automated voice-driven applications and services to record and transmit a user's spoken request. At the same time, several domains and applications may require keeping the content of the user's request confidential and at the same time preserving the speaker's identity. This requires a technology that allows the speaker's voice to be de-identified in the sense that the voice sounds natural and intelligible but does not reveal the identity of the speaker. In this paper we investigate different voice transformation strategies on a large population of speakers to disguise the speakers' identities while preserving the intelligibility of the voices. We apply two automatic speaker identification approaches to verify the success of de-identification with voice transformation, a GMM-based and a Phonetic approach. The evaluation based on the automatic speaker identification systems verifies that the proposed voice transformation technique enables transmission of the content of the users' spoken requests while successfully preserving their identities. Also, the results indicate that different speakers still sound distinct after the transformation. Furthermore, we carried out a human listening test that proved the transformed speech to be both intelligible and securely de-identified, as it hid the identity of the speakers even to listeners who knew the speakers very well. © 2009 IEEE.},
keywords={Common features;  De-identification;  Large population;  Listening tests;  Speaker identification;  Speaker identification systems;  Transformation techniques, Loudspeakers, Speech recognition},
references={Jin, Q., Toth, A., Schultz, T., Black, A., Voice Convergin: Speaker De-Identification by Voice Transformation (2009) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP2009), pp. 3909-3912. , Taipei, Taiwan, 19-24 April; Gross, R., Sweeney, L., Torre, F., Baker, S., Model-Based Face De-Identification (2006) IEEE Workshop on Privacy Research in Vision, pp. 161-168. , June; Uzuner, O., Luo, Y., Szolovits, P., Evaluating the State-of-the-Art in Automatic De-identification (2007) Journal of the American Medical Informatics Association, 14 (5), pp. 550-563; T. Toda, A. W Black, K. Tokuda, Voice Conversion Based on Maximum Likelihood Estimation of Spectral Parameter Trajectory, IEEE Transactions on Audio, Speech, and Language Processing, 15, Issue 8, Nov. 2007, pp. 2222-2235; (2000) Building Synthetic Voices, , http://festvox.org; Laskowski, K., Jin, Q., Modeling Instantaneous Intonation for Speaker Identification Using the Fundamental Frequency Variation Spectrum (2009) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP2009), pp. 4541-4544. , Taipei, Taiwan, 19-24 April; Reynolds, D., Rose, R., Robust Text-independent Speaker Identification Using Gaussian Mixture Speaker Models (1995) IEEE Transactions on Speech and Audio Processing, 3 (1), pp. 72-83. , January; Reynolds, D., Quatieri, T., Dunn, R., Speaker Verification using adapted Gaussian Mixture Models (2000) Digital Signal Processing, 10 (1-3), pp. 19-41. , January; Jin, Q., Schultz, T., Waibel, A., Phonetic Speaker Identification (2002) Proceedings of the ISCA International Conference on Spoken Language Processing (ICSLP2002), pp. 1345-1348. , Denver, Colorado USA, Sept. 16-20; Schultz, T., GlobalPhone: A Multilingual Speech and Text Database developed at Karlsruhe University (2002) Proceedings of the ISCA International Conference on Spoken Language Processing (ICSLP2002), pp. 345-348. , Denver, Colorado USA, Sept. 16-20; Garofalo, J., Graff, D., Paul, D., Pallett, D., CSR-I (WSJ0) Complete, LDC93S6A, , ISBN 1-58563-006-3; CSR-II (WSJ1) Complete, Linguistic Data Consortium, 1994, LDC94S13A; Kominek, J., Black, A., CMU Arctic Databases for Speech Synthesis (2003) CMU Technical Report, , http://festvox.org/cmu-arctic, CMU-LTI-03-177; R.A. Fisher and F. Yates, Statistical tables for biological, in Agricultural and Medical Research (3rd edition), Oliver & Boyd. London, 1948, pp. 26-27},
correspondence_address1={Jin, Q.; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: qjin@ca.cmu.edu},
address={Merano},
isbn={9781424454792},
language={English},
abbrev_source_title={Proc. IEEE Workshop Autom. Speech Recognit. Underst., ASRU},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hsiao2009664,
author={Hsiao, R. and Schultz, T.},
title={Generalized discriminative feature transformation for speech recognition},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2009},
pages={664-667},
note={cited By 4; Conference of 10th Annual Conference of the International Speech Communication Association, INTERSPEECH 2009 ; Conference Date: 6 September 2009 Through 10 September 2009;  Conference Code:78744},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450179629&partnerID=40&md5=1dbbb57442d4f1e9046fae27ea1565f7},
affiliation={InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={We propose a new algorithm called Generalized Discriminative Feature Transformation (GDFT) for acoustic models in speech recognition. GDFT is based on Lagrange relaxation on a transformed optimization problem. We show that the existing discriminative feature transformation methods like feature space MMI/MPE (fMMI/MPE), region dependent linear transformation (RDLT), and a non-discriminative feature transformation, constrained maximum likelihood linear regression (CMLLR) are special cases of GDFT. We evaluate the performance of GDFT for Iraqi large vocabulary continuous speech recognition. Copyright © 2009 ISCA.},
author_keywords={Discriminative training;  Feature transformation;  Speech recognition},
keywords={Acoustic model;  Discriminative features;  Discriminative training;  Feature space;  Lagrange relaxation;  Large vocabulary continuous speech recognition;  Linear transformation;  Maximum likelihood linear regression;  Optimization problems, Continuous speech recognition;  Maximum likelihood, Speech communication},
references={Povey, D., Kanevsky, D., Kingsbury, B., Ramabhadran, B., Saon, G., Visweswariah, K., Boosted MMI for Model and Feature-space Discriminative Training (2008) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing; Povey, D., Kingsbury, B., Mangu, L., Saon, G., Soltau, H., Zweig, G., fMPE: Discriminatively Trained Features for Speech Recognition (2005) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing; Povey, D., Improvements to fMPE for discriminative training of features (2005) Proceedings of the INTERSPEECH; Zhang, B., Matsoukas, S., Schwartz, R., Discriminatively Trained Region Dependent Feature Transforms For Speech Recognition (2006) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing; Zhang, B., Matsoukas, S., Schwartz, R., Recent Progress on the Discriminative Region-dependent Transform for Speech Feature Extraction (2006) Proceedings of the INTERSPEECH; Hsiao, R., Tam, Y.C., Schultz, T., Generalized Baum-Welch Algorithm for Discriminative Training on Large Vocabulary Continuous Speech Recognition System (2009) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing; Gales, M.J.F., Maximum likelihood linear transformations for HMM-based speech recognition (1998) Computer Speech and Language, 12, pp. 75-98; Bach, N., Eck, M., Charoenpornsawat, P., Köhler, T., Stüker, S., Nguyen, T., Hsiao, R., Black, A.W., The CMU TransTac 2007 Eyes-free, and Hands-free Two-way Speech-to-speech Translation System (2007) Proceedings of the IWSLT},
correspondence_address1={Hsiao, R.; InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: wrhsiao@cs.cmu.edu},
address={Brighton},
issn={19909772},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li2009904,
author={Li, R. and Schultz, T. and Jin, Q.},
title={Improving speaker segmentation via speaker identification and text segmentation},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2009},
pages={904-907},
note={cited By 5; Conference of 10th Annual Conference of the International Speech Communication Association, INTERSPEECH 2009 ; Conference Date: 6 September 2009 Through 10 September 2009;  Conference Code:78744},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450207965&partnerID=40&md5=aeae284a94b3695442fffe9ef39149ed},
affiliation={InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Fakultät für Informatik, Universität Karlsruhe (TH), Germany},
abstract={Speaker segmentation is an essential part of a speaker diarization system. Common segmentation systems usually miss speaker change points when speakers switch fast. These errors seriously confuse the following speaker clustering step and result in high overall speaker diarization error rates. In this paper two methods are proposed to deal with this problem: The first approach uses speaker identification techniques to boost speaker segmentation. And the second approach applies text segmentation methods to improve the performance of speaker segmentation. Experiments on Quaero speaker diarization evaluation data shows that our methods achieve up to 45% relative reduction in the speaker diarization error and 64% relative increase in the speaker change detection recall rate over the baseline system. Moreover, both these two approaches can be considered as post-processing steps over the baseline segmentation, therefore, they can be applied in any speaker diarization systems. Copyright © 2009 ISCA.},
author_keywords={Speaker diarization;  Speaker identification;  Speaker segmentation;  Text segmentation},
keywords={Baseline segmentation;  Baseline systems;  Change-points;  Error rate;  Post processing;  Recall rate;  Relative reduction;  Segmentation system;  Speaker change detection;  Speaker clustering;  Speaker diarization;  Speaker identification;  Speaker segmentations;  Text segmentation, Error detection;  Speech communication, Loudspeakers},
references={Gauvain, J.-L., Lamel, L., Adda, G., Partitioning and transcription of broadcast news data (1998) Proc. Int. Conf. Spoken Lang. Process, 4, pp. 1335-1338. , Sydney, Australia, Dec; Meignier, S., Bonastre, J.-F., Fredouille, C., Merlin, T., Evolutive HMM for multispeaker tracking system (2000) Proc. IEEE Int. Conf. Acoust., Speech, Signal Process, 2, pp. 1201-1204. , Istanbul, Turkey, Jun; Barras, C., Zhu, X., Meignier, S., Gauvain, J.-L., Improving speaker diarization, (2004) Proc. Fall Rich Transcription Workshop (RT-04), , Palisades, NY, Nov; Zhu, X., Barras, C., Meignier, S., Gauvain, J.-L., Combining speaker identification and BIC for speaker diarization (2005) Proc. Eur. Conf. Speech Commun. Technol, pp. 2441-2444. , Lisbon, Portugal, Sep; Jin, Q. and Schultz, T., Speaker Segmentation and Clustering in Meetings, in ICSLP, 2004; Chen, S.S., Gopalakrishnam, P.S., Speaker, environment and channel change detection and clustering via the bayesian information criterion (1998) Proc. 1998 DARPA Broadcast News Transcription and Understanding Workshop, pp. 127-132. , Lansdowne, VA; Reynolds, D., Rose, R., Robust Text-independent Speaker Identification Using Gaussian Mixture Speaker Models (1995) IEEE Trans. on Speech and Audio Processing, 3, pp. 72-83; Reynolds, D., Quatieri, T., Dunn, R., Speaker Verification using adapted Gaussian MixtureModels (2000) Digital Signal Processing, 10 (1-3), pp. 19-41; Bimbot, F., Bonastre, J. F. , Fredouille, C. , Gravier, G. , Chagnolleau, Magrin I. , Meignier, S. , Merlin, T. , Ortega-Garcia, J. , Petrovska-Delacretaz, D., and Reynolds,D.A., A Tutorial on Text-Independent Speaker Verification, in EURASIP Journal on Applied Signal Processing, volumn 4, pp. 430-451, 2004; Han, Y., Hämäläinen, A., Boves, L., Trajectory clustering of syllable-length acoustic models for continuous speech recognition, In Proc. of ICASSP-2006, I, pp. 1169-1170, 2006; Gish, H., Ng, K., Parametric trajectory models for speech recognition (1996) Fourth International Conference on Spoken Language, ICSLP, pp. 466-469; Fragkou, P., Petridis, V., Kehagias, A., A Dynamic Programming Algorithm for Linear Text Segmentation (2004) J. Intell. Inf. Syst, 23 (2), pp. 179-197; The naked scientists, , http://www.thenakedscientists.com, online},
correspondence_address1={Li, R.; InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, United States; email: lirx@cs.cmu.edu},
address={Brighton},
issn={19909772},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Toth2009652,
author={Toth, A.R. and Wand, M. and Schultz, T.},
title={Synthesizing speech from electromyography using voice transformation techniques},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2009},
pages={652-655},
note={cited By 17; Conference of 10th Annual Conference of the International Speech Communication Association, INTERSPEECH 2009 ; Conference Date: 6 September 2009 Through 10 September 2009;  Conference Code:78744},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450186044&partnerID=40&md5=725a63b781ddc36f2371daeca13f1843},
affiliation={Cognitive Systems Lab., Universität Karlsruhe, Germany},
abstract={Surface electromyography (EMG) can be used to record the activation potentials of articulatory muscles while a person speaks. This technique could enable silent speech interfaces, as EMG signals are generated even when people pantomime speech without producing sound. Having effective silent speech interfaces would enable a number of compelling applications, allowing people to communicate in areas where they would not want to be overheard or where the background noise is so prevalent that they could not be heard. In order to use EMG signals in speech interfaces, however, there must be a relatively accurate method to map the signals to speech. Up to this point, it appears that most attempts to use EMG signals for speech interfaces have focused on Automatic Speech Recognition (ASR) based on features derived from EMG signals. Following the lead of other researchers who worked with Electro-Magnetic Articulograph (EMA) data and Non-Audible Murmur (NAM) speech, we explore the alternative idea of using Voice Transformation (VT) techniques to synthesize speech from EMG signals. With speech output, both ASR systems and human listeners can directly use EMG-based systems. We report the results of our preliminary studies, noting the difficulties we encountered and suggesting areas for future work. Copyright © 2009 ISCA.},
author_keywords={Electromyography;  Silent speech;  Speech synthesis;  Voice transformation},
keywords={Activation potential;  Automatic speech recognition;  Background noise;  EMG signal;  Human listeners;  Non-audible murmur;  Silent speech;  Speech interface;  Speech output;  Surface electromyography;  Transformation techniques, Electromyography;  Remelting;  Speech communication;  Speech synthesis;  Synthesis (chemical), Speech recognition},
references={Childers, D.G., Yegnanarayana, B., Wu, K., Voice conversion: Factors responsible for quality (1985) ICASSP 1985, pp. 748-751; Toda, T., Black, A., Tokuda, K., Mapping from articulatory movements to vocal tract spectrum with gaussian mixture model for articulatory speech synthesis (2004) 5th ISCA Speech Synthesis Workshop, , June; Nakagiri, M., Toda, T., Kashioka, H., Shikano, K., Improving body transmitted unvoiced speech with statistical voice conversion (2006) Interspeech 2006, pp. 2270-2273. , Pittsburgh, PA; Stylianou, Y., Cappé, O., Moulines, E., Statistical methods for voice quality transformation (1995) Proc. EUROSPEECH95, pp. 447-450. , Madrid, Spain; Kain, A., High resolution voice transformation, (2001), Ph.D. dissertation, OGI School of Science and Engineering, OHSU; Al-Bawab, Z., Raj, B., Stern, R., Analysis-by-synthesis features for speech recognition (2008) Proc. ICASSP2008, , Las Vegas, NV, USA, Mar; Black, A., Lenzo, K., (2000) Building voices in the Festival speech synthesis system, , http://festvox.org/bsv; Toda, T., Black, A., Tokuda, K., Spectral conversion based on maximum likelihood estimation considering global variance of converted parameter (2005) Proc. ICASSP2005, 1, pp. 9-12. , Philadelphia, PA, USA, Mar; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session Independent Non-Audible Speech Recognition Using Surface Electromyography (2005) Proc. ASRU; M. Dietrich and K. V. Abbott, Psychobiological framework of Stress and Voice: A Psychobiological Framework for Studying Psychological Stress and its Relation to Voice Disorders, In: K. Izdebski (Ed.): Emotions in the Human Voice (II, Clinical Evidence, pp. 159-178). San Diego, Plural Publishing, pp. 159 -178, 2007; Dietrich, M., The Effects of Stress Reactivity on Extralaryngeal Muscle Tension in Vocally Normal Participants as a Function of Personality, (2008), Ph.D. dissertation, University of Pittsburgh; (1996) English Broadcast News Speech (HUB4), , Linguistic Data Consortium; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards Continuous Speech Recognition using Surface Electromyography, (2006) Proc. Interspeech, , Pittsburgh, PA, Sep; A. Wrench, The MOCHA-TIMIT articulatory database, 1999, queen Margaret University College, http://www.cstr.ed.ac.uk/artic/mocha.html; Schultz, T., Wand, M., Modeling Coarticulation in Large Vocabulary EMG-based Speech Recognition (2009) Speech Communication Journal, , to Appear},
correspondence_address1={Toth, A. R.; Cognitive Systems Lab., Universität KarlsruheGermany; email: atoth@cs.cmu.edu},
address={Brighton},
issn={19909772},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wölfel2009912,
author={Wölfel, M. and Yang, Q. and Jin, Q. and Schultz, T.},
title={Speaker identification using warped MVDR cepstral features},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2009},
pages={912-915},
note={cited By 11; Conference of 10th Annual Conference of the International Speech Communication Association, INTERSPEECH 2009 ; Conference Date: 6 September 2009 Through 10 September 2009;  Conference Code:78744},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450179598&partnerID=40&md5=e6daa28ddfdbe749c8d79ceae71fbf72},
affiliation={ZKM/Center for Art and Media, Germany; Fakultät für Informatik, Universität Karlsruhe (TH), Germany; Language Technologies Institute, Carnegie Mellon University, United States},
abstract={It is common practice to use similar or even the same feature extraction methods for automatic speech recognition and speaker identification. While the front-end for the former requires to preserve phoneme discrimination and to compensate for speaker differences to some extend, the front-end for the latter has to preserve the unique characteristics of individual speakers. It seems, therefore, contradictory to use the same feature extraction methods for both tasks. Starting out from the common practice we propose to use warped minimum variance distortionless response (MVDR) cepstral coefficients, which have already been demonstrated to perform superior for automatic speech recognition in particular under adverse conditions. Replacing the widely used mel-frequency cepstral coefficients by WMVDR cepstral coefficients improves the speaker identification accuracy by up to 24% relative. We found that the optimal choice of the model order within the WMVDR framework differs between speech recognition and speaker recognition, confirming our intuition that the two different tasks indeed require different feature extraction strategies. Copyright © 2009 ISCA.},
keywords={Automatic speech recognition;  Cepstral coefficients;  Cepstral features;  Feature extraction methods;  Mel-frequency cepstral coefficients;  Minimum variance distortionless response;  Model order;  Optimal choice;  Speaker identification;  Speaker recognition, Feature extraction;  Loudspeakers;  Speech communication;  Weaving, Speech recognition},
references={Reynolds, D.A., Speaker identification and verification using gaussian mixture speaker models (1995) Speech Communication, 17, pp. 91-108; F. Bimbot, J. F. Bonastre, C. Fredouille, G. Gravier, Magrin I. Chagnolleau, S. Meignier, T. Merlin, Ortega J. Garcia, Petrovska Delacretaz, and Reynolds, A tutorial on text-independent speaker verification, EURASIP Journal on Applied Signal Processing, 4, pp. 430-451, 2004; Lee, C.H., Soong, F.K., Paliwal, K.K., (1999) Automatic Speech and Speaker Recognition: Advanced Topics, , Kluwer Academic Publishers, Norwell, MA, USA; Furui, S., Towards robust speech recognition under adverse conditions (1992) ESCA Workshop on Speech Processing in Adverse Conditions, pp. 31-42; Davis, S.B., Mermelstein, P., Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences (1980) IEEE Trans. on Acoustics, Speech, and Signal Processing, 28, pp. 357-366. , Aug; Hermansky, H., Perceptual linear predictive (PLP) analysis of speech (1990) Jour. of Acoustic Society of America, 87 (4), pp. 1738-1752. , Apr; Murthi, M.N., Rao, B.D., All-pole modeling of speech based on the minimum variance distortionless response spectrum (2000) IEEE Trans. on Speech and Audio Processing, 8 (3), pp. 221-239. , May; Wölfel, M., McDonough, J.W., Minimum variance distortionless response spectral estimation, review and refinements (2005) IEEE Signal Processing Magazine, 22 (5), pp. 117-126. , Sept; Wölfel, M., McDonough, J.W., (2009) Distant Speech Recognition, , John Wiley & Sons; Jin, Q., Schultz, T., Robust far-field speaker recognition under mismatched conditions (2008) Proc. of Interspeech; Jin, Q., Kumar, K., Schultz, T., Stern, R., Compensation approaches for far-field speaker identification (2008) NIST SRE Workshop; The naked scientists, , http://www.thenakedscientists.com, online; Wölfel, M., Enhanced speech features by single channel joint compensation of noise and reverberation (2009) IEEE Trans. on Audio, Speech, and Language Processing, 17 (2), pp. 312-323},
correspondence_address1={Wölfel, M.; ZKM/Center for Art and MediaGermany; email: woelfel@zkm.de},
address={Brighton},
issn={19909772},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wand2009648,
author={Wand, M. and Jou, S.-C.S. and Toth, A.R. and Schultz, T.},
title={Impact of different speaking modes on EMG-based speech recognition},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2009},
pages={648-651},
note={cited By 8; Conference of 10th Annual Conference of the International Speech Communication Association, INTERSPEECH 2009 ; Conference Date: 6 September 2009 Through 10 September 2009;  Conference Code:78744},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450169589&partnerID=40&md5=50e8c464222d450460116cf7f6996623},
affiliation={Cognitive Systems Lab., University of Karlsruhe, Germany; ATC, ICL, Industrial Technology Research Institute, Taiwan},
abstract={We present our recent results on speech recognition by surface electromyography (EMG), which captures the electric potentials that are generated by the human articulatory muscles. This technique can be used to enable Silent Speech Interfaces, since EMG signals are generated even when people only articulate speech without producing any sound. Preliminary experiments have shown that the EMG signals created by audible and silent speech are quite distinct. In this paper we first compare various methods of initializing a silent speech EMG recognizer, showing that the performance of the recognizer substantially varies across different speakers. Based on this, we analyze EMG signals from audible and silent speech, present first results on how discrepancies between these speaking modes affect EMG recognizers, and suggest areas for future work. Copyright © 2009 ISCA.},
author_keywords={Articulation;  Silent speech;  Speech recognition;  Surface electromyography},
keywords={Articulation;  EMG signal;  Speech interface;  Surface electromyography, Electric potential;  Electromyography;  Speech communication, Speech recognition},
references={Jou, S.-C., Schultz, T., Waibel, A., Whispery Speech Recognition Using Adapted Articulatory Features (2005) Proc. ICASSP; Nakajima, Y., Kashioka, H., Shikano, K., Campbell, N., Non-Audible Murmur Recognition (2003) Proc. Eurospeech; Hueber, T., Chollet, G., Denby, B., Dreyfus, G., Stone, M., Continuous-Speech Phone Recognition from Ultrasound and Optical Images of the Tongue and Lips (2007) Proc. Interspeech, pp. 658-661; Jorgensen, C., Binsted, K., Web Browser Control Using EMG Based Sub Vocal Speech Recognition (2005) Proceedings of the 38th Hawaii International Conference on System Sciences; Chan, A., Englehart, K., Hudgins, B., Lovely, D., Hidden Markov Model Classification of Myolectric Signals in Speech (2002) Engineering in Medicine and Biology Magazine, IEEE, 21 (9), pp. 143-146; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards Continuous Speech Recognition using Surface Electromyography, (2006) Proc. Interspeech, , Pittsburgh, PA, Sep; Schultz, T., Wand, M., Modeling Coarticulation in Large Vocabulary EMG-based Speech Recognition (2009) Speech Communication Journal, , to Appear; Wand, M., Schultz, T., Towards Speaker-Adaptive Speech Recognition Based on Surface Electromyography (2009) Proc. Biosignals; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session Independent Non-Audible Speech Recognition Using Surface Electromyography (2005) Proc. ASRU; Guenther, F.H., Hampson, M., Johnson, D., A Theoretical Investigation of Reference Frames for the Planning of Speech Movements (1998) Psych.Rev, 105, pp. 611-633; Perkell, J., Matthies, M., Lane, H., Guenther, F., Wilhelms-Tricaricoa, R., Wozniaka, J., Guioda, P., Speech Motor Control: Acoustic Goals, Saturation Effects, Auditory Feedback and Internal Models (1997) Speech Communication Journal, 22, pp. 227-250; Jovicic, S.T., Saric, Z., Acoustic Analysis of Consonants in Whispered Speech (2006) Journal of Voice, 22 (3), pp. 263-274; Jones, J.A., Striemer, D., Speech Disruption During Delayed Auditory Feedback with Simultaneous Visual Feedback (2007) J Acoust Soc Am, 122, pp. 135-141; Dietrich, M., The Effects of Stress Reactivity on Extralaryngeal Muscle Tension in Vocally Normal Participants as a Function of Personality, (2008), Ph.D. dissertation, University of Pittsburgh; Jou, S.-C.S., Schultz, T., Waibel, A., Continuous Electromyographic Speech Recognition with a Multi-Stream Decoding Architecture (2007) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP-2007), , Honolulu, Hawaii, US, April 15-20; Bahl, L.R., de Souza, P.V., Gopalakrishnan, P.S., Nahmoo, D., Picheny, M.A., Decision Trees for Phonological Rules in Continuous Speech (1991) Proc. ICASSP; Toth, A., Wand, M., Schultz, T., Speech Synthesis from EMG Signals (2009) Proc. Interspeech, , Brighton, UK},
correspondence_address1={Wand, M.; Cognitive Systems Lab., University of KarlsruheGermany; email: mwand@ira.uka.de},
address={Brighton},
issn={19909772},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tam20094821,
author={Tam, Y.-C. and Schultz, T.},
title={Incorporating monolingual corpora into bilingual latent semantic analysis for crosslingual LM adaptation},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2009},
pages={4821-4824},
doi={10.1109/ICASSP.2009.4960710},
art_number={4960710},
note={cited By 2; Conference of 2009 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2009 ; Conference Date: 19 April 2009 Through 24 April 2009;  Conference Code:76748},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349199892&doi=10.1109%2fICASSP.2009.4960710&partnerID=40&md5=0038872f6f2cb0366769d6747e894dca},
affiliation={InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={The major limitation in bilingual latent semantic analysis (bLSA) is the requirement of parallel training corpora. Motivated by semi-supervised learning, we propose a cluster-based bLSA training approach to incorporate monolingual corpora. Treating each parallel document pair as centroids of the parallel document clusters, each monolingual document is associated to the closest centroid according to their topic similarity. The resulting parallel document clusters are used as constraints to enforce a one-to-one topic correspondence in variational EM. Slight performance improvement in crosslingual language model adaptation is observed compared to the baseline without monolingual corpora. ©2009 IEEE.},
author_keywords={Bilingual LSA;  Crosslingual LM adaptation;  Crosslingual word trigger;  Monolingual corpora},
keywords={Bilingual latent semantic analysis;  Bilingual LSA;  Cluster-based;  Cross-lingual;  Cross-lingual language model adaptation;  Crosslingual LM adaptation;  Crosslingual word trigger;  Monolingual corpora;  Parallel training;  Performance improvements;  Semi-supervised learning;  Topic similarity;  Variational EM, Acoustics;  Computational linguistics;  Semantics;  Signal processing;  Supervised learning, Lagrange multipliers},
references={Tam, Y.C., Lane, I., Schultz, T., Bilingual LSA-based LM adaptation for spoken language translation (2007) Proc. of ACL; (2006) Semi-Supervised Learning, , O. Chapelle, B. Schölkopf, and A. Zien, Eds, MIT Press, Cambridge, MA; Kim, W., Khudanpur, S., Lexical triggers and latent semantic analysis for cross-lingual language model adaptation (2004) ACM Transactions on Asian Language Information Processing, 3 (2), pp. 94-112. , June; Zhao, B., Xing, E.P., BiTAM: Bilingual topic admixture models for word alignment (2006) Proc. of ACL; C. Quirk, R. U., and A. Menezes, Generative models of noisy translations with applications to parallel fragment extraction, in Proc. of MT Summit, 2007; Blei, D., Ng, A., Jordan, M., Latent Dirichlet Allocation (2003) Journal of Machine Learning Research, pp. 1107-1135; Kneser, R., Peters, J., Klakow, D., Language model adaptation using dynamic marginals (1997) Proceedings of European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1971-1974; Tam, Y.C., Schultz, T., Correlated latent semantic model for unsupervised language model adaptation (2007) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP); Vogel, S., PESA: Phrase pair extraction as sentence splitting (2005) Proc. of MT Summit; Vogel, S., SMT decoder dissected: Word reordering (2003) Proc. of ICNLPKE},
correspondence_address1={Tam, Y.-C.; InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
sponsors={Institute of Electrical and Electronics Engineers; Signal Processing Society},
address={Taipei},
issn={15206149},
isbn={9781424423545},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Fuhs20094589,
author={Fuhs, M.C. and Jin, Q. and Schultz, T.},
title={Detecting bandlimited audio in broadcast television shows},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2009},
pages={4589-4592},
doi={10.1109/ICASSP.2009.4960652},
art_number={4960652},
note={cited By 0; Conference of 2009 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2009 ; Conference Date: 19 April 2009 Through 24 April 2009;  Conference Code:76748},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349199913&doi=10.1109%2fICASSP.2009.4960652&partnerID=40&md5=fe42d7a0503044bb0e312b9cae7a9154},
affiliation={InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={For TV and radio shows containing narrowband speech, Speech-to-text (STT) accuracy on the narrowband audio can be improved by using an acoustic model trained on acoustically matched data. To selectively apply it, one must first be able to accurately detect which audio segments are narrow-band. The present paper explores two different bandwidth classification approaches: a traditional Gaussian mixture model (GMM) approach and a spline-based classifier that categorizes audio segments based on their power spectra. We focus on shows found in the DARPA GALE Mandarin training and test sets, where the ratio of wideband to narrowband shows is very large. In this setting, the spline-based classifier reduces the number of misclassified wideband segments by up to 95% relative to the GMM-based classifier for the same number of misclassified narrowband segments. ©2009 IEEE.},
author_keywords={Pattern classification;  Speech processing;  Speech recognition;  Telephony},
keywords={Acoustic model;  Bandlimited;  Broadcast television;  Classification approach;  Gaussian Mixture Model;  Narrow bands;  Pattern classification;  Power-spectra;  Telephony;  Test sets;  Wide-band, Audio acoustics;  Broadcasting;  Classifiers;  Learning systems;  Magnetostrictive devices;  Signal detection;  Signal processing;  Speech processing;  Splines, Speech recognition},
references={Hain, T., Johnson, S., Tuerk, A., Woodland, P., Young, S., Segment generation and clustering in the htk broadcast news transcription system (1998) Proc. DARPA Broadcast News Transcription and Understanding Workshop; Reynolds, D.A., Torres-Carrasquillo, P., Approaches and applications of audio diarization (2005) Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing; Zhu, X., Barras, C., Meignier, S., Gauvain, J.-L., Combining speaker identification and BIC for speaker diarization (2005) Proc. Interspeech; Hsiao, R., Fuhs, M., Tam, Y.-C., Jin, Q., Schultz, T., The CMU-InterACT 2008 Mandarin transcription system (2008) Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing},
correspondence_address1={Fuhs, M. C.; InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: fuhs@cs.cmu.edu},
sponsors={Institute of Electrical and Electronics Engineers; Signal Processing Society},
address={Taipei},
issn={15206149},
isbn={9781424423545},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hsiao20093769,
author={Hsiao, R. and Tam, Y.-C. and Schultz, T.},
title={Generalized Baum-Welch algorithm for discriminative training on large vocabulary continuous speech recognition system},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2009},
pages={3769-3772},
doi={10.1109/ICASSP.2009.4960447},
art_number={4960447},
note={cited By 4; Conference of 2009 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2009 ; Conference Date: 19 April 2009 Through 24 April 2009;  Conference Code:76748},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349197696&doi=10.1109%2fICASSP.2009.4960447&partnerID=40&md5=301f2b751a8b34f696db5e5ec4c22eaa},
affiliation={InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={We propose a new optimization algorithm called Generalized Baum Welch (GBW) algorithm for discriminative training on hidden Markov model (HMM). GBW is based on Lagrange relaxation on a transformed optimization problem. We show that both Baum-Welch (BW) algorithm for ML estimate ofHMMparameters, and the popular extended Baum-Welch (EBW) algorithm for discriminative training are special cases of GBW.We compare the performance of GBW and EBW for Farsi large vocabulary continuous speech recognition (LVCSR). ©2009 IEEE.},
author_keywords={Discriminative training;  Speech recognition},
keywords={Baum-Welch;  Baum-Welch algorithms;  Discriminative training;  Lagrange relaxation;  Large vocabulary continuous speech recognition;  ML estimate;  Optimization algorithms;  Optimization problems, Acoustics;  Continuous speech recognition;  Electron beam welding;  Electron cyclotron resonance;  Hidden Markov models;  Signal processing;  Vocabulary control, Algorithms},
references={Valtchev, V., Odell, J.J., Woodland, P.C., Young, S.J., MMIE Training of Large Vocabulary Recognition Systems (1997) Speech Communication, 22 (4), pp. 303-314; Povey, D., (2003) Discriminative Training for Large Vocabulary Speech Recognition, , Ph.D. thesis, Cambridge University Engineering Dept; Normandin, Y., Morgera, S.D., An Improved MMIE Training Algorithm for Speaker-independent, Small Vocabulary, Continuous Speech Recognition (1991) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing; Sha, F., Saul, L.K., Large Margin Hidden Markov Models for Automatic Speech Recognition (2007) Advances in Neural Information Processing Systems, 19, pp. 1249-1256; Boyd, S., Vandenberghe, L., (2004) Convex Optimization, , Cambridge University Press},
correspondence_address1={Hsiao, R.; InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: wrhsiao@cs.cmu.edu},
sponsors={Institute of Electrical and Electronics Engineers; Signal Processing Society},
address={Taipei},
issn={15206149},
isbn={9781424423545},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jin20093909,
author={Jin, Q. and Toth, A.R. and Schultz, T. and Black, A.W.},
title={Voice convergin: Speaker de-identification by voice transformation},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2009},
pages={3909-3912},
doi={10.1109/ICASSP.2009.4960482},
art_number={4960482},
note={cited By 24; Conference of 2009 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2009 ; Conference Date: 19 April 2009 Through 24 April 2009;  Conference Code:76748},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349216553&doi=10.1109%2fICASSP.2009.4960482&partnerID=40&md5=5dd508c4cbf834e745eb9526c4ac4d7b},
affiliation={Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={Speaker identification might be a suitable answer to prevent unauthorized access to personal data. However we also need to provide solutions to secure transmission of spoken information. This challenge divides into two major aspects. First, the secure transmission of the content of the spoken input and second the secure transmission of the identity of the speaker. In this paper we concentrate on the latter, i.e. how to securely transmit information via voice without revealing the identity of the speaker to unauthorized listeners. In order to make the first steps toward solving this problem we study in this paper the potential of voice transformation for speaker de-identification. We use two speaker identification approaches to verify the success of de-identification with voice transformation, a GMM-based and a Phonetic approach, and study different voice transformation strategies to disguise speaker identity information while preserving understandability. ©2009 IEEE.},
author_keywords={Secure spoken information transmission;  Speaker de-identification;  Voice transformation},
keywords={De-identification;  Identity information;  Personal data;  Secure spoken information transmission;  Secure transmission;  Speaker de-identification;  Speaker identification;  Understandability;  Voice transformation, Acoustics;  Loudspeakers, Signal processing},
references={Jin, Q., Toth, A., Black, A., Schultz, T., (2008) Is Voice Transformation a Threat to Speaker Identification, , ICASSP; Jin, Q., Schultz, T., Waibel, A., Far-field Speaker Recognition (2007) IEEE Transactions on Audio, Speech, and Language Processing, 15 (7), pp. 2023-2032; Pellom, B., Hansen, J., An Experimental Study of Speaker Verification Sensitivity to Computer Voice-Altered Imposters (1999) ICASSP, pp. 837-840; T. Masuko, K. Tokuda, and T. Kobayashi, Imposture using Synthetic Speech against Speaker Verification based on Spectrum and Pitch, ICSLP, 2000; Genoud, D., Chollet, G., (1998) Speech Pre-Processing Against Intentional Imposture in Speaker Recognition, , ICASLP; Kajarekar, S., Bratt, H., Shriberg, E., Leon, R., A Study of Intentional Voice Modifications for Evading Automatic Speaker Recognition (2006) Odyssey; Bonastre, J., Matrouf, D., Fredouille, C., Artificial Impostor Voice Transformation Effects on False Acceptance Rates (2007) Interspeech, pp. 2053-2056; Gross, R., Sweeney, L., Torre, F., Baker, S., Model-Based Face De-Identification (2006) IEEE Workshop on Privacy Research in Vision; Uzuner, O., Luo, Y., Szolovits, P., Evaluating the State-ofthe-Art in Automatic De-identification (2007) Journal of the American Medical Informatics Association, 14 (5); (2000) FestVox: Building Synthetic Voices, , http://festvox.org; Garofalo, J., Graff, D., Paul, D., Pallett, D., CSR-I (WSJ0) Complete, LDC93S6A, , ISBN 1-58563-006-3; Toda, T., Black, A.W., Tokuda, K., Voice Conversion Based on Maximum Likelihood Estimation of Spectral Parameter Trajectory (2007) IEEE TASLP, 15, pp. 2222-2235. , Nov; Imai, S., Cepstral analysis synthesis on the melfrequency scale (1983) Proceedings of ICASSP, 83, pp. 93-96; Reynolds, D., Rose, R., Robust Text-independent Speaker Identification Using Gaussian Mixture Speaker Models (1995) IEEE Trans. on Speech and Audio Processing, 3, pp. 72-83; Reynolds, D., Quatieri, T., Dunn, R., Speaker Verification using adapted Gaussian Mixture Models (2000) Digital Signal Processing, 10 (1-3), pp. 19-41; Doddington, G., Speaker Recognition based on Idiolectal Differences between Speakers Eurospeech, 2001; Jin, Q., Schultz, T., Waibel, A., Phonetic Speaker Identification (2002) ICSLP, pp. 1345-1348; Adami, A., Mihaescu, R., Reynolds, D., Godfrey, J., Modeling Prosodic Dynamics for Speaker Recognition (2003) ICASSP, pp. 788-791; Verhelst, W., Roelands, M., An overlap-add technique based on waveform similarity (WSOLA) for high quality timescale modification of speech (1993) ICASSP, pp. 554-557},
correspondence_address1={Jin, Q.; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
sponsors={Institute of Electrical and Electronics Engineers; Signal Processing Society},
address={Taipei},
issn={15206149},
isbn={9781424423545},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li20094201,
author={Li, H. and Ma, B. and Lee, K.-A. and Sun, H. and Zhu, D. and Khe, C.S. and You, C. and Tong, R. and Kärkkäinen, I. and Huang, C.-L. and Pervouchine, V. and Guo, W. and Li, Y. and Dai, L. and Nosratighods, M. and Tharmarajah, T. and Epps, J. and Ambikairajah, E. and Chng, E.-S. and Schultz, T. and Jin, Q.},
title={The I4U system in NIST 2008 speaker recognition evaluation},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2009},
pages={4201-4204},
doi={10.1109/ICASSP.2009.4960555},
art_number={4960555},
note={cited By 26; Conference of 2009 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2009 ; Conference Date: 19 April 2009 Through 24 April 2009;  Conference Code:76748},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349203858&doi=10.1109%2fICASSP.2009.4960555&partnerID=40&md5=c1f2c8027992cc2f19434ded3e00f446},
affiliation={Institute for Infocomm Research (IIR), Singapore, Singapore; University of Science and Technology of China (USTC), China; University of New South Wales (UNSW), Australia; Nanyang Technological University (NTU), Singapore, Singapore; Carnegie Mellon University (CMU), United States},
abstract={This paper describes the performance of the I4U speaker recognition system in the NIST 2008 Speaker Recognition Evaluation. The system consists of seven subsystems, each with different cepstral features and classifiers. We describe the I4U Primary system and report on its core test results as they were submitted, which were among the bestperforming submissions. The I4U effort was led by the Institute for Infocomm Research, Singapore (IIR), with contributions from the University of Science and Technology of China (USTC), the University of New South Wales, Australia (UNSW), Nanyang Technological University, Singapore (NTU) and Carnegie Mellon University, USA (CMU). ©2009 IEEE.},
author_keywords={Channel variability;  Classifier;  Speaker recognition;  System fusion},
keywords={Carnegie Mellon University;  Cepstral features;  Channel variability;  Infocomm;  Nanyang Technological University , Singapore;  Primary systems;  Science and Technology;  Singapore;  Speaker recognition;  Speaker recognition system;  System fusion;  Test results;  University of New South Wales, Acoustics;  Classifiers;  Learning systems;  Signal processing, Speech recognition},
references={Reynolds, D.A., Quatieri, T.F., Dunn, R.B., Speaker verification using adapted Gaussian mixture models (2000) Digital Signal Processing, 10, pp. 19-41; Campbell, W.M., Campbell, J.P., Reynolds, D.A., Singer, E., Torres-Carrasquillo, P.A., Support vector machines for speaker and language recognition (2006) Computer Speech and Language, 20, pp. 210-229; Campbell, W.M., Sturim, D., Reynolds, D.A., SVM based speaker verification using a GMM supervector kernel and NAP variability compensation (2006) Proc. ICASSP; Stolcke, A., Ferrer, L., Kajarekar, S., Shriberg, E., Venkataraman, A., MLLR transforms as features in speaker recognition (2005) Proc. ICASSP; Zhu, D., Ma, B., Li, H., Using MAP estimation of feature transformation for speaker recognition Proc. Interspeech, 2008; Lee, K.A., You, C., Li, H., Kinnunen, T., Zhu, D., Characterizing speech utterances for speaker verification with sequence kernel SVM (2008) Proc. Interspeech; You, C.H., Lee, K.A., Li, H., An SVM kernel with GMM-supervector based on the Bhattacharyya distance for speaker recognition (2009) IEEE Signal Processing Letters, 16 (1), pp. 49-52. , Jan; Huang, C.-L., Ma, B., Wu, C.-H., Mak, B., Li, H., Robust speaker verification using short-time frequency with long-time window and fusion of multi-resolutions (2008) Proc. Interspeech; Thiruvaran, T., Ambikairajah, E., Epps, J., Extraction of FM components from speech signals using an all-pole model (2008) IET Electronics Letters, 44 (6), pp. 449-450. , March; Castaldo, F., Colibro, D., Dalmasso, E., Laface, P., Vair, C., Compensation of Nuisance Factors for Speaker and Language Recognition (2007) IEEE Transactions on ASLP, 15 (7); Solomonoff, A., Campbell, W.M., Boardman, I., Advances in channel compensation for SVM speaker recognition (2005) Proc. ICASSP; Kenny, P., Boulianne, G., Ouellet, P., Dumouchel, P., Joint factor analysis versus eigenchannels in speaker recognition (2007) IEEE Trans. Audio Speech and Language Processing, 15 (4), pp. 1435-1447; Lucey, S., Chen, T., Improved Speaker Verification through Probabilistic Subspace Adaptation (2003) Proc. Eurospeech, pp. 2021-2024; Auckenthaler, R., Carey, M., Lloyd-Thomas, H., Score normalization for text-independent speaker verification systems (2000) Digital Signal Processing, 10 (1-3), pp. 42-54. , Jan; Li, K.P., Porter, J.E., Normalizations and selection of speech segments for speaker recognition scoring (1988) Proc. ICASSP, 1, pp. 595-598},
correspondence_address1={Li, H.; Institute for Infocomm Research (IIR), Singapore, Singapore; email: hli@i2r.a-star.edu.sg},
sponsors={Institute of Electrical and Electronics Engineers; Signal Processing Society},
address={Taipei},
issn={15206149},
isbn={9781424423545},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wand2009155,
author={Wand, M. and Schultz, T.},
title={Towards speaker-adaptive speech recognition based on surface electromyography},
journal={BIOSIGNALS 2009 - Proceedings of the 2nd International Conference on Bio-Inspired Systems and Signal Processing},
year={2009},
pages={155-162},
note={cited By 16; Conference of 2nd International Conference on Bio-Inspired Systems and Signal Processing, BIOSIGNALS 2009 ; Conference Date: 14 January 2009 Through 17 January 2009;  Conference Code:76662},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650544129&partnerID=40&md5=1e463747e49107282486f0f0a72c0e9f},
affiliation={Cognitive Systems Lab, University of Karlsruhe, Am Fasanengarten 5, Karlsruhe, Germany},
abstract={We present our recent advances in silent speech interfaces using electromyographic signals that capture the movements of the human articulatory muscles at the skin surface for recognizing continuously spoken speech. Previous systems were limited to speaker- and session-dependent recognition tasks on small amounts of training and test data. In this paper we present speaker-independent and speaker-adaptive training methods which for the first time allows us to use a large corpus of data from many speakers to reliably train acoustic models. On this corpus we compare the performance of speaker-dependent and speaker-independent acoustic models, carry out model adaptation experiments, and investigate the impact of the amount of training data on the overall system performance. In particular, since our data corpus is relatively large compared to previous studies, we are able for the first time to train an EMG recognizer with context-dependent acoustic models. We show that like in acoustic speech recognition, context-dependent modeling significantly increases the recognition performance.},
author_keywords={Electromyography;  Silent speech;  Speech recognition},
keywords={Acoustic model;  Adaptive training;  Context dependent;  Electromyographic signal;  Model Adaptation;  Recognition performance;  Silent speech;  Skin surfaces;  Speech interface;  Surface electromyography;  Test data;  Training data, Acoustics;  Electromyography;  Feature extraction;  Signal processing, Speech recognition},
references={Chan, A.D.C., Englehart, K., Hudgins, B., Lovely, D.F., Hidden Markov model classification of myoelectric signals in speech (2002) IEEE Engineering in Medicine and Biology Magazine, 21 (5), pp. 143-146. , DOI 10.1109/MEMB.2002.1044184; Dietrich, M., (2008) The Effects of Stress Reactivity on Ex-tralaryngeal Muscle Tension in Vocally Normal Participants as a Function of Personality, , PhD thesis, University of Pittsburgh; Dietrich, M., Abbott, K.V., Psychobiological framework of stress and voice: A psychobiological framework for studying psychological stress and its relation to voice disorders (2007) Emotions in the Human Voice, 2, pp. 159-178. , K. Izdebski (Ed.), San Diego, Plural Publishing, Clinical Evidence; Finke, M., Rogina, I., Wide context acoustic modeling in read vs. spontaneous speech (1997) Proc. ICASSP, 3, pp. 1743-1746; Hueber, T., Chollet, G., Denby, B., Dreyfus, G., Stone, M., Continuous-speech phone recognition from ultrasound and optical images of the tongue and lips (2007) Proc. Interspeech, pp. 658-661; Jorgensen, C., Binsted, K., Web browser control using EMG based sub vocal speech recognition (2005) Proceedings of the Annual Hawaii International Conference on System Sciences, p. 294. , Proceedings of the 38th Annual Hawaii International Conference on System Sciences - Abstracts and CD-ROM of Full Papers; Jou, S.-C., Maier-Hein, L., Schultz, T., Waibel, A., Articulatory feature classification using surface electromyography (2006) ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings, 1, pp. I605-I608. , 1660093, Speech and Spoken Language Processing, 2006 IEEE International Conference on Acoustics, Speech, and Signal Processing - Proceedings; Jou, S.-C., Schultz, T., Waibel, A., Whispery speech recognition using adapted articulatory features (2005) ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings, 1, pp. I1009-I1012. , DOI 10.1109/ICASSP.2005.1415287, 1415287, 2005 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP '05 - Proceedings - Speech Processing; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography (2006) Proc. Interspeech, , Pittsburgh, PA; Kingsbury, N.G., A dual-tree complex wavelet transform with improved orthogonality and symmetry properties (2000) Proc. IEEE Conf. on Image Processing, , Vancouver; Leggetter, C.J., Woodland, P.C., Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models (1995) Computer Speech and Language, 9, pp. 171-185; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) Proceedings of ASRU 2005: 2005 IEEE Automatic Speech Recognition and Understanding Workshop, 2005, pp. 307-312. , DOI 10.1109/ASRU.2005.1566521, 1566521, Proceedings of ASRU 2005: 2005 IEEE Automatic Speech Recognition and Understanding Workshop; Shensa Mark, J., The discrete wavelet transform: Wedding the atrous and Mallat algorithms (1992) IEEE Transactions on Signal Processing, 40 (10), pp. 2464-2482. , DOI 10.1109/78.157290; Walliczek, M., Kraft, F., Jou, S.-C., Schultz, T., Waibel, A., Sub-word unit based non-audible speech recognition using surface electromyography (2006) Proc. Interspeech, , Pittsburgh, PA; Wand, M., Jou, S.-C.S., Schultz, T., Wavelet-based front-end for electromyographic speech recognition (2007) Proc. Interspeech; Yu, H., Waibel, A., Streamlining the front end of a speech recognizer (2000) Proc. ICSLP.},
correspondence_address1={Wand, M.; Cognitive Systems Lab, University of Karlsruhe, Am Fasanengarten 5, Karlsruhe, Germany; email: mwand@ira.uka.de},
address={Porto},
isbn={9789898111654},
language={English},
abbrev_source_title={BIOSIGNALS - Proc. Int. Conf. Bio-Inspired Syst. Signal Process.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Porbadnigk2009376,
author={Porbadnigk, A. and Wester, M. and Calliess, J.-P. and Schultz, T.},
title={EEG-based speech recognition:Impact of temporal effects},
journal={BIOSIGNALS 2009 - Proceedings of the 2nd International Conference on Bio-Inspired Systems and Signal Processing},
year={2009},
pages={376-381},
note={cited By 33; Conference of 2nd International Conference on Bio-Inspired Systems and Signal Processing, BIOSIGNALS 2009 ; Conference Date: 14 January 2009 Through 17 January 2009;  Conference Code:76662},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650517041&partnerID=40&md5=15e813f9ccb14849a4f5c1b198f323c7},
affiliation={Cognitive Systems Lab, University of Karlsruhe, Am Fasanengarten 5, 76131 Karlsruhe, Germany},
abstract={In this paper, we investigate the use of electroencephalograhic signals for the purpose of recognizing unspoken speech. The term unspoken speech refers to the process in which a subject imagines speaking a given word without moving any articulatory muscle or producing any audible sound. Early work by Wester (Wester, 2006) presented results which were initially interpreted to be related to brain activity patterns due to the imagination of pronouncing words. However, subsequent investigations lead to the hypothesis that the good recognition performance might instead have resulted from temporal correlated artifacts in the brainwaves since the words were presented in blocks. In order to further investigate this hypothesis, we run a study with 21 subjects, recording 16 EEG channels using a 128 cap montage. The vocabulary consists of 5 words, each of which is repeated 20 times during a recording session in order to train our HMM-based classifier. The words are presented in blockwise, sequential, and random order. We show that the block mode yields an average recognition rate of 45.50%, but it drops to chance level for all other modes. Our experiments suggest that temporal correlated artifacts were recognized instead of words in block recordings and back the above-mentioned hypothesis.},
author_keywords={Electroencephalography;  Speech recognition;  Unspoken speech},
keywords={Audible sound;  Brain activity;  Brain wave;  Recognition performance;  Recognition rates;  Temporal effects;  Unspoken speech, Brain;  Electroencephalography;  Electrophysiology;  Signal processing, Speech recognition},
references={Becker, K., Gebrauchsanweisung fur (2004) VarioPorttrade; Birbaumer, N., Kubler, A., Ghanayim, N., Hinterberger, T., Perelmouter, J., Kaiser, J., Iversen, I., Flor, H., The thought translation device (TTD) for completely paralyzed patients (2000) IEEE Transactions on Rehabilitation Engineering, 8 (2), pp. 190-193. , DOI 10.1109/86.847812, PII S1063652800041112; Birbaumer, N., Ghanayim, N., Hinterberger, T., Iversen, I., Kotchoubey, B., Kubler, A., Perelmouter, J., Flor, H., A spelling device for the paralysed (1999) Nature, 398 (6725), pp. 297-298; Blankertz, B., Dornhege, G., Krauledat, M., Muller, K.-R., Kunzmann, V., Losch, F., Curio, G., The Berlin brain-computer interface: EEG-based communication without subject training (2006) IEEE Transactions on Neural Systems and Rehabilitation Engineering, 14 (2), pp. 147-152. , DOI 10.1109/TNSRE.2006.875557, 1642756; Calliess, J.-P., Further investigations on unspoken speech (2006) Institut fur Theoretische Informatik Universitat Karlsruhe (TH), , Karlsruhe, Germany; (2007) Towards Brain-Computer Interfacing, , Dornhege, G., del R. Millan, J., Hinterberger, T, McFar-land, D., and Muller, K.-R., editors,MIT Press; Lotte, F., Congedo, M., Lecuyer, A., Lamarche, F., Arnaldi, B., A review of classification algorithms for EEG-based brain-computer interfaces (2007) Journal of Neural Engineering, 4 (2), pp. R1-R13. , DOI 10.1088/1741-2560/4/2/R01, PII S1741256007300050, R01; Maier-Hein, L., Speech recognition using surface electromyography (2005) Master's thesis, Institut fur Theoretische Informatik Universitat Karlsruhe (TH), , Karlsruhe, Germany; Neuper, C., Muller, G.R., Kubler, A., Birbaumer, N., Pfurtscheller, G., Clinical application of an EEG-based brain-computer interface: A case study in a patient with severe motor impairment (2003) Clinical Neurophysiology, 114 (3), pp. 399-409. , DOI 10.1016/S1388-2457(02)00387-5; Nijholt, A., Tan, D., Pfurtscheller, G., Brunner, C., Millon, J.D.R., Allison, B., Graimann, B., Muller, K.-R., Brain-computer interfacing for intelligent systems (2008) IEEE Intelligent Systems, 23 (3), pp. 72-79. , DOI 10.1109/MIS.2008.41, 4525145; Porbadnigk, A., Eeg-based speech recognition: Impact of experimental design on performance (2008) Institut fiir Algorithmen und Kognitive Systeme, Universitat Karlsruhe (TH), , Karlsruhe, Germany; Scherer, R., Muller, G.R., Neuper, C., Graimann, B., Pfurtscheller, G., An asynchronously controlled EEG-based virtual keyboard: Improvement of the spelling rate (2004) IEEE Transactions on Biomedical Engineering, 51 (6), pp. 979-984. , DOI 10.1109/TBME.2004.827062; Suppes, P., Lu, Z.-L., Han, B., Brain wave recognition of words (1997) Proceedings of the National Academy of Sciences of the United States of America, 94 (26), pp. 14965-14969. , DOI 10.1073/pnas.94.26.14965; Waibel, A., Bett, M., Metze F, Ries, K., Schaaf, T., Schultz, T., Soltau, H., Zechner, K., Advances in automatic meeting record creation and access (2001) Proc. ICASSP '01, 1, pp. 597-600; Wand, M., Wavelet-based preprocessing of eeg and emg signals for speech recognition (2007) Institut fur Theoretische Informatik Universitat Karlsruhe (TH), , Karlsruhe, Germany; Wester, M., Unspoken speech - speech recognition based on electroencephalography (2006) Master's thesis, Institut fiir Theoretische Informatik Universitat Karlsruhe (TH), , Karlsruhe, Germany; Wolpaw, J.R., Birbaumer, N., McFarland, D.J., Pfurtscheller, G., Vaughan, T.M., Brain-computer interfaces for communication and control (2002) Clinical Neurophysiology, 113 (6), pp. 767-791. , DOI 10.1016/S1388-2457(02)00057-3, PII S1388245702000573; Wolpaw, J.R., McFarland, D.J., Vaughan, T.M., Schalk, G., The Wadsworth Center brain - Computer interface (BCI) research and development program (2003) IEEE Transactions on Neural Systems and Rehabilitation Engineering, 11 (2), pp. 204-207. , DOI 10.1109/TNSRE.2003.814442},
correspondence_address1={Porbadnigk, A.; Cognitive Systems Lab, University of Karlsruhe, Am Fasanengarten 5, 76131 Karlsruhe, Germany; email: annepor@andrew.cmu.edu},
address={Porto},
isbn={9789898111654},
language={English},
abbrev_source_title={BIOSIGNALS - Proc. Int. Conf. Bio-Inspired Syst. Signal Process.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dahlmeier2009450,
author={Dahlmeier, D. and Tou Ng, H. and Schultz, T.},
title={Joint learning of preposition senses and semantic roles of prepositional phrases},
journal={EMNLP 2009 - Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: A Meeting of SIGDAT, a Special Interest Group of ACL, Held in Conjunction with ACL-IJCNLP 2009},
year={2009},
pages={450-458},
doi={10.3115/1699510.1699569},
note={cited By 21; Conference of 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP 2009, Held in Conjunction with ACL-IJCNLP 2009 ; Conference Date: 6 August 2009 Through 7 August 2009;  Conference Code:86731},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953747642&doi=10.3115%2f1699510.1699569&partnerID=40&md5=62379f87c86cf4bd1d83079f4006285c},
affiliation={NUS Graduate School for Integrative Sciences and Engineering, Singapore; Department of Computer Science, National University of Singapore, Singapore; Cognitive Systems Lab., University of Karlsruhe, Germany},
abstract={The sense of a preposition is related to the semantics of its dominating prepositional phrase. Knowing the sense of a preposition could help to correctly classify the semantic role of the dominating prepositional phrase and vice versa. In this paper, we propose a joint probabilistic model for word sense disambiguation of prepositions and semantic role labeling of prepositional phrases. Our experiments on the PropBank corpus show that jointly learning the word sense and the semantic role leads to an improvement over state-of-the-art individual classifier models on the two tasks. © 2009 ACL and AFNLP.},
keywords={Semantics, Individual classifiers;  Joint learning;  Joint probabilistic;  Prepositional phrase;  Semantic role labeling;  Semantic roles;  State of the art;  Word Sense Disambiguation, Natural language processing systems},
references={Andrew, G., Grenager, T., Manning, C.D., Verb Sense and subcategorization: Using joint inference to improve performance on complementary tasks (2004) Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP 2004), pp. 150-157; Dang, H.T., Palmer, M., The role of semantic roles in disambiguating verb senses (2005) Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05), pp. 42-49; Gildea, D., Jurafsky, D., Automatic labeling of semantic roles (2002) Computational Linguistics, 28 (3), pp. 245-288; Jurafsky, D., Martin, J.H., (2008) Speech and Language Processing, , Prentice-Hall, Inc. Upper Saddle River, NJ, USA; Koomen, P., Punyakanok, V., Roth, D., Yih, W.-T., Generalized inference with multiple semantic role labeling systems (2005) Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL 2005), pp. 181-184; Lee, Y.K., Ng, H.T., An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation (2002) Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), pp. 41-48; Litkowski, K.C., Hargraves, O., The preposition project (2005) Proceedings of the 2nd ACLSIGSEM Workshop on the Linguistic Dimensions of Prepositions and Their Use in Computational Linguistic Formalisms and Applications, pp. 171-179; Litkowski, K.C., Hargraves, O., SemEval-2007 Task 06: Word-sense disambiguation of prepositions (2007) Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval 2007), pp. 24-29; Marcus, M.P., Marcinkiewicz, M.A., Santorini, B., Building a large annotated corpus of english: The penn treebank (1993) Computational Linguistics, 19 (2), pp. 313-330; O'hara, T., Wiebe, J., Preposition semantic classification via penn treebank and framenet (2003) Proceedings of the 7th Conference on Computational Natural Language Learning (CoNLL 2003), pp. 79-86; O'hara, T., Wiebe, J., Exploiting semantic role resources for preposition disambiguation (2009) Computational Linguistics, 35 (2), pp. 151-184; Palmer, M., Dang, H.T., Rosenzweig, J., Sense tagging the penn treebank (2000) Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC 2000); Palmer, M., Gildea, D., Kingsbury, P., The proposition bank: An annotated corpus of semantic roles (2005) Computational Linguistics, 31 (1), pp. 71-105; Pradhan, S., Hacioglu, K., Krugler, V., Ward, W., Martin, J.H., Jurafsky, D., Support vector learning for semantic argument classification (2005) Machine Learning, 60 (1-3), pp. 11-39; Toutanova, K., Haghighi, A., Manning, C.D., A global joint model for semantic role labeling (2008) Computational Linguistics, 34 (2), pp. 161-191; Xue, N., Palmer, M., Calibrating features for semantic role labeling (2004) Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP 2004), pp. 88-94; Ye, P., Baldwin, T., Semantic role labeling of prepositional phrases (2006) ACM Transactions on Asian Language Information Processing (TALIP), 5 (3), pp. 228-244; Ye, P., Baldwin, T., MELB-YB: Preposition sense disambiguation using rich semantic features (2007) Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval 2007), pp. 241-244},
correspondence_address1={Dahlmeier, D.; NUS Graduate School for Integrative Sciences and EngineeringSingapore; email: danielhe@comp.nus.edu.sg},
publisher={Association for Computational Linguistics (ACL)},
address={Singapore},
language={English},
abbrev_source_title={EMNLP - Proc. Conf. Empir. Methods Nat. Lang. Process.: Meet. SIGDAT, Spec. Interest Group ACL, Held Conjunction ACL-IJCNLP},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Laskowski2008149,
author={Laskowski, K. and Schultz, T.},
title={Detection of laughter-in-interaction in multichannel close-talk microphone recordings of meetings},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2008},
volume={5237 LNCS},
pages={149-160},
doi={10.1007/978-3-540-85853-9-14},
note={cited By 20; Conference of 5th International Workshop on Machine Learning for Multimodal Interaction, MLMI 2008 ; Conference Date: 8 September 2008 Through 10 September 2008;  Conference Code:74826},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849164116&doi=10.1007%2f978-3-540-85853-9-14&partnerID=40&md5=cb8b6a8dc95b9671539fc86368d431c4},
abstract={Laughter is a key element of human-human interaction, occurring surprisingly frequently in multi-party conversation. In meetings, laughter accounts for almost 10% of vocalization effort by time, and is known to be relevant for topic segmentation and the automatic characterization of affect. We present a system for the detection of laughter, and its attribution to specific participants, which relies on simultaneously decoding the vocal activity of all participants given multi-channel recordings. The proposed framework allows us to disambiguate laughter and speech not only acoustically, but also by constraining the number of simultaneous speakers and the number of simultaneous laughers independently, since participants tend to take turns speaking but laugh together. We present experiments on 57 hours of meeting data, containing almost 11000 unique instances of laughter. © 2008 Springer-Verlag Berlin Heidelberg.},
keywords={Decoding;  Interactive computer systems;  Learning systems;  Robot learning;  Technical presentations;  User interfaces, Human interactions;  Key elements;  Multi channels;  Topic segmentations, Flow interactions},
references={Laskowski, K., Burger, S., Analysis of the occurrence of laughter in meetings (2007) Proc. INTERSPEECH, pp. 1258-1261. , Antwerpen, Belgium, pp; Kennedy, L., Ellis, D., Laughter detection in meetings (2004) Proc. ICASSP Meeting Recognition Workshop, pp. 118-121. , Montreal, Canada, NIST, pp; Russell, J., Bachorowski, J.A., Femandez-Dols, J.M., Facial and vocal expressions of emotion (2003) Annual Review of Psychology, 54, pp. 329-349; Laskowski, K., Burger, S., Annotation and analysis of emotionally relevant behavior in the ISL Meeting Corpus (2006) Proc. LREC, , Genoa, Italy; Galley, M., McKeown, K., Fosler-Lussier, E., Jing, H.: Discourse segmentation of multi-party conversation. In: Dignum, F.P.M. (ed.) ACL 2003. LNCS (LNAI), 2922, pp. 562-569. Springer, Heidelberg (2004); Banerjee, S., Rose, C., Rudnický, A.: The necessity of a meeting recording and playback system, and the benefit of topic-level annotations to meeting browsing. In: Costabile, M.F., Paterno, F. (eds.) INTERACT 2005. LNCS, 3585, pp. 643-656. Springer, Heidelberg (2005); Wrede, B., Shriberg, E., Spotting "hotspots" in meetings: Human judgments and prosodic cues (2003) Proc. EUROSPEECH, pp. 2805-2808. , Geneva, Switzerland, pp; Truong, K., van Leeuwen, D., Automatic detection of laughter (2005) Proc. INTERSPEECH, pp. 485-488. , Lisbon, Portugal, pp; Truong, K., van Leeuwen, D., Automatic discrimination between laughter and speech (2007) Speech Communication, 49 (2), pp. 144-158; Knox, M., Mirghafori, N., Automatic laughter detection using neural networks (2007) Proc. INTER.SPEECH, pp. 2973-2976. , Antwerpen, Belgium, pp; Truong, K., van Leeuwen, D., Evaluating automatic laughter segmentation in meetings using acoustic and acoustics-phonetic features (2007) Proc. ICPhS Workshop on The Phonetics of Laughter, pp. 49-53. , Saarbrücken, Germany, pp; Pfau, T., Ellis, D., Stoicke, A., Multispeaker speech activity detection for the ICSI Meeting Recorder (2001) Proc. ASRU, pp. 107-110. , Madonna di Campiglio, Italy, pp; Janin, A., The ICSI Meeting Corpus (2003) Proc. ICASSP, pp. 364-367. , Hong Kong, China, pp; Shriberg, E., Dhillon, R., Bhagat, S., Ang, J., Carvey, H., The ICSI Meeting Recorder Dialog Act (MRDA) Corpus (2004) Proc. SIGdial, pp. 97-100. , Cambridge MA, USA, pp; Norwine, A.C., Murphy, O.J., Characteristic time intervals in telephonic conversation (1938) Bell System Technical Journal, 17, pp. 281-291; Fiscus, J., Ajot, J., Michel, M., Garofolo, J.: The Rich Transcription 2006 Spring Meeting Recognition Evaluation. In: Renais, S., Bengio, S., Fiscus, J.G. (eds.) MLMI 2006. LNCS, 4299, pp. 309-322. Springer, Heidelberg (2006); Bachorowski, J.-A., Smoski, M., Owren, M., The acoustic features of human laughter (2001) J. of Acoustical Society of America, 110 (3), pp. 1581-1597; Laskowski, K., Burger, S., On the correlation between perceptual and contextual aspects of laughter in meetings (2007) Proc. ICPhS Workshop on the Phonetics of Laughter, , Saarbrücken, Germany; Nwokah, E., Hsu, H.-C., Davies, P., Fogel, A.: The integration of laughter and speech in vocal communication: A dynamic systems perspective. J. of Speech, Language & Hearing Research 42, 880-894 (1999); Laskowski, K., Schultz, T., A supervised factorial acoustic model for simultaneous multiparticipant vocal activity detection in close-talk microphone recordings of meetings (2007), Technical Report CMU-LTI-07-017, Carnegie Mellon University, Pittsburgh PA, USA December; Wrigley, S., Brown, G., Wan, V., Renais, S., Speech and crosstalk detection in multichannel audio (2005) IEEE Trans. Speech and Audio Proc, 13 (1), pp. 84-91; Huang, Z., Harper, M.: Speech activity detection on multichannels of meetings recordings. In: Renais, S., Bengio, S. (eds.) MLMI 2005. LNCS, 3869, pp. 415-427. Springer, Heidelberg (2006); Boakye, K., Stoicke, A., Improved speech activity detection using cross-channel features for recognition of multiparty meetings (2006) Proc. INTERSPEECH, pp. 1962-1965. , Pittsburgh PA, USA, pp; Laskowski, K., Schultz, T., Modeling duration contraints for simultaneous multi-participant vocal activity detection in meetings (2008), Technical report, Carnegie Mellon University, Pittsburgh PA, USA, February; Laskowski, K., Fügen, C., Schultz, T., Simultaneous multispeaker segmentation for automatic meeting recognition (2007) Proc. EUSIPCO, pp. 1294-1298. , Poznan, Poland, pp; Wrigley, S., Brown, G., Wan, V., Renais, S., Feature selection for the classification of crosstalk in multi-channel audio (2003) Proc. EUROSPEECH, pp. 469-472. , Geneva, Switzerland, pp; Dines, J., Vepa, J., Hain, T., The segmentation of multi-channel meeting recordings for automatic speech recognition (2006) Proc. INTER SPEECH, pp. 1213-1216. , Pittsburgh PA, USA, pp},
sponsors={Netherlands Org. for Applied Scientific Research (TNO); Dutch Res. School for Inf. and Knowledge Systems (SIKS); Brno Univ. of Technol., Fac. of Inf. Technol. (FIT BUT)},
address={Utrecht},
issn={03029743},
isbn={3540858520; 9783540858522},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Laskowski200882,
author={Laskowski, K. and Schultz, T.},
title={Recovering participant identities in meetings from a probabilistic description of vocal interaction},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2008},
pages={82-85},
note={cited By 1; Conference of INTERSPEECH 2008 - 9th Annual Conference of the International Speech Communication Association ; Conference Date: 22 September 2008 Through 26 September 2008;  Conference Code:78746},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867211300&partnerID=40&md5=afe341b2fe3ce8d011b564a5ac124280},
affiliation={Cognitive Systems Lab., Universität Karlsruhe, Karlsruhe, Germany; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, United States},
abstract={An important decision in the design of automatic conversation understanding systems is the level at which information streams representing specific participants are merged. In the current work, we explore participant-dependence of low-level interactive aspects of conversation, namely the observed contextual preferences for talkspurt deployment. We argue that strong participant- dependence at this level gives cause for merging participant streams as early as possible. We demonstrate that our probabilistic description of talkspurt deployment preferences is strongly participant-dependent, and frequently predictive of participant identity. Copyright © 2008 ISCA.},
author_keywords={Automatic conversation understanding;  Meetings;  Vocal interaction},
keywords={Automatic conversation understanding;  Information streams;  Meetings;  Probabilistic descriptions;  Vocal interaction, Speech communication},
references={Laskowski, K., Ostendorf, M., Schultz, T., Modeling Vocal Interaction for Text-Independent Classification of Conversation Type (2007) Proc. SIGdial, Antwerpen, Belgium, pp. 194-201; Laskowski, K., Ostendorf, M., Schultz, T., Modeling Vocal Interaction for Text-Independent Participant Characterization in Multi-Party Converation (2008) Proc. SIGdial, Columbus OH, USA, pp. 148-155; Janin, A., Baron, D., Edwards, J., Ellis, D., Gelbart, D., Morgan, N., Peskin, B., Wooters, C., The ICSI Meeting Corpus (2003) Proc. of ICASSP, Hong Kong, China, pp. 364-367; Shriberg, E., Dhillon, R., Bhagat, S., Ang, J., Carvey, H., The ICSI Meeting Recorder Dialog Act (MRDA) Corpus (2004) Proc. of SIGdial, Cambridge MA, USA, pp. 97-100; Norwine, A., Murphy, O., Characteristic Time Intervals in Telephonic Conversation (1938) Bell System Technical Journal, 17, pp. 281-291; Laskowski, K., Schultz, T., Modeling Vocal Interaction for Segmentation in Meeting Recognition (2007) LNCS, 4892, pp. 259-270. , Proc. MLMI, Brno, Czech Republic, Springer; Laskowski, K., Burger, S., Analysis of the Occurrence of Laughter in Meetings (2007) Proc. INTERSPEECH, Antwerpen, Belgium, pp. 1258-1261; Dabbs, J., Ruback, R., Dimensions of group process: Amount and structure of vocal interaction (1987) Advances in Experimental Psychology, 20, pp. 123-169; Rienks, R., Heylen, D., Dominance detection in meetings using easily obtainable features Proc. OfMLMI, Edinburgh, UK, 2005; Rienks, R., Zhang, D., Gatica-Perez, D., Post, W., Detection and application of influence rankings in small-group meetings Proc. of ICMI, Banff, Canada, 2006},
correspondence_address1={Laskowski, K.; Cognitive Systems Lab., Universität Karlsruhe, Karlsruhe, Germany; email: kornel@cs.cmu.edu},
address={Brisbane, QLD},
issn={19909772},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jin20081893,
author={Jin, Q. and Schultz, T.},
title={Robust far-field speaker identification under mismatched conditions},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2008},
pages={1893-1896},
note={cited By 1; Conference of INTERSPEECH 2008 - 9th Annual Conference of the International Speech Communication Association ; Conference Date: 22 September 2008 Through 26 September 2008;  Conference Code:78746},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867205460&partnerID=40&md5=8d09b4b88fc4a09f0461377d74b64d68},
affiliation={InterACT, Language Technologies Institute, Carnegie Mellon University, United States},
abstract={While speaker identification performance has improved dramatically over the past years, the presence of interfering noise and the variety of channel conditions pose a major obstacle. Particularly the mismatch between training and test condition leads to severe performance degradations. In this paper we investigate speaker identification based on data simultaneously recorded with multiple microphones in a farfield setup under different noise and reverberation conditions. Dramatic performance degradation is observed, especially when training and test conditions mismatch. To address this mismatch we apply our robust frame-based score competition approach in which we combine and compete models trained on multiple conditions. To further improve this approach we add simulated, i.e. artificially created training data on a variety of noise conditions for additional model training. Our experimental results show that the extended approach significantly improves speaker identification performance under adverse and mismatching conditions. Copyright © 2008 ISCA.},
author_keywords={Far-field;  Frame-based score competition;  Speaker identification},
keywords={Channel conditions;  Far-field;  Frame-based score competition;  Model training;  Noise conditions;  Performance degradation;  Robust frames;  Speaker identification;  Speaker identification performance;  Test condition;  Training data, Competition;  Computer simulation;  Degradation;  Loudspeakers;  Speech communication, Speech recognition},
references={Lee, C.-H., Soong, F.K., Paliwal, K.K., (1996) Automatic Speech and Speaker Recognition: Advanced Topics, , Springer, ISBN:0792397061; Furui, S., Towards Robust Speech Recognition under Adverse Conditions (1992) ESCA Workshop on Speech Processing in Adverse Conditions, pp. 31-42; Bimbot, F., Bonastre, J.-F., Fredouille, C., Gravier, G., Magrin- Chagnolleau, I., Meignier, S., Merlin, T., Reynolds, D.A., A Tutorial on Text- Independent Speaker Verification (2004) Journal on Applied Signal Processing, 4, pp. 430-451; Reynolds, D., Speaker Identification and Verification Using Gaussian Mixture Speaker Models (1995) Speech Communication, 17 (1-2), pp. 91-108. , August; Jin, Q., Schultz, T., Waibel, A., Far-field Speaker Recognition (2007) IEEE Transactions on Audio, Speech, and Language Processing, 15 (7), pp. 2023-2032; Campbell, D., Palomäki, K., Brown, G., "A MATLAB Simulation of "Shoebox" Room Acoustics for Use in Research and Teaching, , http://cis.paisley.ac.uk/research/journal/V9/V9N3/campbell.doc},
correspondence_address1={Jin, Q.; InterACT, Language Technologies Institute, Carnegie Mellon UniversityUnited States; email: qjin@cs.cmu.edu},
address={Brisbane, QLD},
issn={19909772},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Jou2008305,
author={Jou, S.-C.S. and Schultz, T.},
title={Automatic speech recognition based on electromyographic biosignals},
journal={Communications in Computer and Information Science},
year={2008},
volume={25 CCIS},
pages={305-320},
doi={10.1007/978-3-540-92219-3_23},
note={cited By 2; Conference of 1st International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2008 ; Conference Date: 28 January 2008 Through 31 January 2008;  Conference Code:82178},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049373521&doi=10.1007%2f978-3-540-92219-3_23&partnerID=40&md5=053b38246e03ef95074d2d92ef4ab3ac},
affiliation={International Center for Advanced Communication Technologies, Carnegie Mellon University, Pittsburgh, PA, United States; Cognitive Systems Laboratory, Karlsruhe University, Karlsruhe, Germany},
abstract={This paper presents our studies of automatic speech recognition based on electromyographic biosignals captured from the articulatory muscles in the face using surface electrodes. We develop a phone-based speech recognizer and describe how the performance of this recognizer improves by carefully designing and tailoring the extraction of relevant speech feature toward electromyographic signals. Our experimental design includes the collection of audibly spoken speech simultaneously recorded as acoustic data using a close-speaking microphone and as electromyographic signals using electrodes. Our experiments indicate that electromyographic signals precede the acoustic signal by about 0.05-0.06 seconds. Furthermore, we introduce articulatory feature classifiers, which had recently shown to improved classical speech recognition significantly. We describe that the classification accuracy of articulatory features clearly benefits from the tailored feature extraction. Finally, these classifiers are integrated into the overall decoding framework applying a stream architecture. Our final system achieves a word error rate of 29.9% on a 100-word recognition task. © 2008 Springer-Verlag.},
keywords={Acoustic data;  Acoustic signals;  Articulatory features;  Automatic speech recognition;  Biosignals;  Classification accuracy;  Electromyographic;  Electromyographic signal;  Experimental design;  Speech features;  Speech recognizer;  Stream architecture;  Surface electrode;  Word error rate;  Word recognition, Biomedical engineering;  Classifiers;  Electrodes;  Face recognition;  Feature extraction;  Speech processing, Speech recognition},
references={Fromkin, V., Ladefoged, P., Electromyography in speech research (1966) Phonetica, 15; Chan, A., Englehart, K., Hudgins, B., Lovely, D., Hidden Markov model classification of myoelectric signals in speech (2002) IEEE Engineering in Medicine and Biology Magazine, 21 (4), pp. 143-146; Jorgensen, C., Lee, D., Agabon, S., Sub auditory speech recognition based on EMG signals Proc. IJCNN, Portland, Oregon (July 2003); Jorgensen, C., Binsted, K., Web browser control using EMG based sub vocal speech recognition Proc. HICSS, Hawaii (January 2005); Betts, B., Jorgensen, C., Small vocabulary communication and control using surface electromyography in an acoustically noisy environment Proc. HICSS, Hawaii (January 2006); Manabe, H., Hiraiwa, A., Sugimura, T., Unvoiced speech recognition using EMG-Mime speech recognition Proc. CHI, Ft. Lauderdale, Florida (April 2003); Manabe, H., Zhang, Z., Multi-stream HMM for EMG-based speech recognition Proc. IEEE EMBS, San Francisco, California (September 2004); Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography Proc. ASRU, San Juan, Puerto Rico (November 2005); Becker, K., (2005) Varioport, , http://www.becker-meditec.de; Walliczek, M., Kraft, F., Jou, S.C., Schultz, T., Waibel, A., Sub-word unit based non-audible speech recognition using surface electromyography Proc. Interspeech, Pittsburgh, PA (September 2006); Yu, H., Waibel, A., Streaming the front-end of a speech recognizer Proc. ICSLP, Beijing, China (2000); Metze, F., Waibel, A., A flexible stream architecture for ASR using articulatory features Proc. ICSLP, Denver, CO (September 2002); Jou, S.C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography Proc. Interspeech, Pittsburgh, PA (September 2006); Jou, S.C., Schultz, T., Waibel, A., Continuous electromyographic speech recognition with a multi-stream decoding architecture Proc. ICASSP, Honolulu, Hawai'i (April 2007)},
correspondence_address1={Jou, S.-C. S.; International Center for Advanced Communication Technologies, Carnegie Mellon University, Pittsburgh, PA, United States; email: tanja@cs.cmu.edu},
address={Funchal, Madeira},
issn={18650929},
isbn={3540922180; 9783540922186},
language={English},
abbrev_source_title={Commun. Comput. Info. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Honal2008100,
author={Honal, M. and Schultz, T.},
title={Determine task demand from brain activity},
journal={BIOSIGNALS 2008 - Proceedings of the 1st International Conference on Bio-inspired Systems and Signal Processing},
year={2008},
volume={1},
pages={100-107},
note={cited By 8; Conference of BIOSIGNALS 2008 - 1st International Conference on Bio-inspired Systems and Signal Processing ; Conference Date: 28 January 2008 Through 31 January 2008;  Conference Code:73994},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350482624&partnerID=40&md5=b19b54808361b9f5edd8549ef902cf5e},
affiliation={Carnegie Mellon University, 407 South Craig Street, Pittsburgh 15213, PA, United States; Karlsruhe University, Am Fasanengarten 5, 78131 Karlsruhe, Germany},
abstract={Our society demands ubiquitous mobile devices that offer seamless interaction with everybody, everything, everywhere, at any given time. However, the effectiveness of these devices is limited due to their lack of situational awareness and sense for the users' needs. To overcome this problem we develop intelligent transparent human-centered systems that sense, analyze, and interpret the user's needs. We implemented learning approaches that derive the current task demand from the user's brain activity by measuring the electroencephalogram. Using Support Vector Machines we can discriminate high versus low task demand with an accuracy of 92.2% in session dependent experiments, 87.1% in session independent experiments, and 80.0% in subject independent experiments. To make brain activity measurements less cumbersome, we built a comfortable headband with which we achieve 69% classification accuracy on the same task.},
author_keywords={Brain activity;  EEG;  Human-centered systems;  Meeting and lecture scenario;  Task demand identification},
keywords={Brain activity;  EEG;  Human-centered systems;  Meeting and lecture scenario;  Task demand identification, Electroencephalography;  Experiments;  Mobile devices;  Signal processing, Brain},
references={Becker, K., VarioPort™, , http://www.becker-meditec.com; Berka, C, Levendowski, D., Cvetinovic, M., et al., 2004. Real-Time Analysis of EEG Indexes of Alertness, Cognition, and Memory Acquired With a Wireless EEG Headset. Int. Journal of HCI, 17(2):151-170; Delorme, A., Makeig, S., EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics (2004) Journal of Neuroscience Methods, 134, pp. 9-21; Duta, M., Alford, C., Wilon, S., Tarassenko, L., Neural Network Analysis of the Mastoid EEG for the Assessment of Vigilance. Int. Journal of HCI (2004), 17 (2). , 171-195; Electro-Cap™, Electro-Cap International, Inc.: http://www.electro- cap.com/; Fukunaga, K., (1972) Introduction to Statistical Pattern Recognition, , Academic Press, New York, London; Honal, M., Schultz, T., Identifying User State using Electroencephalographic Data (2005) Proceedings of the International Conference on Multimodal Input (ICMI), , Trento, Italy, October; Hyväarinen, A., Oja, E., Independent Component Analysis: Algorithms and Applications (2000) Neural Networks, 13 (4-5), pp. 411-430; Iqbal, S., Zheng, X., Bailey, B., Task evoked pupillary response to mental workload in human computer interaction (2004) Proceedings of Conference of Human Factors in Computer Systems, , CHI; Izzetoglu, K., Bunce, S., Onaral, B., Pourrezaei, K., Chance, B., Functional Optical Brain Imaging Using Near-Infrared During Cognitive Tasks (2004) International Journal of HCI, 17 (2), pp. 211-227; Jasper, H.H., The ten-twenty electrode system of the International Federation (1958) Electroencephalographic Clinical Neurophysiolgy, 10, pp. 371-375; Joachims, T., (1999) Making Large-Scale SVM Learning Practical, , chapter 11 .MIT-Press; Jung, T., Makeig, S., Humphries, C., Lee, T., Mckeown, M., Iragui, V., Sejnowski, T., Removing Electroencephalographic Artifacts by Blind Source Separation (2000) Psychophysiology, 37 (2), pp. 163-217; Jung, T.P., Makeig, S., Stensmo, M., Sejnowski, T.J., Estimating Alertness from the EEG Power Spectrum (1997) IEEE Transactions on Biomedical Engineering, 4 (L), pp. 60-69. , January; Pleydell-Pearce, C.W., Whitecross, S.E., Dickson, B.T., Multivariate Analysis of EEG: Predicting Cognition on the basis of Frequency Decomposition, Inter-electrode Correlation, Coherence, Cross Phase and Cross Power (2003) Proceedings of 38th HICCS; (1997) Physiologie des Menschen, , Schmidt, F. and Thews, G, editors, Springer; Smith, M., Gevins, A., Brown, H., Karnik, A., Du, R., Monitoring Task Loading with Multivariate EEG Measures during Complex Forms of Human-Computer Interaction (2001) Human Factors, 43 (3), pp. 366-380; Tsochantaridis, I., Hofmann, T., Joachims, T., Altun, Y., (2004) Support Vector Machine Learning for Interdependent and Structured Output Spaces, , Proceedings of the ICML; Vesanto, J., Himberg, J., Alhoniemi, E., Parhan-kangas, J., (2000), SOM Toolbox for Matlab 5. Technical report, Helsinki University of Technology; Zschocke, S., (1995) Klinische Elektroenzephalographie, , Springer},
correspondence_address1={Honal, M.; Carnegie Mellon University, 407 South Craig Street, Pittsburgh 15213, PA, United States; email: honal@ira.uka.de},
address={Funchal, Madeira},
isbn={9789898111180},
language={English},
abbrev_source_title={BIOSIGNALS - Proc. Int. Conf. Bio-inspired Syst. Signal Process.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jou20083,
author={Jou, S.-C.S. and Schultz, T.},
title={Ears: Electromyographical automatic recognition of speech},
journal={BIOSIGNALS 2008 - Proceedings of the 1st International Conference on Bio-inspired Systems and Signal Processing},
year={2008},
volume={1},
pages={3-12},
note={cited By 2; Conference of BIOSIGNALS 2008 - 1st International Conference on Bio-inspired Systems and Signal Processing ; Conference Date: 28 January 2008 Through 31 January 2008;  Conference Code:73994},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350464264&partnerID=40&md5=72a33aa3a8f8add014b65ec906cd13e1},
affiliation={International Center for Advanced Communication Technologies, Carnegie Mellon University, Pittsburgh, PA, United States; Karlsruhe University, Karlsruhe, Germany},
abstract={In this paper, we present our research on automatic speech recognition of surface electromyographic signals that are generated by the human articulatory muscles. With parallel recorded audible speech and electromyographic signals, experiments are conducted to show the anticipatory behavior of electromyographic signals with respect to speech signals. Additionally, we demonstrate how to develop phone-based speech recognizers with carefully designed electromyographic feature extraction methods. We show that articulatory feature (AF) classifiers can also benefit from the novel feature, which improve the F-score of the AF classifiers from 0.467 to 0.686. With a stream architecture, the AF classifiers are then integrated into the decoding framework. Overall, the word error rate improves from 86.8% to 29.9% on a 100 word vocabulary recognition task.},
author_keywords={Articulatory feature;  Electromyography;  Feature extraction;  Speech recognition},
keywords={Articulatory feature;  Articulatory features;  Automatic recognition;  Automatic speech recognition;  Electromyographic;  Electromyographic signal;  F-score;  Feature extraction methods;  Speech recognizer;  Speech signals;  Stream architecture;  Vocabulary recognition;  Word error rate, Classifiers;  Electromyography;  Feature extraction;  Learning systems;  Muscle;  Signal processing, Speech recognition},
references={Becker, K., (2005) Varioport, , http://www.becker-meditec.de; Betts, B., Jorgensen, C., Small vocabulary communication and control using surface electromyogra-phy in an acoustically noisy environment (2006) Proc. HICSS, , Hawaii; Chan, A., Englehart, K., Hudgins, B., Lovely, D., Hidden Markov model classification of my-oelectric signals in speech (2002) IEEE Engineering in Medicine and Biology Magazine, 21 (4), pp. 143-146; Fromkin, V. and Ladefoged, P. (1966). Electromyography in speech research. Phonetica, 15; Jorgensen, C., Binsted, K., Web browser control using EMG based sub vocal speech recognition (2005) Proc. HICSS, , Hawaii; Jorgensen, C., Lee, D., Agabon, S., Sub auditory speech recognition based on EMG signals (2003) Proc. IJCNN, , Portland, Oregon; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) Proc. ASRU, , San Juan, Puerto Rico; Manabe, H., Hiraiwa, A., Sugimura, T., Unvoiced speech recognition using EMG-Mime speech recognition (2003) Proc. CHI, , Ft. Lauderdale, Florida; Manabe, H., Zhang, Z., Multi-stream HMM for EMG-based speech recognition (2004) Proc. IEEE EMBS, , San Francisco, California; Metze, F., Waibel, A., A flexible stream architecture for ASR using articulatory features (2002) Proc. ICSLP, , Denver, CO; Yu, H., Waibel, A., Streaming the front-end of a speech recognizer (2000) Proc. ICSLP, , Beijing, China},
correspondence_address1={Jou, S.-C. S.; International Center for Advanced Communication Technologies, Carnegie Mellon University, Pittsburgh, PA, United States; email: scjou@cs.emu.edu},
address={Funchal, Madeira},
isbn={9789898111180},
language={English},
abbrev_source_title={BIOSIGNALS - Proc. Int. Conf. Bio-inspired Syst. Signal Process.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Charoenpornsawat2008241,
author={Charoenpornsawat, P. and Schultz, T.},
title={Improving word segmentation for thai speech translation},
journal={2008 IEEE Workshop on Spoken Language Technology, SLT 2008 - Proceedings},
year={2008},
pages={241-244},
doi={10.1109/SLT.2008.4777885},
art_number={4777885},
note={cited By 5; Conference of 2008 IEEE Workshop on Spoken Language Technology, SLT 2008 ; Conference Date: 15 December 2008 Through 19 December 2008;  Conference Code:76987},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649524990&doi=10.1109%2fSLT.2008.4777885&partnerID=40&md5=2a2a3967dc4ef6d1d9d815e353e6f519},
affiliation={InterACT, Language Technologies Institute, Carnegie Mellon University, United States},
abstract={A vocabulary list and language model are primary components in a speech translation system. Generating both from plain text is a straightforward task for English. However, it is quite challenging for Chinese, Japanese, or Thai which provide no word segmentation, i.e. the text has no word boundary delimiter. For Thai word segmentation, Maximal Matching, a lexicon-based approach, is one of the popular methods. Nevertheless this method heavily relies on the coverage of the lexicon. When text contains an unknown word, this method usually produces a wrong boundary. When extracting words from this segmented text, some words will not be retrieved because of wrong segmentation. In this paper, we propose statistical techniques to tackle thisproblem. Based on different word segmentation methods we develop various speech translation systems and show that the proposed method can significantly improve thetranslation accuracy by about 6.42% BLEU points compared to the baseline system. ©2008 IEEE.},
author_keywords={Speech Recognition;  Spoken language translation;  Text Processing;  Word Segmentation},
keywords={Baseline systems;  Language model;  Maximal matching;  Plain text;  Speech translation;  Speech translation systems;  Spoken language translation;  Statistical techniques;  Vocabulary lists;  Word Segmentation, Computational linguistics;  Speech analysis;  Text processing;  Translation (languages);  Word processing, Speech recognition},
references={Aroonmanakun, W., Collocation and thai word segmentation (2002) Proc. of the 5th SNLP Workshop, , Pathumthani, Thailand; Theeramunkong, T., Usanavasin, S., Non-dictionary-based thai word segmentation using decision trees (2001) Proc. of HLT, , San Diego, USA; Meknavin, S., Charoenpornsawat, P., Kijsirikul, B., Featurebased thai word segmentation (1997) Proc. of NLPRS; Haruechaiyasak, C., Kongyoung, S., Dailey, M.N., A comparative study on thai word segmentation approaches (2008) Processdings of ECTI-CON; Suebvisai, S., Thai automatic speech recogntion (2005) Proc. of ICASSP, , Philadelphia, PA; Dangsaart, S., Intelligent thai text - Thai sign translation for language learning (2008) Comput. Educ. 51, 3. , Nov; Charoenpornsawat, P., Schultz, T., Example-based grapheme-to-phoneme conversion for thai (2006) Proc. of Interspeech, , Pittsburgh PA; Sornlertlamvanich, V., Potipiti, T., Charoenporn, T., Automatic corpus-based Thai word extraction with the c4.5 learning algorithm (2000) Proc. of the 18th Conference on Computational Linguistics - Volume 2; Takezawa, T., Towards a broad-coverage bilingual corpus for speech translation of travel conversations in the real world (2002) Proc. of LREC, , Canary Islands, Spain; P. Charoenpornsawat, (2003) SWATH: Smart Word Analysis for Thai, , http://www.cs.cmu.edu/~paisarn/software.htm; (2003) Thai-English Dictionary, , http://lexitron.nectec.or.th, Lexitron Version 2.0; Vogel, S., The CMU statistical Machine Translation System (2003) Proceedings of MT summit IX; Koehn, P., Pharaoh: A beam search decoder for phrase- based statistical machine translation models (2004) Proc. of AMTA, , Washington, DC},
correspondence_address1={Charoenpornsawat, P.; InterACT, Language Technologies Institute, Carnegie Mellon UniversityUnited States; email: paisarn@cs.cmu.edu},
sponsors={Institute of Electrical and Electronics Engineers; IEE Signal Processing Society},
address={Goa},
isbn={9781424434725},
language={English},
abbrev_source_title={IEEE Workshop Spoken Lang. Technol., SLT - Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Laskowski2008149,
author={Laskowski, K. and Ostendorf, M. and Schultz, T.},
title={Modeling vocal interaction for text-independent participant characterization in multi-party conversation},
journal={ACL-08: HLT - Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue},
year={2008},
pages={149-155},
note={cited By 31; Conference of 9th SIGdial Workshop on Discourse and Dialogue ; Conference Date: 19 June 2008 Through 20 June 2008;  Conference Code:88805},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857728013&partnerID=40&md5=aa156665a95cea29756e9aab7ce14e4d},
affiliation={Cognitive Systems Labs, Universität Karlsruhe, Karlsruhe, Germany; Dept. of Electrical Engineering, University of Washington, Seattle WA, United States},
abstract={An important task in automatic conversation understanding is the inference of social structure governing participant behavior. We explore the dependence between several social dimensions, including assigned role, gender, and seniority, and a set of low-level features descriptive of talkspurt deployment in a multiparticipant context. Experiments conducted on two large, publicly available meeting corpora suggest that our features are quite useful in predicting these dimensions, excepting gender. The classification experiments we present exhibit a relative error rate reduction of 37% to 67% compared to choosing the majority class. © 2008 Association for Computational Linguistics.},
keywords={Low-level features;  Multi-party conversations;  Relative error rates;  Social dimensions;  Social structure;  Vocal interaction, Experiments},
references={Bales, R., (1950) Interaction Process Analysis, , Addison-Wesley Press, Inc; Banerjee, S., Rudnicky, A., Using simple speech based features to detect the state of a meeting and the roles of the meeting participants (2004) Proc. Interspeech, pp. 2189-2192; Berger, J., Rosenholtz, S., Zelditch Jr., M., Status organizing processes (1980) Annual Review of Sociology, 6, pp. 479-508; Carletta, J., Garrod, S., Fraser-Krauss, H., Communication and placement of authority in workplace groups - The consequences for innovation (1998) Small Group Research, 29 (5), pp. 531-559; Carletta, J., Unleashing the killer corpus: Experiences in creating the multi-everything AMI meeting corpus (2007) Language Resources and Evaluation Journal, 41 (2), pp. 181-190; Janin, A., Baron, D., Edwards, J., Ellis, D., Gelbart, D., Morgan, N., Peskin, B., Wooters, C., The ICSI meeting corpus (2003) Proc. ICASSP, pp. 364-367; Laskowski, K., Ostendorf, M., Schultz, T., Modeling vocal interaction for text-independent classification of conversation type (2007) Proc. SIGdial, pp. 194-201; Laskowski, K., Fügen, C., Schultz, T., Simultaneous multispeaker segmentation for automatic meeting recognition (2007) Proc. EUSIPCO, pp. 1294-1298; Norwine, A., Murphy, O., Characteristic time intervals in telephonic conversation (1938) Bell System Technical Journal, 17, pp. 281-291; Rienks, R., Heylen, D., Dominance detection in meetings using easily obtainable features (2005) Proc. MLMI; Rienks, R., Zhang, D., Gatica-Perez, D., Post, W., Detection and application of influence rankings in small-group meetings (2006) Proc. ICMI; Rotman, J., (1995) An Introduction to the Theory of Groups, , Springer-Verlag New York, Inc; Shriberg, E., Dhillon, R., Bhagat, S., Ang, J., Carvey, H., The ICSI meeting recorder dialog act (MRDA) corpus (2004) Proc. SIGdial, pp. 97-100; Tannen, D., (1996) Gender & Discourse, , Oxford University Press, USA; Vinciarelli, A., Speakers role recognition in multiparty audio recordings using social network analysis and duration distribution modeling (2007) IEEE Trans. Multimedia, 9 (6), pp. 1215-1226; Wyatt, D., Bilmes, J., Choudhury, T., Kautz, H., A privacy-sensitive approach to modeling multi-person conversations (2007) Proc. IJCAI, pp. 1769-1775; Zancanaro, M., Lepri, B., Pianesi, F., Automatic detection of group functional roles in face to face interactions (2006) Proc. ICMI},
correspondence_address1={Laskowski, K.; Cognitive Systems Labs, Universität Karlsruhe, Karlsruhe, Germany; email: kornel@ira.uka.de},
sponsors={Microsoft Research; John Benjamins Publishing Company; SemanticEdge - First Choice; Powerset - Natural Language Search},
address={Columbus, OH},
isbn={9781932432176},
language={English},
abbrev_source_title={ACL: HLT - Proc. SIGdial Workshop Discourse Dialogue},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kominek20081833,
author={Kominek, J. and Badaskar, S. and Schultz, T. and Black, A.W.},
title={Improving speech systems built from very little data},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2008},
pages={1833-1836},
note={cited By 0; Conference of INTERSPEECH 2008 - 9th Annual Conference of the International Speech Communication Association ; Conference Date: 22 September 2008 Through 26 September 2008;  Conference Code:78746},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867221199&partnerID=40&md5=826e938120dbf1139cacec7650417393},
affiliation={Language Technologies Institute, Carnegie Mellon University, United States; Cognitive Systems Lab., Karlsruhe University, Germany},
abstract={This paper studies two ways for helping non-specialist users develop speech systems from limited data for new languages. Focused web re-crawling finds additional examples of text matching the domain as specified by the user. This improves the language model and cuts word error rate nearly in half. Iterative voice building with interleaved lexicon construction uses the voice from a previous iteration to help construct an improved voice. 4.5 hours of the user's time reduces transcription error rate from 32% to 4%. Copyright © 2008 ISCA.},
keywords={Error rate;  Language model;  Limited data;  Speech systems;  Text-matching;  Word error rate, Computational linguistics, Speech communication},
references={SPICE, , http://cmuspice.org; Schultz, T., Black, A., Badaskar, S., Hornyak, M., Kominek, J., SPICE: Web-based Tools for Rapid Language Adaptation in Speech Processing Systems (2007) Interspeech, , Antwerp; Kominek, J., Schultz, T., Black, A., Voice Building from Insufficient Data - Classroom Experiences with Web-based Language Development Tools (2007) ISCA Speech Synthesis Workshop, 6. , Bonn, German; Kominek, J., Schultz, T., Black, A., Synthesizer Voice Quality of New Languages Calibrated with Mean Mel Cepstral Distortion SLTU-2008 Workshop, Hanoi, Vietnam; Chakrabarti, S., Van Den Berg, M., Dom, B., Focused crawling: A new approach to topic-specific (web) resource discovery (1999) Computer Networks, 31 (11-16); Schultz, T., GlobalPhone: A Multilingual Speech and Text Database developed at Karlsruhe University ICSLP, Denver CO, USA, 2002; Davel, M., Barnard, E., The Efficient generation of pronunciation dictionaries: Human factors during bootstrapping (2004) Interspeech, , Jeju, Korea},
correspondence_address1={Kominek, J.; Language Technologies Institute, Carnegie Mellon UniversityUnited States},
address={Brisbane, QLD},
issn={19909772},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hsiao20081445,
author={Hsiao, R. and Fuhs, M. and Tam, Y.-C. and Jin, Q. and Schultz, T.},
title={The CMU-InterACT 2008 Mandarin transcription system},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2008},
pages={1445-1448},
note={cited By 6; Conference of INTERSPEECH 2008 - 9th Annual Conference of the International Speech Communication Association ; Conference Date: 22 September 2008 Through 26 September 2008;  Conference Code:78746},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867208407&partnerID=40&md5=7d92bd564c50aa285e417aac0cc028fc},
affiliation={InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={We present our Mandarin BN/BC transcription system recently developed for the GALE07 evaluation. The system employs a 3-pass decoding strategy trained with over 1300 hours of quickly transcribed audio. We successfully apply discriminative training, dynamic unsupervised language model adaptation, and system combination techniques in our system. We furthermore achieve improvements by combining an Initial-Final system with a genre dependent phone system. On the GALE07 phase 2 retest evaluation, our system achieves a character error rate(CER) of 13.3% on dev07 test set and 13.5% on eval07 unsequestered test set. Our system also allows combination with other sites and in this paper, we investigate different system combination strategies which significantly improve the final recognition performance. Copyright © 2008 ISCA.},
author_keywords={Broadcast conversation;  Broadcast news;  Gale evaluation;  Mandarin transcription system},
keywords={Broadcast conversation;  Broadcast news;  Character error rates;  Decoding strategy;  Discriminative training;  Gale evaluation;  Language model adaptation;  Mandarin transcription system;  Phone systems;  Recognition performance;  System combination;  Test sets, Computational linguistics;  Speech communication;  Transcription, Telephone systems},
references={Tam, Y.C., Schultz, T., Language model adaptation using variational bayes inference Proceedings of Interspeech, 2005; Tam, Y.C., Schultz, T., Unsupervised language model adaptation using latent semantic marginals Proceedings of the Interspeech, 2006; Jin, Q., Schultz, T., Speaker segmentation and clustering in meetings Proceedings of the International Conference on Spoken Language Processing, 2004; Yu, H., Tam, Y.C., Schaaf, T., Stüker, S., Jin, Q., Noamany, M., Schultz, T., The ISL RT04 mandarin broadcast news evaluation system EARS Rich Transcription Workshop, Palisades, NY, USA, November 2004; Gales, M., Semi-tied covariance matrices for hidden Markov models (1999) IEEE Transactions Speech and Audio Processing, 7, pp. 272-281; Valtchev, V., Odell, J., Woodland, P., Young, S., MMIE training of large vocabulary speech recognition systems (1997) Speech Communication, 22, pp. 303-314; Povey, D., Kanevsky, D., Kingsbury, B., Ramabhadran, B., Saon, G., Visweswariah, K., Boosted MMI for model and feature-space discriminative training Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, 2008; Povey, D., (2003) Discriminative Training for Large Vocabulary Speech Recognition, , Ph.D. dissertation, Cambridge University Engineering Dept; Stolcke, A., SRILM - An extensible language modeling toolkit Proceedings of ICSLP, 2002; Tam, Y.C., Schultz, T., Correlated latent semantic model for unsupervised language model adaptation Proc. of ICASSP, 2007; Kneser, R., Peters, J., Klakow, D., Language model adaptation using dynamic marginals Proc. of Eurospeech, 1997, pp. 1971-1974; Mangu, L., Brill, E., Stolcke, A., Finding consensus in speech recognition: Word error minimization and other applications of confusion networks (2000) Computer Speech and Language, (4), pp. 373-400. , October; Paulik, M., Rao, S., Lane, I., Vogel, S., Schultz, T., Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, 2008},
correspondence_address1={Hsiao, R.; InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: wrhsiao@cs.cmu.edu},
address={Brisbane, QLD},
issn={19909772},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jin20084845,
author={Jin, Q. and Toth, A.R. and Black, A.W. and Schultz, T.},
title={Is voice transformation a threat to speaker identification?},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2008},
pages={4845-4848},
doi={10.1109/ICASSP.2008.4518742},
art_number={4518742},
note={cited By 37; Conference of 2008 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP ; Conference Date: 31 March 2008 Through 4 April 2008;  Conference Code:73384},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-51449115100&doi=10.1109%2fICASSP.2008.4518742&partnerID=40&md5=daa6b5cd15369ac797011a3dcfc6c2f4},
affiliation={Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={With the development of voice transformation and speech synthesis technologies, speaker identification systems are likely to face attacks from imposters who use voice transformed or synthesized speech to mimic a particular speaker. Therefore, we investigated in this paper how speaker identification systems perform on voice transformed speech. We conducted experiments with two different approaches, the classical GMM-based speaker identification system and the Phonetic speaker identification system. Our experimental results showed that current standard voice transformation techniques are able to fool the GMM-based system but not the Phonetic speaker identification system. These findings imply that future speaker identification systems should include idiosyncratic knowledge in order to successfully distinguish transformed speech from natural speech and thus be armed against imposter attacks. ©2008 IEEE.},
author_keywords={Phonetic speaker identification;  Speaker identification;  Voice transformation},
keywords={Acoustics;  Computer crime;  Computer networks;  Linguistics;  Loudspeakers;  Magnetostrictive devices;  Signal processing;  Speech synthesis, Phonetic speaker identification;  Speaker identification;  Voice transformation, Speech recognition},
references={Pellom, B., Hansen, J., An Experimental Study of Speaker Verification Sensitivity to Computer Voice-Altered Imposters (1999) ICASSP, pp. 837-840; T. Masuko, K. Tokuda, and T. Kobayashi, Imposture using Synthetic Speech against Speaker Verification based on Spectrum and Pitch, ICSLP, 2000; Kajarekar, S., Bratt, H., Shriberg, E., Leon, R., A Study of Intentional Voice Modifications for Evading Automatic Speaker Recognition (2006) Odyssey; Reynolds, D., Rose, R., Robust Text -independent Speaker Identification Using Gaussian Mixture Speaker Models (1995) IEEE Transactions on Speech and Audio Processing, 3 (1), pp. 72-83. , Jan; Reynolds, D., Quatieri, T., Dunn, R., Speaker verification using adapted Gaussian mixture models (2000) Digital Signal Processing, 10 (1-3), pp. 19-41; Doddington, G., Speaker recognition based on idiolectal differences between speakers Eurospeech, 2001; Jin, Q., Navratil, J., Reynolds, D., Campbell, J., Andrews, W., Abramson, J., Combining Cross-Stream and Time Dimensions in Phonetic Speaker Recognition (2003) ICASSP, pp. 800-803; Adami, A., Mihaescu, R., Reynolds, D., Godfrey, J., Modeling prosodic dynamics for speaker recognition (2003) ICASSP, pp. 788-791; Schultz, T., Waibel, A., Language Independent and Language Adaptive Acoustic Modeling for Speech Recognition (2001) Speech Communication, 35, pp. 31-51; Garofalo, J., Graff, D., Paul, D., David Pallett, C.S.R.-I., (WSJ0) Complete, LDC93S6A, , ISBN 1-58563-006-3; http://www.festvox.org; Toth, A., Jin, Q., Schultz, T., Black, A., Evaluation of Identity of Synthetic and Transformed Voices Using Speaker Identification Systems, , Submitted to ICASSP 2008; Jin, Q., Schultz, T., Waibel, A., Phonetic Speaker Identification (2002) ICSLP, pp. 1345-1348},
correspondence_address1={Jin, Q.; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: qjin@cs.cmu.edu},
address={Las Vegas, NV},
issn={15206149},
isbn={1424414849; 9781424414840},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Paulik20085105,
author={Paulik, M. and Rao, S. and Lane, I. and Vogel, S. and Schultz, T.},
title={Sentence segmentation and punctuation recovery for spoken language translation},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2008},
pages={5105-5108},
doi={10.1109/ICASSP.2008.4518807},
art_number={4518807},
note={cited By 14; Conference of 2008 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP ; Conference Date: 31 March 2008 Through 4 April 2008;  Conference Code:73384},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-51449090378&doi=10.1109%2fICASSP.2008.4518807&partnerID=40&md5=a6f2815dcf8323d51480c9955a36dd78},
affiliation={Interactive Systems Laboratories (interACT), Carnegie Mellon University, United States; Interactive Systems Laboratories (interACT), Universität Karlsruhe, Germany},
abstract={Sentence segmentation and punctuation recovery are critical components for effective spoken language translation (SLT). In this paper we describe our recent work on sentence segmentation and punctuation recovery for three different language pairs, namely for English-to-Spanish, Arabic-to-English and Chinese-to-English. We show that the proposed approach works equally well in these very different language pairs. Furthermore, we introduce two features computed from the translation beam-search lattice that indicate if phrasal and target language model context is jeopardized when segmenting at a given word boundary. These features enable us to introduce short intra-sentence segments without degrading translation performance. ©2008 IEEE.},
author_keywords={Punctuation Recovery;  Sentence Segmentation;  Spoken Language Translation;  Tight Coupling},
keywords={Acoustics;  Computational linguistics;  Information retrieval systems;  Signal processing;  Speech;  Translation (languages), Punctuation Recovery;  Sentence Segmentation;  Spoken Language Translation;  Tight Coupling, Speech recognition},
references={Global Autonomous Language Exploitation, , http://www.darpa.mil/ipto/programs/gale, GALE; STAR, T., Technology and Corpora for Speech to Speech Translation, , http://www.tc-star.org; Och, F.J., Ney, H., Improved Statistical Alignment Models (2000) Proc. of ACL, , Hongkong, China; Koehn, P., Monz, C., Manual and Automatic Evaluation of Machine Translation between European Languages (2006) Proc. on the Workshop on SMT, , New York City, USA; Paulik, M., Rottmann, K., Niehues, J., Hildebrand, S., Vogel, S., The ISL Phrase-Based MT System for the 2007 ACL Workshop on SMT (2007) Proc. of the ACL 2007 Second Workshop on SMT, , Prague, Czech Republic, June; Stüker, S., Fügen, C., Hsiao, R., Ikbal, S., Jin, Q., Kraft, F., Paulik, M., Wölfel, M., The ISL TC-STAR Spring 2006 ASR Evalutation Systems (2006) Proc. of the TC-STAR Workshop on Speech-to-Speech Translation, , Barcelona, Spain; Al-Onaizan, Y., Mangu, L., Arabic ASR and MT Integration For GALE (2007) Proc. ICASSP, , Hawaii, USA, April; Matusov, E., Leusch, G., Bender, O., Ney, H., Evaluating Machine Translation Output with Automatic Sentence Segmentation (2005) Proc. of IWSLT, , Pittsburgh, PA; Quinlan, J.R., The C4.5 Induction System - C4.5 Release 8, , http://rulequest.com/Personal; Rao, S., Lane, I., Schultz, T., Optimizing Sentence Segmentation For Spoken Language Translation (2007) Proc. INTERSPEECH, , Antwerp, Belgium, August; Fügen, C., Kolss, M., The Influence of Utterance Chunking on Machine Translation Performance (2007) Proc. INTERSPEECH, , Antwerp, Belgium, August},
correspondence_address1={Paulik, M.; Interactive Systems Laboratories (interACT), Carnegie Mellon UniversityUnited States; email: paulik@cs.cmu.edu},
address={Las Vegas, NV},
issn={15206149},
isbn={1424414849; 9781424414840},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Laskowski2008259,
author={Laskowski, K. and Schultz, T.},
title={Modeling vocal interaction for segmentation in meeting recognition},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2008},
volume={4892 LNCS},
pages={259-270},
doi={10.1007/978-3-540-78155-4_23},
note={cited By 5; Conference of 4th International Workshop on Machine Learning for Multimodal Interaction, MLMI 2007 ; Conference Date: 28 June 2007 Through 30 June 2007;  Conference Code:71518},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-40249088282&doi=10.1007%2f978-3-540-78155-4_23&partnerID=40&md5=caeaa8e4cf5546a581f604f8a7a34695},
affiliation={InterACT, Carnegie Mellon University, Pittsburgh, PA, United States},
abstract={Automatic segmentation is an important technology for both automatic speech recognition and automatic speech understanding. In meetings, participants typically vocalize for only a fraction of the recorded time, but standard vocal activity detection algorithms for close-talk microphones in meetings continue to treat participants independently. In this work we present a multispeaker segmentation system which models a particular aspect of human-human communication, that of vocal interaction or the interdependence between participants' on-off speech patterns. We describe our vocal interaction model, its training, and its use during vocal activity decoding. Our experiments show that this approach almost completely eliminates the problem of crosstalk, and word error rates on our development set are lower than those obtained with human-generatated reference segmentation. We also observe significant performance improvements on unseen data. © 2008 Springer-Verlag Berlin Heidelberg.},
keywords={Algorithms;  Crosstalk;  Decoding;  Mathematical models, Speech understanding;  Vocal activity detection, Speech recognition},
references={Boakye, K., Stolcke, A., Improved speech activity detection using cross-channel features for recognition of multiparty meetings (2006) Proc. of INTERSPEECH, pp. 1962-1965. , Pittsburgh PA, USA, pp; Brady, P., A model for generating on-off speech patterns in two-way conversation (1969) Bell Systems Technical Journal, 48 (7), pp. 2445-2472; Burger, S., MacLaren, V., Yu, H., The ISL Meeting Corpus: The impact of meeting type on speech style (2002) Proc. of ICSLP, pp. 301-304. , Denver CO, USA, pp; Çetin, O., Shriberg, E.: Overlap in meetings: ASR effects and analysis by dialog factors, speakers, and collection site. In: Renais, S., Bengio, S., Fiscus, J.G. (eds.) MLMI 2006. LNCS, 4299, pp. 200-211. Springer, Heidelberg (2006); Dabbs Jr., J., Ruback, R., Dimensions of group process: Amount and structure of vocal interaction (1987) Advances in Experimental Social Psychology, 20, pp. 123-169; Dines, J., Vepa, J., Hain, T., The segmentation of multi-channel meeting recordings for automatic speech recognition (2006) Proc. of INTERSPEECH, pp. 1213-1216. , Pittsburgh PA, USA, pp; Fay, N., Garrod, S., Carletta, J., Group discussion as interactive dialogue or serial monologue: The influence of group size (2000) Psychological Science, 11 (6), pp. 487-492; Fügen, C., Ikbal, S., Kraft, F., Kumatani, K., Laskowski, K., McDonough, J., Ostendorf, M., Stüker, S., Wölfel, M.: The ISL RT-06S Speech-to-Text System. In: Renais, S., Bengio, S., Fiscus, J.G. (eds.) MLMI 2006. LNCS, 4299, pp. 407-418. Springer, Heidelberg (2006); Laskowski, K., Jin, Q., Schultz, T., Crosscorrelation-based multispeaker speech activity detection (2004) Proc. of ISLP, pp. 973-976. , Jeju Island, South Korea, pp; Laskowski, K., Schultz, T.: Unsupervised learning of overlapped speech model parameters for multichannel speech activity detection in meetings. In: Proc. of ICASSP 2006, Toulouse, France, I, pp. 993-996 (2006); Laskowski, K., Schultz, T., A geometric interpretation of normalized maximum crosscorrelation for vocal activity detection in meetings (2007) Proc. of HLT-NAACL, pp. 89-92. , Short Papers, Rochester NY, USA, pp; Pfau, T., Ellis, D., Stolcke, A., Multispeaker speech activity detection for the ICSI Meeting Recorder (2001) Proc. of ASRU, pp. 107-110. , Madonna di Campiglio, Italy, pp; Sacks, H., Schegloff, E., Jefferson, G., A simplest sematics for the organization of turn-taking for conversation (1974) Language, 50 (4), pp. 696-735; Wrigley, S., Brown, G., Wan, V., Renais, S., Feature selection for the classification of crosstalk in multi-channel audio (2003) Proc. of EUROSPEECH, pp. 469-472. , Geneva, Switzerland, pp},
correspondence_address1={Laskowski, K.; InterACT, Carnegie Mellon University, Pittsburgh, PA, United States; email: kornel@cs.cmu.edu},
address={Brno},
issn={03029743},
isbn={3540781544; 9783540781547},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Fung200889,
author={Fung, P. and Schultz, T.},
title={Multilingual spoken language processing},
journal={IEEE Signal Processing Magazine},
year={2008},
volume={25},
number={3},
pages={89-97},
doi={10.1109/MSP.2008.918417},
note={cited By 34},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032751388&doi=10.1109%2fMSP.2008.918417&partnerID=40&md5=106e7518d63799ba6a464c67d8d01295},
affiliation={Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology (HKUST), Hong Kong; Department of Computer Science, Karlsruhe University, Karlsruhe, Germany},
abstract={Research on multilingual speech processing is now a hot topic as there has been an increase the widespread use of speech and language technologies in a wide variety of applications. There are new algorithms and tools that support the simultaneous recognition of mixed-language input, as well as summarization of multilingual text and spoken documents, the generation of output in the appropriate language, or the accurate translation from one language to another. Many approaches have been proposed to tailor language models toward particular domains by text selection or various interpolation schemes. Meanwhile, spoken document summarization is the recognition, distillation, and presentation of spoken documents in a structural textual form. Multilingual speech translation must use a framework that can handle multiple pairs of source and target languages.},
keywords={Interpolation;  Natural language processing systems;  Speech recognition;  Text processing;  Translation (languages), Multilingual text;  Spoken documents;  Summarization, Speech processing},
correspondence_address1={Fung, P.; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology (HKUST)Hong Kong; email: pascale@ece.ust.hk},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={10535888},
coden={ISPRE},
language={English},
abbrev_source_title={IEEE Signal Process Mag},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Gehrig2008,
author={Gehrig, D. and Schultz, T.},
title={Selecting relevant features for human motion recognition},
journal={Proceedings - International Conference on Pattern Recognition},
year={2008},
doi={10.1109/icpr.2008.4761290},
art_number={4761290},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957941535&doi=10.1109%2ficpr.2008.4761290&partnerID=40&md5=9bf3c8ba2b1b78083ab63574b0c8f4e6},
affiliation={Cognitive Systems Laboratory, Universität Karlsruhe (TH), Karlsruhe, Germany},
abstract={Recently, there is a growing interest in automatic recognition of human motion for applications, such as humanoid robots, human activity monitoring, and surveillance. In this paper we investigate motion recognition based on joint angle trajectories derived from marker-based video recordings. The goal of this paper is to improve the generalization and robustness of human motion recognition even if only limited amount of training data is available. We achieve this goal by significantly reducing the amount of input features. We leverage on recent studies in the area of neuroscience which indicate that human motions display only a few independent degrees of freedom (DOF). We examine which DOF are relevantfor recognizing upper body human motions and to what extend the dimensionality of the feature vectors can be reduced in order to simplify the data acquisition and improve the robustness of the recognition process. Ourfinal results indicate that careful selection offeatures proves to reduce the number of features by a factor of up to 3, while at the same time significantly improving the recognition performance. © 2008 IEEE.},
keywords={Anthropomorphic robots;  Data acquisition;  Degrees of freedom (mechanics);  Pattern recognition;  Video recording, Automatic recognition;  Feature vectors;  Human Activity Monitoring;  Human motion recognition;  Input features;  Motion recognition;  Recognition process;  Relevant features, Motion estimation},
references={Collaborative Research Center 588 Humanoid Robots - Learning and Cooperating Multimodal Robots, , http://www.sfb588.uni-karlsruhe.de/; Aggarwal, J., Park, S., Human motion: Modeling and recognition of actions and interactions (2004) 3DPVT, pp. 640-647; Finke, M., Geutner, P., Hild, H., Kemp, T., Ries, K., Westphal, M., The Karlsruhe-Verbmobil speech recognition engine ICASSP-97, 1, pp. 83-86. , vol.1; Fukunaga, K., (1990) Introduction to Statistical Pattern Recognition, , Academic Press, 2 edition; Ivanenko, Y., Poppele, R., Lacquaniti, F., (2004) Five Basic Muscle Activation Pattern Account for Muscle Activity during Human Locomotion; Park, A.-N., Mukovskiy, A., Omlor, L., Giese, M.A., Self organized character animation based on learned synergies from foil-body motion capture data (2008) Proceedings of the 2008 International Conference on Cognitive Systems, pp. 145-152. , April; Soltau, H., Metze, F., Füugen, C., Waibel, A., A one-pass decoder based on polymorphic linguistic context assignment (2001) ASRU, pp. 214-217; Starner, T., Pentland, A., Real-time American sign language recognition from video using hidden Markov models (1995) ISCV'95. Proceedings of the International Symposium on Computer Vision, p. 265. , Washington, DC, USA, IEEE Computer Society; Yamato, J., Ohya, J., Ishii, K., Recognizing human action in time-sequential images using hidden Markov model (1992) CVPR, pp. 379-385; Yu, X., Yang, S.X., A study of motion recognition from video sequences (2005) Comput. Vis. Sci., 8 (1), pp. 19-25},
correspondence_address1={Gehrig, D.; Cognitive Systems Laboratory, Universität Karlsruhe (TH), Karlsruhe, Germany; email: dgehrig@ira.uka.de},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={10514651},
isbn={9781424421756},
coden={PICRE},
language={English},
abbrev_source_title={Proc. Int. Conf. Pattern Recognit.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nallasamy20082888,
author={Nallasamy, U. and Black, A.W. and Schultz, T. and Frederking, R.},
title={NineOneOne: Recognizing and classifying speech for handling minority language emergency calls},
journal={Proceedings of the 6th International Conference on Language Resources and Evaluation, LREC 2008},
year={2008},
pages={2888-2891},
note={cited By 1; Conference of 6th International Conference on Language Resources and Evaluation, LREC 2008 ; Conference Date: 28 May 2008 Through 30 May 2008;  Conference Code:131723},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037548739&partnerID=40&md5=0a09ab494583540c0c7b2f2f85546acd},
affiliation={Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA  15213, United States},
abstract={In this paper, we describe NineOneOne (9-1-1), a system designed to recognize and translate Spanish emergency calls for better dispatching. We analyze the research challenges in adapting speech translation technology to 9-1-1 domain. We report our initial research towards building the system and the results of our initial experiments.},
keywords={Emergency calls;  Minority languages;  Research challenges;  Speech translation, Translation (languages)},
funding_details={National Science FoundationNational Science Foundation, NSF, IIS-0627957},
funding_text 1={This project is funded by NSF Grant No: IIS-0627957 “NineOneOne: Exploratory Research on Recognizing Non-English Speech for Emergency Triage in Disaster Response”. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of sponsors.},
references={Burges, C.J.C., A tutorial on support vector machines for pattern recognition (1998) Proc. Data Mining and Knowledge Discovery, (2), p. 2. , 955-974, USA; Finke, M., Geutner, P., Hild, H., Kemp, T., Ries, K., Westphal, M., The Karlsruhe-verbmobil speech recognition engine (1997) Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 83-86. , Germany; Frederking, R., Rudnicky, A., Hogan, C., Lenzo, K., Interactive speech translation in the diplomat project (2000) Machine Translation Journal, 15 (1-2), pp. 61-66. , Special Issue on Spoken Language Translation, USA; Gao, Y., Zhou, B., Sarikaya, R., Afify, M., Kuo, H., Zhu, W., Deng, Y., Besacier, L., IBM MASTOR SYSTEM: Multilingual automatic speech-to-speech translator (2006) Proc. First International Workshop on Medical Speech Translation, pp. 53-56. , USA; Garner, S.R., WEKA: The Waikato environment for knowledge analysis (1995) Proc. New Zealand Computer Science Research Students Conference, pp. 57-64. , New Zealand; Langley, C., (2003) Domain Action Classification and Argument Parsing for Interlingua-based Spoken Language Translation, , PhD thesis, Carnegie Mellon University, Pittsburgh, PA; [http://www.languageline.com]; Lavie, A., Balducci, F., Coletti, P., Langley, C., Lazzari, G., Pianesi, F., Taddei, L., Waibel, A., Architecture and design considerations in NESPOLE!: A speech translation system for E-commerce applications (2001) Proc. Human Language Technologies (HLT), pp. 31-34. , USA; Levin, L., Langley, C., Lavie, A., Gates, D., Wallace, D., Peterson, K., Domain specific speech acts for spoken language translation (2003) Proc. 4th SIGdial Workshop on Discourse and Dialogue, pp. 208-217. , Japan; Liu, Y., Carbonell, J., Jin, R., A pairwise ensemble approach for accurate genre classification (2003) Proc. 14th European Conference on Machine Learning (ECML), , Croatia; Schultz, T., Westphal, M., Waibel, A., The GlobalPhone project: Multilingual LVCSR with JANUS-3 (1997) Proc. Multilingual Information Retrieval Dialogs: 2nd SQEL Workshop, pp. 20-27. , Czech Republic; Soltau, H., Metze, F., Fugen, C., Waibel, A., A one pass-decoder based on polymorphic linguistic context assignment (2001) Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), , Italy; Stallard, D., Choi, F., Kao, C., Krstovski, K., Natarajan, P., Prasad, R., Saleem, S., Subramanian, S., The BBN 2007 displayless English/Iraqi speech-to-speech translation system (2007) Proc. International Conference on Spoken Language Processing (Interspeech), , Belgium},
sponsors={et al.; European Media Laboratory GmbH (EML); Instituut voor Nederlandse Lexicologie (INL); Linguatec; Microsoft; Nuance},
publisher={European Language Resources Association (ELRA)},
isbn={2951740840; 9782951740846},
language={English},
abbrev_source_title={Proc. Int. Conf. Lang. Resourc. Eval., LREC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tam20072444,
author={Tam, Y.-C. and Schultz, T.},
title={Bilingual LSA-based translation lexicon adaptation for spoken language translation},
journal={International Speech Communication Association - 8th Annual Conference of the International Speech Communication Association, Interspeech 2007},
year={2007},
volume={4},
pages={2444-2447},
note={cited By 1; Conference of 8th Annual Conference of the International Speech Communication Association, Interspeech 2007 ; Conference Date: 27 August 2007 Through 31 August 2007;  Conference Code:73788},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-56149096062&partnerID=40&md5=15af53662c74b316d0aa498014ee92fb},
affiliation={InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={We present a bilingual LSA (bLSA) framework for translation lexicon adaptation. The idea is to apply marginal adaptation on a translation lexicon so that the lexicon marginals match to in-domain marginals. In the framework of speech translation, the bLSA method transfers topic distributions from the source to the target side, such that the translation lexicon can be adapted before translation based on the source document. We evaluated the proposed approach on our Mandarin RT04 spoken language translation system. Results showed that the conditional likelihood on the test sentence pairs is improved significantly using an adapted translation lexicon compared to an unadapted baseline. The proposed approach showed improvement on BLEU-score in SMT. When both the target-side LM and the translation lexicon were adapted and applied simultaneously for SMT decoding, the gain on BLEU-score was more than additive compared to the scenarios when the adapted models were individually applied.},
author_keywords={Bilingual lsa;  LM;  Marginal adaptation;  Translation lexicon},
keywords={Base-lines;  Bilingual lsa;  Conditional likelihoods;  Do-mains;  LM;  Marginal adaptation;  Marginal adaptations;  Marginals;  Speech translations;  Spoken language translations;  Translation lexicon;  Translation lexicons, Decoding;  Lagrange multipliers;  Linguistics;  Speech;  Surface mount technology;  Translation (languages), Speech communication},
references={Tam, Y.C., Lane, I., Schultz, T., Bilingual LSA-based LM adaptation for spoken language translation (2007) Proc. of ACL; Kneser, R., Peters, J., Klakow, D., Language model adaptation using dynamic marginals (1997) Proc. of Eu-rospeech, pp. 1971-1974; Zhao, B., Xing, E.P., BiTAM: Bilingual topic admixture models for word alignment (2006) Proc. of ACL; Tam, Y.C., Schultz, T., Correlated latent semantic model for unsupervised language model adaptation (2007) Proc. of ICASSP; Blei, D., Ng, A., Jordan, M., Latent Dirichlet Allocation (2003) Journal of Machine Learning Research, pp. 1107-1135; Vogel, S., SMT decoder dissected: Word reordering (2003) Proc. of ICNLPKE; Vogel, S., PESA: Phrase pair extraction as sentence splitting (2005) Proc. of the Machine Translation Summit; Venugopal, A., Zollman, A., Waibel, A., Training and evaluation error minimization rules for statistical machine translation (2005) Proc. of ACL},
correspondence_address1={Tam, Y.-C.; InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: yct@cs.emu.edu},
address={Antwerp},
isbn={9781605603162},
language={English},
abbrev_source_title={Int. Speech Commun. Assoc. - Annu. Conf. Int. Speech Commun. Assoc., Interspeech},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Laskowski20071294,
author={Laskowski, K. and Fügen, C. and Schultz, T.},
title={Simultaneous multispeaker segmentation for automatic meeting recognition},
journal={European Signal Processing Conference},
year={2007},
pages={1294-1298},
note={cited By 3; Conference of 15th European Signal Processing Conference, EUSIPCO 2007 ; Conference Date: 3 September 2007 Through 7 September 2007;  Conference Code:91100},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863757369&partnerID=40&md5=53fea44f7684d4988d05910f46a9f49b},
affiliation={Inter ACT, Universität Karlsruhe, am Fasanengarten 5, 76131 Karlsruhe, Germany; Inter ACT, Carnegie Mellon University, 407 South Craig St., Pittsburgh PA 15213, United States},
abstract={Vocal activity detection is an important technology for both automatic speech recognition and automatic speech understanding. In meetings, participants typically vocalize for only a fraction of the recorded time, and standard vocal activity detection algorithms for close-talk microphones have shown to be ineffective. This is primarily due to the problem of crosstalk, in which a participant's speech appears on other participants' microphones, making it hard to attribute detected speech to its correct speaker. We describe an automatic multichannel segmentation system for meeting recognition, which accounts for both the observed acoustics and the inferred vocal activity states of all participants using joint multi-participant models. Our experiments show that this approach almost completely eliminates the crosstalk problem. Recent improvements to the baseline reduce the development set word error rate, achieved by a state-of-the-art multi-pass speech recognition system, by 62% relative to manual segmentation. We also observe significant performance improvements on unseen data. © 2007 EURASIP.},
keywords={Activity detection;  Automatic speech recognition;  Cross-talk problem;  Manual segmentation;  Multi-channel;  Multi-pass;  Performance improvements;  Segmentation system;  Speech recognition systems;  Speech understanding;  Word error rate, Microphones;  Signal processing, Speech recognition},
references={Boakye, K., Stolcke, A., Improved speech activity detection using cross-channel features for recognition of multiparty meetings (2006) Proc. of INTERSPEECH, pp. 1962-1965. , Pittsburgh PA, USA; Burger, S., MacLaren, V., Yu, H., The ISL meeting corpus: The impact of meeting type on speech style (2002) Proc. of ICSLP, pp. 301-304. , Denver CO, USA; Dines, J., Vepa, J., Hain, T., The segmentation of multi-channel meeting recordings for automatic speech recognition (2006) Proc. of INTERSPEECH, pp. 1213-1216. , Pittsburgh PA, USA; Fügen, C., Ikbal, S., Kraft, F., Kumatani, K., Laskowski, K., McDonough, J., Ostendorf, M., Wölfel, M., The ISL RT-06S speech-to-text evaluation system (2006) Proc. of MLMI (Springer Lecture Notes in Computer Science 4299), pp. 407-418. , Washington DC, USA; Laskowski, K., Jin, Q., Schultz, T., Crosscorrelation-based multispeaker speech activity detection (2004) Proc. of INTERSPEECH, pp. 973-976. , Jeju Island, South Korea; Laskowski, K., Schultz, T., Unsupervised learning of overlapped speech model parameters for multichannel speech activity detection in meetings (2006) Proc. of ICASSP, 1, pp. 993-996. , Toulouse, France; Laskowski, K., Schultz, T., A geometric interpretation of non-target-normalized maximum cross-channel correlation for vocal activity detection in meetings (2007) Proc. of HLT-NAACL, Short Papers, pp. 89-92. , Rochester NY, USA; Laskowski, K., Schultz, T., Modeling vocal interaction for segmentation in meeting recognition (2007) Proc. of MLMI, , Brno, Czech Republic; Pfau, T., Ellis, D., Stolcke, A., Multispeaker speech activity detection for the ICSI meeting recorder (2001) Proc. of ASRU, pp. 107-110. , Madonna di Campiglio, Italy; Wrigley, S., Brown, G., Wan, V., Renals, S., Feature selection for the classification of crosstalk in multi-channel audio (2003) Proc. of EUROSPEECH, pp. 469-472. , Geneva, Switzerland},
correspondence_address1={Laskowski, K.; Inter ACT, Universität Karlsruhe, am Fasanengarten 5, 76131 Karlsruhe, Germany; email: kornel@ira.uka.de},
sponsors={Eur. Off. Aerosp. Res. Dev., Air Force Off.; Sci. Res., United States Air Force Res. Lab.; Poznan Supercomputing and Networking Center; Univ. of Technology, Foundation for Development of Poznan; Mercure Accor Hotels; NOVOTEL ACCOR Hotels - Poznan Centrum},
address={Poznan},
issn={22195491},
isbn={9788392134022},
language={English},
abbrev_source_title={European Signal Proces. Conf.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tam2007520,
author={Tam, Y.-C. and Lane, I. and Schultz, T.},
title={Bilingual-LSA based LM adaptation for spoken language translation},
journal={ACL 2007 - Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics},
year={2007},
pages={520-527},
note={cited By 17; Conference of 45th Annual Meeting of the Association for Computational Linguistics, ACL 2007 ; Conference Date: 23 June 2007 Through 30 June 2007;  Conference Code:89523},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860503859&partnerID=40&md5=86817a95abed033e46b24179b1df6dd0},
affiliation={InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={We propose a novel approach to crosslingual language model (LM) adaptation based on bilingual Latent Semantic Analysis (bLSA). A bLSA model is introduced which enables latent topic distributions to be efficiently transferred across languages by enforcing a one-to-one topic correspondence during training. Using the proposed bLSA framework crosslingual LMadaptation can be performed by, first, inferring the topic posterior distribution of the source text and then applying the inferred distribution to the target language N-gram LMvia marginal adaptation. The proposed framework also enables rapid bootstrapping of LSA models for new languages based on a source LSA model from another language. On Chinese to English speech and text translation the proposed bLSA framework successfully reduced word perplexity of the English LMby over 27% for a unigram LMand up to 13.6% for a 4-gram LM. Furthermore, the proposed approach consistently improved machine translation quality on both speech and text based adaptation. © 2007 Association for Computational Linguistics.},
keywords={Bilingual latent semantic analysis;  Cross-lingual;  Language model;  Machine translations;  Marginal adaptation;  Posterior distributions;  Source text;  Spoken language translation;  Target language;  Word perplexity, Computational linguistics;  Speech transmission, Translation (languages)},
references={Blei, D., Ng, A., Jordan, M., Latent dirichlet allocation (2003) Journal of Machine Learning Research, pp. 1107-1135; Kim, W., Khudanpur, S., LM adaptation using cross-lingual information (2003) Proc. of Eurospeech; Kim, W., Khudanpur, S., Cross-lingual latent semantic analysis for LM (2004) Proc. of ICASSP; Kneser, R., Peters, J., Klakow, D., Language model adaptation using dynamic marginals (1997) Proc. of Eurospeech, pp. 1971-1974; Papineni, K., Roukos, S., Ward, T., Zhu, W., BLEU: A method for automatic evaluation of machine translation (2002) Proc. of ACL; Paulik, M., Fügen, C., Schaaf, T., Schultz, T., Stüker, S., Waibel, A., Document driven machine translation enhanced automatic speech recognition (2005) Proc. of Interspeech; Tam, Y.C., Schultz, T., Unsupervised language model adaptation using latent semantic marginals (2006) Proc. of Interspeech; Tam, Y.C., Schultz, T., Correlated latent semantic model for unsupervised languagemodel adaptation (2007) Proc. of ICASSP; Venugopal, A., Zollmann, A., Waibel, A., Training and evaluation error minimization rules for statistical machine translation (2005) Proc. of ACL; Vogel, S., SMT decoder dissected: Word reordering (2003) Proc. of ICNLPKE; Vogel, S., PESA: Phrase pair extraction as sentence splitting (2005) Proc. of the Machine Translation Summit; Zhao, B., Xing, E.P., BiTAM: Bilingual topic admixture models for word alignment (2006) Proc. of ACL},
correspondence_address1={Tam, Y.-C.; InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: yct@cs.cmu.edu},
address={Prague},
isbn={9781932432862},
language={English},
abbrev_source_title={ACL - Proc. Annu. Meet. Assoc. Comput. Linguist.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schultz2007,
author={Schultz, T. and Stone, M. and Zhai, C.},
title={Preface from the program co-chairs},
journal={NAACL HLT 2007 - Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, Proceedings of the Main Conference},
year={2007},
pages={VII-VIII},
note={cited By 0; Conference of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, NAACL HLT 2007 ; Conference Date: 22 April 2007 Through 27 April 2007;  Conference Code:88927},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858422381&partnerID=40&md5=3212e85a74b83f5eff14e7fbeb3c3f61},
affiliation={Carnegie Mellon University, United States; Rutgers University, United States; University of Illinois, Urbana-Champaign, United States},
correspondence_address1={Schultz, T.; Carnegie Mellon UniversityUnited States},
sponsors={Eastman Kodak Company; Microsoft Research; Powerset - Natural Language Search; Thomson; Association For Machine Translation in the Americas (AMTA)},
address={Rochester, NY},
language={English},
abbrev_source_title={NAACL HLT - Hum. Lang. Technol.: Conf. North Am. Chapter Assoc. Comput. Linguist., Proc. Main Conf.},
document_type={Editorial},
source={Scopus},
}

@CONFERENCE{Laskowski2007194,
author={Laskowski, K. and Ostendorf, M. and Schultz, T.},
title={Modeling vocal interaction for text-independent classification of conversation type},
journal={Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue},
year={2007},
pages={194-201},
note={cited By 15; Conference of 8th SIGdial Workshop on Discourseand Dialogue ; Conference Date: 1 September 2007 Through 2 September 2007;  Conference Code:88804},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349209213&partnerID=40&md5=4c59aeadfbc7fa68c37c6c710723f2d0},
affiliation={Inter ACT Universität Karlsruhe, Karlsruhe, Germany; Dept. of Electrical Engineering, University of Washington, Seattle WA, United States; Inter ACT Carnegie Mellon University, Pittsburgh PA, United States},
abstract={We describe a system for conversation type classification which relies exclusively on multi-participant vocal activity patterns. Using a variation on a well-studied model from stochastic dynamics, we extract features which represent the transition probabilities that characterize the evolution of participant interaction. We also show how vocal interaction can be modeled between specific participant pairs. We apply the proposed system to the task of classifying meeting types in a large multi-party meeting corpus, and achieve a three-way classification accuracy of 84%. This represents a relative error reduction of more than 50% over a baseline which uses only individual speaker times (i.e. no interaction dynamics). Random guessing on this data yields an accuracy of 43%. © 2007 Association for Computational Linguistics.},
keywords={Activity patterns;  Classification accuracy;  Data yields;  Interaction dynamics;  Relative errors;  Stochastic dynamics;  Transition probabilities;  Vocal interaction, Dynamics;  Stochastic models, Text processing},
references={Banerjee, S., Rudnicky, A., Using simple speech based features to detect the state of a meeting and the roles of the meeting participants (2004) Proceedings of INTERSPEECH, , Jeju Island, South Korea; Basu, S., (2002) Conversational Scene Analysis, , doctoral thesis, MIT; Brdiczka, O., Maisonnasse, J., Reignier, P., Automatic detection of interaction groups (2005) Proceedings of ICMI, , Trento, Italy; Burger, S., MacLaren, V., Yu, H., The ISL meeting corpus: The impact of meeting type on speech style (2002) Proceedings of ICSLP, pp. 301-304. , Denver CO, USA; Dabbs, J., Ruback, R., Dimensions of group process: Amount and structure of vocal interaction (1987) Advances in Experimental Psychology, 20, pp. 123-169; Fay, N., Garrod, S., Carletta, J., Group discussion as interactive dialogue or as serial monologue: The influence of group size (2000) Psychological Science, 11 (6), pp. 487-492; Glauber, R., Time-dependent statistics of the ising model (1963) Journal of Mathematical Physics, 4 (2), pp. 294-307; Hertz, J., Krogh, A., Palmer, R., (1991) Introduction to the Theory of Neural Compuations, , Addison-Wesley Longman; Janin, A., Baron, D., Edwards, J., Ellis, D., Gelbart, D., Morgan, N., Peskin, B., Wooters, C., The ICSI meeting corpus (2003) Proceedings of ICASSP, pp. 364-367. , Hong Kong, China; Jin, Q., Laskowski, K., Schultz, T., Speaker segmentation and clustering in meetings (2004) Proceedings of ICASSP NIST RT-04s Spring Meeting Recognition Evaluation Workshop, , Montreal, Canada; Laskowski, K., Schultz, T., Unsupervised learning of overlapped speech model parameters for multichannel speech activity detection in meetings (2006) Proceedings of ICASSP, pp. 993-996. , Toulouse, France; Laskowski, K., Schultz, T., Modeling vocal interaction for segmentation in meeting recognition (2007) Proceedings of MLMI, , (to appear), Brno, Czech Republic; Laskowski, K., Schultz, T., Analysis of the occurrence of laughter in meetings (2007) Proceedings of INTERSPEECH, , (to appear), Antwerpen, Belgium; McCowan, I., Gatica-Perez, D., Bengio, S., Lathoud, G., Barnard, M., Zhang, D., Automatic analysis of multimodal group actions in meetings (2005) IEEE Transactions on Pattern Analysis and Machine Intelligence, 27 (3), pp. 305-317. , DOI 10.1109/TPAMI.2005.49; Mirghafori, N., Wooters, C., Nuts and flakes: A study of data characteristics in speaker diarization (2006) Proceedings ICASSP, pp. 1017-1020. , Toulouse, France; Renals, S., Ellis, D., Audio information access from meeting rooms (2003) Proceedings ICASSP, pp. 744-747. , Hong Kong, China; Rienks, R., Heylen, D., Dominance detection in meetings using easily obtainable features (2005) Proceedings MLMI, , Edinburgh, UK; Sacks, H., Schegloff, E., Jefferson, G., A simplest semantics for the organization of turn-taking for conversation (1974) Language, 50 (4), pp. 696-735; Shriberg, E., Dhillon, R., Bhagat, S., Ang, J., Carvey, H., The ICSI meeting recorder dialog act (MRDA) corpus (2004) Proceedings SIGdial, pp. 97-100. , Cambridge MA, USA; Wyatt, D., Bilmes, J., Choudhury, T., Kautz, H., A privacy-sensitive approach to modeling multi-person conversations (2007) Proceedings IJCAI, pp. 1769-1775. , Hyderabad, India; Zancanaro, M., Lepri, B., Pianesi, F., Automatic detection of group functional roles in face to face interactions (2006) Proceedings ICMI, , Banff, Canada},
correspondence_address1={Laskowski, K.; Inter ACT Universität Karlsruhe, Karlsruhe, Germany; email: kornel@ira.uka.de},
sponsors={Tilburg University; Microsoft Research; John Benjamins Publishing Company},
address={Antwerp},
isbn={9789074029322},
language={English},
abbrev_source_title={Proc. SIGdial Workshop Discourse Dialogue},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tam2007187,
author={Tam, Y.-C. and Lane, I. and Schultz, T.},
title={Bilingual LSA-based adaptation for statistical machine translation},
journal={Machine Translation},
year={2007},
volume={21},
number={4},
pages={187-207},
doi={10.1007/s10590-008-9045-2},
note={cited By 27},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-57249108199&doi=10.1007%2fs10590-008-9045-2&partnerID=40&md5=c1f059682ff1a82c13349294acdeb27e},
affiliation={Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213, United States},
abstract={We propose a novel approach to cross-lingual language model and translation lexicon adaptation for statistical machine translation (SMT) based on bilingual latent semantic analysis. Bilingual LSA enables latent topic distributions to be efficiently transferred across languages by enforcing a one-to-one topic correspondence during training. Using the proposed bilingual LSA framework, model adaptation can be performed by, first, inferring the topic posterior distribution of the source text and then applying the inferred distribution to an n-gram language model of the target language and translation lexicon via marginal adaptation. The background phrase table is enhanced with the additional phrase scores computed using the adapted translation lexicon. The proposed framework also features rapid bootstrapping of LSA models for new languages based on a source LSA model of another language. Our approach is evaluated on the Chinese-English MT06 test set using the medium-scale SMT system and the GALE SMT system measured in BLEU and NIST scores. Improvement in both scores is observed on both systems when the adapted language model and the adapted translation lexicon are applied individually. When the adapted language model and the adapted translation lexicon are applied simultaneously, the gain is additive. At the 95% confidence interval of the unadapted baseline system, the gain in both scores is statistically significant using the medium-scale SMT system, while the gain in the NIST score is statistically significant using the GALE SMT system. © 2008 Springer Science+Business Media B.V.},
author_keywords={Bilingual latent semantic analysis;  Cross-lingual language model adaptation;  Latent Dirichlet-tree allocation;  Lexicon adaptation;  Statistical machine translation;  Topic distribution transfer},
keywords={Computational linguistics;  Computer aided language translation;  Information retrieval systems;  Information theory;  Query languages;  Semantics;  Speech transmission;  Surface mount technology;  Translation (languages), Bilingual latent semantic analysis;  Cross-lingual language model adaptation;  Latent Dirichlet-tree allocation;  Lexicon adaptation;  Statistical machine translation;  Topic distribution transfer, Linguistics},
references={Bellegarda, J.R., Large vocabulary speech recognition with multispan statistical language models (2000) IEEE Trans Speech Audio Process, 8, pp. 76-84; Blei, D., Ng, A., Jordan, M., Latent Dirichlet allocation (2003) J Mach Learn Res, 3, pp. 1107-1135; Brown, P.F., Della Pietra, S.A., Della Pietra, V.J., Mercer, R.L., The mathematics of statistical machine translation: Parameter estimation (1994) Comput Linguist, 19, pp. 263-311; Darroch, J.N., Ratcliff, D., Generalized iterative scaling for log-linear models (1972) Ann Math Stat, 43, pp. 1470-1480; Deerwester, S.C., Dumais, S.T., Landauer, T.K., Furnas, G.W., Harshman, R.A., Indexing by latent semantic analysis (1990) J Am Soc Inf Sci, 41, pp. 391-407; Doddington, G., Automatic evaluation of MT quality using n-gram co-occurrence statistics (2002) Proceedings of Human Language Technology Conference 2002, pp. 138-145. , San Diego, CA; Griffiths, T.L., Steyvers, M., Blei, D.M., Tenenbaum, J.B., Saul, L.K., Weiss, Y., Bottou, L., Integrating topics and syntax (2004) Advances in Neural Information Processing Systems 17, Proceedings of the 2004 Conference, pp. 537-544. , MIT Press Cambridge MA; Hofmann, T., Probabilistic latent semantic indexing (1999) UAI '99, Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, pp. 289-296. , Stockholm, Sweden; Hsu, B.-J., Glass, J., Style & topic language model adaptation using HMM-LDA (2006) EMNLP 2006, 2006 Conference on Empirical Methods in Natural Language Processing, pp. 373-381. , Sydney, Australia; Iyer, R., Ostendorf, M., Modeling long distance dependence in language: Topic mixtures vs. dynamic cache models (1996) ICSLP 96, Fourth International Conference on Spoken Language Processing, pp. 236-239. , Philadelphia, PA; Jolliffe, I.T., (2002) Principal Component Analysis, , 2 Springer New York; Kim, W., Khudanpur, S., LM adaptation using cross-lingual information (2003) 8th European Conference on Speech Communication and Technology (Eurospeech 2003 - Interspeech 2003), pp. 3129-3132. , Geneva, Switzerland; Kim, W., Khudanpur, S., Cross-lingual latent semantic analysis for LM (2004) 2004 IEEE International Conference on Acoustics, Speech, and Signal Processing, 1, pp. 257-260. , Montreal, Quebec, Canada; Kneser, R., Peters, J., Klakow, D., Language model adaptation using dynamic marginals (1997) Proceedings of Eurospeech '97, 5th European Conference on Speech Communication and Technology, pp. 1971-1974. , Rhodes, Greece; Mrva, D., Woodland, P.C., Unsupervised language model adaptation for Mandarin broadcast conversation transcription (2006) Interspeech 2006 - ICSLP, Ninth International Conference on Spoken Language Processing, , Pittsburgh, Pennsylvania, paper 1549-Thu1A2O.3; Och, F.J., Minimum error rate training in statistical machine translation (2003) ACL-03, 41st Annual Meeting of the Association for Computational Linguistics, pp. 160-167. , Sapporo, Japan; Papineni, K., Roukos, S., Ward, T., Zhu, W., BLEU: A method for automatic evaluation of machine translation (2002) 40th Annual Meeting of the Association of Computational Linguistics, pp. 311-318. , Philadelphia, Pennsylvania; Paulik, M., Fügen, C., Schaaf, T., Schultz, T., Stüker, S., Waibel, A., Document driven machine translation enhanced automatic speech recognition (2005) Proceedings of Interspeech'2005 - Eurospeech, 9th European Conference on Speech Communication and Technology, pp. 2261-2264. , Lisbon, Portugal; Rottmann, K., Vogel, S., Word reordering in statistical machine translation with a POS-based distortion model (2007) TMI 2007, Proceedings of the 11th International Conference on Theoretical and Methodological Issues in Machine Translation, pp. 171-180. , Skövde; Stolcke, A., SRILM - An extensible language modeling toolkit (2002) Proceedings of the 7th International Conference on Spoken Language Processing ICSLP/Interspeech, pp. 901-904. , Denver, Colorado; Tam, Y.C., Schultz, T., Language model adaptation using variational Bayes inference (2005) Proceedings of Interspeech'2005 - Eurospeech, 9th European Conference on Speech Communication and Technology, pp. 5-8. , Lisbon, Portugal; Tam, Y.C., Schultz, T., Unsupervised language model adaptation using latent semantic marginals (2006) Interspeech 2006 - ICSLP, Ninth International Conference on Spoken Language Processing, , Pittsburgh, Pennsylvania, paper 1705-Thu1A2O.2; Tam, Y.C., Schultz, T., Correlated latent semantic model for unsupervised language model adaptation (2007) Proceedings of ICASSP 2007, International Conference on Acoustics, Speech, and Signal Processing, 4, pp. 41-44. , Honolulu, Hawaii; Tseng, H., Chang, P., Andrew, G., Jurafsky, D., Manning, C., A conditional random field word segmenter (2005) IJCNLP-05, Fourth SIGHAN Workshop on Chinese Language Processing, pp. 168-171. , Jeju Island, Korea; Vogel, S., Zhang, Y., Huang, F., Tribble, A., Venugopal, A., Zhao, B., Waibel, A., The CMU statistical translation system (2003) MT Summit IX, Proceedings of the Ninth Machine Translation Summit, pp. 402-409. , New Orleans; Zhang, Y., Vogel, S., Measuring confidence intervals for the machine translation evaluation metrics (2004) Proceedings of the Tenth Conference on Theoretical and Methodological Issues in Machine Translation TMI-04, pp. 85-94. , Baltimore, Maryland; Zhao, B., Xing, E.P., BiTAM: Bilingual topic admixture models for word alignment (2006) Coling • ACL 2006, 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Main Conference Poster Sessions, pp. 969-976. , Sydney, Australia; Zhao, B., Xing, E.P., HM-BiTAM: Bilingual topic exploration, word alignment, and translation (2007) Twenty-second Annual Conference on Neural Information Processing Systems, , Vancouver BC, Canada},
correspondence_address1={Tam, Y.-C.; Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213, United States; email: yct@cs.cmu.edu},
issn={09226567},
coden={MACTE},
language={English},
abbrev_source_title={Mach Transl},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Rao20072812,
author={Rao, S. and Lane, I. and Schultz, T.},
title={Optimizing sentence segmentation for spoken language translation},
journal={International Speech Communication Association - 8th Annual Conference of the International Speech Communication Association, Interspeech 2007},
year={2007},
volume={4},
pages={2812-2815},
note={cited By 7; Conference of 8th Annual Conference of the International Speech Communication Association, Interspeech 2007 ; Conference Date: 27 August 2007 Through 31 August 2007;  Conference Code:73788},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-56149113663&partnerID=40&md5=d9e2c22bfc4f996b48e57a8c8858cb8d},
affiliation={Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={The conventional approach in text-based machine translation (MT) is to translate complete sentences, which are conveniently indicated by sentence boundary markers. However, since such boundary markers are not available for speech, new methods are required that define an optimal unit for translation. Our experimental results show that with a segment length optimized for a particular MT system, intra-sentence segmentation can improve translation performance (measured in BLEU) by up to 11% for Arabic Broadcast Conversation (BC) and 6% for Arabic Broadcast News (BN). We show that acoustic segmentation that minimizes Word Error Rate (WER) may not give the best translation performance. We improve upon it by automatically resegmenting the ASR output in a way that is optimized for translation and argue that it might be necessary for different stages of a Spoken Language Translation (SLT) system to define their own optimal units.},
author_keywords={Automatic speech recognition;  Optimal segment length;  Sentence segmentation;  Statistical machine translation},
keywords={Acoustic segmentations;  Arabic broadcast news;  Automatic speech recognition;  Conventional approaches;  Machine translations;  Mt systems;  Optimal segment length;  Resegmenting;  Segment lengths;  Sentence boundaries;  Sentence segmentation;  Sentence segmentations;  Spoken language translations;  Statistical machine translation;  Word Error Rates, Computer aided language translation;  Information theory;  Linguistics;  Photolithography;  Speech;  Speech recognition;  Speech transmission;  Telluric prospecting;  Translation (languages), Speech communication},
references={Y. Liu, E. Shriberg, A. Stolcke, D. Hillard, M. Ostendorf, and M. Harper, Enriching speech recognition with automatic detection of sentence boundaries and disfluencies, IEEE Transactions on Audio, Speech, and Language Processing, 14, pp. 1526-1540, September 2006; Huang, J., Zweig, G., Maximum entropy model for punctuation annotation from speech (2002) Proc. of ICLSP, pp. 917-920; J. Xu, R. Zens, and H. Ney, Sentence Segmentation Using IBM Word Alignment Model 1, In Proceedings of the 10th Annual Conference of the European Association for Machine Translation (EAMT 2005), pp. 280-287, Budapest, May 2005; Doi, T., Sumita, E., Splitting Input for Machine Translation Using N-gram Language Model Together with Utterance Similarity Coling 2004; Kim, S.D., Byoung-Tak Zhang, Y., Kim, T., Reducing Parsing Complexity by Intra-Sentence Segmentation Using Genetic Learning (2000) 38th Annual Meeting of the Association for Computational Linguistics, pp. 164-171. , p.p, Hong Kong; Lavie, A., Gates, D., Coccaro, N., Levin, L.S., Input Segmentation of Spontaneous Speech in JANUS: A Speech-to-speech Translation System (1996) Workshop on Dialogue Processing in Spoken Language Systems, pp. 86-99; Ostendorf, M., Hillard, D., Scoring structural mde: Towards more meaningful error rates (2004) EARS Rich Transcription Workshop; Noamany, M., Schaaf, T., Schultz, T., Advances in the CMU-InterACT Arabic Gale Transcription System (2007) Proceedings of the HLT/NAACL; Sebastian Stuker, Christian Fugen, Roger Hsiao, Shajith Ikbal, Qin Jin, Florian Kraft, Matthias Paulik, Martin Raab, Yik-Cheung Tam, and Matthias Wolfel, The ISL TC-STAR Spring 2006 ASR Evaluation Systems, In Proc. of the TC-STAR Workshop on Speech-to-Speech Translation, Barcelona, 2006; Eck, M., Lane, I., Bach, N., Hewavitharana, S., Kolss, M., Zhao, B., Silja Hildebrand, A., Waibel, A., The UKA/CMU Statistical Machine Translation System for IWSLT 2006 (2006) Proc. of the IWSLT, , Kyoto; Och, F., Minimum error rate training in statistical machine translation (2003) Proc. of the 41st Annual Meeting of the Association for Computational Linguistics, pp. 160-167. , ACL, pp, Japan, July; Papineni, K., Roukos, S., Ward, T., Zhu, W., Bleu: A method for automatic evaluation of machine translation, (2001), Technical Report RC22I76, IBM Research Division, Thomas J. Watson Research Center},
correspondence_address1={Rao, S.; Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
address={Antwerp},
isbn={9781605603162},
language={English},
abbrev_source_title={Int. Speech Commun. Assoc. - Annu. Conf. Int. Speech Commun. Assoc., Interspeech},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wand20071773,
author={Wand, M. and Szu-Jou, C.S. and Schultz, T.},
title={Wavelet-based front-end for electromyographic speech recognition},
journal={International Speech Communication Association - 8th Annual Conference of the International Speech Communication Association, Interspeech 2007},
year={2007},
volume={3},
pages={1773-1776},
note={cited By 0; Conference of 8th Annual Conference of the International Speech Communication Association, Interspeech 2007 ; Conference Date: 27 August 2007 Through 31 August 2007;  Conference Code:73788},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-56149098896&partnerID=40&md5=1a9ca48d383acb16bde56a82b232b224},
affiliation={International Center for Advanced Communication Technologies, Carnegie Mellon University, USA and Universität Karlsruhe, Germany},
abstract={In this paper we present our investigations on the potential of wavelet-based preprocessing for surface electromyographic speech recognition. We implemented several variants of the Discrete Wavelet Transform and applied them to electromyographical data. First we examined different transforms with various filters and decomposition levels and found that the Redundant Discrete Wavelet Transform performs the best among all tested wavelet transforms. Furthermore, we compared the best wavelet transform to our EMG optimized spectral- and time-domain features. The results showed that the best wavelet transform slightly outperforms the optimized features with 30.9% word error rate compared to 32% for the optimized EMG spectral and time-domain features. Both numbers were achieved on a 108 word vocabulary test set using phone based acoustic models trained on continuously spoken speech captured by EMG.},
author_keywords={Electromyography;  Preprocessing;  Speech recognition;  Wavelets},
keywords={Discrete wavelet transforms;  Mathematical transformations;  Speech;  Speech analysis;  Speech communication;  Speech recognition;  Time domain analysis;  Wavelet decomposition, Acoustic models;  Decomposition levels;  Domain features;  Electromyographic;  Preprocessing;  Test sets;  Wavelets;  Word Error Rates, Wavelet transforms},
references={Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session Independent Non-Audible Speech Recognition Using Surface Electromyography (2005) Proc. ASRU; Jorgensen, C., Binsted, K., Web Browser Control Using EMG Based Sub Vocal Speech Recognition (2005) Proceedings of the 38th Hawaii International Conference on System Sciences; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards Continuous Speech Recognition using Surface Electromyography (2006) Proc. Interspeech, , Pittsburgh, PA, September; Jou, S.-C., Maier-Hein, L., Schultz, T., Waibel, A., Articulatory Feature Classification using Surface Electromyography (2006) Proc. ICASSP, , Toulouse, France, May; Walliczek, M., Kraft, F., Jou, S.-C., Schultz, T., Waibel, A., Sub-Word Unit based Non-audible Speech Recognition using Surface Electromyography (2006) Proc. Interspeech, , Pittsburgh, PA, September; Shensa, M.J., The Discrete Wavelet Transform: Wedding the À Trous and Mallat Algorithms (1992) IEEE Transactions on Signal Processing, 40, pp. 2464-2482. , October; S. G. Mallat. A Theory for Multiresolution Signal Decomposition: The Wavelet Representation. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 11, no. 7, pp. pp. 674-693, July 1989; Selesnick, I.W., Baraniuk, R.G., Kingsbury, N.G., The Dual-Tree Complex Wavelet Transform (2005) IEEE Signal Processing Magazine, 22 (6), pp. 123-151. , November; Daubechies, I., Orthonormal Bases of Compactly Supported Wavelets (1988) Comm. Pure Appl. Math, 41; Kingsbury, N.G., A Dual-Tree Complex Wavelet Transform with Improved Orthogonality and Symmetry Properties (2000) Proc. IEEE Conf. on Image Processing, , Vancouver},
correspondence_address1={Wand, M.; International Center for Advanced Communication Technologies, Carnegie Mellon University, USA and Universität KarlsruheGermany; email: michael.wand@stud.uni-karlsruhe.de},
address={Antwerp},
isbn={9781605603162},
language={English},
abbrev_source_title={Int. Speech Commun. Assoc. - Annu. Conf. Int. Speech Commun. Assoc., Interspeech},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bach20071057,
author={Bach, N. and Noamany, M. and Lane, I. and Schultz, T.},
title={Handling OOV words in Arabic ASR via flexible morphological constraints},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2007},
volume={2},
pages={1057-1060},
note={cited By 3; Conference of 8th Annual Conference of the International Speech Communication Association, Interspeech 2007 ; Conference Date: 27 August 2007 Through 31 August 2007;  Conference Code:73788},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-56149107612&partnerID=40&md5=4d69d0da5d3620c6bd51019cd918a469},
affiliation={Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={We propose a novel framework to detect and recognize out-of-vocabulary (OOV) words in automated speech recognition (ASR). In the proposed framework a hybrid language model combining words and sub-word units is incorporated during ASR decoding then three different OOV words recognition methods are applied to generate OOV word hypotheses. Specifically, dictionary lookup, morphological composition, and direct phoneme-to-grapheme. The proposed approach successfully reduced WER by 1.9% and 1.6% for ASR systems with recognition vocabularies of 30K and 219K. Moreover, the proposed approach correctly recognized 5% of OOV words.},
author_keywords={Out-of-vocabulary words recognition;  Speech recognition;  Subword language modeling},
keywords={ASR decoding;  Asr systems;  Automated speech recognitions;  Dictionary lookups;  Hybrid language models;  OOV words;  Out-of-vocabulary words recognition;  Recognition methods;  Subword language modeling, Computational linguistics;  Decoding;  Linguistics;  Query languages;  Remelting;  Speech;  Speech analysis;  Speech recognition, Speech communication},
references={Yazgan, A., Saraclar, M., Hybrid language models for out of vocabulary word detection in large vocabulary conversational speech recognition (2004) Proc. Int. Conf. of Acoustics, Speech, and Signal Processing, Canada, May, 1, pp. 745-748; Bazzi, I., Glass, J., Modeling out of vocabulary words for robust speech recognition (2000) Proc. Int. Conf. on Sposken Language Processing, , Bejing, China; Noamany, M., Schaaf, T., Schultz, T., Advances in the CMU/InterACT Arabic GALE transcription system (2007) Proc. of Human Language Technology, , to appear in, Rochester, New York, USA; Huang, F., Vogel, S., Waibel, A., Towards named entity extraction and translation in spoken language translation (2004) Proc. of the Human Language Technology, , Boston, USA; Favre, B., Bechet, F., Nocere, P., Robust named entity extraction from large spoken archives (2005) Proc. of the Emperical Methods in Natural Language Processing, , Vancouver, Canada; Bisani, M., Ney, H., Open Vocabulary Speech Recognition with Flat Hybrid Models (2005) Proc. of the European Conf. on Speech Communication and Technology, pp. 725-728. , Lisbon, Portugal, September; Stolcke, A., SRILM - An Extensible Language Modeling Toolkit (2002) Proc. Intl. Conf. on Spoken Language Processing, 2, pp. 901-904. , Denver, USA; Schaaf, T., Detection Of OOV Words Using Generalized Word Models And A Semantic Class Language Model (2001) Proc. of the European Conf. on Speech Communication and Technology, , Aalborg, Denmark; Zollmann, A., Venugopal, A., Vogel, S., Bridging the Inflection Morphology Gap for Arabic Statistical Machine Translation (2006) Proc. of the Human Language Technology, , New York, USA; Bing, X., Kham, N., Long, N., Richard, S., John, M., Morphological Decomposition for Arabic Broadcast News Transcription (2006) Proc. Int. Conf. of Acoustics, Speech, and Signal Processing, , Toulouse, France; Bazzi, I., Glass, J., A Multi-Class Approach for Modelling out-of-Vocabulary Words (2002) Proc. Int. Conf. on Sposken Language Processing, pp. 1613-1616. , Denver, USA; Soltau, H., Metze, F., Fugen, C., Waibel, A., A one pass-decoder based on polymorphic linguistic context as-signment (2001) Proc. of the IEEE Workshop on Automatic Speech Recognition and Understanding},
correspondence_address1={Bach, N.; Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: nbach@cs.cmu.edu},
address={Antwerp},
issn={19909772},
isbn={9781605603162},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schultz2007125,
author={Schultz, T. and Black, A.W. and Badaskar, S. and Homyak, M. and Kominek, J.},
title={SPICE: Web-based tools for rapid language adaptation in speech processing systems},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2007},
volume={1},
pages={125-128},
note={cited By 22; Conference of 8th Annual Conference of the International Speech Communication Association, Interspeech 2007 ; Conference Date: 27 August 2007 Through 31 August 2007;  Conference Code:73788},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-56149096853&partnerID=40&md5=4eaaabb3b8b5ee24683a043a4b068d58},
affiliation={InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, United States},
abstract={In this paper we describe the design and implementation of a user interface for SPICE, a web-based toolkit for rapid prototyping of speech and language processing components. We report on the challenges and experiences gathered from testing these tools in an advanced graduate hands-on course, in which we created speech recognition, speech synthesis, and smalldomain translation components for 10 different languages within only 6 weeks.},
author_keywords={Language Adaptation;  Multilingual Speech Recognition;  Multilingual Speech Synthesis;  Multilinguality;  Web Interfaces},
keywords={Language Adaptation;  Multilingual Speech Recognition;  Multilingual Speech Synthesis;  Multilinguality;  Web Interfaces, Computational linguistics;  Concurrent engineering;  Information theory;  Job analysis;  Linguistics;  Query languages;  Rapid prototyping;  Speech;  Speech analysis;  Speech processing;  Speech recognition;  Speech synthesis;  Telecommunication equipment;  User interfaces, Speech communication},
references={(2006) Multilingual Speech Processing, , Schultz T. and Kirchhoff, K, Eds, Academic Press; Schultz, T., Black, A., (2006) Challenges with Rapid Adaptation of Speech Translation Systems to New Language Pairs, , ICASSP, Toulouse, France; Schultz, T., GlobalPhone: A Multilingual Speech and Text Database developed at Karlsruhe University. ICSLP, Denver, CO, 2002; Black, A., Lenzo, K., The FestVox Project: Building Synthetic Voices, , http://festvox.org/bsv/2000; Soltau, H., Metze, F., Fügen, C., Waibel, A., A one-pass decoder based on polymorphic linguistic context assignment (2001) IEEE ASRU Workshop, , Madonna di Campiglio, Italy; Davel, M., Barnard, E., Efficient generation of pronunciation dictionaries: Machine learning factors during bootstrapping, , ICSLP2004, Jeju, Korea; Kominek, J., Black, A., Learning Pronunciation Dictionaries: Language Complexity and Word Selection Strategies Proceedings of the Human Language Technology Conference of the NAACL, pp. 232-239. , New York City, USA; Black, A., CLUSTERGEN: a statistical parametric synthesizer using trajectory modeling, INTERSPEECH-2006, Pittsburgh, PA, September 2006; Black, A., Schultz, T., Speaker Clustering for Multilingual Synthesis MULTILING 2006, Stellenbosch, S. Africa; Engelbrecht, H., Schultz, T., Rapid Development of an Afrikaans-English Speech-to-Speech Translator (2005) Proceedings of International Workshop of Spoken Language Translation (IWSLT), , Pittsburgh, PA, October; Mircheva, A., (2006) Bulgarian Speech Recognition and Multilingual Language Modeling, Studienarbeit, Institut für Theoretische Informatik, Lehrstuhl Prof. Waibel, , Universität Karlsruhe, March; Viet-Bac, L., Besacier, L., Schultz, T., Acoustic-Phonetic Unit Similarities for Context-Dependent Acoustic Model Portability (2006) Proceeding on Acoustics, Speech, and Signal Processing (ICASSP-2006), , Toulouse, France, May; http://www.elda.org/catalogue/en/text/W0038.html, EMILLE; http://www.speech.cs.cmu.edu/cgi-bin/cmudict, CMUDICT; Schultz, T., Towards Rapid Language Portability of Speech Processing Systems (2004) Conference on Speech and Language Systems for Human Communication, , Delhi, India, November},
correspondence_address1={Schultz, T.; InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, United States; email: tanja@cs.cmu.edu},
address={Antwerp},
issn={19909772},
isbn={9781605603162},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Jin20072023,
author={Jin, Q. and Schultz, T. and Waibel, A.},
title={Far-field speaker recognition},
journal={IEEE Transactions on Audio, Speech and Language Processing},
year={2007},
volume={15},
number={7},
pages={2023-2032},
doi={10.1109/TASL.2007.902876},
art_number={4291604},
note={cited By 68},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-50449087648&doi=10.1109%2fTASL.2007.902876&partnerID=40&md5=e3f04ace906e7ef24f3c56856f45e4e3},
affiliation={Interactive Systems Laboratories, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={In this paper, we study robust speaker recognition in far-field microphone situations. Two approaches are investigated to improve the robustness of speaker recognition in such scenarios. The first approach applies traditional techniques based on acoustic features. We introduce reverberation compensation as well as feature warping and gain significant improvements, even under mismatched training-testing conditions. In addition, we performed multiple channel combination experiments to make use of information from multiple distant microphones. Overall, we achieved up to 87.1% relative improvements on our Distant Microphone database and found that the gains hold across different data conditions and microphone settings. The second approach makes use of higher-level linguistic features. To capture speaker idiosyncrasies, we apply n-gram models trained on multilingual phone strings and show that higher-level features are more robust under mismatching conditions. Furthermore, we compared the performances between multilingual and multiengine systems, and examined the impact of a number of involved languages on recognition results. Our findings confirm the usefulness of language variety and indicate a language independent nature of this approach, which suggests that speaker recognition using multilingual phone strings could be successfully applied to any given language. © 2006 IEEE.},
author_keywords={Far-field microphones;  Mismatched conditions;  Multilingual phone strings;  Robust speaker recognition},
keywords={Acoustic features;  Far-field microphones;  Linguistic features;  Mismatched conditions;  Multilingual phone strings;  Multiple channels;  N-gram models;  Robust speaker recognition;  Speaker recognition;  Testing conditions;  Traditional techniques, Linguistics;  Query languages;  Telephone sets, Microphones},
references={Pelecanos, J., Sridharan, S., Feature warping for robust speaker verification (2001) Proc. Speaker Odyssey 2001 Conf, pp. 213-218. , Jun; Furui, S., Cepstral analysis technique for automatic speaker verification (1981) IEEE Trans. Acoust., Speech, Signal Process, ASSP-19 (2), pp. 254-272. , Apr; Janin, A., Baron, D., Edwards, J., Ellis, D., Gelbart, D., Morgan, N., Peskin, B., Wooters, C., The ICSI meeting corpus Proc. ICASSP, p. 2003. , pp. I-364-I-367; Reynolds, D., Speaker identification and verification using Gaussian mixture speaker models (1995) Speech Commun, 17 (1-2), pp. 91-108. , Aug; Jin, Q., Pan, Y., Schultz, T., Far-field speaker recognition Proc. ICASSP, p. 2006. , pp. I-937-I-940; Xiang, B., Chaudhari, U., Navratil, J., Ramaswamy, G., Gopinath, R., Short-time Gaussianization for robust speaker verification Proc. ICASSP, p. 2002. , pp. I-681-I-684; Weber, F., Manganaro, L., Peskin, B., Shriberg, E., Using prosodic and lexical information for speaker identification Proc. ICASSP, p. 2002. , pp. I-141-I-144; Jin, Q., Schultz, T., Waibel, A., Phonetic speaker recognition (2002) Proc. ICSLP, pp. 1345-1348; Jin, Q., Schultz, T., Waibel, A., Speaker identification using multilingual phone strings Proc. ICASSP, p. 2002. , pp. I-145-I-148; Andrews, W., Kohler, M., Compbell, J., Phonetic Speaker Recognition (2001) Proc. Eurospeech, pp. 2517-2520; Andrews, W., Kohler, M., Campbell, J., Godfrey, J., Hernandez-Cordero, J., Gender-dependent phonetic refraction for speaker recognition Proc. ICASSP, p. 2002. , pp. I-149-I-152; Doddington, G., Speaker recognition based on idiolectal differences between speakers (2001) Proc. Eurospeech, pp. 2521-2524; Q. Jin, J. Navratil, D. Reynolds, W. Andrews, J. Campbell, and J. Abramson, Combining cross-stream and time dimensions in phonetic speaker recognition, in Proc. ICASSP, 2003, pp. IV-800-IV-803; Jin, Q., Robust speaker recognition, (2007), Ph.D. dissertation, School of Computer Science, Carnegie Mellon Univ, Pittsburgh, PA; Clarkson, P., Rosenfeld, R., Statistical language modeling using the CMU-Cambridge toolkit (1997) Proc. Eurospeech, pp. 2707-2710; Schultz, T., Kirchhoff, K., (2006) Multilingual Speech Processing, , 1st ed. New York: Elsevier/Academic; Schultz, T., Waibel, A., Language independent and language adaptive acoustic modeling for speech recognition (2001) Speech Commun, 35, pp. 31-51; Schultz, T., Jin, Q., Laskowski, K., Tribble, A., Waibel, A., Improvements in non-verbal cue identification using multilingual phone strings (2002) Proc. Speech-to-Speech Translation Workshop, ACL, pp. 101-108; Pan, Y., Robust speech recognition on distant microphones, (2007), Ph.D. dissertation, Lang. Technol. Inst, Carnegie Mellon Univ, Pittsburgh, PA, thesis in submission},
correspondence_address1={Jin, Q.; Interactive Systems Laboratories, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: qjin@cs.cmu.edu},
issn={15587916},
language={English},
abbrev_source_title={IEEE Trans. Audio Speech Lang. Process.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Tam2007,
author={Tam, Y.-C. and Schultz, T.},
title={Correlated latent semantic model for unsupervised LM adaptation},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2007},
volume={4},
pages={IV41-IV44},
doi={10.1109/ICASSP.2007.367158},
art_number={4218032},
note={cited By 30; Conference of 2007 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '07 ; Conference Date: 15 April 2007 Through 20 April 2007;  Conference Code:69976},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547552501&doi=10.1109%2fICASSP.2007.367158&partnerID=40&md5=1e65303408474841e6ff2033f42acb86},
affiliation={InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={We propose a Latent Dirichlet-Tree Allocation (LDTA) model - a correlated latent semantic model - for unsupervised language model adaptation. The LDTA model extends the Latent Dirichlet Allocation (LDA) model by replacing a Dirichlet prior with a Dirichlet-Tree prior over the topic proportions. Latent topics under the same subtree are expected to be more correlated than topics under different subtrees. The LDTA model falls back to the LDA model using a depth-one Dirichlet-Tree, and the model fits to the variational Bayes inference framework employed in the LDA model. Empirical results show that the LDTA model has a faster training convergence than the LDA model with the same initial flat model. Experimental results show that LDTA-adapted LM performed better than LDAadapted LM on the Mandarin RT04-eval set when the models were trained using a small text corpus, while both models had the same recognition performance when the models were trained using a big text corpus. We observed 0.4% absolute CER reduction after LM adaptation using LSA marginals. © 2007 IEEE.},
author_keywords={Correlated topics;  Dirichlet-Tree;  LSA;  Unsupervised LM adaptation},
keywords={Correlated topics;  Dirichlet-Tree;  Latent Dirichlet Allocation (LDA);  Unsupervised LM adaptation, Adaptive systems;  Computer programming languages;  Speech recognition;  Trees (mathematics);  Unsupervised learning, Semantics},
references={Blei, D., Ng, A., Jordan, M., Latent Dirichlet Allocation (2003) Journal of Machine Learning Research, pp. 1107-1135; Tam, Y.C., Schultz, T., Language model adaptation using variational bayes inference (2005) Proc. of Interspeech; Tam, Y.C., Schultz, T., Unsupervised Language Model Adaptation using Latent Semantic Marginals (2006) Proceedings of the Interspeech; Mrva, D., Woodland, P.C., Unsupervised language model adaptation for mandarin broadcast conversation transcription (2006) Proc. of Interspeech; Connor, R.J., Mosimann, J.E., Concepts of independence for proportions with a generalization of the dirichlet distribution (1969) Journal of the American Statistical Association, 64, pp. 194-206; Minka, T., (1999) The dirichlet-tree distribution, , http://research.microsoft.com/~minka/papers/dirichlet/minkadirtree.pdf; Blei, D., Lafferty, J., Correlated topic models (2005) Advances in Neural Information Processing Systems; Li, W., McCallum, A., Pachinko allocation: DAG-structured mixture models of topic correlations (2006) International Conference on Machine Learning; Kneser, R., Peters, J., Klakow, D., Language model adaptation using dynamic marginals (1997) Proc. of Eurospeech, pp. 1971-1974; Yu, H., Tam, Y.C., Schaaf, T., Stüker, S., Jin, Q., Noamany, M., Schultz, T., The ISL RT04 Mandarin Broadcast News Evaluation System (2004) EARS Rich Transcription Workshop},
correspondence_address1={Tam, Y.-C.; InterACT, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: yct@cs.cmu.edu},
sponsors={Inst. of Electrical and Electronics Eng. Signal Proces. Soc.},
address={Honolulu, HI},
issn={15206149},
isbn={1424407281; 9781424407286},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jou2007,
author={Jou, S.-C.S. and Schultz, T. and Waibel, A.},
title={Continuous electromyographic speech recognition with a multi-stream decoding architecture},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2007},
volume={4},
pages={IV401-IV404},
doi={10.1109/ICASSP.2007.366934},
art_number={4218122},
note={cited By 17; Conference of 2007 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '07 ; Conference Date: 15 April 2007 Through 20 April 2007;  Conference Code:69976},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547522669&doi=10.1109%2fICASSP.2007.366934&partnerID=40&md5=05ab46221e6b02d419ae8e3d0773f92e},
affiliation={International Center for Advanced Communication Technologies, Carnegie Mellon University, Pittsburgh, PA, United States},
abstract={In our previous work, we reported a surface electromyographic (EMG) continuous speech recognition system with a novel EMG feature extraction method, E4. which is more robust to EMG noise than traditional spectral features. In this paper, we show that articulatory feature (AF) classifiers can also benefit from the E4 feature, which improve the F-score of the AF classifiers from 0.492 to 0.686. We also show that the E4 feature is less correlated across EMG channels and thus channel combination gains larger improvement in F-score. With a stream architecture, the AF classifiers are then integrated into the decoding framework and improve the word error rate by 11.8% relative from 33.9% to 29.9%. © 2007 IEEE.},
author_keywords={Articulatory features;  Articulatory muscles;  Electromyography;  Feature extraction;  Speech recognition},
keywords={Articulatory features;  Articulatory muscles, Bit error rate;  Electromyography;  Feature extraction;  Robust control;  Signal encoding, Speech recognition},
references={Nakajima, Y., Kashioka, H., Shikano, K., Campbell, N., Non-audible murmur recognition input interface using stethoscopic microphone attached to the skin (2003) Proc. ICASSP, , Hong Kong; Jou, S.-C., Schultz, T., Waibel, A., Whispery speech recognition using adapted articulatory features (2005) Proc. ICASSP, , Philadelphia, PA, March; Chan, A.D.C., Englehart, K., Hudgins, B., Lovely, D.F., Hidden Markov model classification of myoelectric signals in speech (2002) IEEE Engineering in Medicine and Biology Magazine, 21 (4), pp. 143-146; Betts, B., Jorgensen, C., Small vocabulary communication and control using surface electromyography in an acoustically noisy environment (2006) Proc. HICSS, , Hawaii, Jan; Manabe, H., Hiraiwa, A., Sugimura, T., Unvoiced speech recognition using EMG-mime speech recognition (2003) Proc. HFCS, , Ft. Lauderdale, Florida; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) Proc. ASRU, , San Juan. Puerto Rico, Nov; Jou, S.-C., Maier-Hein, L., Schultz, T., Waibel, A., Articulatory feature classification using surface electromyography (2006) Proc. ICASSP, , Toulouse. France, May; Jou, S.-C., Schultz, T., Walliczek, M., Kraft, F., Waibel, A., Towards continuous speech recognition using surface electromyography, (2006) Proc. Interspeech, , Pittsburgh, PA, Sep; Becker, K., Varioport, , http://www.becker-meditec.de; Yu, H., Waibel, A., Streaming the front-end of a speech recognizer (2000) Proc. ICSLP, , Beijing, China; Metze, F., Waibel, A., A flexible stream architecture for ASR using articulatory features (2002) Proc. ICSLP, , Denver. CO. Sep},
correspondence_address1={Jou, S.-C.S.; International Center for Advanced Communication Technologies, Carnegie Mellon University, Pittsburgh, PA, United States; email: scjou@cs.cmu.edu},
sponsors={Inst. of Electrical and Electronics Eng. Signal Proces. Soc.},
address={Honolulu, HI},
issn={15206149},
isbn={1424407281; 9781424407286},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Paulik200730,
author={Paulik, M. and Stüker, S. and Fügen, C. and Schultz, T. and Waibel, A.},
title={Translating language with technology's help},
journal={IEEE Potentials},
year={2007},
volume={26},
number={3},
pages={30-35},
doi={10.1109/MP.2007.361642},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249291297&doi=10.1109%2fMP.2007.361642&partnerID=40&md5=1d892e63b6c4a2223803db91d77b4886},
affiliation={Department of Computer Science, Universität Karlsruhe, Germany; IEEE, Germany; IEEE Karlsruhe Student Branch, Germany; ISCA, Germany; Carnegie Mellon, United States; Computer Science Department, Carnegie Mellon University, Karlsruhe University, Germany; InterACT, International Center of Advanced Communication Technologies},
abstract={Some of the latest technological advancements, such as automatic transcription systems and techniques that help human interpreters translate languages at lower costs and efforts, were analyzed. Machine translation (MT) was one of the latest technologies that automatically translated text from a source. Some of the latest language translation technologies were also utilizing statistical models to solve the problem of automatic language translation. Automatic Speech Recognition (ASR) systems were also being used to translate and recognize the speech of interpreters and speakers. Several basic adaptation techniques were compared in a study, to improve the performance of the ASR system's main components on the basis of a written source language representation. Techniques to adapt the ASR component using knowledge provided by the MT component and adapt the MT component using knowledge derived from ASR, were also analyzed.},
keywords={Automatic Speech Recognition (ASR) systems;  Automatic transcription systems;  Statistical models, Cost accounting;  Information analysis;  Mathematical models;  Pattern recognition systems;  Speech recognition;  Statistical methods;  Text processing, Computer aided language translation},
correspondence_address1={Paulik, M.; Department of Computer Science, Universität KarlsruheGermany; email: paulik@ira.uka.de},
issn={02786648},
coden={IEPTD},
language={English},
abbrev_source_title={IEEE Potentials},
document_type={Article},
source={Scopus},
}

@ARTICLE{Schultz200747,
author={Schultz, T.},
title={Speaker characteristics},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2007},
volume={4343 LNAI},
pages={47-74},
doi={10.1007/978-3-540-74200-5_3},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-36248938764&doi=10.1007%2f978-3-540-74200-5_3&partnerID=40&md5=f23cea38a9bdf24f6d9cc1ddb9603f3f},
affiliation={Carnegie Mellon University, Pittsburgh, PA, United States},
abstract={In this chapter, we give a brief introduction to speech-driven applications in order to motivate why it is desirable to automatically recognize particular speaker characteristics from speech. Starting from these applications, we derive what kind of characteristics might be useful. After categorizing relevant speaker characteristics, we describe in more detail language, accent, dialect, idiolect, and sociolect. Next, we briefly summarize classification approaches to illustrate how these characteristics can be recognized automatically, and conclude with a practical example of a system implementation that performs well on the classification of various speaker characteristics. © Springer-Verlag Berlin Heidelberg 2007.},
author_keywords={Automatic speaker classification;  Language-dependent speaker characteristics;  Multilingual phonetic recognition;  Real-world applications},
keywords={Classification (of information);  Formal languages, Automatic speaker classification;  Language-dependent speaker characteristics;  Multilingual phonetic recognition;  Real-world applications, Speech recognition},
references={Sacks, O.W., (1985) The Man who Mistook His Wife for a Hat - and other Clinical Trials, , New York summit Books; Krauss, R.M., Freyberg, R., Morsella, E., Inferring speakers' physical attributes from their voices (2002) Journal of Experimental Social Psychology, 38, pp. 618-625; Nass, C., Brave, S., (2005) Wired for Speech: How Voice Activates and Advances the Human-Computer Relationship, , MIT Press, Cambridge; Sproat, R.: Review in Computational Linguist 17.65 on Nass and Brave 2005. Linguist List 17.65 (2006) http://linguistlist.org/issues/17/17-65.html; Nass, C., Gong, L., Speech Interfaces from an Evolutionary Perspective: Social Psychological Research and Design Implications (2000) Communications of the ACM, 43 (9), pp. 36-43; Nass, C., Lee, K.M., Does computer-generated speech manifest personality? an experimental test of similarity-attraction (2000) CHI '00: Proceedings of the SIGCHI conference on Human factors in computing systems, pp. 329-336. , ACM Press, New York; Tokuda, K., Hidden Markov model-based Speech Synthesis as a Tool for constructing Communicative Spoken Dialog Systems (2006) Proc. 4th Joint Meeting of the Acoustical Society of America and the Acoustical Society of Japan, Special Session on Speech Communication: Communicative Speech Synthesis and Spoken Dialog, invited paper, , Honolulu, Hawaii; Doddington, G., Speaker Recognition - Identifying People by their Voices (1985) Proceedings of the IEEE, 73 (11), pp. 1651-1664; Meng, H., Li, D., Multilingual Spoken Dialog Systems (2006) Multilingual Speech Processing, pp. 399-447. , Elsevier, Academic Press; Seneff, S., Hirschman, L., Zue, V.W., Interactive problem solving and dialogue in the ATIS domain (1991) Proceedings of the Fourth DARPA Speech and Natural Language Workshop, Defense Advanced Research Projects Agency, pp. 1531-1534. , Morgan Kaufmann, Pacific Grove; Rudnicky, A., Thayer, E., Constantinides, P., Tchou, C., Shern, R., Lenzo, K., Xu, W., Oh, A., Creating natural dialogs in the Carnegie Mellon Communicator system (1999) Proc. of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1531-1534. , Budapest, Hungary, pp; Litman, D., Forbes, K., Recognizing Emotions from Student Speech in Tutoring Dialogues (2003) Proc. IEEE Workshop on Automatic Speech Recognition and Understanding, St, , Thomas, Virgin Islands; Zue, V., Seneff, S., Glass, J., Polifroni, J., Pao, C., Hazen, T., Hetherington, L., JUPITER: A telephone-based conversational interface for weather information (2000) IEEE Transactions on Speech and Audio Processing, 8 (1); Hazen, T., Jones, D., Park, A., Kukolich, L., Reynolds, D., Integration of Speaker Recognition into Conversational Spoken Dialog Systems (2003) Proc. of the European Conference on Speech Communication and Technology (EUROSPEECH), , Geneva, Switzerland; Muthusamy, Y.K., Barnard, E., Cole, R.A., Reviewing Automatic Language Identification (1994) IEEE Signal Processing Magazin; Gorin, A.L., Riccardi, G., Wright, J.H., How may I help you? (1997) Speech Communication, 23 (1-2), pp. 113-127; Batliner, A., Fischer, K., Huber, R., Spilker, J., Noth, E., How to find trouble in communication (2004) Speech Communication, 40, pp. 117-143; Polzin, T., Waibel, A., Emotion-sensitive Human-Computer Interfaces (2000) Proc. ISCA Workshop on Speech and Emotion, , A Conceptual Framework for Research, Belfast, Northern Ireland; Raux, A., Langner, B., Black, A.W., Eskenazi, M., LET'S GO: Improving Spoken Language Dialog Systems for the Elderly and Non-natives (2003) Proc. of the European Conference on Speech Communication and Technology (EUROSPEECH), , Geneva, Switzerland; ELLS: The e-language learning system. ELLS Web-server. Retrieved December, 2006 (2004) from http://ott.educ.msu.edu/elanguage/; Eskenazi, M., Issues in the Use of Speech Recognition for Foreign Language Tutors (1999) Language Learning and Technology Journal, 2 (2), pp. 62-76; Barnard, E., Cloete, J.P.L, Patel, H.: Language and Technology Literacy Barriers to Accessing Government Services. In: Traunmüller, R. (ed.) EGOV 2003. LNCS, 2739, pp. 37-42. Springer, Heidelberg (2003); CHIL: Computers in the human interaction loop. CHIL Web-server. Retrieved December, 2006 (2006), from http://chil.server.de; Schultz, T., Waibel, A., Bett, M., Metze, F., Pan, Y., Ries, K., Schaaf, T., Zechner, K., The ISL Meeting Room System (2001) Proceedings of the Workshop on Hands-Free Speech Communication (HSC-2001), , Kyoto, Japan; Waibel, A., Bett, M., Finke, M., Stiefelhagen, R., Meeting browser: Tracking and summarizing meetings (1998) Proceedings of the Broadcast News Transcription and Understanding Workshop, Lansdowne, pp. 281-286. , Penrose, D.E.M, ed, Virginia, pp, Morgan Kaufmann, San Francisco; AMI: Augmented multi-party interaction. AMI Web-server. Retrieved December, 2006 (2006), from http://amiproject.org/; Vogel, S., Schultz, T., Waibel, A., Yamamoto, S., Speech-to-Speech Translation (2006) Multilingual Speech Processing, pp. 317-398. , Elsevier, Academic Press, pp; GALE: Global autonomous language exploitation. GALE Program. Retrieved December, 2006 (2006), from http://www.darpa.mil/ipto/Programs/gale/index.htm; Verbmobil: Foundations of Speech-to-Speech Translation (2000) LNCS (LNAI, , Wahlster, W, ed, Springer, Berlin Heidelberg New York; Waibel, A., Soltau, H., Schultz, T., Schaaf, T., Metze, F., Multilingual Speech Recognition (2000) The Verbmobil Book, , Springer, Heidelberg; McNair, A., Hauptmann, A., Waibel, A., Jain, A., Saito, H., Tebelskis, J., Janus: A Speech-To-Speech Translation System Using Connectionist And Symbolic Processing Strategies (1991) Proc. of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), , Toronto, Canada; Cincarek, T., Toda, T., Saruwatari, H., Shikano, K., Acoustic Modeling for Spoken Dialog Systems based on Unsupervised Utterance-based Selective Training (2006) Proc. of the International Conference on Spoken Language Processing (ICSLP), , Pittsburgh, PA; Kemp, T., Waibel, A., Unsupervised Training of a Speech Recognizer using TV Broadcasts (1998) Proc. of the International Conference on Spoken Language Processing (ICSLP), pp. 2207-2210. , Sydney, Australia, pp; Schultz, T., Waibel, A., Language Independent and Language Adaptive Acoustic Modeling for Speech Recognition (2001) Speech Communication, 35 (1-2), pp. 31-51; Doddington, G., Speaker recognition based on idiolectal differences between speakers (2001) Proceedings of Eurospeech; Goronzy, S., Tomokiyo, L.M., Barnard, E., Davel, M.: Other Challenges: Nonnative Speech, Dialects, Accents, and Local Interfaces. In: Multilingual Speech Processing. Elsevier, Academic Press, pp. 273-315 (2006); Jessen, M., Speaker Classification in Forensic Phonetics and Acoustics (2007) LNCS(LNAI, 4343. , Müller, C, ed, Speaker Classification I, Springer, Heidelberg , this issue; Eriksson, E., Rodman, R., Hubal, R.C., Emotions in Speech: Juristic Implications (2007) LNCS(LNAI, 4343. , Müller, C, ed, Speaker Classification I, Springer, Heidelberg , this issue; Reynolds, D.: Tutorial on SuperSID. In: JHU 2002 Workshop. Retrieved December, 2006 (2002) from http://www.clsp.jhu.edu/ws2002/groups/supersid/ SuperSID_Tutorial.pdf; Batliner, A., Huber, R., Niemann, H., Nöth, E., Spilker, J., Fischer, K., The Recognition of Emotion (2000) The Verbmobil Book, pp. 122-130. , Springer, Heidelberg; Katzenmaier, M., Schultz, T., Stiefelhagen, R., Human-Human-Robot Interaction (2004) International Conference on Multimodal Interfaces, , Penn State University, State College, PA; Kirchhoff, K., Language Characteristics (2006) Multilingual Speech Processing, pp. 5-32. , Elsevier, Academic Press, pp; Goronzy, S., Robust Adaptation to Non-Native Accents in Automatic Speech Recognition (2002) LNCS (LNAI, 2560. , Springer, Heidelberg; Wang, Z., Schultz, T., Non-Native Spontaneous Speech Recognition through Polyphone Decision Tree Specialization (2003) Proc. of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1449-1452. , Geneva, Switzerland, pp; Fischer, V., Gao, Y., Janke, E., Speaker-independent upfront dialect adaptation in a large vocabulary continuous speech recognizer (1998) Proc. of the International Conference on Spoken Language Processing (ICSLP); Sander, M.L., Fowler, C.A., Gestural drift in bilingual speaker of Brazilian Portuguese and English (1997) Journal of Phonetics, 25, pp. 421-436; Cohen, P., Dharanipragada, S., Gros, J., Monkowski, M., Neti, C., Roukos, S., Ward, T., Towards a universal speech recognizer for multiple languages (1997) Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 591-598; Fügen, C., Stüker, S., Soltau, H., Metze, F., Schultz, T., Efficient handling of multilingual language models (2003) Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 441-446; Navrátil, J., Automatic Language Identification (2006) Multilingual Speech Processing, pp. 233-272. , Elsevier, Academic Press, pp; Reynolds, D., An Overview of Automatic Speaker Recognition Technology (2002) Proc. of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 4072-4075. , Orlando, FL, pp; Huang, X.D., Acero, A., Hon, H.-W., (2001) Spoken Language Processing, , Prentice Hall PTR, New Jersey; Reynolds, D., (1993) A Gaussian mixture modeling approach to text-independent using automatic acoustic segmentation, , PhD thesis, Georgia Institute of Technology; Kohler, M.A., Andrews, W.D., Campbell, J.P., Hernander-Cordero, L., Phonetic Refraction for Speaker Recognition (2001) Proceedings of Workshop on Multilingual Speech and Language Processing, , Aalborg, Denmark; Jin, Q., Navratil, J., Reynolds, D., Andrews, W., Campbell, J., Abramson, J., Cross-stream and Time Dimensions in Phonetic Speaker Recognition (2003) Proc. of the International Conference on Acoustics, Speech, and Signal Processing, , ICASSP, HongKong, China; Campbell, J.P., Speaker recognition: A tutorial (1997) Proceedings of the IEEE, 85, pp. 1437-1462; Jin, Q., (2007) Robust Speaker Recognition, , PhD thesis, Carnegie Mellon University, Language Technologies Institute, Pittsburgh, PA; Cimarusti, D., Ives, R., Development of an automatic identification system of spoken languages: Phase 1 (1982) Proc. of the International Conference on Acoustics, Speech, and Signal Processing, , ICASSP, Paris; Zissman, M.A., Language Identification Using Phone Recognition and Phonotactic Language Modeling (1995) Proc. of the International Conference on Acoustics, Speech, and Signal Processing, 5, pp. 3503-3506. , ICASSP, Detroit, MI; Hazen, T.J., Zue, V.W., Segment-based automatic language identification (1997) Journal of the Acoustical Society of America, 101 (4), pp. 2323-2331; Navrátil, J., Spoken language recognition - a step towards multilinguality in speech processing (2001) IEEE Trans. Audio and Speech Processing, 9 (6), pp. 678-685; Parandekar, S., Kirchhoff, K., Multi-stream language identification using datadriven dependency selection (2003) Proc. of the International Conference on Acoustics, Speech, and Signal Processing, , ICASSP; Torres-Carrasquillo, P., Reynolds, D., Deller Jr., J., Language identification using gaussian mixture model tokenization (2002) Proc. of the International Conference on Acoustics, Speech, and Signal Processing, , ICASSP; Eady, S.J., Differences in the f0 patterns of speech: Tone language versus stress language (1982) Language and Speech, 25 (1), pp. 29-42; Schultz, T., Rogina, I.A.W., Lvcsr-based language identification (1996) Proc. of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), , Atlanta, Georgia, IEEE; Schultz, T., Globalphone: A multilingual text and speech database developed at karlsruhe university (2002) Proc. of the International Conference on Spoken Language Processing (ICSLP), , Denver, CO; Jin, Q., Schultz, T., Waibel, A., Speaker Identification using Multilingual Phone Strings (2002) Proc. of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), , Orlando, FL; NIST: Speaker recognition evaluation plan. Retrieved December, 2006 (1999) from http://www.itl.nist.gov/iaui/894.01/spk99/spk99plan.html; Tomokiyo- Mayfield, L.: Recognizing Non-Native Speech: Characterizing and Adapting to Non-Native Usage in LVCSR. PhD thesis, CMU-LTI-01-168, Language Technologies Institute, Carnegie Mellon, Pittsburgh, PA (2001); Schultz, T., Jin, Q., Laskowski, K., Tribble, A., Waibel, A., Speaker, accent, and language identification using multilingual phone strings (2002) Proceedings of the Human Language Technologies Conference (HLT), , San Diego, Morgan Kaufman, San Francisco; Schultz, T., Jin, Q., Laskowski, K., Tribble, A., Waibel, A., Improvements in non-verbal cue identification using multilingual phone strings (2002) Proceedings of the 40nd Annual Meeting of the Association for Computational Linguistics, , Philadelphia, PA, The Association for Computational Linguistics},
correspondence_address1={Schultz, T.; Carnegie Mellon University, Pittsburgh, PA, United States; email: tanja@cs.cmu.edu},
publisher={Springer Verlag},
issn={03029743},
isbn={3540741860; 9783540741862},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Jin20071027,
author={Jin, Q. and Jou, S.-C.S. and Schultz, T.},
title={Whispering speaker identification},
journal={Proceedings of the 2007 IEEE International Conference on Multimedia and Expo, ICME 2007},
year={2007},
pages={1027-1030},
doi={10.1109/icme.2007.4284828},
art_number={4284828},
note={cited By 29; Conference of IEEE International Conference onMultimedia and Expo, ICME 2007 ; Conference Date: 2 July 2007 Through 5 July 2007;  Conference Code:72463},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-46449124039&doi=10.1109%2ficme.2007.4284828&partnerID=40&md5=05461aedac618803bc89f32e6c3864b7},
affiliation={Interactive Systems Laboratories, LTI SCS, Carnegie Mellon University},
abstract={This paper describes a study of automatically identifying whispering speakers. People usually whisper in order to avoid being identified or overheard by lowering their voices. The study compares performances between normal and whispered speech mode in clean and noisy environment under matched and mismatched training conditions, and describes the impact of feature warping and throat microphone on noise reduction. Score combination strategies are used when only little whisper data is available to improve performance. In sum, we achieved 8% to 33% relative improvements in identification accuracy with only 5 to 10 seconds noisy whispered speech data per speaker. © 2007 IEEE.},
keywords={Noise abatement, Combination strategies;  Identification accuracy;  Improve performance;  Noisy environment;  Speaker identification;  Throat microphones;  Training conditions;  Whispered speech, Speech recognition},
references={Reynolds, D., Speaker Identification and Verification Using Gaussian Mixture Speaker Models (1995) Speech Communication, 17 (1-2), pp. 91-108. , August; Pelecanos, J., Sridharan, S., Feature warping for robust speaker verification (2001) Proc. Speaker Odyssey 2001 conference, , June; Xiang, B., Chaudhari, U., Navratil, J., Ramaswamy, G., Gopinath, R., Short-time Gaussianization for Robust Speaker Verification, in Proc ICASSP, 2002; Jin, Q., Pan, Y., Schultz, T., Far-field Speaker Recognition (2006) Proc. ICASSP; Valbret, H., Moulines, E., Tubach, J.P., Voice Transformation Using PSOLA Technique (1992) Speech Communication, 11, pp. 175-187; Gales, M.J.F., Maximum Likelihood Linear Transformations for HMM-Based Speech Recognition (1998) Computer Speech and Language, 12, pp. 75-98; Jou, S.-C., Schultz, T., Waibel, A., Adaptation for Soft Whisper Recognition Using a Throat Microphone (2004) Proc. ICSLP; Kajarekar, S., Bratt, H., Shriberg, E., Leon, R., A Study of Intentional Voice Modifications for Evading Autmatic Speaker Recognition Proc. IEEE Odyssey 2006 Speaker and Language Recognition Workshop; Coleman, J., Grabe, E., Braun, B., Larynx Movements and Intonation in Whispered Speech Summary of research supported by British Academy grant, SG-36269, p. 2002; Ito, T., Takeda, K., Itakura, F., Analysis and Recognition of Whispered Speech (2005) Speech Communication, 45, pp. 139-152; Zheng, Y., Liu, Z., Zhang, Z., Sinclair, M., Droppo, J., Deng, L., Acero, A., Huang, X., Air- and Bone-Conductive Integrated Microphones for Robust Speech Detection and Enhancement (2003) Proc. ASRU; Rabiner, L., Juang, B.-H., (1993) Fundamentals of Speech Recognition, , Prentice Hall, New Jersey},
correspondence_address1={Jin, Q.; Interactive Systems Laboratories, LTI SCS, Carnegie Mellon UniversityUnited States; email: qjin@cs.cmu.edu},
sponsors={Circuits and Systems Society; Communications Society; Computer Society; Institute of Electrical and Electronics Engineers; Signal Processing Society},
publisher={IEEE Computer Society},
address={Beijing},
isbn={1424410177; 9781424410170},
language={English},
abbrev_source_title={Proc. IEEE Int. Conf. Multimedia and Expo, ICME},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schultz2006,
author={Schultz, T. and Black, A.W.},
title={Challenges with rapid adaptation of speech translation systems to new language pairs},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2006},
volume={5},
pages={V1213-V1216},
art_number={1661500},
note={cited By 12; Conference of 2006 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2006 ; Conference Date: 14 May 2006 Through 19 May 2006;  Conference Code:69350},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947643681&partnerID=40&md5=f41451519fa8a3a0a3d90c4ecfafe84b},
affiliation={Interactive Systems Laboratories, Language Technologies Institute, Carnegie Mellon University, United States},
abstract={Although we have far from solved the issues in porting speech translation systems to new languages, we have gathered sufficient experience by new to identify a number of major challenges in the process. Although well-defined processes exist for building speech recognition, speech synthesis and statistical machine translation models, they still require both significant native speaker involvement and linguistic expertise. As the core technology improves we believe we will see increasing cultural and social issues in contributions from native speakers. This paper identifies some of these issues and presents our initial attempts to build tools that we hope will eventually allow linguistically naive native informants build complete speech translation systems. © 2006 IEEE.},
keywords={Social issues;  Speakers;  Statistical machine translation models, Linguistics;  Mathematical models;  Social aspects;  Speech analysis;  Speech recognition;  Speech synthesis;  Statistical methods, Translation (languages)},
references={Barnard, E., and Barnard, E., The efficient generation of pronunciation dictionaries: human factors during bootstrapping. ICSLP, Jeju, Korea, 2004; Black, A., Lenzo, K., Pagel, V., Issues in Building General Letter to Sound Rules (1998) Proc. ESCA Workshop on Speech Synthesis, pp. 77-80. , Jenolan Caves, Australia; Black, A., Lenzo, K., (2000) The FestVox Project: Building Synthetic Voices, , http://festvox.org/bsv; Davel, M., and Barnard, E., The efficient generation of pronunciation dictionaries: machine learning factors during bootstrapping, 1CSLP, Jeju, Korea, 2004; Engelbrecht, H., Schultz, T., Rapid Development of an Afrikaans-English Speech-to-Speech Translator (2005) Proceedings of the IWSLT, , Pittsburgh, PA, October; Finke, M., Geutner, P., Hild, H., Kemp, T., Ries, K., Westphal, M., (1997) The Karlsruhe Verbmobil Speech Recognition Engine, , ICASSP, Munich, Germany; Kominek, J., Black, A., (2005) Measuring Unsupervised and Acoustic Clustering through Phoneme Pair Merge-and-Split Tests, , Interspeech, Lisbon, Portugal; Latorre, J., Iwano, K., Furui, S., (2005) Polyglot synthesis using a micture of monolingual corpora, , ICASSP, Philadelphia, PA; Le, V.B., Besacier, L., Schultz, T., Acoustic-Phonetic Unit Similarities for Context Dependent Acoustic Model Portability, , Submitted to ICASSP 2006; Maskey. S, Black, A. and Tomokiyo, L. Bootstrapping Phonetic Lexicons for New Languages. ICSLP, Jeju, Korea, 2004; Maamouri, M., Graff, D., Hubert, J., Cieri, C., Buckwalter, T., (2004) Dialectal Arabic Orthography-based Transcription and CTS Levantine Arabic Collection, , COLING, Geneva, Switzerland; Schultz, T., Waibel, A., Language Independent and Language Adaptive Acoustic Modeling for Speech Recognition (2001) Speech Communication, 35 (1-2), pp. 31-51. , August; Schultz, T., GlobalPhone: A Multilingual Speech and Text Database developed at Karlsruhe University. ICSLP, Denver, CO, 2002; Schultz, T., Towards Rapid Language Portability of Speech Processing Systems (2004) Conference on Speech and Language Systems for Human Communication, , Delhi, India, November; Schultz, T., Black, A., Woszczyna, M., Flexible Speech Translation Systems. Special Issue in Speech Translation, IEEE Transactions of Speech and Audio Processing (2005), Accepted for publication, August; Vogel, S., Zhang, Y., Huang, F., Tribble, A., Venugopal, A., Zhao, B., Waibel, A., The CMU Statistical Translation System (2003) Proceedings of the MT Summit, 9. , New Orleans, LA. September; Waibel, A., Schultz, T., Vogel, S., Fügen, C., Honal, M., Kolss, M., Reichert, J., Stüker, S., (2004) Towards Language Portability in Statistical Speech Translation, , ICASSP, Montreal, Canada, May},
correspondence_address1={Schultz, T.; Interactive Systems Laboratories, Language Technologies Institute, Carnegie Mellon UniversityUnited States; email: tanja@cs.cmu.edu},
sponsors={The Inst. of Elec. and Elec. Eng. Signal Proc. Soc.},
address={Toulouse},
issn={15206149},
isbn={142440469X; 9781424404698},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Laskowski2006,
author={Laskowski, K. and Schultz, T.},
title={Unsupervised learning of overlapped speech model parameters for multichannel speech activity detection in meetings},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2006},
volume={1},
pages={I993-I996},
art_number={1660190},
note={cited By 12; Conference of 2006 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2006 ; Conference Date: 14 May 2006 Through 19 May 2006;  Conference Code:69350},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947615205&partnerID=40&md5=5134f9b769e5c5b24a7d9ade9e0574da},
affiliation={interACT, Carnegie Mellon University, Pittsburgh, PA, United States},
abstract={The study of meetings, and multi-party conversation in general, is currently the focus of much attention, calling for more robust and more accurate speech activity detection systems. We present a novel multichannel speech activity detection algorithm, which explicitly models the overlap incurred by participants taking turns at speaking. Parameters for overlapped speech states are estimated during decoding by using and combining knowledge from other observed states in the same meeting, in an unsupervised manner. We demonstrate on the NIST Rich Transcription Spring 2004 data set that the new system almost halves the number of frames missed by a competitive algorithm within regions of overlapped speech. The overall speech detection error on unseen data is reduced by 36% relative. © 2006 IEEE.},
keywords={Data structures;  Error detection;  Knowledge acquisition;  Parameter estimation;  Speech coding, Multichannel speech activity detection;  Speech activity detection systems;  Speech models;  Speech states, Speech recognition},
references={Pfau, T., Ellis, D., Stolcke, A., Multispeaker speech activity detecction for the icsi meeting recorder (2001) Proc. IEEE Automatic Speecch Recognition and Understanding Workshop, , Madonna di Campiglio, Italy; Stolcke, A., Further progress in meeting recognition: The icsi-sri spring 2005 speech-to-text evaluation system (2005) Proc. NIST MLMl Meeting Recognition Workshop, pp. 39-50. , Edinburgh, Scotland, July; Wrigley, S., Brown, G., Wan, V., Renais, S., Feature selection for the classification of crosstalk in multi-channel audio (2003) Proc. EuroSpeech; Wrigley, S., Brown, G., Wan, V., Renals, S., Speech and crosstalk detection in multi-channel audio (2002) IEEE Transactions on Speech and Audio Processing; Laskowski, K., Schultz, T., Crosscorrelation-based multi-speaker speech activity detection (2004) Proc. ICSLP, , Jeju Island, Korea, October; Rich transcription, , http://www.nist.gov/speech/tests/ rt, NIST},
correspondence_address1={Laskowski, K.; interACT, Carnegie Mellon University, Pittsburgh, PA, United States; email: kornel@cs.cmu.edu},
sponsors={The Inst. of Elec. and Elec. Eng. Signal Proc. Soc.},
address={Toulouse},
issn={15206149},
isbn={142440469X; 9781424404698},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Le2006,
author={Le, V.B. and Besacier, L. and Schultz, T.},
title={Acoustic-phonetic unit similarities for context dependent acoustic model portability},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2006},
volume={1},
pages={I1101-I1104},
art_number={1660217},
note={cited By 11; Conference of 2006 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2006 ; Conference Date: 14 May 2006 Through 19 May 2006;  Conference Code:69350},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947657936&partnerID=40&md5=8903cc0dda117e61857cb4809ec13dcb},
affiliation={CLIPS-IMAG Laboratory, UMR CNRS 5524, BP 53, 38041 Grenoble Cedex 9, France; Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA, United States},
abstract={This paper addresses particularly the use of acoustic-phonetic unit similarities for portability of context dependent acoustic models to new languages. Since the IPA-based method is limited to a source/target phoneme mapping table construction, an estimation method of the similarity between two phonemes is proposed in this paper. Based on these phoneme similarities, some estimation methods for polyphone similarity and clustered polyphonic model similarity are investigated. For a new language, first a polyphonic decision tree is built with a small amount of speech data. Then, clustered models in the target language are duplicated from the nearest clustered models in the source language and adapted with limited data to the target language. Results obtained from the experiments demonstrate the feasibility of these methods. © 2006 IEEE.},
keywords={Data reduction;  Decision trees;  Linguistics;  Mathematical models;  Parameter estimation, Acoustic model;  Estimation method;  Phonemes;  Polyphonic model, Speech analysis},
references={Beyerlein, P., Towards language independent acoustic modeling (1999) ASRU'99, , Keystone, CO, USA, December; Bayeh, R., Towards multilingual speech recognition using data driven source/target acoustical units association (2004) ICASSP'04, 1, pp. 521-524. , Montreal, Canada, May; Le, V.B., Besacier, L., First steps in fast acoustic modeling for a new target language: Application to Vietnamese (2005) ICASSP'05, 1, pp. 821-824. , Philadelphia, PA, USA, March; Köhler, J., Multi-lingual phoneme recognition exploiting acoustic-phonetic similarities of sounds (1996) ICSLP'96, pp. 2195-2198. , Philadelphia, PA, USA, October; Imperl, B., Agglomerative vs. Tree-based clustering for the definition of multilingual set of triphones (2000) ICASSP'00, 3, pp. 1273-1276. , Istanbul, Turkey, June; Schultz, T., Waibel, A., Language independent and language adaptive acoustic modeling for speech recognition (2001) Speech Communication, 35 (1-2), pp. 31-51. , August; Sooful, J.J., Botha, E.C., An acoustic distance measure for automatic cross-language phoneme mapping (2001) PRASA'01, pp. 99-102. , South Africa, November; Finke, M., The Karlsruhe-Verbmobil Speech Recognition Engine (1997) ICASSP'97, 1, pp. 83-86. , Munich, Germany; V. B. Le et al, Spoken and written language resources for Vietnamese, LREC'04, pp. 509-602, Lisbon, Portugal, May 2004; Stolcke, A., SRILM - An Extensible Language Modeling Toolkit (2002) ICSLP'02, 2, pp. 901-904. , Denver, CO, USA, September},
correspondence_address1={Le, V.B.; CLIPS-IMAG Laboratory, UMR CNRS 5524, BP 53, 38041 Grenoble Cedex 9, France; email: viet-bac.le@imag.fr},
sponsors={The Inst. of Elec. and Elec. Eng. Signal Proc. Soc.},
address={Toulouse},
issn={15206149},
isbn={142440469X; 9781424404698},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Qin2006,
author={Qin, J. and Yue, P. and Schultz, T.},
title={Far-field speaker recognition},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2006},
volume={1},
pages={I937-I940},
art_number={1660176},
note={cited By 14; Conference of 2006 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2006 ; Conference Date: 14 May 2006 Through 19 May 2006;  Conference Code:69350},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947644327&partnerID=40&md5=09c07040147a814cb342ca8498cd5d79},
affiliation={Language Technologies Institute, School of Computer Science, Carnegie Mellon University, United States},
abstract={In this paper we study robust speaker recognition in far-field microphone situations such as meeting scenarios. By applying reverberation compensation and feature warping we achieved significant improvements under mismatched training-testing conditions. To capture useful information from multiple distant microphones, two approaches for multiple channel combination are investigated. This leads to 84.1% and 78.1% relative improvements on the Distant Microphone database. Furthermore, we tested the resulting system on the ICSI Meeting Corpus. The improvements are also very high on this task, which indicates that our system is robust to changing conditions in a remote microphone setting. © 2006 IEEE.},
keywords={Channel combination;  Feature warping;  Reverberation compensation, Communication channels (information theory);  Database systems;  Information retrieval;  Microphones;  Reverberation, Speech recognition},
references={NIST Annual Speaker Recognition Evaluation, , http://www.nist.gov/speech/ tests/spk/index.htm; Pelecanos, J., Sridharan, S., Feature warping for robust speaker verification (2001) Proc. Speaker Odyssey 2001 conference, , June; Furui, S., Cepstral analysis technique for automatic speaker verification (1981) IEEE Transactions on Acoustics, Speech and Signal Processing, 19, pp. 254-272; Hermansky, H., Morgan, N., RASTA processing of speech (1994) IEEE Trans. Speech and Audio Processing, 2 (4), pp. 578-589; Janin, A., Baron, D., Edwards, J., Ellis, D., Gelbart, D., Morgan, N., Peskin, B., Wooters, C., The ICSI Meeting Corpus (2003) Proc. ICASSP; Reynolds, D., Speaker Identification and Verification Using Gaussian Mixture Speaker Models (1995) Speech Communication, 17 (1-2), pp. 91-108. , August; Xiang, B., Chaudhari, U., Navratil, J., Ramaswamy, G., Gopinath, R., Short-time Gaussianization for Robust Speaker Verification (2002) Proc. ICASSP},
correspondence_address1={Qin, J.; Language Technologies Institute, School of Computer Science, Carnegie Mellon UniversityUnited States; email: qjin@cs.cmu.edu},
sponsors={The Inst. of Elec. and Elec. Eng. Signal Proc. Soc.},
address={Toulouse},
issn={15206149},
isbn={142440469X; 9781424404698},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jou2006,
author={Jou, S.-C. and Maier-Hein, L. and Schultz, T. and Waibel, A.},
title={Articulatory feature classification using surface electromyography},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2006},
volume={1},
pages={I605-I608},
art_number={1660093},
note={cited By 28; Conference of 2006 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2006 ; Conference Date: 14 May 2006 Through 19 May 2006;  Conference Code:69350},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947674793&partnerID=40&md5=d63b119edd41b47e2c7554651257ea4d},
affiliation={International Center for Advanced Communication Technologies, Carnegie Mellon University, United States; Universität Karlsruhe, Germany},
abstract={In this paper, we present an approach for articulatory feature classification based on surface electromyographic signals generated by the facial muscles. With parallel recorded audible speech and electromyographic signals, experiments are conducted to show the anticipatory behavior of electromyographic signals with respect to speech signals. On average, we found that the signals to be time delayed by 0.02 to 0.12 second. Furthermore, it is shown that different articulators have different anticipatory behavior. With offset-aligned signals, we improved the average F-score of the articulatory feature classifiers in our baseline system from 0.467 to 0.502. © 2006 IEEE.},
keywords={Audio recordings;  Electromyography;  Speech processing, Articulators;  Electromyographic signals;  Facial muscles;  Feature classifiers, Feature extraction},
references={Nakajima, Y., Kashioka, H., Shikano, K., Campbell, N., Non-audible murmur recognition input interface using stethoscopic microphone attached to the skin (2003) Proc. ICASSP, , Hong Kong; Chan, A.D.C., Englehart, K., Hudgins, B., Lovely, D.F., Hidden Markov model classification of myoelectric signals in speech (2002) IEEE Engineering in Medicine and Biology Magazine, 21 (4), pp. 143-146; Jorgensen, C., Binsted, K., Web browser control using EMG based sub vocal speech recognition (2005) Proc. HICSS, , Hawaii, Jan; Manabe, H., Hiraiwa, A., Sugimura, T., Unvoiced speech recognition using EMG-mime speech recognition (2003) Proc. HFCS, , Ft. Lauderdale, Florida; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) Proc. ASRU, , San Juan, Puerto Rico, Nov; Varioport, http://www.becker-meditec.de; Kirchhoff, K., (1999) Robust Speech Recognition Using Articulatory Information, , Ph.D. thesis, University of Bielefeld, Germany, July; Metze, F., Waibel, A., A flexible stream architecture for ASR using articulatory features (2002) Proc. ICSLP, , Denver, CO, Sep; Yu, H., Waibel, A., Streaming the front-end of a speech recognizer (2000) Proc. ICSLP, , Beijing, China},
correspondence_address1={Jou, S.-C.; International Center for Advanced Communication Technologies, Carnegie Mellon UniversityUnited States; email: scjou@cs.cmu.edu},
sponsors={The Inst. of Elec. and Elec. Eng. Signal Proc. Soc.},
address={Toulouse},
issn={15206149},
isbn={142440469X; 9781424404698},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Schultz2006403,
author={Schultz, T. and Black, A.W. and Vogel, S. and Woszczyna, M.},
title={Flexible speech translation systems},
journal={IEEE Transactions on Audio, Speech and Language Processing},
year={2006},
volume={14},
number={2},
pages={403-411},
doi={10.1109/TSA.2005.860768},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947134833&doi=10.1109%2fTSA.2005.860768&partnerID=40&md5=8989b65406882b230003809b65fa4799},
affiliation={Interactive Systems Laboratory, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={Speech translation research has made significant progress over the years with many high-visibility efforts showing that translation of spontaneously spoken speech from and to diverse languages is possible and applicable in a variety of domains. As language and domains continue to expand, practical concerns such as portability and reconfigurability of speech come into play: system maintenance becomes a key issue and data is never sufficient to cover the changing domains over varying languages. In this paper, we discuss strategies to overcome the limits of today's speech translation systems. In the first part, we describe our layered system architecture that allows for easy component integration, resource sharing across components, comparison of alternative approaches, and the migration toward hybrid desktop/PDA or stand-alone PDA systems. In the second part, we show how flexibility and reconfigurability is implemented by more radically relying on learning approaches and use our English-Thai two-way speech translation system as a concrete example. © 2006 IEEE.},
author_keywords={Multilinguality;  Portability;  Speech translation;  System deployment},
keywords={Computer architecture;  Computer software portability;  Learning systems;  Personal digital assistants;  Resource allocation;  Translation (languages), Multilinguality;  Reconfigurability of speech;  System architecture;  System deployment, Speech synthesis},
funding_details={Chiang Mai UniversityChiang Mai University, CMU},
funding_details={Defense Advanced Research Projects AgencyDefense Advanced Research Projects Agency, DARPA},
funding_text 1={Manuscript received June 20, 2004; revised June 16, 2005. This work was supported in part by the Defense Advanced Research Projects Agency (DARPA) under Grant “Mobile Speech-to-Speech Translation for Military Field Applications” (formerly called “Babylon”) and LASER-ACTD under Grant “CMU Thai Speech Translator for Multilingual Coalition Conversation.” The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Giuseppe (G. E.) Riccardi.},
references={Finke, M., Geutner, P., Hild, H., Kemp, T., Ries, K., Westphal, M., The Karlsruhe Verbmobil speech recognition engine (1997) Proc. ICASSP, pp. 83-86. , Munich, Germany; Soltau, H., Metze, F., Fügen, C., Waibel, A., A one-pass decoder based on polymorphic linguistic context assignment (2001) Proc. ASRU, pp. 214-217. , Madonna di Campiglio, Italy; Finke, M., Fritsch, J., Roll, D., Waibel, A., Modeling and efficient decoding of large vocabulary conversational speech (1999) Proc. Eurospeech, pp. 467-470. , Budapest, Hungary; Mangu, L., Brill, E., Stolcke, A., Finding consensus in speech recognition: Word error minimization and other applications of confusion networks (2000) Comput. Speech Lang, 14 (4), p. 373; Fügen, C., Stüker, S., Soltau, H., Metze, F., Schultz, T., Efficient handling of multilingual language models (2003) Proc. ASRU, pp. 441-446. , St. Thomas, VI; Lavie, A., Levin, L., Schultz, T., Langley, C., Han, B., Tribble, A., Gates, D., Peterson, K., Domain portability in speech-to-speech translation (2001) Proc. HLT, pp. 82-86. , San Diego, CA; Levin, L., Gates, D., Lavie, A., Waibel, A., An interlingua based on domain actions for machine translation of task-oriented dialogues (1998) Proc. ICSLP, pp. 1155-1158. , Sydney, Australia; Brown, P.F., Delia Pietra, S.A., Delia Pietra, V.J., Mercer, R.L., The mathematics of statistical machine translation: Parameter estimation (1993) Comput. Linguist, 19 (2), pp. 263-311; Wu, D., Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora (1995) Proc. UCAI, pp. 1328-1335. , Montreal, QC, Canada; Vogel, S., Key, H., Tillmann, C., HMM-based word alignment in statistical translation (1996) Proc. COLING, pp. 836-841. , Copenhagen, Denmark; Och, F.J., Ney, H., Improved statistical alignment models (2000) Proc. ACL, pp. 440-470. , Hong Kong, China; Vogel, S., Zhang, Y., Huang, F., Tribble, A., Venogupal, A., Zhao, B., Waibel, A., The CMU statistical translation system (2003) Proc. MT-Summit IX, pp. 402-409. , New Orleans, LA; Vogel, S., SMT decoder dissected: Word reordering (2003) Proc. Natural Language Processing and Knowledge Engineering, pp. 561-566. , Beijing, China; Lavie, A., Langley, C., Waibel, A., Pianesi, F., Lazzari, G., Coletti, P., Taddei, L., Balducci, F., Architecture and design considerations in Nespole!: A speech translation system for E-commerce applications (2001) Proc. HLT, pp. 31-34. , San Diego, CA; Papineni, K., Roukos, S., Ward, T., Zhu, W., BLEU: A method forautomatic evaluation of machine translation (2002) Proc. 40th Annu. Meeting Association for Computational Linguistics, pp. 311-318. , Philadelphia, PA; Black, A., Lenzo, K., (2000) Building Voices in the Festival Speech Synthesis System, , http://festvox.org/bsv, Online] Available; Black, A., Taylor, P., Caley, R., (1999) The Festival Speech Synthesis System, , http://festvox.org/festival, Online] Available; Cepstral, LLC, Theta: Small Footprint Text-to-Speech Synthesizer, Cepstral, LLC, Pittsburgh, PA, 2004; Aberdeen, J., Condon, S., Doran, C., Harper, L., Oshika, B., Phillips, J., (2004) DARPA Cast Final Rep, , Bedford, MA: The MITRE Corp; Waibel, A., Badran, A., Black, A., Frederking, R., Gates, D., Lavie, A., Levin, L., Zhang, J., Speechalator: Two-way speech-to-speech translation on a consumer PDA (2003) Proc. Eurospeech, pp. 369-372. , Geneva, Switzerland; Schultz, T., Alexander, D., Black, A., Peterson, K., Suebvisai, S., Waibel, A., A Thai speech translation system for medical dialogs (2004) Proc. HIT, pp. 263-264. , Boston, MA; Charoenpornsawat, P., Kijsirikul, B., Meknavin, S., Feature-based Thai unknown word boundary identification using winnow (1998) Proc. APCCAS, pp. 547-550. , Chieng Mai, Thailand; Schultz, T., Waibel, A., Language independent and language adaptive acoustic modeling for speech recognition (2001) Speech Commun, 35 (1-2), pp. 31-51; Schultz, T., GlobalPhone: A multilingual speech and text database developed at Karlsruhe University (2002) Proc. ICSLP, pp. 345-348. , Denver, CO; Suebvisai, S., Charoenpornsawat, P., Black, A., Woszczyna, M., Schultz, T., Thai automatic speech recognition (2005) Proc. ICASSP, pp. 857-860. , Philadelphia, PA; Maskey, S., Black, A., Mayfield Tomokiyo, L., Optimally constructing phonetic lexicons in new languages (2004) Proc. ICSLP, pp. 1227-1230. , Jeju Island, South Korea; Kanthak, S., Ney, H., Context-dependent acoustic modeling using graphemes for large vocabulary speech recognition (2002) Proc. ICASSP, pp. 845-1-845-8. , Orlando, FL; Hain, T., Implicit pronunciation modeling in ASR (2002) ISCA Pronunciation Modeling Workshop; Saraclar, M., Nock, H.J., Khudanpur, S., Pronunciation modeling by sharing Gaussian densities across phonetic models (2000) Comput. Speech Lang, 14, pp. 137-160; Yu, H., Schultz, T., Enhanced tree clustering with single pronunciation dictionary for conversational speech recognition (2003) Proc. Eurospeech, pp. 1896-1899. , Geneva, Switzerland; Killer, M., Stüker, S., Schultz, T., Grapheme based speech recognition (2003) Proc. Eurospeech, pp. 3141-3144. , Geneva, Switzerland; Mimer, B., Stüker, S., Schultz, T., Flexible tree clustering for grapheme-based speech recognition (2004) Proc. Elektronische Sprachverarbeitung, , Cottbus, Germany; J. Kominek and A. Black, The CMU ARCTIC speech databases for speech synthesis research, Lang. Technol. Inst., Carnegie Mellon Univ., Pittsburgh, PA, Tech. Rep. CMU-LTI-03-177. [Online]. Available: http://festvox.org/cmu_arctic., 2003; Black, A., Lenzo, K., Limited domain synthesis (2000) Proc. ICSLP, pp. 411-414. , Beijing, China; Black, A., Lenzo, K., Pagel, V., Issues in building general letter to sound rules (1998) Proc. 3rd ESCA Workshop Speech Synthesis, pp. 77-80. , Jenolan Caves, Australia; A. Black and P. Taylor, Automatically clustering similar units for unit selection in speech synthesis, in Proc. Eurospeech, Rhodes, Greece, 1997, pp. 601-604; Logan, J., Greene, B., Pisoni, D., Segmental intelligibility of synthetic speech produced by rule (1989) J. Acoust. Soc. Amer, 86 (2), pp. 566-581; Benoit, C., Grice, M., Hazan, V., The SUS test: A method for the assessment of text-to-speech synthesis intelligibility using semantically unpredictable sentences (1996) Speech Commun, 18, pp. 381-392; Tomokiyo, L., Black, A., Lenzo, K., Arabic in my hand: Small-foot-print synthesis of Egyptian Arabic (2003) Proc. Eurospeech, pp. 2049-2052. , Geneva, Switzerland; Gavalda, M., Soup: A parser for real-world spontaneous speech (2004) New Developments in Parsing Technology, , H. Bunt and J. Carroll, Eds. Norwell, MA: Kluwer},
correspondence_address1={Schultz, T.; Interactive Systems Laboratory, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: tanja@cs.cmu.edu},
issn={15587916},
language={English},
abbrev_source_title={IEEE Trans. Audio Speech Lang. Process.},
document_type={Article},
source={Scopus},
}

@BOOK{Schultz2006,
author={Schultz, T. and Kirchhoff, K.},
title={Multilingual Speech Processing},
journal={Multilingual Speech Processing},
year={2006},
page_count={508},
doi={10.1016/B978-0-12-088501-5.X5000-8},
note={cited By 94},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013700737&doi=10.1016%2fB978-0-12-088501-5.X5000-8&partnerID=40&md5=8cca307671bea3138e94b7db9cc7e39c},
affiliation={Carnegie Mellon University, Pittsburgh, PA, United States; University of Washington, Seattle, WA, United States},
abstract={Tanja Schultz and Katrin Kirchhoff have compiled a comprehensive overview of speech processing from a multilingual perspective. By taking this all-inclusive approach to speech processing, the editors have included theories, algorithms, and techniques that are required to support spoken input and output in a large variety of languages. This book presents a comprehensive introduction to research problems and solutions, both from a theoretical as well as a practical perspective, and highlights technology that incorporates the increasing necessity for multilingual applications in our global community. Current challenges of speech processing and the feasibility of sharing data and system components across different languages guide contributors in their discussions of trends, prognoses and open research issues. This includes automatic speech recognition and speech synthesis, but also speech-to-speech translation, dialog systems, automatic language identification, and handling non-native speech. The book is complemented by an overview of multilingual resources, important research trends, and actual speech processing systems that are being deployed in multilingual human-human and human-machine interfaces. Researchers and developers in industry and academia with different backgrounds but a common interest in multilingual speech processing will find an excellent overview of research problems and solutions detailed from theoretical and practical perspectives. © 2006 Elsevier Inc.},
references={Ackermann, U., Angelini, B., Brugnara, F., Federico, M., Giuliani, D., Gretter, R., Lazzari, G., Niemann, H., Speedata: Multilingual spoken dataentry (1996) Proceedings of the International Conference on Spoken Language Processing, pp. 2211-2214; Ackermann, U., Angelini, B., Brugnara, F., Federico, M., Giuliani, D., Gretter, R., Niemann, H., Speedata: A prototype for multilingual spoken data-entry (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 355-358; Adda, G., Adda-Decker, M., Gauvain, J., Lamel, L., Text normalization and speech recognition in French (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 2711-2714; Adda-Decker, M., Towards multilingual interoperability in automatic speech recognition (1999) Proceedings of the ESCA-NATO Tutorial Research Workshop on Multi-lingual Interoperability in Speech Technology; Adda-Decker, M., A corpus-based decompounding algorithm for German lexical modeling in LVCSR (2003) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); Adda-Decker, M., de Mareuil, P.B., Adda, G., Lamel, L., Investigating syllabic structures and their variation in spontaneous French (2005) Speech Communication, 46 (2), pp. 119-139; Adda-Decker, M., Lamel, L., Pronunciation variation across system configuration, language and speaking style (1999) Speech Communication, 29, pp. 83-98; (2004) The AECMA simplified English standard, psc-85-16598, , http://www.simplifiedenglish-aecma.org/Simplifie_English.htm; Akiba, Y., Watanabe, T., Sumita, E., Using language and translation models to select the best among outputs from multiple MT system (2002) Proceedings of COLING, pp. 8-14; Akmajian, A., Harnish, R., Demers, R., Farmer, A., (1995) Linguistics: An Introduction to Language and Communication, , MIT Press, Cambridge, MA; Allauzen, A., Gauvain, J., Adaptation automatique du modèle de langage d'un système de transcription de journaux parèls (2003) Traitement Automatique des langues, 44 (1), pp. 11-31; Allauzen, A., Gauvain, J., Open vocabulary ASR for audiovisual document indexation (2005) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Allauzen, A., Gauvain, J., Diachronic vocabulary adaptation for broadcast news transcription (2005) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH-INTERSPEECH); (2004) The Unicode Standard, Version 4.0, , Addison Wesley, J. Allen, J. Becker (Eds.); Allen, J., Hunnicut, S., Klatt, D., (1987) Text-to-Speech: The MITalk system, , Cambridge University Press, Cambridge, U.K; Alshawi, H., Bangalore, S., Douglas, S., Learning dependency translation models as collections of finite-state head transducers (2000) Computational Linguistics, 26 (1), pp. 45-60; Amdall, I., Korkmazskiy, F., Surendran, A.C., Joint pronuciation modeling of non-native speakers using data-driven methods (2000) Proceedings of the International Conference on Spoken Language Processing; AMSR2001 (2001) Workshop on Adaptation Methods for Speech Recognition; Andersen, O., Dalsgaard, P., Language-identification based on crosslanguage acoustic models and optimised information combination (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 67-70; Andersen, O., Dalsgaard, P., Barry, W., Data-driven identification of poly- and mono-phonemes for four European lanugages (1993) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 759-762; Andersen, O., Dalsgaard, P., Barry, W., On the use of data-driven clustering techniques for language identification of poly- and mono-phonemes for Four European languages (1994) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 121-124; Angkititrakul, P., Hansen, J., Use of trajectory models for automatic accent classification (2003) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); The real digital divide (2005) The Economist; Aretoulaki, M., Harbeck, S., Gallwitz, F., Nöth, E., Niemann, H., Ivanecky, J., Ipsic, I., Matousek, V., SQEL: A multilingual and multifunctional dialogue system (1998) Proceedings of the International Conference on Spoken Language Processing; Armstrong, S., Kempen, M., McKelvie, D., Petitpierre, D., Rapp, R., Thompson, H., Multilingual corpora for cooperation (1998) Proceedings of the First International Conference on Language Resources and Evaluation (LREC), pp. 975-980; Association, I.P., (1999) Handbook of the International Phonetic Association: A Guide to Use of the International Phonetic Alphabet, , Cambridge University Press, Cambridge, UK; Auberg, S., Correa, N., Locktionova, V., Molitor, R., Rothenberg, M., The accent coach: An English pronunciation training system for Japanese speakers (1998) Proceedings of the ESCA Workshop on Speech Technology in Language Learning (STiLL); Aust, H., Oerder, M., Seide, F., Steinbiss, V., The Phillips Automatic Train Timetable Information System (1995) Speech Communication, 17, pp. 249-262; http://www.darpa.mil/ipto/research/babylon/approach.html, Web site; Bacchiani, M., Ostendorf, M., Joint acoustic unit design and lexicon generation (1998) Proceedings of the ESCA Workshop on Speech Synthesis, pp. 7-12; Bahl, L., Brown, P., DeSouza, P., Mercer, R., A tree-based statistical language model for natural language speech recognition (1989) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 1001-1008; Barnard, E., Cloete, J., Patel, H., Language and technology literacy barriers to accessing government services (2003) Vol. 2739 of Lecture notes in Computer Science, pp. 37-42; Barnett, J., Corrada, A., Gao, G., Gillick, L., Ito, Y., Lowe, S., Manganaro, L., Peskin, B., Multilingual speech recognition at Dragon Systems (1996) Proceedings of the International Conference on Spoken Language Processing, pp. 2191-2194; Bayer, S., Doran, C., George, B., Exploring speech-enabled dialog with the GALAXY communicator infrastructure (2001) Proceedings of the Human Language Technologies Conference (HLT), pp. 79-81; Bazzi, J., Glass, J., Modeling out-of-vocabulary words for robust speech recognition (2000) Proceedings of the International Conference on Spoken Language Processing (ICSLP-00), pp. 401-404; Beaugendre, F., Clase, T., van Hamme, H., Dialect adaptation for Mandarin Chinese (2000) Proceedings of the International Conference on Spoken Language Processing; Beebe, L.M., Myths about interlanguage phonology (1987) Interlanguage Phonology: The Acquisition of a Second Language Sound System. Issues in Second Language Research, , Newbury House, Cambridge, MA, G. Ioup, S.H. Weinberger (Eds.); Bellegarda, J., Statistical language model adaptation: Review and perspectives (2004) Speech Communication, 42 (1), pp. 93-108; Bellegarda, J.R., Exploiting latent semantic information in statistical language modeling (2000) Proceedings of the IEEE, 88, pp. 1279-1296; Berkling, K., Arai, T., Barnard, E., Cole, R., Analysis of phoneme-based features for language identification (1994) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 289-292; Berry, M.W., Dumais, S.T., O'Brien, G.W., Using linear algebra for intelligent information retrieval (1995) SIAM Review, 37 (4), pp. 573-595; Besling, S., (1994), pp. 23-31. , Heuristical and statistical methods for grapheme-to-phoneme conversion In: Konvens. Vienna, Austria; Billa, J., Ma, K., McDonough, J.W., Zavaliagkos, G., Miller, D.R., Ross, K.N., El-Jaroudi, A., Multilingual speech recognition: The 1996 Byblos Callhome System (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 363-366; Billa, J., Noamany, M., Srivastava, A., Liu, D., Stone, R., Xu, J., Makhoul, J., Kubala, F., Audio indexing of Arabic broadcast news (2002) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 5-8; Bisani, M., Ney, H., Multigram-based grapheme-to-phoneme conversion for LVCSR (2003) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 933-936; Black, A., Font Llitjós, A., Unit selection without a phoneme set (2002) IEEE Workshop on Speech Synthesis; Black, A., Hunt, A., Generating F0 contours from ToBI labels using linear regression (1996) Proceedings of the International Conference on Spoken Language Processing, pp. 1385-1388; Black, A., Lenzo, K., (2000) Building Voices in the Festival Speech Synthesis System, , http://festvox.org/bsv/; Black, A., Lenzo, K., Limited domain synthesis (2000) Proceedings of the International Conference on Spoken Language Processing, pp. 411-414; Black, A., Lenzo, K., Optimal data selection for unit selection synthesis (2001) Proceedings of the 4th ESCA Workshop on Speech Synthesis; Black, A., Lenzo, K., Pagel, V., Issues in building general letter to sound rules (1998) Proceedings of the ESCA Workshop on Speech Synthesis, pp. 77-80; Black, A., Taylor, P., Automatically clustering similar units for unit selection in speech synthesis (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 601-604; Black, A., Taylor, P., Caley, R., (1998) The Festival Speech Synthesis System, , http://festvox.org/festival; Black, A.W., Predicting the intonation of discourse segments from examples in dialogue speech (1997) Computing Prosody, pp. 117-128. , Springer Verlag, Y. Sagisaka, N. Campbell, N. Higuchi (Eds.); Black, A.W., Campbell, N., Optimising selection of units from speech databases for concatenative synthesis (1995) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 581-584; Black, A.W., Taylor, P., CHATR: A genetic speech synthesis system (1994) Proceedings of COLING, pp. 983-986; Bonaventura, P., Gallocchio, F., Micca, G., Multilingual Speech Recognition for Flexible Vocabularies (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 355-358; Boula de Mareil, P., Habert, B., Bnard, F., Adda-Decker, M., Baras, C., Adda, G., Paroubek, P., A quantitative study of disfluencies in French broad-cast interviews (2005) ISCA DiSS '05-Disfluency in Spontaneous Speech; Braun, J., Levkowitz, H., Automatic language identification with perceptually guided training and recurrent neural networks (1998) Proceedings of the International Conference on Spoken Language Processing, pp. 3201-3205; Breiman, L., Friedman, J., Olshen, R., Stone, C., (1984) Classification and Regression Trees, , Wadsworth & Brooks, Pacific Grove, CA; Brière, E., An investigation of phonological interference (1966) Language, 42 (4), pp. 768-796; Brill, E., Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging (1995) Computational Linguistics, 21 (4), pp. 543-566; Brown, P., Cocke, J., Pietra, S.A.D., Pietra, V.J.D., Jelinek, F., Lafferty, J.D., Mercer, R.L., Roossin, P.S., A statistical approach to machine translation (1993) Computational Linguistics, 16, pp. 79-85; Brown, P., Pietra, S.D., Pietra, V.D., Mercer, R., The mathematics of statistical machine translation: Parameter estimation (1993) Computational Linguistics, 19 (2), pp. 263-311; Bub, U., Köhler, J., Imperl, B., In-service adaptation of multilingual hidden-Markov-models (1997) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 1451-1454; Buckwalter, T., (2002) Arabic morphological analyzer, , http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2002L49, Available at; Byrne, W., Doermann, D., Franz, M., Gustman, S., Hajiĉ, J., Picheny, D.O.M., Psutka, J., Zhu, W.-J., Automatic recognition of spontaneous speech for access to multilingual oral history archives (2004) IEEE Transactions on Speech and Audio Processing, 12 (4), pp. 420-435; Byrne, W., Hajiĉ, J., Ircing, P., Jelinek, F., Khudanpur, S., Krbec, P., Psutka, J., On large vocabulary conversational speech recognition of a highly inflectional language-Czech (2001) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 487-489; Byrne, W., Hajiĉ, J., Ircing, P., Jelinek, F., Khudanpur, S., McDonough, J., Peterek, N., Psutka, J., Large vocabulary speech recognition for read and broadcast Czech (1999) Lecture Notes in Computer Science, 1692, pp. 235-240. , Springer Verlag, V. Matousek, P. Mautner, J. Ocelikova, Sojka (Eds.) Text Speech and Dialog; Byrne, W., Hajiĉ, J., Krbec, P., Ircing, P., Psutka, J., Morpheme based language models for speech recognition of Czech (2000) Lecture Notes in Computer Science, 1902, pp. 211-216. , Springer Verlag, P. Sojka, I. Kopecek, K. Pala (Eds.) Text Speech and Diaglog; Byrne, W., Knodt, E., Khudanpur, S., Bernstein, J., Is automatic speech recognition ready for non-native speech? A data collection effort and initial experiments in modeling conversational Hispanic speech (1998) Proceedings of the ESCA-ITR Workshop on Speech Technology in Language Learning, pp. 37-40; Byrne, W., Knodt, E., Khudanpur, S., Bernstein, J., Is automatic speech recognition ready for non-native speech? A data collection effort and initial experiments in modeling conversational Hispanic English (1998) Proceedings of the ESCA Workshop on Speech Technology in Language Learning (STiLL); Campbell, W., Generalized linear discriminant sequence kernels for speaker recognition (2002) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Campbell, W.E.S.P.A.T.-C., Reynolds, D., Language recognition with support vector machines (2004) Speaker Odyssey-The Speaker and Language Recognition Workshop; Canavan, A., Zipperlen, G., Graff, D., (1997) The CALLHOME Egyptian Arabic speech corpus, , http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC97S45, Available at; Candea, M., Vasilescu, I., Adda-Decker, M., Inter- and intra-language acoustic analysis of autonomous fillers (2005) Proceedings of the DiSS'05-ISCA Tutorial and Research Workshop on Disfluency in Spontaneous Speech; Çarki, K., Geutner, P., Schultz, T., Turkish LVCSR: Towards better speech recognition for agglutinative languages (2000) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 1563-1566; Caseiro, D., Trancoso, I., Identification of spoken European languages (1998) Proceedings of the Ninth European Signal Processing Conference (EUSIPCO); Chan, J., Ching, P., Lee, T., Meng, H., Detection of language boundary in code-switching utterances by bi-phone probabilities (2004) Proceedings of the International Conference on Spoken Language Processing; Charniak, E., Immediate-head parsing for language models (2001) Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics; Chase, L., A review of the American Switchboard and Callhome speech recognition evaluation programs (1998) Proceedings of the First International Conference on Language Resources and Evaluation (LREC), pp. 789-793. , A. Rubio, N. Gallardo, R. Castro, A. Tejada (Eds.); Chelba, C., Jelinek, F., Structured language modeling (2000) Computer Speech and Language, 14 (4), pp. 283-332; Chen, L., Gauvain, J.-L., Gilles Adda, L.L., Dynamic language modeling for broadcast news (2004) Proceedings of the International Conference on Spoken Language Processing, pp. 1281-1284; Chen, L., Gauvain, J.-L., Lamel, L., Adda, G., Unsupervised language model adaptation for broadcast news (2003) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. I; Chen, S., Goodman, J., An empirical study of smoothing techniques for language modeling (1998) Tech. Rep. TR-10-98, , http://www.2.cs.cmu.edu/~sfc/papers/h015a, available at, Computer Science Group, Harvard University; Choi, W.N., Wong, Y.W., Lee, T., Ching, P.C., Lexical tree decoding with class-based language models for Chinese speech recognition (2000) Proceedings of the International Conference on Spoken Language Processing, pp. 174-177; Chotimongkol, A., Black, A., Statistically trained orthographic to sound models for Thai (2000) Proceedings of the International Conference on Spoken Language Processing, pp. 551-554; Choukri, K., Mapelli, V., Deliverable 5.2: Report contributing to the design of an overall coordination and strategy in the field of LR (2003) ENABLER project report; Chow, Y., Schwartz, R., Roukos, S., Kimball, O., Price, P., Kubala, F., Dunham, M., Makhoul, J., The role of word-dependent coarticulatory effects in a phoneme-based speech recognition system (1986) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 1593-1596; Chung, G., Seneff, S., Wang, C., Automatic acquisition of names using speak and spell mode in spoken dialogue systems (2003) Proceedings of the Human Language Technologies Conference (HLT), pp. 197-200; Cimarusti, D., Ives, R., Development of an automatic identification system of spoken languages: Phase 1 (1982) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Clarkson, P.R., Robinson, A.J., Language model adaptation using mixtures and an exponentially decaying cache (1997) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 799-802; (1998) Carnegie Mellon Pronuncing Dictionary, , http://www.speech.cs.cmu.edu/cgi-bin/cmudict; Coccaro, N., Jurafsky, D., Toward better integration of semantic predictors in statistical language modeling (1998) Proceedings of the International Conference on Spoken Language Processing, pp. 2403-2406; Cohen, M., Phonological structures for speech recognition (1989) Ph.D. thesis, , University of California, Berkeley, Berkeley, CA; Cohen, P., Dharanipragada, S., Gros, J., Monkowski, M., Neti, C., Roukos, S., Ward, T., Towards a universal speech recognizer for multiple languages (1997) Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 591-598; Cohen, P.R., Cheyer, J., Wang, M., Baeg, S.C., An Open Agent Architecture (1994) Working Notes of the AAAT spring Symposium: Software Agents, pp. 1-8; Compernolle, D.V., Recognizing speech of goats, wolves, sheep and.. non-natives (2001) Speech Communication, 35 (1-2), pp. 71-79; Comrie, B., (1989) Language Universals and Linguistic Typology, , University of Chicago Press; Comrie, B., (1990) The World's Major Languages, , Oxford University Press, Oxford, UK; Constantinescu, A., Chollet, G., On cross-language experiments and data-driven units for ALISP (automatic language independent speech processing) (1997) Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 606-613; Corredor-Ardoy, C., Gauvain, J., Adda-Decker, M., Lamel, L., Language identification with language-independent acoustic models (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); Costantini, E., Burger, S., Pianesi, F., NESPOLE!'s multi-lingual and multi-modal corpus (2002) Proceedings of Third International Conference on Language Resources and Evaluation (LREC), pp. 165-170; Coulmas, F., (1996) The Blackwell Encyclopedia of Writings Systems, , Blackwell Publishing, Oxford; Cremelie, N., ten Bosch, L., Improving the recognition of foreign names and non-native speech by combining multiple grapheme-to-phoneme converters (2001) AMSR2001, pp. 151-154; Cremelie, N., Martens, J., In search of pronunciation rules (1998) ESCA Workshop on Modeling Pronunciation Variation for Automatic Speech Recognition, pp. 23-28; Croft, W., (1990) Typology and Universals, , Cambridge University Press, Cambridge, UK; Cucchiarini, C., Strik, H., Boves, L., Quantitative assessment of second language learners' fluency: An automatic approach (1998) Proceedings of the International Conference on Spoken Language Processing; Culhane, C., DoD workshops on conversational and multilingual speech recognition (1996) Proceedings of the DARPA Speech Recognition Workshop, pp. 148-153; Cummins, F., Gers, F., Schmidhuber, J., Language identification from prosody without explicit features (1999) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); Cutler, A., The comparative perspective on spoken-language processing (1997) Speech Communication, 21 (1-2), pp. 3-16; Cutrell, E., Czerwinski, M., Horvitz, E., Notification disruption and memory: Effects of messaging on memory and performance (2001) Proceedings of the Human-Computer Interaction (Interact-01), pp. 263-269. , IOS Press, Tokyo; Dalsgaard, P., Andersen, O., Identification of mono- and poly-phonemes using acoustic-phonetic features derived by a self-organising neural network (1992) Proceedings of the International Conference on Spoken Language Processing, pp. 547-550; Dalsgaard, P., Andersen, O., Application of inter-language phoneme similarities for language identification (1994) Proceedings of the International conference on Spoken Language Processing, pp. 1903-1906; Dalsgaard, P., Andersen, O., Barry, W., Cross-language merged speech units and their descriptive phonetic correlates (1998) Proceedings of the International Conference on Spoken Language Processing; Dalsgaard, P., Andersen, O., Hesselager, H., Petek, B., Language-identification using language-dependent phonemes and language-independent speech units (1996) Proceedings of the International Conference on Spoken Language Processing; Daniels, P., Bright, W., (1996) The World's Writing Systems, , Oxford University Press, Oxford; Dantzich, M., Robbins, D., Horvitz, E., Czerwinski, M., Scope: Providing awareness of multiple notifications at a glance (2002) Proceedings of the ACM International Working Conference on Advanced Visual Interfaces (AVI-02); Davel, M., Barnard, E., Bootstrapping for language resource generation (2003) Proceedings of the Symposium of the Pattern Recognition Association of South Africa, pp. 97-100; Davel, M., Barnard, E., The efficient generation of pronunciation dictionaries: Machine learning factors during bootstrapping (2004) Proceedings of the International Conference on Spoken Language Processing; Davel, M., Barnard, E., The efficient generation of pronunciation dictionaries: Human factors during bootstrapping (2004) Proceedings of the International Conference on Spoken Language Processing, pp. 2797-2800; de Mareil, P.B., Habert, B., Banard, F., Adda-Decker, M., Baras, C., Adda, G., Paroubek, P., A quantitative study of disfluencies in French broadcast interviews (2005) Proceedings of the DISS'05-ISCA Tutorial and Research Workshop on Disfluency in Spontaneous Speech; Decadt, B., Duchateau, J., Daelemans, W., Wambacq, P., Transcription of out-of-vocabulary words in large vocabulary, speech recognition based on phoneme-to-grapheme conversion (2002) Processings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 861-864; Deerwester, S., Dumains, S.T., Landauer, T.K., Furnas, G.W., Harshman, R.A., Indexing by latent semantic analysis (1990) Journal of the American Society for Information Science, 41 (6), pp. 391-407; DeFrancis, J., (1984) The Chinese Language: Fact and Fantasy, , University of Hawaii Press, Honolulu; Deng, L., Integrated-multilingual speech recognition using universal phonological features in a production model (1997) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 1007-1010; Deng, L., A dynamic feature-based approach to speech modeling and recognition (1997) Proceedings of the IEEE Workshop on automatic Speech Recognition and Understanding, pp. 107-114; Deng, L., Sun, D.X., A statistical approach to automatic speech recognition using the atomic speech units constructed from overlapping articulatory features (1994) Journal of the Acoustical Society of America (JASA), 95, pp. 2702-2719; Deng, Y., Khudanpur, S., Latent semantic information in maximum entropy language models for conversational speech recognition (2003) Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pp. 56-63; Dharanipragada, S., Jelinek, F., Khudanpur, S., Language model adaptation using the minimum divergence principles (1996) research notes no 1, 1995 language modeling summer research workshop technical report, , Center for Language and Speech Processing, Johns Hopkins University; Dijkstra, J., van Son, R.J.J.H., Pols, L., Frisian TTS, an example of bootstrapping TTS for minority languages (2004) Proceedings of the 5th ISCA Workshop on Speech Synthesis; Doddington, G., Speaker recognition based on idiolectal differences between speakers (2001) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); Dugast, C., Aubert, X., Kneser, R., The Philips large-vocabulary recognition system for American English, French and German (1995) Processings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 197-200; Dung, L., Sprachen identifikation mittels prosodie (1997) Master's thesis, , 114-96 D-03, Technische Universität Ilmenau; Durand, J., Laks, B., (2002) Phonetics, Phonology, and Cognition, , Oxford University Press; Dutoit, T., Pagel, V., Pierret, N., van der Vreken, O., Bataille, F., The MBROLA project: Towards a set of high-quality speech synthesizers free of use for non-commercial purposes (1996) Proceedings of the International Conference on Spoken Language Processing, pp. 1393-1397; Eady, S., Differences in the F0 patterns of speech: Tone language versus stress language (1982) Language and Speech, 25 (1), pp. 29-42; Ehsani, F., Bernstein, J., Najimi, A., Todic, O., SUBARASHII: Japanese Interactive Spoken Language Education (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); Eide, E., Distinctive features for use in an automatic speech recognition system (2001) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); (2005) Evaluations and Language Resources Distribution Agency, , http://www.elda.org; (1994) European Corpus Initiative Multilingual Corpus I (ECI/MCI), part mul06, , http://www.elsnet.org/eci.html; (2005) Endangered data vs enduring practice: Creating linguistic resources that last, , http://emeld.org/events/lsa_symposium.cfm, E-MELD Web site. Retrieved May 25, 2005, from; Eskenazi, M., Hansma, S., The fluency pronunciation trainer (1998) Proceedings of the ESCA Workshop on Speech Technology in Language Learning (STiLL); Farinas, J., Pellegrino, F., Rouas, J.-L., Andre-Obrecht, R., Merging segmental and rhythmic features for automatic language identification (2002) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Federico, M., Bertoldi, N., Broadcast news LM adaptation over time (2004) Computer Speech and Language, 18 (4), pp. 417-435; Ferguson, C.A., Language teaching and theories of language (1989) Georgetown University Round Table on Languages and Linguistics 1989, , Georgetown University Press, J.E. Alatis (Ed.); Fernandez, F., De Cordoba, R., Ferreiros, J., Sama, V., D'Haro, L.F., Macias-Guarasa, J., Language identification techniques base on full recognition in an air traffic control task (2004) Proceedings of the International conference on Spoken Language Processing; Finke, M., Waibel, A., Speaking mode dependent pronunciation modeling in large vocabulary conversational speech recognition (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 2379-2382; Fischer, V., Gao, Y., Janke, E., Speaker-independent upfront dialect adaptation in a large vocabulary continuous speech recognizer (1998) Proceedings of the International Conference on Spoken Language Processing; Fisher, W., Doddington, G., Goudie-Marshall, K., The DARPA speech recognition research database: Specifications and status (1986) Proceedings of the DARPA Workshop on Speech Recognition, pp. 93-99; Fitt, S., The pronunciation of unfamiliar native and non-native town names (1995) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 2227-2230; Flege, J., (1995) Speech Perception and Linguistic Experience: Theoretical and Methodological Issues in Cross-Language Speech Research, pp. 233-272. , Ch. Second-language speech learning: Theory, findings and problems, York Press Inc., Timonium, MD; Foil, J., Language identification using noisy speech (1986) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 861-864; Fox, R.A., Flege, J.E., The perception of English and Spanish vowels by native English and Spanish listeners (1995) J. Acoust. Soc. Am., 97 (4), pp. 2540-2551; Franco, H., Neumeyer, L., Kim, Y., Ronen, O., Automatic Pronunciation Scoring for Language Instruction (1997) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Fromkin, V., Rodman, R., Hyams, N., (2003) An Introduction to Language, , Heinle, Boston; Fügen, C., Stüker, S., Soltau, H., Metze, F., Schultz, T., Efficient handling of multilingual language models (2003) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 441-446; Fukunaga, K., (1972) Introduction to Statistical Pattern Recognition, , Academic Press, New York and London; Funahashi, K., On the approximate realization of continuous mapping by neural networks (1989) Neural Networks, 2, pp. 183-192; Fung, P., Liu, W.K., Fast accent identification and accented speech recognition (1999) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Fung, P., Shi, B.E., Wu, D., Lam, W.B., Wong, S.K., Dealing with multilinguality in a spoken language query translator (1997) Proceedings of the ACL/EACL Workshop on Spoken Language Translation, pp. 40-47; Fung, T.Y., Meng, H., Concatenating syllables for response generation in domain-specific applications (2000) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 933-936; Furuse, O., Iida, H., Constituent boundary parsing for example-based machine translation (1994) Proceedings of COLING, pp. 105-111; Furuse, O., Iida, H., Incremental translation utilizing constituent boundary patterns (1996) Proceedings of COLING, pp. 412-417; García-Romero, D., Fiérrez-Aguilar, J., Gozález-Rodríguez, J., Ortega-García, J., Support vector machine fusion of idiolectal and acoustic speaker information in Spanish conversational speech (2003) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 229-232; Garrett, N., ICALL and Second Language Acquisition (1995) Intelligent Language Tutors: Theory Shaping Technology, , Lawrence Erlbaum, Mahwah, NJ, V.M. Holland, J.D. Kaplan, M.R. Sams (Eds.); Gauvain, J., Adda, G., Adda-Decker, M., Allauzen, A., Gendner, V., Lamel, L., Schwenk, H., Where are we in transcribing French broadcast news (2005) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH-INTERSPEECH); Gauvain, J., Adda, G., Lamel, L., Adda-Decker, M., Transcribing broadcast news: The LIMSI nov96 Hub4 System (1997) Proceedings of the ARPA Spoken language Systems Technology Workshop, pp. 56-63; Gauvain, J., Lamel, L., Large vocabulary continuous speech recognition: From laboratory systems towards real-world applications (1996) Trans. IEICE J79-D-II, pp. 2005-2021; Gauvain, J., Lamel, L.F., Adda, G., Matrouf, D., The LIMSI 1995 Hub3 System (1996) Proceedings of the ARPA Spoken Language Technology Workshop, pp. 105-111; Gauvain, J., Messaoudi, A., Schwenk, H., Language recognition using phone lattices (2004) Proceedings of the International Conference on Spoken Language Processing; Gauvain, J.-L., Lamel, L., Adda, G., The LIMSI broadcast news transcription system (2002) Speech Communication, 37 (1-2), pp. 89-108; Gavalda, M., Growing semantic grammars (2000) Ph.D. dissertation, , Language Technologies Institute. Carnegie Mellon University; Gavalda, M., Soup: A parser for real-world spontaneous speech (2004) New Developments in Parsing Technology, , Kluwer, H. Bunt, J. Carroll, G.S. (Eds.); Gendrot, C., Adda-Decker, M., Impact of duration on F1/F2 formant values of oral vowels: An automatic analysis of large broadcast news corpora in French and German (2005) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH-INTERSPEECH); Germann, U., Jahr, M., Knight, K., Marcu, D., Yamada, K., Fast decoding and optimal decoding for machine translation (2001) Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pp. 220-227; Geutner, P., Using morphology towards better large-vocabulary speech recognition systems (1995) Proceedings of the International Conference on Spoken Language Processing, pp. 445-448; Geutner, P., Finke, M., Scheytt, P., Adaptive vocabularies for transcribing multilingual broadcast news (1998) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 925-928; Glass, J., Flammia, G., Goodine, D., Phillips, M., Polifroni, J., Sakai, S., Seneff, S., Zue, V., Multilingual spoken-language understanding in the MIT Voyager system (1995) Speech Communication, 17 (1-2), pp. 1-18; Goddeau, D., Meng, H., Polifroni, J., Seneff, S., Busayapongchai, S., A form-based dialog manager for spoken language applications (1996) Proceedings of the International Conference on Spoken Language Processing, pp. 701-704; Godfrey, J., Holliman, E., McDaniel, J., SWITCHBOARD: Telephone speech corpus for research and development (1992) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 517-520; Gokcen, S., Gokcen, J., A multilingual phoneme and model set: towards a universal base for automatic speech recognition (1997) Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 599-603; (2005) Ethnologue: Languages of the World, , SIL International, Dallas, TX, R. Gordon (Ed.); Goronzy, S., Robust Adaptation to Non-Native Accents in Automatic Speech Recognition (2002) Lecture Notes on Artificial Intelligence, 2560. , Springer Verlag; Goronzy, S., Eisle, K., Automatic pronunciation modeling for multiple non-native accents (2003) Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding; Goronzy, S., Rapp, S., Kompe, R., Generating non-native pronunciation variants for lexicon adaptation (2004) Speech Communication, 42 (1), pp. 109-123; Graff, D., Brid, S., Many uses, many annotations for large corpora: Switchboard and TDT as case studies (2000) Proceedings of the First International Conference on Language Resources and Evaluation (LREC); Graff, D., Cieri, C., Strassel, S., Martey, N., The TDT-3 text and speech corpus (2000) 1999 TDT Evaluation System Summary Papers, , http://www.itl.nist.gov/iaui/894.01/tests/tdt/tdt99/papers/LDC.ps, available at, National Institutes of Standards and Technology; Grosz, B., Sidner, C., Discourse structure and the proper treatment of interruptions (1985) Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI-85), pp. 832-839; Hain, T., Implicit modelling of pronunciation variation in automatic speech recognition (2005) Speech Communication, 46 (2), pp. 171-188; Hajič, J., (1999) Morphological analysis of Czech word forms, , http://nlp.cs.jhu.edu/~hajic/morph.html, Available at, from the Center for Language and Speech Processing, Johns Hopkins University; Hajič, J., Brill, E., Collins, M., Hladká, B., Jones, D., Kuo, C., Ramshaw, L., Zeman, D., Core natural language processing technology applicable to multiple languages (1998) Final report of the 1998 summer workshop on language engineering, , http://www.clsp.jhu.edu/ws98/projects/nlp/report/, available at; Harper, M.P., Jamieson, L.H., Mitchell, C.D., G.Ying, S.P., Srinivasan, P.N., Chen, R., Zoltowski, C.B., Helzerman, R.A., Integrating language models with speech recognition (1994) Proceedings of the 1994 American Association for Artificial Intelligence Workshop on the Integration of Natural Language and Speech Processing; Hartmann, C., Varshney, P., Mehrotra, K., Gerberich, C., Application of information theory to the construction of efficient decision trees (1982) IEEE Trans. on Information Theory, IT-28 (4), pp. 565-577; Hazen, T., Automatic language identification using a segment-based approach (1993) Master's thesis, , Massachusetts Institute of Technology; Hazen, T., Bazzi, I., A comparison and combination of methods for OOV word detection and word conference scoring (2001) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 397-400; Hazen, T., Zue, V., Recent improvements in an approach to segment-based automatic language identification (1994) Proceedings of the International Conference on Spoken Language Processing, pp. 1883-1886; Hazen, T., Zue, V., Segment-based automatic language identification (1997) Journal of the Acoustical Society of America, 101 (4), pp. 2323-2331; He, D., Oard, D., Wang, J., Luo, J., Demner-Fushman, D., Darwish, K., Resnik, P., Leuski, A., Making MIRACLEs: Interactive translingual search for Cebuano and Hindi (2003) ACM Transactions on Asian Language Information Processing, 2 (3), pp. 219-244; He, X., Zhao, Y., Model complexity optimization for nonnative English speakers (2001) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1461-1463; Hieronymus, J., ASCII phonetic symbols for the world's languages: Worldbet (1994) Tech. rep., , AT&T Bell Laboratories; Hieronymus, J., Kadambe, S., Spoken language identification using large vocabulary speech recognition (1996) Proceedings of the International Conference on Spoken Language Processing; Hieronymus, J., Kadambe, S., Robust spoken language identification using large vocabulary speech recognition (1997) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 1111-1114; Hofstede, G., (1997) Cultures and Organizations: Software of the Mind, , McGraw-Hill, New York; Holter, T., Svendsen, T., Incorporating linguistic knowledge and automatic baseform generation in acoustic subword unit based speech recognition (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1159-1162; Holter, T., Svendsen, T., Maximum likelihood modelling of pronunciation variation (1999) Speech Communication, 29, pp. 177-191; Honal, M., Schultz, T., Correction of disfluencies in spontaneous speech using a noisy-channel approach (2003) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); Honal, M., Schultz, T., Automatic disfluency removal on recognized spontaneous speech-rapid adaptation to speaker-dependent disfluencies (2005) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Horvitz, E., Principles of mixed-initiative user interfaces (1999) Proceedings of the ACM Conference on Human Factors in Computing Systems (CHI-99), pp. 159-166. , ACM Press, Pittsburgh, PA; Horvitz, E., Kadie, C., Paek, T., Hovel, D., (2003) Models of Attention in Computing and Communication: From Principles to Applications, pp. 52-59; Huang, C., Chang, E., Zhou, J., Lee, K.-F., Accent Modeling Based on Pronunciation Dictionary Adaptation for Large Vocabulary Mandarin Speech Recognition (2000) Proceedings of the International Conference on Spoken Language Processing, pp. 818-821; Huang, X., Hwang, M.-Y., Jiang, L., Mahajan, M., Deleted interpolation and density sharing for continuous hidden Markov models (1996) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Huang, X.D., Acero, A., Hon, H.-W., (2001) Spoken Language Processing, , Prentice Hall, NJ; Humphries, J.J., Woodland, P.C., Pearce, D., Using Accent-Specific Pronunciation Modelling for Robust Speech Recognition (1996) Proceedings of the International Conference on Spoken Language Processing, pp. 2324-2327; Hunt, A., Black, A., Unit selection in a concatenative speech synthesis system using a large speech database (1996) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 373-376; Huot, H., (2001) Morphologie-forme et sens des mots du francais, , Armand Colin, Paris, France; Ikeno, A., Pellom, B., Cer, D., Thornton, A., Brenier, J.M., Jurafsky, D., Ward, W., Byrne, W., Issues in recognition of Spanish-accented spontaneous English (2003) Proceedings of the ISCA and IEEE Workshop on Spontaneous Speech Processing and Recognition; Imamura, K., Hierarchical phrase alignment harmonized with parsing (2001) Proceedings of the Sixth Natural Language Processing Pacific Rim Symposium, pp. 377-384; Imamura, K., Application of translation knowledge acquired by hierarchical phrase alignment (2002) Proceedings of the Sixth International Conference on Theoretical and Methodological Issues in Machine Translation, pp. 74-84; Imamura, K., Feedback cleaning of machine translation rules using automatic evaluation (2003) Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pp. 447-454; Imperl, B., Clustering of Context Dependent Speech Units for Multilingual Speech Recognition (1999) Proceedings of the ESCA-NATO Tutorial Research Workshop on Multi-lingual Interoperability in Speech Technology, pp. 17-22; Imperl, B., Horvat, B., The clustering algorithm for the definition of multilingual set of context dependent speech models (1999) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 887-890; International, W., http://www.wordwave.co.uk/linguistic; The International Phonetic Association-IPA chart (revised to 1993) (1993) Journal of the International Phonetic Association, 1 (23); Ircing, P., Psutka, J., Two-pass recognition of Czech speech using adaptive vocabulary (2001) Lecture Notes in Computer Science, 2166, pp. 273-277. , Springer Verlag, V. Matousek, P. Mautner, R. Moucek, K. Tauser (Eds.) Text Speech and Dialog; http://www.informatik.uni; Issar, S., Estimation of language models for new spoken language applications (1996) Proceedings of the International Conference on Spoken Language Processing, pp. 869-872; Itahashi, S., Liang, D., Language identification based on speech fundamental frequency (1995) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1359-1362; Itahashi, S., Zhou, J., Tanaka, K., Spoken language discrimination using speech fundamental frequency (1994) Proceedings of the International Conference on Spoken Language Processing, pp. 1899-1902; Iyer, R., Ostendorf, M., Modeling long distance dependence in language: Topic mixtures versus dynamic cache models (1999) IEEE Transactions on Speech and Audio Processing, 7 (1), pp. 30-39; Iyer, R., Ostendorf, M., Gish, H., Using out-of-domain data to improve in-domain language models (1997) IEEE Signal Processing Letters, 4 (8), pp. 221-223; James, C., (1980) Contrastive Analysis, , Longman, London; Jekat, S., Hahn, W., Multilingual Verbmobil-dialogs: Experiments, data collection and data analysis (2000) Vermobil: Foundations of Speech-to-Speech Translations, pp. 575-582. , Springer, Berlin, W. Wahlater (Ed.); Jelinek, F., (1997) Statistical Methods for Speech Recognition, , MIT Press, Cambridge, MA; Jelinek, F., Lafferty, J., Computation of the probability of initial substring generation by stochastic context-free grammars (1991) Computational Linguistics, 17 (3), pp. 315-323; Jelinek, F., Mercer, R., Bahl, L., Baker, J., Perplexity-a measure of difficulty of speech recognition tasks (1997) 94th Meeting of the Acoustic Society of America; Jin, Q., Navrátil, J., Reynolds, D., Campbell, J., Andrews, W., Abramson, J., Combining cross-stream and time dimensions in phonetic speaker recognition (2003) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing; Jitsuhiro, T., Matsui, T., Nakamura, S., Automatic generation of non-uniform context-dependent HMM topologies based on the MDL criterion (2003) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 2721-2724; Jones, R.J., Downey, S., Mason, J.J., Continuous speech recognition using syllables (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1215-1218; Jurafsky, D., Ward, W., Jianping, Z., Herold, K., Xiuyang, Y., Sen, Z., What kind of pronunciation variation is hard for triphones to model (2001) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 577-580; Kain, A., High resolution voice transformation (2001) Ph.D. thesis, , OGI School of Science and Engineering, Oregon Health and Science University; Kaji, H., Kida, Y., Morimoto, Y., Learning translation templates from bilingual text (1992) Proceedings of the Conference on Computational Linguistcs, pp. 672-678; Kanthak, S., Ney, H., Context-dependent acoustic modeling using graphemes for large vocabulary speech recognition (2002) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 845-848; Kanthak, S., Ney, H., Multilingual acoustic modeling using graphemes (2003) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1145-1148; Katzner, K., (2002) The Languages of the World, , Routledge, London/New York; Kawai, G., Spoken language processing applied to nonnative language pronunciation learning (1999) Ph.D. thesis, , University of Tokyo; Khudanpur, S., Kim, W., Contemporaneous text as side-information in statistical language modeling (2004) Computer Speech and Language, 18 (2), pp. 143-162; Kiecza, D., Schultz, T., Waibel, A., Data-driven determination of appropriate dictionary units for Korean LVCSR (1999) Proceedings of the International Conference on Speech Processing (ICSP '99), pp. 323-327; Kikui, G., Sumita, E., Takezawa, T., Yamamoto, S., Creating corpora for speech-to-speech translation (2003) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 381-384; Killer, M., Stüker, S., Schultz, T., Grapheme based speech recognition (2003) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); Kim, J., Flynn, S., What makes a non-native accent?: A study of Korean English (2004) Proceedings of the International Conference on Spoken Language Processing; Kim, W., Khudanpur, S., Lexical triggers and latent semantic analysis for cross-lingual language model adaptation (2004) ACM Transactions on Asian Language Information Processing, 3 (2), pp. 94-112; Kim, Y., Franco, H., Neumeyer, L., Automatic pronunciation scoring of specific phone segments for language instruction (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); Kimball, O., Kao, C., Iyer, R., Arvizo, T., Makhoul, G., Using quick transcriptions to improve conversational speech models (2004) Proceedings of the International Conference on Spoken Language Processing; Kipp, A., Lamel, L., Mariani, J., Schiel, F., (1993) Translanguage English Database (TED), , LDC; Kirchhoff, K., Combining articulatory and acoustic information for speech recognition in noisy and reverberant environments (1998) Proceedings of the International Conference on Spoken Language Processing; Kirchhoff, K., Bilmes, J., Das, S., Duta, N., Egan, M., Ji, G., He, F., Vergyri, D., Novel approaches to Arabic speech recognition: Report from the 2002 Johns-Hopkins summer workshop (2003) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 344-347; Kirchhoff, K., Bilmes, J., Das, S., Egan, M., Ji, G., He, F., Henderson, J., Schwartz, R., Novel speech recognition models for Arabic (2002) Final report of the 2001 summer workshop on language engineering, , http://www.clsp.jhu.edu/wfinal.pdf002/groups/arabic/arabic-final.pdf, available at; Kirchhoff, K., Parandekar, S., Multi-stream statistical N-gram modeling with application to automatic language identification (2001) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); Kirchhoff, K., Parandekar, S., Bilmes, J., Mixed-memory Markov models for automatic language identification (2002) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Knight, K., Automating knowledge acquisition for machine translation (1997) AI Magazine, 18 (4), pp. 81-96; Koehn, P., Och, F.J., Marcu, D., Statistical phrase-based translation (2003) Proceedings of the HLT/NAACL; Köhler, J., Multi-lingual phoneme recognition exploiting acoustic-phonetic similarities of sounds (1996) Proceedings of the International Conference on Spoken Language Processing, pp. 2195-2198; Köhler, J., Multilingual phone modelling for telephone speech (1997) Proceedings of the SQEL, 2nd Workshop on Multi-Lingual Information Retrieval Dialogs, pp. 16-19; Köhler, J., Language adaptation of multilingual phone models for vocabulary independent speech recognition tasks (1998) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 417-420; Köhler, J., Comparing three methods to create multilingual phone models for vocabulary independent speech recognition tasks (1999) Proceedings of the ESCA-NATO Tutorial Research Workshop on Multi-lingual Interoperability in Speech Technology, pp. 79-84; Kominek, J., Bennett, C., Black, A., Evaluating and correcting phoneme segmentation for unit selection synthesis (2003) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); Kominek, J., Black, A., The CMU ARCTIC speech databases for speech synthesis research (2003) Tech. Rep. CMU-LTI-03-177, , http://festvox.org/cmu_arctic/, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA; Krauwer, S., Elsnet and ELRA: A common past and a common future (1998) ELRA Newsletter, 3 (2); Krauwer, S., Maegaard, B., Chouki, K., Report on basic language resource kit (BLARK) for Arabic (2004) Proceedings from NEMLAR International Conference on Arabic Language Resources and Tools; Kubala, F., Austin, S., Barry, C., Makhoul, J., Placeway, P., Schwartz, R., BYBLOS Speech Recognition Benchmark Results (1991) Proceedings from DARPA Speech and Natural Language Workshop; Kuboň, V., Plátek, M., A grammar based approach to a grammar checking of free word order languages (1994) Proceedings of the 15th International Conference on Computational Linguistics, pp. 906-910; Kumpf, K., King, R., Foreign speaker accent classification using phoneme-dependent accent discrimination models and comparisons with human perception benchmarks (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); Küpfmüller, K., Die Entropie der deutschen Sprache (1954) FTZ, 6 (6), pp. 265-272; Kwan, H., Hirose, K., Use of recurrent network for unknown language rejection in language identification systems (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 63-67; Labov, W., The Social Stratification of English in New York City (1966) Ph.D. thesis, , Columbia University, NY; Labov, W., (2004) Phonological Atlas of North America, , http://www.ling.upenn.edu/phono_atlas/home/html; Lamel, L., Adda, G., On designing pronunciation lexicons for large vocabulary, continuous speech recognition (1996) Proceedings of the International Conference on Spoken Language Processing, pp. 6-9; Lamel, L., Adda-Decker, M., Gauvain, J.L., Issues in large vocabulary multilingual speech recognition (1995) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 185-188; Lamel, L., Adda-Decker, M., Gauvain, J., Adda, G., Spoken language processing in a multilingual context (1996) Proceedings of the International Conference on Spoken Language Processing, pp. 2203-2206; Lamel, L., Gauvain, J., Language identification using phone-based acoustic likelihoods (1994) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Lamel, L., Gauvain, J., Alternate phone models for conversational speech (2005) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Landauer, T.K., Littman, M.L., Fully automatic cross-language document retrieval using latent semantic indexing (1995) Proceedings of the Sixth Annual Conference of the UW Centre for the New Oxford English Dictionary and Text Research, pp. 31-38; Lander, T., Cole, R., Oshika, B., Noel, M., The OGI 22 language telephone speech corpus (1995) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); Langlais, P., Öster, A.-M., Granström, B., Automatic detection of mispronunciation in non-native Swedish speech (1998) Proceedings of the ESCA Workshop on Speech Technology in Language Learning (STiLL); Langner, B., Black, A., Improving the understandability of speech synthesis by modeling speech in noise (2005) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Latorre, J., Iwano, K., Furui, S., Polyglot synthesis using a mixture of monolingual corpora (2005) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Lau, R., Seneff, S., A unified system for sublexical and linguistic modeling using ANGIE and TINA (1998) Proceedings of the International Conference on Spoken Language Processing, pp. 2443-2446; Lavie, A., Font, A., Peterson, E., Carbonell, J., Probst, K., Levin, L., Reynolds, R., Vogel, S., Experiments with a Hindi-to-English transferbased (MT) system under a miserly data scenario (2003) ACM Transactions on Asian Language Information Processing, , TALIP; Lavie, A., Langley, C., Waibel, A., Pianesi, F., Lazzari, G., Coletti, P., Taddei, L., Balducci, F., Architecture and design considerations in NESPOLE!: A speech translation system for e-commerce applications (2001) Proceedings of HLT: Human Language Technology; Lavie, A., Levin, L., Schultz, T., Waibel, A., Domain portability in speech-to-speech translation (2001) Proceedings of HLT: Human Language Technology; http://www.ldc.upenn.edu/Catalog/LDC97S44.html; (2000) The Linguistic Data Consortium. Internet, , http://www.ldc.upenn.edu; http://www.ldc.upenn.edu; Lee, K.-F., Large-vocabulary speaker-independent continuous speech recognition: The sphinx system (1988) Ph.D. thesis, , Carnegie Mellon University, Pittsburg, PA; Lefevre, F., Gauvain, J.L., Lamel, L., Improving genericity for task-independent speech recognition (2001) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1241-1244; Leggetter, C.J., Woodland, P.C., MLLR for speaker adaptation of CDHMMs (1995) Computer Speech and Language, 9, pp. 171-185; Lesh, N., Rich, C., Sidner, C., Collaborating with focused and unfocused users under imperfect communication (2001) Proceedings of the International Conference on User Modeling, pp. 63-74. , Springer; Levin, E., Narayanan, S., Pieraccini, R., Biatov, K., Bocchieri, E., Fabbrizio, G.D., Eckert, W., Walker, M., The AT&T DARPA Communicator Mixed-Initiative Spoken Dialog System (2000) Proceedings of the International Conference on Spoken Language Processing, pp. 122-125; Levin, L., Gates, D., Lavie, A., Waibel, A., An interlingua based on domain actions for machine translation of task-oriented dialogues (1998) Proceedings of the International Conference on Spoken Language Processing; Li, J., Zheng, F., Wu, W., Context-independent Chinese initial-final acoustic modeling (2000) Proceedings of the International Symposium on Chinese Spoken Language processing, pp. 23-26; Li, K., Automatic language identification using syllabic spectral features (1994) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 297-300; Linguistic Data Consortium (1996) Celex2, , http://www.ldc.upenn.edu/Catalog/LDC96L14.html; Linguistic Data Consortium http://www.ldc.upenn.edu/Catalog/docs/LDC97T19/ar-trans.txt; Liu, F.H., Gao, Y., Gu, L., Picheny, M., Noise robustness in speech to speech translation (2003) IBM Technical Report RC22874; Liu, W.K., Fung, P., MLLR-based accent model adaptation without accented data (2000) Proceedings of the International Conference on Spoken Language Processing; Livescu, K., (1999) Analysis and modeling of non-native speech for automatic speech recognition, , MIT Department of Electrical Engineering and Computer Science; Livescu, K., Glass, J., Lexical modeling of non-native speech for automatic speech recognition (2000) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Lo, W.K., Meng, H., Ching, P.C., Subsyllabic acoustic modeling across Chinese dialects (2000) Proceedings of the International Symposium on Chinese Spoken Language Processing; Lyons, J., (1990) Introduction to Theoretical Linguistics, , Oxford University Press, Oxford; Ma, K., Zavaliagkos, G., Iyer, R., BBN pronunciation modeling (1998) 9th Hub-5 Conversational Speech Recognition Workshop; Macon, M., Kain, A., Cronk, A., Meyer, H., Müeller, K., Säeuberlich, B., Black, A., Rapid prototyping of a German TTS system (1998) Unpublished report Oregon Graduate Institute, , http://www.cslu.ogi.edu/tts/research/multiling/de-report.html; Maddieson, I., Vasilescu, I., Factors in human language identification (2002) Proceedings of the International Conference on Spoken Language Processing; Maegaard, B., NEMLAR-an Arabic language resources project (2004) Proceedings of the IV International Conference on Language Resource on Speech Communication and Technology (EUROSPEECH), pp. 2631-2634; Manning, C.D., Schütze, H., (1999) Foundations of Statistical Natural Language Processing, , The MIT Press, Cambridge, Massachusetts; Manos, A., Zue, V., A segment-based spotter using phonetic filler models (1997) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 899-902; Mapelli, V., Choukri, K., Deliverable 5.1 report on a (minimal) set of LRS to be made available for as many languages as possible, and map of the actual gaps (2003) Internal ENABLER project report; Marcu, D., Wong, W., A phrase-based, joint probability model for statistical machine translation (2002) Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 6-7; Marcus, A., Gould, E., Crosscurrents: Cultural dimensions and global Web user-interface design (2000) ACM Interactions, 7 (4), pp. 32-48; Mariani, J., (2003), Proposal for an ERA-net in the field of human language technologies. In: LangNet; Mariani, J., Lamel, L., An overview of EU programs related to conversational/interactive systems (1998) Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, pp. 247-253; Mariani, J., Paroubek, P., Human language technologies evaluation in the European framework (1998) Proceedings of the DARPA Broadcast News Workshop, pp. 237-242; Marino, J., Nogueiras, A., Bonafonte, A., The demiphone: An efficient subword unit for continuous speech recognition (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1171-1174; Markov, A., An example of statistical invesetigation in the text of "Eugene Onegin" illustrating coupling "tests" in chains (1913) Proceedings of the Academy of Science of St. Petersburg., pp. 153-162; Martin, A., Doddington, G., Kamm, T., Ordowski, M., Przybocki, M., The DET curve in assessment of detection task performance (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1895-1898; Maskey, S., Black, A., Mayfield Tomokiyo, L., Optimally constructing phonetic lexicons in new languages (2004) Proceedings of the International Conference on Spoken Language Processing; Matrouf, D., Adda-Decker, M., Lamel, L., Gauvain, J., Language identification incorporating lexical information (1998) Proceedings of the International Conference on Spoken Language Processing, pp. 181-184; Matsumoto, Y., Ishimoto, H., Usturo, T., Structural matching of parallel text (1993) Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pp. 23-30; Mayfield Tomokiyo, L., Linguistic properties of non-native speech (2000) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Mayfield Tomokiyo, L., Recognizing Non-native Speech Characterizing and Adapting to Non-native Usage in Speech Recognition (2001) Ph.D. thesis, , Carnegie Mellon University; Mayfield Tomokiyo, L., Burger, S., Eliciting natural speech from nonnative users: Collecting speech data for LVCSR (1999) Proceedings of the ACL-IALL Joint Workshop in Computer-Mediated Language Assessment and Evaluation in Natural Language Processing; Mayfield Tomokiyo, L., Jones, R., You're not from 'round here, are you? Naive Bayes detection of non-native utterance text (2001) Proceedings of the NAACL Conference; Mayfield Tomokiyo, L., Waibel, A., Adaptation methods for Non-native speech (2001) Proceedings of the ISCA Workshop on Multilinguality in Spoken Language Processing; McTear, M., Spoken dialog technology: enabling the conversational user interface (2002) ACM Computing Survey, 34 (1), pp. 90-169; Meng, H., Busayapongchai, S., Glass, J., Goddeau, D., Hetherington, L., Hurley, E., Pao, C., Zue, V., WHEELS: A Conversational System on Electronic Automobile Classifieds (1996) Proceedings of the International Conference on Spoken Language Processing, pp. 542-545. , IEEE; Meng, H., Chan, S.F., Wong, Y.F., Chan, C.C., Wong, Y.W., Fung, T.Y., Tsui, W.C., Chi, H.S., ISIS: A learning system with combined interaction and delegation dialogs (2001) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1551-1554; Meng, H., Ching, P.C., Wong, Y.F., Chan, C.C., A multi-modal, trilingual, distributed spoken diaglog system developed with CORBA, JAVA, XML and KQML (2002) Proceedings of the International Conference on Spoken Language Processing, pp. 2561-2564; Meng, H., CU VOCAL: Corpus-based syllable concatenation for Chinese speech synthesis across domains and dialects (2002) Proceedings of the International Conference on Spoken Language Processing; Meng, H., Lam, W., Wai, C., To believe is to understand (1999) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 2015-2018; Meng, H., Lee, S., Wai, C., CU FOREX: A bilingual spoken dialog system for foreign exchange enquires (2000) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 229-232; Meng, H., Lo, T.H., Keung, C.K., Ho, M.C., Lo, W.K., CU VOCAL Web service: A text-to-speech synthesis Web service for voice-enabled Web-mediated application (2003) Proceedings of the 12th International World Wide Web Conference (WWW-03), , http://www2003.org/cdrom/papers/poster/p056/p56-meng.htmlkerning2; Messaoudi, A., Lamel, L., Gauvain, J., Transcription of Arabic broadcast news (2004) Proceedings of the International Conference on Spoken Language Processing, pp. 521-524; Metze, F., Langley, C., Lavie, A., McDonough, J., Soltau, H., Waibel, A., Burger, S., Taddei, L., The NESPOLE! speech-to-speech translation system (2003) Proceedings of the HLT: Human Language Technology; Metze, F., Waibel, A., A flexible stream architecture for ASR using articulatory features (2002) Proceedings of the International Conference on Spoken Language Processing; Micca, G., Palme, E., Frasca, A., Multilingual vocabulariese in automatic speech recognition (1999) Proceedings of the ESCA-NATO Tutorial Research Workshop on Multi-lingual Interoperability in Speech Technology, pp. 65-68; Miller, G.A., Fellbaum, C., Tengi, R., Wolff, S., Wakefield, P., Langone, H., Haskell, B., (2005) Wordnet a lexical database for the English language, , http://wordnet.princeton.edu; Mimer, B., Stüker, S., Schultz, T., Flexible tree clustering for grapheme-based speech recognition (2004) Elektronische Sprachverarbeitung, , ESSV, Cottbus, Germany; Mitteni, R., (1992) Computer-Usable Version of Oxford Advanced Learner's Dictionary of Current English, Oxford Text Archive; Mohri, M., Pereira, F., Riley, M., Weighted finite state transducers in speech recognition (2000) ISCA ITRW Workshop on Automatic Speech Recognition: Challenges for the Millenium., pp. 97-106; Mohri, M., Riley, M., Weighted determinization and minimization for large vocabulary speech recognition (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 131-134; Morimoto, T., Uratani, N., Takezawa, T., Furuse, O., Sobashima, Y., Iida, H., Nakamura, A., Yamazaki, Y., A speech and language database for speech translation research (1994) Proceedings of the International Conference on Spoken Language Processing, pp. S30; Muthusamy, Y., A segmental approach to automatic language identification (1993) Ph.D. thesis, , Oregon Graduate Identification of Science and Technology, Portland, Oregon; Muthusamy, Y., Barnard, E., Cole, R., Reviewing automatic language recognition (1994) IEEE Signal Processing Magazine, 11 (4), pp. 33-41; Muthusamy, Y., Cole, R., Oshika, B., The OGI multi-language telephone speech corpus (1992) Proceedings of the International Conference on Spoken Language Processing; Muthusamy, Y., Jain, N., Cole, R., Perceptual benchmarks for automatic language identification (1994) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 333-336; Nagao, M., A framework of a mechanical translation between Japanese and English by analogy principle (1984) Artifcial and Human Intelligence, pp. 173-180. , North-Holland, Amsterdam, R. Banerji (Ed.); Nagata, M., A stochastic Japanese morphological analyzer using a forward-DP backwarad-A N-best search algorithm (1994) Proceedings of COLING, pp. 201-207; Nakamura, A., Matsunaga, S., Shimizu, T., Tonomura, M., Sagisaka, Y., Japanese speech databases for robust speech recognition (1996) Proceedings of the International Conference on Spoken Language Processing, pp. 2199-2202; Nass, C., Brave, S., (2005) Wired for Speech: How Voice Activates and Advances the Human-Computer Relationship, , MIT Press, Cambridge, MA; Nass, C., Gong, L., Speech interfaces from an evolutionary perspective: Social psychological research and design implications (2000) Communications of the ACM, 43 (9), pp. 36-43; National Institute of Standards and Technology (2004) DARPA EARS Rich-Transcription workshops, , http://www.nist.gov/speech/tests/rt/index.htm, Described at; NATO Non-Native Speech Corpus (2004), http://www.homeworks.be/docs/tr-ist; Navrátil, J., Improved phonotactic analysis in automatic language identification (1996) Proceedings of the European Signal Processing Conference (EUSIPCO-96), pp. 1031-1034; Navrátil, J., Untersuchungen zur automatischen sprachen-identifikation auf der basis der phonotaktik, akustik und prosodie (1998) Ph.D. thesis, , Technical University of Ilmenau, Germany; Navrátil, J., Spoken language recognition-a step towards multilinguality in speech processing (2001) IEEE Transactions on Speech and Audio Processing, 9 (6), pp. 678-685; Navrátil, J., Zühlke, W., Phonetic-context mapping in language identification (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 71-74; Navrátil, J., Zühlke, W., An efficient phonotactic-acoustic system for language identification (1998) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 781-784; Ney, H., Speech translation: Coupling of recognition and translation (1999) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 517-520; Ney, H., Stochastic modeling: From pattern classification to language translation (2001) Proceedings of the ACL 2001 Workshop on DDMT, pp. 33-37; Nießen, S., Vogel, S., Ney, H., Tillmann, C., A DP based search algorithm for statistical machine translation (1998) Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17 th International Conference on Computational Linguistics, pp. 960-967; (1997) The 1997 Hub-5NE evaluation plan for recognition of conversational speech over the telephone, in non-English languages, , http://www.nist.gov/speech/tests/ctr/hub5me_97/current-plan.htm; (2002) Automatic evaluation of machine translation quality using N-gram co-occurence statistics, , http://www.nist.gov/speech/tests/mt/doc/ngram-study.pdf; (2003) The 2003 NIST language evaluation plan, , http://www.nist.gov/speech/tests/lang/doc/LangRec_EvalPlan.v1.pdf; Oard, D.W., The surprise language exercises (2003) ACM Transactions on Asian Language Information Processing, 2 (2), pp. 79-84; Och, F., Tillmann, C., Ney, H., Improved alignment models for statistical machine translation (1999) Proceedings of the EMNLP/WVLC; Och, F.J., Ney, H., Improved statistical alignment models (2000) Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pp. 440-447; Oflazer, K., Two-level description of Turkish morphology (1994) Literary and Linguistic Computing, 9 (2), pp. 137-148; Olive, J., Greenwood, A., Coleman, J., Greenwood, A., (1993) Acoustics of American English Speech: A Dynamic Approach, , Springer Verlag; Ostendorf, M., Moving beyond the beads-on-a-string model of speech (1999) Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding; Osterholtz, L., Augustine, C., McNair, A., Rogina, I., Saito, H., Sloboda, T., Tebelskis, J., Woszczyna, M., Testing generality in JANUS: A multi-lingual speech translation system (1992) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Oviatt, S., Cohen, P., Multimodal interfaces that process what comes naturally (2000) Communications of the ACM, 43 (3), pp. 45-53; Oviatt, S., DeAngeli, A., Kuhn, K., Integration and synchronization of input modes during multimodal human-computer interaction (1997) Proceedings of the ACM Conference on Human Factors in Computing Systems (CHI-97), pp. 415-422. , ACM Press; Pallett, D., Fiscus, J., Martin, A., Przybocki, M., 1997 broadcast news benchmark test results: English and non-English (1998) Proceedings of the DARPA Broadcast News Transcription & Understanding Workshop, pp. 5-11; Papineni, K.A., Roukos, S., Ward, R.T., Free-flow dialog management using forms (1998) Proceedings of the International Conference on Spoken Language Processing, pp. 1411-1414; Papineni, K., Roukos, S., Ward, T., Zhu, W., BLEU: A method for automatic evaluation of machine translation (2002) Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311-318; Parandekar, S., Kirchhoff, K., Multi-stream language identification using data-driven dependency selection (2003) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; http://www.speech.sri.com/projects/translation/full.html; Pastor, M., Sanchis, A., Casacuberta, F., Vidal, E., EuTrans: A speech-to-speech translator prototyp (2001) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 2385-2389; Paul, D., Baker, J., The design for the Wall Street Journal-based CSR corpus (1992) Proceedings of the International Conference on Spoken Language Processing, pp. 899-902; Pellergrino, F., Farinas, J., Obrecht, R., Comparison of two phonetic approaches to language identification (1999) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); Pérennou, G., Le projet bdlex de base de données lexicales et phonologiques (1988) lères journées du GRECO-PRC CHM; Pfau, T., Beham, M., Reichl, W., Ruske, G., Creating large subword units for speech recognition (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1191-1194; Pfister, B., Romsdorfer, H., Mixed-lingual text analysis for Polyglot TTS synthesis (2003) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); (1993) Fundamentals of Speech Recognition, , Prentice Hall, Englewood Cliffs, NJ, L. Rabiner, B. Juang (Eds.); Ramsey, S.R., (1987) The Languages of China, , Princeton University Press; Ramus, F., Mehler, J., Language identification with suprasegmental cues: A study based on speech resynthesis (1999) Journal of the Acoustical Society of America, 105 (1), pp. 512-521; Rao, A.S., Georgeff, M.P., BDI Agents: From Theory to Practice (1995) Tech. rep., , Australian Artificial Intelligence Institute, Melbourne, Australia; Rao, P.S., Monkowski, M.D., Roukos, S., Language model adaptation via minimum discrimination information (1995) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 161-164; Raux, A., Black, A., A unit selection approach to F0 modeling and its application to emphasis (2003) Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding; Reynolds, D.A., A Gaussian mixture modeling approach to text-independent speaker identification (1992) Ph.D. thesis, , Georgia Institute of Technology, Atlanta, GA; Reynolds, D., Exploiting high-level information for high-performance speaker recognition, SuperSID project final report (2003) Summer workshop 2002, , http://www.clsp.jhu.edu/w002/groups/supersid/SuperSID_Final_Report_CLSP_ WS02_2003_10_06.pdf; Riccardi, G., Gorin, A.L., Ljolje, A., Riley, M., A spoken language system for automated call routing (1997) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 1143-1146; Riley, M., Byrne, W., Finke, M., Khudanpur, S., Ljolie, A., McDonough, J., Nock, H., Zavaliagkos, G., Stochastic pronunciation modelling from hand-labelled phonetic Corpora (1999) Speech Communication, 29, pp. 209-224; Riley, M., Ljojle, A., (1996) Automatic Speech and Speaker Recognition, pp. 285-301. , Ch. Automatic Generation of Detailed Pronunciation Lexicons, Kluwer; Ringger, E.K., Robust loose coupling for speech recognition and natural language understanding (1995) Technical Report 592, , University of Rochester Computer Science Department; Roark, B., Probabilistic top-down parsing and language modeling (2001) Computational Linguistics, 27 (2), pp. 249-276; Roark, B., Saraclar, M., Collins, M., Johnson, M., Discriminative language modeling with conditional random fields and the perceptron algorithm (2004) Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics; Ronen, O., Neumeyer, L., Franco, H., Automatic detection of mispronunciation for Language Instruction (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); Rosenfeld, R., Adaptive statistical language modeling: A maximum entropy approach (1994) Ph.D. thesis, , Computer Science Department, Carnegie Mellon University; Rosenfeld, R., Two decades of statistical language modeling; where do we go from here? (2000) Proceedings of the IEEE, 88, pp. 1270-1278; Rosset, S., Bennacef, S., Lamel, L., Design strategies for spoken language dialog systems (1999) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1535-1538; Rouas, J.-L., Farinas, J., Pellegrino, F., Andre-Obrecht, R., Modeling prosody for language identification on read and spontaneous speech (2003) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Roux, J., Xhosa: A tone or pitch-accent language? (1998) South African Journal of Linguistics, (SUPP. 36), pp. 33-50; Rudnicky, A., Thayer, E., Constantinides, P., Tchou, C., Shern, R., Lenzo, K., Xu, W., Oh, A., Creating natural dialogs in the Cernegie Mellon Communicator System (1999) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1531-1534; Sadek, M.D., Mori, R., Dialog systems (1997) Spoken Dialog with Computers, pp. 523-561. , Acadamic Press, R. de Mori (Ed.); Sagisaka, Y., Kaiki, N., Iwahashi, N., Mimura, K., ATR ν-talk speech synthesis system (1992) Proceedings of the International Conference on Spoken Language Processing, pp. 483-486; Sahakyan, M., Variantenlexikon italienischer und deutscher Lerner des Englischen für die automatische Spracherkennung (2001) Master's thesis, , IMS, University of Stuttgart; Sampson, G., (1985) Writing Systems: A Linguistic Introduction, , Stanford University Press, Stanford, CA; Schaden, S., A database for the analysis of cross-lingual pronunciation variants of European city names (2002) Proceedings of the Third International Conference on Language Resources and Evaluation (LREC), pp. 1277-1283; Schaden, S., Rule-based lexical modelling of foreign-accented pronunciation variants (2003) Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pp. 159-162; Schaden, S., Generating non-native pronunciation lexicons by phonological rules (2003) Proceedings of the 15th International Conference of Phonetic Sciences (ICPhS), pp. 2545-2548; Schillo, C., Fink, G.A., Kummert, F., Grapheme-based speech recognition for large vocabularies (2000) Proceedings of the International Conference on Spoken Language Processing, pp. 584-587; Schukat-Talamazzini, E., Hendrych, R., Kompe, R.H.N., Permugram language models (1995) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1773-1776; Schultz, T., Globalphone: A multilingual text and speech database developed at Karlsruhe University (2002) Proceedings of the International Conference on Spoken Language Processing; Schultz, T., Towards rapid language portability of speech processing systems (2004) Conference on Speech and Language Systems for Human Communication (SPLASH); Schultz, T., Black, A., Vogel, S., Woszczyna, M., (2005), Flexible speech translation systems. In: IEEE Transactions on Speech and Audio Processing, to appear; Schultz, T., Jin, Q., Laskowski, K., Tribble, A., Waibel, A., Improvements in non-verbal cue identification using multilingual phone strings (2002) Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics; Schultz, T., Rogina, I., Waibel, A., LVCSR-based language identification (1996) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Schultz, T., Waibel, A., Fast bootstrapping of LVCSR systems with multilingual phoneme sets (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 371-373; Schultz, T., Waibel, A., The Globalphone project: Multilingual LVCSR with Janus-3 (1997) Multilingual Information Retrieval Dialogs: 2nd SQEL Workshop, pp. 20-27; Schultz, T., Waibel, A., Multilingual and crosslingual speech recognition (1998) Proceedings of the DARPA Workshop on Broadcast News Transcription and Understanding, pp. 259-262; Schultz, T., Waibel, A., Adaptation of pronunciation dictionaries for recognition of unseen languages (1998) Proceedings of the SPIIRAS International Workshop on Speech and Computer, pp. 207-210; Schultz, T., Waibel, A., Language independent and language adaptive large vocabulary speech recognition (1998) Proceedings of the International Conference on Spoken Language Processing, pp. 1819-1822; Schultz, T., Waibel, A., Polyphone decision tree specialization for language adaptation (2000) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Schultz, T., Waibel, A., Language independent and language adaptive acoustic modeling for speech recognition (2001) Speech Communication, 35 (1-3), pp. 31-51; Schwartz, R., Chow, Y., Roucos, S., Improved hidden Markov modelling of phonemes for continuous speech recognition (1984) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 35.6.1-35.6.4; Schwartz, R., Jin, H., Kubala, F., Matsoukas, S., Modeling those F-conditions-or not (1997) Proceedings of the 1997 DARPA Speech Recognition Workshop; Seneff, S., Chuu, C., Cyphers, D.S., Orion: From on-line interaction to off-line delegation (2000) Proceedings of the International Conference on Spoken Language Processing, pp. 142-145; Seneff, S., Lau, R., Meng, H., ANGIE: A new framework for speech analysis based on morpho-phonological modeling (1996) Proceedings of the International Conference on Spoken Language Processing, pp. 110-113; Seneff, S., Lau, R., Polifroni, J., Organization, communication and control in the Galaxy-II Conversational System (1999) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1271-1274; Seneff, S., Wang, C., Peabody, M., Zue, V., Second language acquisition through human-computer dialogue (2004) Proceedings of the International Symposium on Chinese Spoken Language Processing; Seymore, K., Rosenfeld, R., Using story topics for language model adaptation (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1987-1990; Shafran, I., Ostendorf, M., Acoustic model clustering based on syllable structure (2003) Computer Speech and Language, 17 (4), pp. 311-328; Shimohata, M., Sumita, E., Matsumoto, Y., Building a paraphrase corpus for speech translation (2004) Proceedings of the First International Conference on Language Resources and Evaluation (LREC), pp. 453-457; Shoup, J., Phonological aspects of speech recognition (1980) Trends in Speech Recognition, pp. 125-138. , Prentice-Hall, Englewood Cliffs, NJ, L. W. (Ed.); Shozakai, M., Speech interface VLSI for car applications (1999) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. I; Shriberg, E., Stolcke, A., Word predictability after hesitations (1996) Proceedings of the International Conference on Spoken Language Processing; Silberztein, M., (1993) Dictionnaires électroniques et analyse automatique de textes: Ĩe système INTEX, , Masson; SimplifiedEnglish, , http://www.userlab.com/se.html; Singer, E., Torres-Carrasquillo, P., Gleason, T., Campbell, W., Reynolds, D., Acoustic, phonetic, and discriminative approaches to automatic language identification (2003) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); Singh, R., Raj, B., Stern, R., Automatic generation of subword units for speech recognition systems (2002) IEEE Transactions on Speech and Audio Processing, 10, pp. 98-99; Somers, H., Review article: Example-based machine translation (1999) Journal of Machine Translation, 14 (2), pp. 113-157; http://www.cmuspice.org; Spiegel, M., Using the Orator synthesizer for a public reverse-directory service: Design, lessons, and recommendations (1993) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1897-1900; Spiegel, M., Macchi, M., Development of the Orator synthesizer for network applications: Name pronunciation accuracy, morphological analysis, custimization for business listing, and acronym pronunciation (1990) Proceedings of the AVOIS; Sproat, R., Emerson, T., The first international Chinese word segmentation bakeoff (2003) Proceedings of the Second SIGHAN Workshop on Chinese Language Processing, pp. 1-11; Sproat, R., Zheng, F., Gu, L., Li, J., Zheng, Y., Su, Y., Zhou, H., Jurafsky, D., Dialectal Chinese speech recognition: Final report (2004) Final report of the 2004 summer workshop on language engineering, , http://www.clsp.jhu.edu/w004/groups/ws04casr/report.pdf, available at; Stolcke, A., Bratt, H., Butzberger, J., Franco, H., Gadde, Rao, V.R., Plauche, M., Zheng, J., The SRI March 2000 Hub-5 conversational speech transcription system (2000) Proceedings of the NIST Speech Transcription Workshop; (1995) Strange Corpus, , http://www.phonetik.uni-muenchen.de/bas/bassc1deu.html; Strassel, S., Glenn, M., Creating the annotated TDT-4 Y2003 evaluation corpus (2003) 2003 TDT Evaluation System Summary Papers, , http://www.nist.gov/speech/tests/tdt/tdt2003/papers/ldc.ppt, available at, National Institutes of Standards and Technology; Strassel, S., Maxwell, M., Cieri, C., Linguistic resource creation for research and technology development: A recent experiment (2003) ACM Transactions on Asian Language Information Processing, 2 (2), pp. 101-117; Stüker, S., Metze, F., Schultz, T., Waibel, A., Integrating multilingual articulatory features into speech recognition (2001) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); Stüker, S., Schultz, T., A grapheme based speech recognition system for Russian (2004) Proceedings of the Specom; Stüker, S., Schultz, T., Metze, F., Waibel, A., Multilingual articulatory features (2003) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Stylianou, Y., Cappé, O., Moulines, E., Statistical methods for voice quality transformation (1995) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 447-450; Sugaya, F., Takezawa, T., Yokoo, A., Sagisaka, Y., Yamamoto, S., Evaluation of the ATR-MATRIX speech translation system with a pair comparison method between the system and humans (2000) Proceedings of the International Conference on Spoken Language Processing, pp. 1105-1108; Sugiyama, M., Automatic language identification using acoustic features (1991) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 813-816; Sumita, E., Example-based machine translation using DP-matching between word sequences (2001) Proceedings of the Association for Computational Linguistics Workshop on DDMT, pp. 1-8; Sumita, E., Iida, H., Experiments and prospects of example-based machine translation (1991) Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pp. 185-192; Sumita, E., Yamada, S., Yamamoto, K., Paul, M., Kashioka, H., Ishikawa, K., Shirai, S., Solutions to problems inherent in spoken-language translation: The ATR-matrix approach (1999) Proceedings of the MT Summit VII, pp. 229-235; Takami, J., Sagayama, S., A successive state splitting algorithm for efficient allophone modeling (1992) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 573-576; Takezawa, T., Kikui, G., Collecting machine-translation-aided bilingual dialogues for corpus-based speech translation (2003) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 2757-2760; Takezawa, T., Morimoto, T., Sagisaka, Y., Speech and language databases for speech translation research in ATR (1998) Proceedings of the 1st International Workshop on East-Asian Lnaguage Resources and Evaluation (EARLREW); Takezawa, T., Morimoto, T., Sagisaka, Y., Speech and language database for speech translation research in ATR (1998) Proceedings of the Oriental COCOSDA Workshop'98, pp. 148-155; Takezawa, T., Morimoto, T., Sagisaka, Y., Campbell, N., Iida, H., Sugay, F., Yokoo, A., Yamamoto, S., A Japanese-to-English speech translation system: ATR-MATRIX (1998) Proceedings of the International Conference on Spoken Language Processing; Takezawa, T., Sumita, E., Sugaya, F., Yamamoto, H., Yamamoto, S., Toward a broad-coverage bi-lingual corpus for speech translation of travel conversations in the real world (2002) Proceedings of the Third International Conference on Language Resources and Evaluation (LREC), pp. 147-152; Tan, G., Takechi, M., Brave, S., Nass, C., Effects of voice vs. remote on U.S. and Japanese user satisfaction with interactive HDTV systems (2003) Extended Abstracts of the Computer-Human, Interaction (CHI) Conference, pp. 714-715; Tarone, E., Cohen, A.D., Dumas, G., A closer look at some interlanguage terminology: A framework for communication strategies (1983) Strategies in Interlanguage Communication, , Longman, C. F{high symbol}rch, G. Kasper (Eds.); Taylor, P., Black, A., Assigning phrase breaks from part-of-speech sequences (1998) Computer Speech and Language, 12, pp. 99-117; Taylor, P., Black, A., Caley, R., The architecture of the festival speech synthesis system (1998) Proceedings of the 3rd ESCA/COCOSDA Workshop on Speech Synthesis, pp. 147-151; Teixeira, C., Trancoso, I., Serralheiro, A., Accent identification (1996) Proceedings of the International Conference on Spoken Language Processing; Thyme-Gobbel, A., Hutchins, S., On using prosodic cues in automatic language identification (1996) Proceedings of the International Conference on Spoken, Language Processing; Tian, J., Kiss, I., Viikki, O., Pronunciation and acoustic model adaptation for improving multilingual speech recognition (2001) Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding; Toda, T., Kawai, H., Tsuzaki, M., Optimizing sub-cost functions for segment selection based on perceptual evaluations in concatenative speech synthesis (2004) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 657-660; Toda, T., Kawai, H., Tsuzaki, M., Shikano, K., Segment selection considering local degradation of naturalness in concatenative speech synthesis (2003) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 696-699; http://www.ets.org/toeic/; Tokuda, K., Kobayashi, T., Imai, S., Speech parameter generation from HMM using dynamic features (1995) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 660-663; Tokuda, K., Yoshimura, T., Masuko, T., Kobayashi, T., Kitamura, T., Speech parameter generation algorithms for HMM-based speech synthesis (2000) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 1315-1318; Tomokiyo, L., Badran, A., (2003) Egyptian Arabic romanization conventions, internal report, p. 2003. , Carnegie Mellon; Tomokiyo, L., Black, A., Lenzo, K., Foreign accents in synthesis: Development and evaluation (2005) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); Torres-Carrasquillo, P., Reynolds, D., Deller, J., Language identification using, Gaussian mixture model tokenization (2002) Proceedings, of the IEEE International Conference on Acoustics, Speech and Signal Processing; Torres-Carrasquillo, P., Singer, E., Kohler, M., Greene, R., Reynolds, D., Deller, J., Approaches to language identification using Gaussian mixture models and shifted delta cepstral features (2002) Proceedings of the International Conference on Spoken Language Processing; Tsopanoglou, A., Fakotakis, N., Selection of the most effective set of subword units for an HMM-based speech recognition system (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1231-1234; Tsuzuki, R., Zen, H., Tokuda, K., Kitamura, T., Bulut, M., Narayanan, S., Constructing emotional speech synthesizers with limited speech database (2004) Proceedings of the International Conference on Spoken Language Processing; Übler, U., Schüßler, M., Niemann, H., Bilingual and dialectal adaptation and retraining (1998) Proceedings of the International Conference on Spoken Language, Processing; Uebler, U., Boros, M., Recognition of non-native German speech with multilingual recognizers (1999) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 907-910; Ueda, Y., Nakagava, S., Prediction for phoneme/syllable/word-category and identification of language using HMM (1990) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1209-1212; Vapnik, V., (1999) The Nature of Statistical Learning Theory, , Springer; Venugopal, A., Vogel, S., Waibel, A., Effective phrase translation extraction from alignment models (2003) Proceedings of the 41 st Annual Meeting of the Association for Computational Linguistics, pp. 319-326; Vergyri, D., Kirchhoff, K., Automatic diacritization of Arabic for acoustic modeling in speech recognition (2004) Proceedings of the COLING Workshop on Arabic Script Based Languages; Vergyri, D., Kirchhoff, K., Duh, K., Stolcke, A., Morphology-based language modeling for Arabic speech recognition (2004) Proceedings of the International Conference on Spoken Language Processing; Vogel, S., SMT decoder dissected: Word reordering (2003) Proceedings of the International Conference on Natural Language Processing and Knowledge Engineering (NLP-KE); Vogel, S., Hewavitharana, S., Kolss, M., Waibel, A., The ISL statistical translation system for spoken language translation (2004) International Workshop on Spoken Language Translation, pp. 65-72; Vogel, S., Ney, H., Tillmann, C., HMM-based word alignment in statistical translation (1996) Proceedings of the 16th International Conference on Computational Linguistics, pp. 836-841; Vogel, S., Zhang, Y., Huang, F., Tribble, A., Venogupal, A., Zhao, B., Waibel, A., The CMU statistical translation system (2003) Proceedings of the MT-Summit IX; Waibel, A., Badran, A., Black, A.W., Frederking, R., Gates, D., Lavie, A., Levin, L., Zhang, J., Speechalator: Two-way speech-to-speech translation on a consumer PDA (2004) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); Waibel, A., Geutner, P., Mayfield Tomokiyo, L., Schultz, T., Woszczyna, M., Multilinguality in speech and spoken language systems (2000) Proceedings of the IEEE, 88, pp. 1297-1313; Walter, H., (1997) L'aventure des mots venus d'ailleurs, , Robert Laffont, Paris, France; Wang, Y., Waibel, A., Fast decoding for statistical machine translation (1998) Proceedings of the International Conference on Spoken Language Processing; Wang, Y.-Y., Waibel, A., Decoding algorithm in statistical translation (1997) Proceedings of the ACL/EACL '97, pp. 366-372; Wang, Z., Schultz, T., Non-native spontaneous speech recognition through polyphone decision tree specialization (2003) Proceedings of the International Conference on Spoken Language, Processing, pp. 1449-1452; Wang, Z., Topkara, U., Schultz, T., Waibel, A., Towards universal speech recognition (2002) 4th IEEE International Conference on Multimodal Interfaces, pp. 247-252; Ward, T., Roukos, S., Neti, C., Epstein, M., Dharanipragada, S., Towards speech understanding across multiple languages (1998) Proceedings of the International Conference on Spoken Language Processing; Watanabe, T., Sumita, E., Bidirectional decoding for statistical machine translation (2002) Proceedings of COLING, pp. 1079-1085; Watanae, T., Sumita, E., Statistical machine translation based on hierarchical phrase alignment (2002) Proceedings of the TMI-2002, pp. 188-198; Watanae, T., Sumita, E., Example-based decoding for statistical machine translation (2003) Proceedings of the IX-the MT Summit; Weingarten, R., http://www.ruediger-weingarten.de/texte/latinisierung.pdf; Wells, J., SAMPA compute readable phonetic alphabet (1997) Handbook of Standards and Resources for Spoken Language Systems, , Mouton de Gruyter, Berlin and New York, D. Gibbon, R. Moore, R. Winski (Eds.); Weng, F., Bratt, H., Neumeyer, L., Stolke, A., A study of multilingual speech recognition (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 359-362; Whaley, L., (1997) Introduction to Typology: The Unity and Diversity of Language, , Sage, Thousand Oaks, CA; Wheatley, B., Kondo, K., Anderson, W., Muthusamy, Y., An evaluation of cross-language adaptation for rapid HMM development in a new language (1994) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 237-240; Whittaker, E.W.D., Statistical language modelling for automatic speech recognition of Russian and English (2000) Ph.D. thesis, , Cambridge University Engineering Department, Cambridge, UK; Williams, G., Terry, M., Kaye, J., Phonological elements as a basis for language-independent ASR (1998) Proceedings of the International Conference on Spoken Language Processing; Witt, S., Young, S., Offine acoustic modeling of non-native accents (1999) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); Wong, E., Sridharan, S., Methods to improve Gaussian mixture model based language identification system (2002) Proceedings of the International Conference on Spoken Language Processing; Woodland, P.C., Speaker adaptation: Techniques and challenges (1999) Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 85-90; Woodland, P.C., Speaker adaptation for condtinuous density HMMs: A review (2001) AMSR2001; (1994) Proceedings of the ARPA Spoken Language Technology Workshop; Wu, D., Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora (1995) Proceedings of the 14th International Joint Conference on, Artificial Intelligence (IJCAI-95); Xu, P., Jelinek, F., Using random forests in the structure language model (2004) Advances in Neural Information Processing Systems (NIPS). Vol. 17. Vancouver, Canada; Yamamoto, H., Isogai, S., Sagisaka, Y., Multi-class composite N-gram language model (2003) Speech Communication, 41, pp. 369-379; Yan, Y., Barnard, E., An appraoch to automatic language identification based on language-dependent phone recognition (1995) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Yan, Y., Barnard, E., Cole, R., Development of an approach to automatic language identification based on phone recognition (1996) Computer Speech and Language, 10 (1), pp. 37-54; Yoshimura, T., Tokuda, K., Masuku, T., Kobayashi, T., Kitamura, T., Simultaneous modeling of spectrum, pitch and duration in HMM-based speech synthesis (1999) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 2347-2350; Young, S., Adda-Decker, M., Aubert, X., Dugast, C., Gauvain, J.-L., Kershaw, D., Lamel, L., Woodland, P., Multilingual large vocabulary speech recognition: The European SQUALE project (1997) Computer Speech and Language, 11, pp. 73-89; Young, S., Odell, J., Woodland, P., Tree-based state tying for high accuracy acoustic modeling (1994) Proceedings of the ARPA Spoken Language Technology Workshop, pp. 307-312; Yu, H., Schultz, T., Enhanced tree clustering with single pronunciation dictionary for conversational speech recognition (2003) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); Zellig, H., From phoneme to morpheme (1955) Language, 31 (2), pp. 190-222; Zhang, J.S., Markov, K., Matsui, T., Nakamura, S., A study on acoustic modeling of pauses for recognizing noisy conversational speech (2003) IEICE Trans. on Inf. & Syst., (3); Zhang, J.S., Mizumachi, M., Soong, F., Nakamura, S., An introduction to AATRPTH: A phonetically rich sentence set based Chinese Putonghua speech database developed by ATR (2003) Proceedings of the ASJ Meeting, pp. 167-168; Zhang, J.S., Zhang, S.W., Sagisaka, Y., Nakamura, S., A hybrid approach to enhance taks portability of acoustic models in Chinese speech recognition (2001) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1661-1663; Zhang, Y., Vogel, S., Waibel, A., Integrated phrase segmentation and alignment model for statistical machine translation (2003) Proceedings of the International Conference on Natural Language Processing and Knowledge Engineering (NLP-KE); Zissman, M., Automatic language identification using Gaussian mixture and hidden Markov models (1993) Proceedings of the International Conference on Acoustics, Speech and Signal Processing, pp. 399-402; Zissman, M., Comparison of four approaches to automatic language identification (1996) IEEE Transactions on Speech and Audio Processing, 4 (1), pp. 31-44; Zissman, M., Predicting, diagnosing and improving automatic language identification performance (1997) Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH); Zissman, M., Singer, E., Automatic language identification of telephone speech messages using phoneme recognition and N-gram modeling (1994) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 305-308; Zissman, M., Singer, E., Language identification using phoneme recognition and phonotactic language modeling (1995) Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 3503-3506; Zue, V., Seneff, S., Glass, J., Polifroni, J., Pao, C., Hazen, T.J., Hetherington, L., JUPITER: A telephone-based conversational interface for weather information (2000) IEEE Transactions on Speech and Audio Processing, 8 (1), pp. 147-151},
correspondence_address1={Schultz, T.; Carnegie Mellon University, Pittsburgh, PA, United States},
publisher={Elsevier Inc.},
isbn={9780120885015},
language={English},
abbrev_source_title={Multilingual Speech Proces.},
document_type={Book},
source={Scopus},
}

@CONFERENCE{Tam20062206,
author={Tam, Y.-C. and Schultz, T.},
title={Unsupervised language model adaptation using latent semantic marginals},
journal={INTERSPEECH 2006 and 9th International Conference on Spoken Language Processing, INTERSPEECH 2006 - ICSLP},
year={2006},
volume={5},
pages={2206-2209},
note={cited By 58; Conference of INTERSPEECH 2006 and 9th International Conference on Spoken Language Processing, INTERSPEECH 2006 - ICSLP ; Conference Date: 17 September 2006 Through 21 September 2006;  Conference Code:72042},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-44949236270&partnerID=40&md5=a6d64d4c29c9f63051f3f2e7326f017e},
affiliation={InterACT, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={We integrated the Latent Dirichlet Allocation (LDA) approach, a latent semantic analysis model, into unsupervised language model adaptation framework. We adapted a background language model by minimizing the Kullback-Leibler divergence between the adapted model and the background model subject to a constraint that the marginalized unigram probability distribution of the adapted model is equal to the corresponding distribution estimated by the LDA model - the latent semantic marginals. We evaluated our approach on the RT04 Mandarin Broadcast News test set and experimented with different LM training settings. Results showed that our approach reduces the perplexity and the character error rates using supervised and unsupervised adaptation.},
author_keywords={Latent dirichlet allocation;  LSA marginals;  Mandarin broadcast news;  Unsupervised LM adaptation},
keywords={Computational linguistics;  Probability distributions;  Statistics, Broadcast news;  Kullback Leibler divergence;  Language model adaptation;  Latent Dirichlet allocation;  Latent dirichlet allocations;  Latent Semantic Analysis;  Marginals;  Unsupervised LM adaptation, Semantics},
references={Kneser, R., Peters, J., Klakow, D., Language model adaptation using dynamic marginals (1997) Proc. of Eurospeech, pp. 1971-1974; Pietra, S.A.D., Pietra, V.J.D., Mercer, R.L., Roukos, S., Adaptive language modeling using minimum discriminant estimation (1992) Proc. of ICASSP, pp. 1633-1636; Blei, D., Ng, A., Jordan, M., Latent Dirichlet Allocation (2003) Journal of Machine Learning Research, pp. 1107-1135; Tam, Y.C., Schultz, T., Language model adaptation using variational bayes inference (2005) Proc. of Interspeech; Federico, M., Language model adaptation through topic decomposition and mdi estimation (2002) Proc. of ICASSP; Bellegarda, J., Exploiting latent semantic information in statistical language modeling (2000) IEEE Trans. on ASSP, 88 (8), pp. 63-75. , Aug; Bellegarda, J., Latent semantic mapping: Dimensionality reduction via globally optimal continuous parameter modeling (2005) Proc of IEEE Automatic Speech Recognition and Understanding Workshop, pp. 127-132; Hofmann, T., Probabilistic latent semantic indexing (1999) Proc. of UAI; Blei, D., Lafferty, J., Correlated topic models (2006) Advances in Neural Information Processing Systems; Jordan, M.I., Ghahramani, Z., Jaakkola, T., Saul, L.K., An introduction to variational methods for graphical models (1999) Machine Learning, 37 (2), pp. 183-233; Rosenfeld, R., Adaptive statistical language modeling: A maximum entropy approach, (1994), Ph.D. dissertation, School of Computer Science, Carnegie Mellon University, Pittsburgh, April; Yu, H., Tam, Y.C., Schaaf, T., Stüker, S., Jin, Q., Noamany, M., Schultz, T., The ISL RT04 Mandarin Broadcast News Evaluation System (2004) EARS Rich Transcription Workshop; Stolcke, A., Srilm - an extensible language modeling toolkit (2002) Proc. of ICSLP},
correspondence_address1={Tam, Y.-C.; InterACT, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: yct@cs.cmu.edu},
sponsors={},
publisher={International Speech Communication Association},
address={Pittsburgh, PA},
isbn={9781604234497},
language={English},
abbrev_source_title={INTERSPEECH Intl. Conf. Spoken Lang. Proc., INTERSPEECH ICSLP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Charoenpornsawat20061268,
author={Charoenpornsawat, P. and Schultz, T.},
title={Example-based grapheme-to-phoneme conversion for Thai},
journal={INTERSPEECH 2006 and 9th International Conference on Spoken Language Processing, INTERSPEECH 2006 - ICSLP},
year={2006},
volume={3},
pages={1268-1271},
note={cited By 8; Conference of INTERSPEECH 2006 and 9th International Conference on Spoken Language Processing, INTERSPEECH 2006 - ICSLP ; Conference Date: 17 September 2006 Through 21 September 2006;  Conference Code:72042},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-44949084747&partnerID=40&md5=65ed9a77f34b83a6de0eb580cb9df910},
affiliation={Interactive Systems Laboratories, Carnegie Mellon University},
abstract={Several characteristics of the Thai writing system make Thai grapheme-to-phoneme (G2P) conversion very challenging. In this paper, we propose an Example-Based Grapheme-to-Phoneme conversion approach. It generates the pronunciation of a word by selecting, modifying and combining pronunciations from syllables from training corpus. The best system achieves 80.99% word accuracy and 94.19% phone accuracy which significantly outperform previous approaches for Thai.},
author_keywords={Example-based;  Grapheme-to-phoneme conversion;  Letter-to-sound rules;  Pronunciation;  Thai},
keywords={Example based;  Grapheme-to-phoneme conversion;  Letter-to-sound rules;  Pronunciation;  Thai, Speech recognition},
references={Black, A.W., Lenzo, K., Pagel, V., Issues in Building General Letter to Sound Rules (1998) The 3rd ESCA Workshop on Speech Synthesis, , Jenolan Caves, Australia; Chotimongkol, A., Black, A.W., Statistically trained Orthographic to sound Models for Thai (2000) Proceedings of ICSLP; Chen, S.F., Conditional and Joint Models for Grapheme-to-Phoneme Conversion (2003) The 8th Eurospeech, , Geneva, Switzerland; Taylor, P., Markov, H., Models for Grapheme to Phoneme Conversion (2005) Proceedings of Interspeech, , Lisbon, Portugal; Dedina, M.J., Nusbaum, H.C., PRONOUNCE: A program for pronunciation by analogy (1996) Computer Speech and Language, 5, pp. 55-64; Yvon, F., Grapheme-to-Phoneme conversion using multiple unbounded overlapping chunks (1996) Proceedings of NeM-LaP-2, , Ankara, Turkey; Marchand, Y., Damper, R.I., A Multi-Strategy Approach to Improving Pronunciation by Analogy (2000) Computational Linguistics, 26 (2); Pongthai Tarsaku, Virach Sornlertlamvanich and Rachod Tongprasert. Thai Grapheme-to-Phoneme Using Probabilistic GLR Parser. In Proceedings of Eurospeech Aalborg, Denmark. 2001; Tesprasit, V., Charoenpornsawat, P., Sornlertlamvanich, V., A Context-Sensitive Homograph Disambiguation in Thai Text-to-Speech Synthesis (2003) Proceedings of HLT-NAACL, , Edmonton, Canada; Aroonmanakun, W., Rivepiboon, W., A Unified Model of Thai Word Segmentation and Romanization (2004) Proceedings of The 18th PACLIC, , Tokyo, Japan; Sawamipak, D., (1990) Developing software for analyzing Thai syntax in Unix. Thamasart University Publishing, , Bangkok, Thailand},
correspondence_address1={Charoenpornsawat, P.; Interactive Systems Laboratories, Carnegie Mellon UniversityUnited States; email: paisarn@cs.cmu.edu},
sponsors={},
publisher={International Speech Communication Association},
address={Pittsburgh, PA},
isbn={9781604234497},
language={English},
abbrev_source_title={INTERSPEECH Intl. Conf. Spoken Lang. Proc., INTERSPEECH ICSLP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Woszczyna20061882,
author={Woszczyna, M. and Charoenpornsawat, P. and Schultz, T.},
title={Spontaneous thai speech recognition},
journal={INTERSPEECH 2006 and 9th International Conference on Spoken Language Processing, INTERSPEECH 2006 - ICSLP},
year={2006},
volume={4},
pages={1882-1885},
note={cited By 0; Conference of INTERSPEECH 2006 and 9th International Conference on Spoken Language Processing, INTERSPEECH 2006 - ICSLP ; Conference Date: 17 September 2006 Through 21 September 2006;  Conference Code:72042},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-44949122773&partnerID=40&md5=84ab83d78580bd600a9f3a86e1c4cbd8},
affiliation={Multimodal Technologies, Inc.; Carnegie Mellon University},
abstract={This paper expands previous work on Thai speech recognition, investigating pronunciation changes such as syllable and phoneme elisions as well as phoneme shifts in Thai spontaneous speech. We compare several approaches to model these effects in large vocabulary continuous speech recognition across multiple domains. This work includes experiments on two new speech databases that significantly alleviate the data sparseness problem of earlier publications. We found that given sufficient training data, a fully data driven approach using an allophone cluster tree yields the best results. Explicit modeling of pronunciation changes does not improve performance across domains.},
author_keywords={Acoustic model sharing;  Pronunciation modeling;  Speech recognition;  Spontaneous speech;  Thai},
keywords={Continuous speech recognition;  Deep neural networks;  Speech;  Trees (mathematics), Acoustic model;  Data sparseness problem;  Data-driven approach;  Improve performance;  Large vocabulary continuous speech recognition;  Pronunciation modeling;  Spontaneous speech;  Thai, Speech recognition},
references={Schultz, T., Black, A.W., Vogel, S., Woszczcyna, M., Flexible Speech Translation Systems (2006) IEEE Transactions on Audio, Speech, and Language Processing, 14 (2). , March; Suebvisai, S., Charoenpornsawat, P., Black, A., Woszczyna, M., Schultz, T., (2005) Thai Automatic Speech Recognition, , ICASSP, Philadelphia, USA; Kasuriya, S., Kanokphara, S., Thatphithakkul, N., Cotsomrong, P., Sunpethiniyom, T., Context-independent Acoustic Models for Thai Speech Recognition, , ISCIT2004, Sapporo, Japan; Schultz, T., Alexander, D., Black, A.W., Peterson, K., Suebvisai, S., Waibel, A., A Thai Speech Translation System For Medical Dialogs, , HLT 2004, Boston, USA; Kanokphara, S., Tesprasit, V., Thongprasirt, R., Pronunciation Variation Speech Recognition without Dictionary Modification on Sparse Database ICASSP 2003, , Hong Kong, China; Kanokphara, S., Syllable Structure Based Phonetic Units for Context-Dependent Continuous Thai Speech Recognition (2003) Eurospeech, , Geneva, Switzerland; Paisam Charoenpornsawat, Sanjika Hewaviharana and Tanja Schultz. Thai Grapheme-Based Speech Recognition. HLT-NAACL 2006, New York, USA; Yu, H., Schultz, T., Enhanced Tree Clustering with Single Pronunciation dictionary for Conversational Speech Recognition (2003) Eurospeech, , Geneva, Switzerland; Finke, M., Fritsch, J., Koll, D., Waibel, A., Modeling and Efficient Decoding of Large Vocabulary Conversational Speech (1999) Eurospeech, , Budapest, Hungary; Charoenpornsawat, P., (2003) SWATH: Thai Word Segmentation Program, , http://www.cs.cmu.edu/~paisarn/software.htm},
correspondence_address1={Woszczyna, M.; Multimodal Technologies, Inc.email: monika@cs.emu.edu},
sponsors={},
publisher={International Speech Communication Association},
address={Pittsburgh, PA},
isbn={9781604234497},
language={English},
abbrev_source_title={INTERSPEECH Intl. Conf. Spoken Lang. Proc., INTERSPEECH ICSLP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jou2006573,
author={Jou, S.-C. and Schultz, T. and Walliczek, M. and Kraft, F. and Waibel, A.},
title={Towards continuous speech recognition using surface electromyography},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2006},
volume={2},
pages={573-576},
note={cited By 80; Conference of INTERSPEECH 2006 and 9th International Conference on Spoken Language Processing, INTERSPEECH 2006 - ICSLP ; Conference Date: 17 September 2006 Through 21 September 2006;  Conference Code:72042},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-44949257531&partnerID=40&md5=d4ce7f89dd42e3607fa19763d778e45c},
affiliation={International Center for Advanced Communication Technologies, Carnegie Mellon University, United States; International Center for Advanced Communication Technologies, Universität Karlsruhe, Germany},
abstract={We present our research on continuous speech recognition of the surface electromyographic signals that are generated by the human articulatory muscles. Previous research on electromyographic speech recognition was limited to isolated word recognition because it was very difficult to train phoneme-based acoustic models for the electromyographic speech recognizer. In this paper, we demonstrate how to train the phoneme-based acoustic models with carefully designed electromyographic feature extraction methods. By decomposing the signal into different feature space, we successfully keep the useful information while reducing the noise. Additionally, we also model the anticipatory effect of the electromyographic signals compared to the speech signal. With a 108-word decoding vocabulary, the experimental results show that the word error rate improves from 86.8% to 32.0% by using our novel feature extraction methods.},
author_keywords={Articulatory muscles;  Electromyography;  Feature extraction;  Speech recognition},
keywords={Biomedical signal processing;  Continuous speech recognition;  Deep neural networks;  Electromyography;  Extraction;  Feature extraction;  Muscle;  Signal processing;  Speech;  Speech communication, Electromyographic;  Electromyographic signal;  Feature extraction methods;  Isolated word recognition;  Speech recognizer;  Speech signals;  Surface electromyography;  Word error rate, Speech recognition},
references={Nakajima, Y., Kashioka, H., Shikano, K., Campbell, N., Non-audible murmur recognition input interface using stethoscopic microphone attached to the skin (2003) Proc. ICASSP, , Hong Kong; Jou, S.-C., Schultz, T., Waibel, A., Whispery speech recognition using adapted articulatory features (2005) Proc. ICASSP, , Philadelphia, PA, March; Chan, A.D.C., Englehart, K., Hudgins, B., Lovely, D.F., Hidden Markov model classification of myoelectric signals in speech (2002) IEEE Engineering in Medicine and Biology Magazine, 21 (4), pp. 143-146; Betts, B., Jorgensen, C., Small vocabulary communication and control using surface electromyography in an acoustically noisy environment (2006) Proc. HICSS, , Hawaii, Jan; Manabe, H., Hiraiwa, A., Sugimura, T., Unvoiced speech recognition using EMG-mime speech recognition (2003) Proc. HFCS, , Ft. Lauderdale, Florida; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) Proc. ASRU, , San Juan, Puerto Rico, Nov; Jou, S.-C., Maier-Hein, L., Schultz, T., Waibel, A., Articulatory feature classification using surface electromyography (2006) Proc. ICASSP, , Toulouse, France, May; Walliczek, M., Kraft, F., Jou, S.-C., Schultz, T., Waibel, A., Sub-word unit based non-audible speech recognition using surface electromyography (2006) Proc. Interspeech, , Pittsburgh, PA, Sep; Varioport, http://www.becker-meditec.de; Yu, H., Waibel, A., Streaming the front-end of a speech recognizer (2000) Proc. ICSLP, , Beijing, China},
correspondence_address1={Jou, S.-C.; International Center for Advanced Communication Technologies, Carnegie Mellon UniversityUnited States; email: scjou@cs.cmu.edu},
sponsors={},
publisher={International Speech Communication Association},
address={Pittsburgh, PA},
issn={19909772},
isbn={9781604234497},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Walliczek20061487,
author={Walliczek, M. and Kraft, F. and Jou, S.-C. and Schultz, T. and Waibel, A.},
title={Sub-word unit based non-audible speech recognition using surface electromyography},
journal={INTERSPEECH 2006 and 9th International Conference on Spoken Language Processing, INTERSPEECH 2006 - ICSLP},
year={2006},
volume={3},
pages={1487-1490},
note={cited By 16; Conference of INTERSPEECH 2006 and 9th International Conference on Spoken Language Processing, INTERSPEECH 2006 - ICSLP ; Conference Date: 17 September 2006 Through 21 September 2006;  Conference Code:72042},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-44949179587&partnerID=40&md5=70d9e0b6f6e160f16ed7602eaeebc5dd},
affiliation={Interactive Systems Labs., Universität Karlsruhe (TH), Germany; Carnegie Mellon University, United States},
abstract={In this paper we present a novel approach for a surface electromyographic speech recognition system based on sub-word units. Rather than using full word models as integrated in our previous work we propose here smaller sub-word units as prerequisites for large vocabulary speech recognition. This allows the recognition of words not seen in the training set based on seen sub-word units. Therefore we report on experiments with syllables and phonemes as sub-word units. We also developed a new feature extraction method that gains significant improvement for words and sub-word units.},
author_keywords={Electromyography;  Non-audible speech recognition;  Silent speech;  Sub-word unit comparison},
keywords={Deep neural networks;  Electromyography;  Feature extraction;  Speech, Electromyographic;  Feature extraction methods;  Large vocabulary speech recognition;  Silent speech;  Speech recognition systems;  Sub-word units;  Surface electromyography;  Training sets, Speech recognition},
references={Jorgensen, C., Binsted, K., Web Browser Control Using EMG Based Sub Vocal Speech Recognition (2005) Proc. of the 38th Hawaii International Conference on System Sciences; Ganapathiraju, A., Hamaker, J., Picone, J., Ordowski, M., Doddington, G.R., Syllable-Based Large Vocabulary Continuous Speech Recognition (2001) IEEE Transactions on speech and audio processing, 9 (4). , May; Jou, S.-C., Maier-Hein, L., Schultz, T., Waibel, A., Articulatory feature classification using surface electromyographic (2006) Proc. ICASSP 06, Toulouse; France, , IEEE; Maier-Hein, L., Metze, F., Schultz, T., Waibel, A., Session independent non-audible speech recognition using surface electromyography (2005) Proc. ASRU, , Costa Rica, Nov; Becker, K., Varioport, , http://www.becker-medilec.de; Finke, M., Geutner, P., Hild, H., Kemp, T., Ries, K., Westphal, M., The Karlsruhe Verbmobil Speech Recognition Engine (1997) Proc. ICASSP 97, München; Germany, , IEEE; S.-C. Jou, T. Schultz, M. Walliczek, F. Kraft and A. Waibel, Towards Continuous Speech Recognition Using Surface Electromyography, in Proc. ICSLP 06, Pittsburgh; USA, 2006},
correspondence_address1={Walliczek, M.; Interactive Systems Labs., Universität Karlsruhe (TH)Germany; email: walliczek@ira.uka.de},
sponsors={},
publisher={International Speech Communication Association},
address={Pittsburgh, PA},
isbn={9781604234497},
language={English},
abbrev_source_title={INTERSPEECH Intl. Conf. Spoken Lang. Proc., INTERSPEECH ICSLP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hsiao2006765,
author={Hsiao, R. and Venugopal, A. and Köhler, T. and Zhang, T. and Charoenpornsawat, P. and Zollmann, A. and Vogel, S. and Black, A.W. and Schultz, T. and Waibel, A.},
title={Optimizing components for handheld two-way speech translation for an English-Iraqi Arabic system},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
year={2006},
volume={2},
pages={765-768},
note={cited By 8; Conference of INTERSPEECH 2006 and 9th International Conference on Spoken Language Processing, INTERSPEECH 2006 - ICSLP ; Conference Date: 17 September 2006 Through 21 September 2006;  Conference Code:72042},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-44949221433&partnerID=40&md5=c6a6a3f76fdb4ec869dbb23596f6f039},
affiliation={InterACT, Carnegie Mellon University, Pittsburgh, PA 15213, United States; Mobile Technologies LLC, Pittsburgh, PA, United States},
abstract={This paper described our handheld two-way speech translation system for English and Iraqi. The focus is on developing a field usable handheld device for speech-to-speech translation. The computation and memory limitations on the handheld impose critical constraints on the ASR, SMT, and TTS components. In this paper we discuss our approaches to optimize these components for the handheld device and present performance numbers from the evaluations that were an integral part of the project. Since one major aspect of the TransTac program is to build fieldable systems, we spent significant effort on developing an intuitive interface that minimizes the training time for users but also provides useful information such as back translations for translation quality feedback.},
author_keywords={English-Iraqi speech translation;  Handheld devices;  Iraqi speech recognition;  Pocket translation;  Translation interface},
keywords={Hand held computers;  Program translators;  Speech;  Speech recognition, Back translations;  Critical constraints;  Hand held device;  Intuitive interfaces;  Speech translation;  Speech translation systems;  Speech-to-speech translation;  Translation quality, Translation (languages)},
references={Waibel, A., Badran, A., Black, A., Frederking, R., Gates, D., Lavie, A., Levin, L., Zhang, J., Speechalator: Two-way speech-to-speech translation on a consumer PDA (2003) Proceedings of the European Conference on Speech Communication and Technology; Schultz, T., Black, A., Vogel, S., Woszczyna, M., Flexible speech-to-speech translation systems (2006) IEEE Transactions in Speech and Audio Processing, 14 (2), pp. 403-411. , Mar; Soltau, H., Metze, F., Fügen, C., Waibel, A., A one pass-decoder based on polymorphic linguistic context assignment (2001) Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding; Köhler, T.W., Fügen, C., Stüker, S., Waibel, A., Rapid porting of ASR-systems to mobile devices (2005) Proceedings of Eurospeech, 9th European Conference on Speech Communication and Technology; Metze, F., Fügen, C., Pan, Y., Schultz, T., Yu, H., The ISL RT-04S meeting transcription system (2004) Proceedings of the NIST RT-04S Evaluation Workshop, , Montreal, Canada, May, NIST; Chen, S.F., Goodman, J., An empirical study of smoothing techniques for language modeling (1996) Proceedings of the Thirty-Fourth Annual Meeting of the Association for Computational Linguistics, pp. 310-318. , Arivind Joshi and Martha Palmer, Eds, San Francisco, Morgan Kaufmann Publishers; Leppnen, J., Kiss, I., Gaussian selection with non-overlapping clusters for ASR in embedded devices (2006) Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing; Vogel, S., Zhang, Y., Huang, F., Tribble, A., Venogupal, A., Zhao, B., Waibel, A., The CMU statistical translation system (2003) Proceedings of MT Summit IX, , New Orleans, LA, September; Vogel, S., PESA: Phrase pair extraction as sentence splitting (2005) Proceedings: The tenth Machine Translation Summit, , Phuket, Thailand; Zollmann, A., Venugopal, A., Vogel, S., Bridging the inflection morphology gap for arabic statistical machine translation (2006) Short Papers in the Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference (HLT/NAACL); Josef Och, F., Minimum error rate training in statistical machine translation (2003) Proc. of the Association for Computational Linguistics, , Sapporo, Japan, July 6-7; Black, A., Lenzo, K., Pagel, V., Issues in building general letter to sound rules (1998) Proc. ESCA Workshop on Speech Synthesis, pp. 77-80. , Jenolan Caves, Australia},
correspondence_address1={Hsiao, R.; InterACT, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: wrhsiao@cs.emu.edu},
sponsors={},
publisher={International Speech Communication Association},
address={Pittsburgh, PA},
issn={19909772},
isbn={9781604234497},
language={English},
abbrev_source_title={Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tam20055,
author={Tam, Y.-C. and Schultz, T.},
title={Dynamic language model adaptation using variational bayes inference},
journal={9th European Conference on Speech Communication and Technology},
year={2005},
pages={5-8},
note={cited By 52; Conference of 9th European Conference on Speech Communication and Technology ; Conference Date: 4 September 2005 Through 8 September 2005;  Conference Code:67499},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745203547&partnerID=40&md5=10d48b4ff429875296f37dc1f5a0b770},
affiliation={Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={We propose an unsupervised dynamic language model (LM) adaptation framework using long-distance latent topic mixtures. The framework employs the Latent Dirichlet Allocation model (LDA) which models the latent topics of a document collection in an unsupervised and Bayesian fashion. In the LDA model, each word is modeled as a mixture of latent topics. Varying topics within a context can be modeled by re-sampling the mixture weights of the latent topics from a prior Dirichlet distribution. The model can be trained using the variational Bayes Expectation Maximization algorithm. During decoding, mixture weights of the latent topics are adapted dynamically using the hypotheses of previously decoded utterances. In our work, the LDA model is combined with the trigram language model using linear interpolation. We evaluated the approach on the CCTV episode of the RT04 Mandarin Broadcast News test set. Results show that the proposed approach reduces the perplexity by up to 15.4% relative and the character error rate by 4.9% relative depending on the size and setup of the training set.},
keywords={Language model (LM);  Latent Dirichlet Allocation model (LDA);  Mixture weights;  Variational Bayes Expectation Maximization algorithmS, Adaptive algorithms;  Coding errors;  Context sensitive languages;  Decoding;  Interpolation;  Linear programming;  Mathematical models, Speech analysis},
references={Kuhn, R., Mori, R.D., A cache-based natural language model for speech reproduction (1990) IEEE Trans. on Pattern Analysis and Machine Intelligence; Clarkson, P.R., Robinson, A.J., Language model adaptation using mixtures and an exponentially decaying cache (1997) Proc. of ICASSP, 2; Lyer, R., Ostendorf, M., Modeling long distance dependence in language: Topic mixtures vs. dynamic cache models (1996) Proc. of ICSLP, 1; Bellegarda, J.R., Exploiting latent semantic information in statistical language modeling (2000) IEEE Trans. on ASSP, 88 (8), pp. 63-75. , Aug; Blei, D., Ng, A., Jordan, M., Latent dirichlet allocation (2003) Journal of Machine Learning Research; Gildea, D., Hofmann, T., Topic-based language models using em (1999) Proc. of Eurospeech; Mrva, D., Woodland, P.C., A PLSA-based language model for conversational telephone speech (2004) Proc. of ICSLP; Jordan, M.I., Ghahramani, Z., Jaakkola, T., Saul, L.K., An introduction to variational methods for graphical models (1999) Machine Learning, 37 (2); Yu, H., Tam, Y.C., Schaaf, T., Stüker, S., Jin, Q., Noamany, M., Schultz, T., The ISL RT04 mandarin broadcast news evaluation system (2004) EARS Rich Transcription Workshop; Clarkson, P., Robinson, A., The applicability of adaptive language modelling for the broadcast news task (1998) Proc. of ICSLP},
correspondence_address1={Tam, Y.-C.; Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: yct@cs.cmu.edu},
sponsors={Vodafone},
address={Lisbon},
language={English},
abbrev_source_title={Eur. Conf. Speech Commun. Technol.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Maier-Hein2005307,
author={Maier-Hein, L. and Metze, F. and Schultz, T. and Waibel, A.},
title={Session independent non-audible speech recognition using surface electromyography},
journal={Proceedings of ASRU 2005: 2005 IEEE Automatic Speech Recognition and Understanding Workshop},
year={2005},
volume={2005},
pages={307-312},
doi={10.1109/ASRU.2005.1566521},
art_number={1566521},
note={cited By 87; Conference of ASRU 2005: 2005 IEEE Automatic Speech Recognition and Understanding Workshop ; Conference Date: 27 November 2005 Through 1 December 2005;  Conference Code:68918},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846185567&doi=10.1109%2fASRU.2005.1566521&partnerID=40&md5=1fbb0b74935afcff05b92efeb45993e0},
affiliation={Interactive Systems Labs, Universität Karlsruhe (TH), Carnegie Mellon University, Germany},
abstract={In this paper we introduce a speech recognition system based on myoelectric signals. The system handles audible and non-audible speech. Major challenges in surface electromyography based speech recognition ensue from repositioning electrodes between recording sessions, environmental temperature changes, and skin tissue properties of the speaker. In order to reduce the impact of these factors, we investigate a variety of signal normalization and model adaptation methods. An average word accuracy of 97.3% is achieved using seven EMG channels and the same electrode positions. The performance drops to 76.2% after repositioning the electrodes if no normalization or adaptation is performed. By applying our adaptation methods we manage to restore the recognition rates to 87.1%. Furthermore, we compare audibly to non-audibly spoken speech. The results suggest that large differences exist between the corresponding muscle movements. Still, our recognition system recognizes both speech manners accurately when trained on pooled data. © 2005 IEEE.},
keywords={Muscle movement;  Myoelectric signals;  Speech recognition systems, Bioelectric phenomena;  Electrodes;  Electromyography;  Pattern recognition systems;  Speech processing, Speech recognition},
references={Chan, A.D.C., Englehart, K., Hudgins, B., Lovely, D.F., Myoelectric Signals to Augment Speech Recognition (2001) Medical and Biological Engineering and Computing, 39, pp. 500-506; Jorgensen, C., Lee, D., Agabon, S., Sub Auditory Speech Recognition Based on EMG/EPG Signals (2003) Proc. of the International Joint Conference on Neural Networks; B. Leveau and G.B.J. Andersson, Output Forms: Data Analysis and Applications, in Selected Topics in Surface Electromyography for Use in the Occupational Setting: Expert Perspective. U.S. Department of Health and Human Services, 3 1992, DHHS(NIOSH) Publication No 91-100; Coleman, J., Grabe, E., Braun, B., Larynx movements and intonation in whispered speech (2002) Summary of research supported by British Academy grant SG-36269; De Luca, C., Surface Electromyography: Detection and Recording (2002), Tech. Rep, DelSys Inc; Chan, A.D.C., Englehart, K., Hudgins, B., Lovely, D.F., Hidden Markov Model Classification of Myolectric Signals in Speech Engineering in Medicine and Biology Magazine, IEEE, 21, pp. 143-146,9-2002; Jorgensen, C., Binsted, K., Web Browser Control Using EMG Based Sub Vocal Speech Recognition (2005) Proc. of the 38th Annual Hawaii International Conference on System Sciences; Manabe, H., Hiraiwa, A., Sugimura, T., Unvoiced Speech Recognition using EMG - Mime Speech Recognition (2003) Proc. of the 2003 Conference on Human Factors in Computing Systems, , Ft. Lauderdale, Florida, USA; Manabe, H., Zhang, Z., Multi-stream HMM for EMG-Based Speech Recognition (2004) Proc. of the 26th IEEE EMBS Conference, San Francisco, CA, USA; Dissection of the Speech Production Mechanism (2002), UCLA Phonetics Laboratory, Tech. Rep, Department of Linguistics, University of California, Los Angeles; Becker, K., Varioport™, , http://www.becker-meditec.de; Finke, M., Geutner, P., Hild, H., emp, T.K., Ries, K., Westphal, M., The Karlsruhe Verbmobil Speech Recognition Engine (1997) Proc. of the ICASSP, 4. , München; Germany, IEEE; Jin, H., Matsoukas, S., Schwartz, R., Kubala, F., Fast Robust Inverse Transform SAT and Multi-stage Adaptation (1998) Proc. DARPA Broadcast News Transcription and Understanding Workshop, , Lansdowne, VA; USA},
correspondence_address1={Maier-Hein, L.; Interactive Systems Labs, Universität Karlsruhe (TH), Carnegie Mellon UniversityGermany; email: lena@ira.uka.de},
address={Cancun},
isbn={0780394798; 9780780394797},
language={English},
abbrev_source_title={Proc. ASRU IEEE Autom. Speech Recog. Understanding Workshop},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Paulik20052261,
author={Paulik, M. and Fügen, C. and Stüker, S. and Schultz, T. and Schaaf, T. and Waibel, A.},
title={Document driven machine translation enhanced ASR},
journal={9th European Conference on Speech Communication and Technology},
year={2005},
pages={2261-2264},
note={cited By 12; Conference of 9th European Conference on Speech Communication and Technology ; Conference Date: 4 September 2005 Through 8 September 2005;  Conference Code:67499},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745195228&partnerID=40&md5=bf5fe83d271b2766ef2d74d4f8558e64},
affiliation={Institut für Logik, Komplexität und Deduktionssysteme, Universität Karlsruhe (TH), Germany; Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, United States},
abstract={In human-mediated translation scenarios a human interpreter translates between a source and a target language using either a spoken or a written representation of the source language. In this paper we improve the recognition performance on the speech of the human translator spoken in the target language by taking advantage of the source language representations. We use machine translation techniques to translate between the source and target language resources and then bias the target language speech recognizer towards the gained knowledge, hence the name Machine Translation Enhanced Automatic Speech Recognition. We investigate several different techniques among which are restricting the search vocabulary, selecting hypotheses from n-best lists, applying cache and interpolation schemes to language modeling, and combining the most successful techniques into our final, iterative system. Overall we outperform the baseline system by a relative word error rate reduction of 37.6%.},
keywords={Error rates;  Language modeling;  Machine Translation Enhanced Automatic Speech Recognition;  Translation techniques, Automation;  Human computer interaction;  Interpolation;  Iterative methods;  Mathematical models;  Program interpreters;  Speech recognition;  Vocabulary control, Translation (languages)},
references={Dymetman, M., Brousseaux, J., Foster, G., Isabelle, P., Normandin, Y., Plamondon, P., Towards an automatic dictation system for translators: The TransTalk project (1994) ICSLP, , Yokohama, Japan; Brousseaux, J., Foster, G., Isabelle, P., Kuhn, R., Normandin, Y., Plamondon, P., French speech recognition in an automatic dictation system for translators: The TransTalk project (1995) Eurospeech, , Madrid, Spain; Brown, P., Chen, S., Della Pietra, S., Della Pietra, V., Kehler, S., Mercer, R., Automatic speech recognition in machine aided translation (1994) Computer Speech and Language, 8; Placeway, P., Lafferty, J., Cheating with imperfect transcripts (1996) ICSLP, , Philadelphia, PA, USA; Ludovik, Y., Zacharski, R., MT and topic-based techniques to enhance speech recognition systems for professional translators (2000) CoLing, , Saarbrücken, Germany; Soltau, H., Metze, F., Fügen, C., Waibel, A., A one-pass decoder based on polymorphic linguistic context assignment (2001) ASRU 2001, , Madonna di Campiglio, Italy; Metze, F., Jin, Q., Fügen, C., Laskowski, K., Pan, Y., Schultz, T., Issues in meeting transcription - The ISL meeting transcription system (2004) ICSLP, , Jeju Island, Korea; Vogel, S., Hewavitharana, S., Kolß, M., Waibel, A., The ISL statistical machine translation system for spoken language translation (2004) IWSLT, , Kyoto, Japan},
correspondence_address1={Paulik, M.; Institut für Logik, Komplexität und Deduktionssysteme, Universität Karlsruhe (TH), Karlsruhe, Germany; email: paulik@ira.uka.de},
sponsors={Vodafone},
address={Lisbon},
language={English},
abbrev_source_title={Eur. Conf. Speech Commun. Technol.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Honal2005969,
author={Honal, M. and Schultz, T.},
title={Automatic disfluency removal on recognized spontaneous speech -rapid adaptation to speaker-dependent disfluencies},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2005},
volume={I},
pages={969-972},
doi={10.1109/ICASSP.2005.1415277},
art_number={1415277},
note={cited By 26; Conference of 2005 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP '05 ; Conference Date: 18 March 2005 Through 23 March 2005;  Conference Code:67354},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646762857&doi=10.1109%2fICASSP.2005.1415277&partnerID=40&md5=8e9098fa77c559516228187348bd8e8f},
abstract={In this paper we investigate methods to adapt a system for disfluency removal to different data properties. A gradient descent algorithm for parameter optimization is presented which achieves 85.1% recall and 93.1% precision on the English Verbmobil corpus and 53.0% recall and 79.0% precision on the Mandarin Chinese CallHome corpus. This compares to the results produced with hand-optimization on the test set. Furthermore we investigated the impact of cross-validation and training set selection on recognizer output. Finally we examined speaker dependent disfluency production behavior and clustered training data accordingly in order to improve the overall system. ©2005 IEEE.},
keywords={Algorithms;  Data reduction;  Optimization;  Parameter estimation, Automatic disfluency removal;  Clustered training data;  Hand optimization;  Speaker dependent disfluencies;  Spontaneous speech, Speech recognition},
references={Stolcke, A., Shriberg, E., Bates, R., Ostendorf, M., Hakkani, D., Plauche, M., Tür, G., Lu, Y., Automatic detection of sentence boundaries and disfluencies based on recognized words (1998) Proceedings of the ICSLP, 5, pp. 2247-2250; Liu, Y., Shriberg, E., Stolcke, A., Automatic disfluency identification in conversational speech using multiple knowlege sources (2003) Proceedings of the 8th Eurospeech Conference, , Geneva; Spilker, J., Klarner, M., Görz, G., Processing self-corrections in a speech-to-speech system (2000) Verbmobil: Foundations of Speech-to-Speech Translation, , W. Wahlster, Ed., Springer Verlag, Berlin; Honal, M., Schultz, T., Correction of disfluencies in spontaneous speech using a noisy-channel approach (2003) Proceedings of the 8th Eurospeech Conference, , Geneva; Zechner, K., (2001) Automatic Summarization of Spoken Dialogues in Unrestricted Domains, , Ph.D. thesis, Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh; Wang, Y., Waibel, A., Decoding algorithm in statistical machine translation (1997) Proceedings of the 35th Annual Meeting of the ACE},
sponsors={IEEE Signal Processing Society},
publisher={Institute of Electrical and Electronics Engineers Inc.},
address={Philadelphia, PA},
issn={15206149},
isbn={0780388747; 9780780388741},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jou2005,
author={Jou, S.-C. and Schultz, T. and Waibel, A.},
title={Whispery speech recognition using adapted articulatory features},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2005},
volume={I},
pages={I1009-I1012},
doi={10.1109/ICASSP.2005.1415287},
art_number={1415287},
note={cited By 32; Conference of 2005 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP '05 ; Conference Date: 18 March 2005 Through 23 March 2005;  Conference Code:67354},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646764944&doi=10.1109%2fICASSP.2005.1415287&partnerID=40&md5=34aa76e30a7c06c8a4855a8e0248c16b},
affiliation={Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA, United States},
abstract={This paper describes our research on adaptation methods applied to articulatory feature detection on soft whispery speech recorded with a throat microphone. Since the amount of adaptation data is small and the testing data is very different from the training data, a series of adaptation methods is necessary. The adaptation methods include: maximum likelihood linear regression, featurespace adaptation, and re-training with downsampling, sigmoidal low-pass filter, and linear multivariate regression. Adapted articulatory feature detectors are used in parallel to standard senonebased HMM models in a stream architecture for decoding. With these adaptation methods, articulatory feature detection accuracy improves from 87.82% to 90.52% with corresponding F-measure from 0.504 to 0.617, while the final word error rate improves from 33.8% to 31.2%. © 2005 IEEE.},
keywords={Decoding;  Low pass filters;  Markov processes;  Maximum likelihood estimation;  Microphones;  Regression analysis, Adapted articulatory features;  Featurespace adaptation;  Linear multivariate regression, Speech recognition},
references={Nakajima, Y., Kashioka, H., Shikano, K., Campbell, N., Non-audible murmur recognition input interface using stethoscopic microphone attached to the skin (2003) Proc. ICASSP, , Hong Kong; Zheng, Y., Liu, Z., Zhang, Z., Sinclair, M., Droppo, J., Deng, L., Acero, A., Huang, X., Air- And bone-conductive integrated microphones for robust speech detection and enhancement (2003) Proc. ASRU, , St. Thomas, U.S. Virgin Islands, Dec; Jou, S.-C., Schultz, T., Waibel, A., Adaptation for soft whisper recognition using a throat microphone (2004) Proc. ICSLP, , Jeju Island, Korea, Oct; Kirchhoff, K., (1999) Robust Speech Recognition Using Articulatory Information, , Ph.D. thesis, University of Bielefeld, Germany, July; Metze, F., Waibel, A., A flexible stream architecture for ASR using articulatory features (2002) Proc. ICSLP, , Denver, CO, Sep; http://voicetouch.myweb.hinet.net/english/prod01.htm; Yu, H., Waibel, A., Streaming the front-end of a speech recognizer (2000) Proc. ICSLP, , Beijing, China; Leggetter, C.J., Woodland, P.C., Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models (1995) Computer Speech and Language, 9, pp. 171-185; Valbret, H., Moulines, E., Tubach, J.P., Voice transformation using PSOLA technique (1992) Speech Communication, 11, pp. 175-187; Gales, M.J.F., Maximum likelihood linear transformations for HMM-based speech recognition (1998) Computer Speech and Language, 12, pp. 75-98; Heracleous, P., Nakajima, Y., Lee, A., Saruwatari, H., Shikano, K., Accurate hidden Markov models for nonaudible murmur (NAM) recognition based on iterative supervised adaptation (2003) Proc. ASRU, , St. Thomas, U.S. Virgin Islands, Dec; Stüker, S., Metze, F., Schultz, T., Waibel, A., Integrating multilingual articulatory features into speech recognition (2003) Proc. Eurospeech, , Geneva, Switzerland, Sep},
correspondence_address1={Jou, S.-C.; Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA, United States; email: scjou@cs.cmu.edu},
sponsors={IEEE Signal Processing Society},
publisher={Institute of Electrical and Electronics Engineers Inc.},
address={Philadelphia, PA},
issn={15206149},
isbn={0780388747; 9780780388741},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Suebvisai2005,
author={Suebvisai, S. and Charoenpornsawat, P. and Black, A. and Woszczyna, M. and Schultz, T.},
title={Thai automatic speech recognition},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2005},
volume={I},
pages={I857-I860},
doi={10.1109/ICASSP.2005.1415249},
art_number={1415249},
note={cited By 12; Conference of 2005 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP '05 ; Conference Date: 18 March 2005 Through 23 March 2005;  Conference Code:67354},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646815003&doi=10.1109%2fICASSP.2005.1415249&partnerID=40&md5=168734384fac24cf1abdefafd301bf0a},
affiliation={Interactive Systems Laboratories, Carnegie Mellon University, United States; Multimodal Technologies Inc, Pittsburgh PA, United States; Cepstral LLC, Pittsburgh PA, United States},
abstract={We describe the development of a robust and flexible Thai Speech Recognizer as integrated into our English-Thai Speech-to-Speech translation system. We focus on the discussion of the rapid deployment of ASR for Thai under limited time and data resources, including rapid data collection issues, acoustic model bootstrap, and automatic generation of pronunciations. Issues relating to the translation and overall system will be reported elsewhere. © 2005 IEEE.},
keywords={Acoustic devices;  Data acquisition;  Database systems;  Robustness (control systems);  Translation (languages), Automatic speech recognition;  Data collection;  Data resources;  Pronunciations;  Thai Speech Recognizer, Speech recognition},
references={Schultz, T., Alexander, D., Black, A., Peterson, K., Suebvisai, S., Waibel, A., A Thai speech translation system for medical dialogs (2004) Proceedings of the Human Language Technologies (HLT), , Boston, MA, May 2004; Tesprasit, V., Charoenpornsawat, P., Sornlertlamvanich, V., A context-sensitive homograph disambiguation in Thai text-to-speech synthesis (2003) Proceedings of Human Language Technology Conference (HLT&NAACL 2003), , Edmonton, Canada; Schultz, T., Globalphone: A multilingual speech and text database developed at Karlsruhe university (2002) International Conference of Spoken Language Processing (ICSLP-2002), , Denver, CO; Charoepornsawat, P., Together: Thai Word Segmentation Program, , http://www.thai.net/pcharoen/together, [Online]; Meknavin, S., Charoenpornsawat, P., Kijsirikul, B., Feature-based thai word segmentation (1997) Proceedings of the Natural Language Processing Pacific Rim Symposium (NLPRS 1997), , Phuket, Thailand; Chotimongkol, A., Black, A., Statistically trained orthographic to sound models for Thai (2000) ICSLP2000, , Beijing, China; Maskey, S., Black, A., Tomokiyo, L., Bootstrapping phonetic lexicons for new languages (2004) ICSLP2004, , Jeju, Korea; Schultz, T., Waibel, A., Language independent and language adaptive acoustic modeling for speech recognition (2001) Speech Communication, 35 (1-2), pp. 31-51. , August 2001},
correspondence_address1={Suebvisai, S.; Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA, United States},
sponsors={IEEE Signal Processing Society},
publisher={Institute of Electrical and Electronics Engineers Inc.},
address={Philadelphia, PA},
issn={15206149},
isbn={0780388747; 9780780388741},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Paulik2005121,
author={Paulik, M. and Stüker, S. and Fügen, C. and Schultz, T. and Schaaf, T. and Waibel, A.},
title={Speech translation enhanced automatic speech recognition},
journal={Proceedings of ASRU 2005: 2005 IEEE Automatic Speech Recognition and Understanding Workshop},
year={2005},
volume={2005},
pages={121-126},
doi={10.1109/ASRU.2005.1566488},
art_number={1566488},
note={cited By 20; Conference of ASRU 2005: 2005 IEEE Automatic Speech Recognition and Understanding Workshop ; Conference Date: 27 November 2005 Through 1 December 2005;  Conference Code:68918},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846214441&doi=10.1109%2fASRU.2005.1566488&partnerID=40&md5=42603c5ae2d0f4d02c86fd89eeca5037},
affiliation={Interactive Systems Laboratories, Universität Karlsruhe, Germany; Interactive Systems Laboratories, Carnegie Mellon University, United States},
abstract={Nowadays official documents have to be made available in many languages, like for example in the EU with its 20 official languages. Therefore, the need for effective tools to aid the multitude of human translators in their work becomes easily apparent. An ASR system, enabling the human translator to speak his translation in an unrestricted manner, instead of typing it, constitutes such a tool. In this work we improve the recognition performance of such an ASR system on the target language of the human translator by taking advantage of an either written or spoken source language representation. To do so, machine translation techniques are used to translate between the different languages and then the involved ASR systems are biased towards the gained knowledge. We present an iterative approach for ASR improvement and outperform our baseline system by a relative word error rate reduction of 35.8% / 29.9% in the case of a written / spoken source language representation. Further, we show how multiple target languages, as for example provided by different simultaneous translators during European Parliament debates, can be incorporated into our system design for an improvement of all involved ASR systems. © 2005 IEEE.},
keywords={Formal languages;  Knowledge representation;  Speech recognition;  Systems analysis;  Translation (languages), Human translators;  Machine translation techniques;  Speech translation, Speech processing},
references={Dymetman, M., Brousseaux, J., Foster, G., Isabelle, P., Normandin, Y., Plamondon, P., Towards an automatic dictation system for translators: The transtalk project (1994) Proceedings of ICSLP, , Yokohama, Japan; Brown, P., Della Pietra, S., Chen, S., Della Pietra, V., Kehler, S., Mercer, R., Automatic speech recognition in machine aided translation (1994) Computer Speech and Language, 8; Paulik, M., Fügen, C., Stüker, S., Schultz, T., Schaaf, T., Waibel, A., Document driven machine translation enhanced asr (2005) Proceedings of the 9th European Conference on Speech Communication and Technology, , Lisbon, Portugal, September; Kikui, G., Sumita, E., Takezawa, T., Yamamoto, S., Creating corpora for speech-to-speech translation (2003) Proceedings of Eurospeech, , Geneve, Switzerland; Soltau, H., Metze, F., Fügen, C., Waibel, A., A one-pass decoder based on polymorphic linguistic context assignment (2001) Proceedings of ASRU, , Madonna di Campiglio, Italy; Vogel, S., Hewavitharana, S., Kolss, M., Waibel, A., The isl statistical machine translation system for spoken language translation (2004) Proceedings of IWSLT, , Kyoto, Japan; Yua, H., Tam, Y., Schaaf, T., Stüker, S., Jin, Q., Noamany, M., Schultz, T., The isl rt04 mandarin broadcast news evaluation system (2004) EARS Rich Transcription Workshop, , Palisades, NY, USA},
correspondence_address1={Paulik, M.; Interactive Systems Laboratories, Universität KarlsruheGermany; email: paulik@ira.uka.de},
publisher={IEEE Computer Society},
address={Cancun},
isbn={0780394798; 9780780394797},
language={English},
abbrev_source_title={Proc. ASRU IEEE Autom. Speech Recog. Understanding Workshop},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Waibel2004,
author={Waibel, A. and Schultz, T. and Vogel, S. and Fügen, C. and Honal, M. and Kolss, M. and Reichert, J. and Stüker, S.},
title={Towards language portability in statistical speech translation},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2004},
volume={3},
pages={III765-III768},
note={cited By 1; Conference of Proceedings - IEEE International Conference on Acoustics, Speech, and Signal Processing ; Conference Date: 17 May 2004 Through 21 May 2004;  Conference Code:63500},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-4544300731&partnerID=40&md5=0359c57e9e055fac98c9d2d3d69ac0cc},
abstract={An approach towards the tighter coupling of statically based speech translation was discussed. Multiple layers of reduction and transformation were used by cascading several stochastic source-channel models. The It was observed that end-to-end performance can be improved if the interlingua language is enriched with additional linguistic information. The result of translation suggested that MT systems can be successfully constructed for any language pair by cascading multiple MT systems via english.},
keywords={Algorithms;  Data acquisition;  Data reduction;  Linguistics;  Mathematical models;  Probability;  Project management;  Speech recognition, Bootstraping;  Disfluency cleaning;  Grammars;  Speech translation systems, Translation (languages)},
references={Brill, E., (1995) A Case Study in Part of Speech Tagging, , [Brill], Association for Computational Linguistics; Honal, M., Schultz, T., Correction of disfluencies in spontaneous speech using a noisy-channel approach Proc. Eurospeech 2003, , [Honal], Geneva, Switzerland; Killer, M., Stüker, S., Schultz, T., Grapheme-based speech recognition Proc. Eurospeech 2003, , [Killer], Geneva, Switzerland; NIST MT Evaluation Kit Version 9, , http://www.nist.gov/speech/tests/mt/, [MTeval]; Schultz, T., Waibel, A., Language independent and language adaptive acoustic modeling for speech recognition (2001) Speech Communication, 35. , [Schultz], August; Shriberg, E., (1994) Preliminaries to a Theory of Speech Disfluencies, , [Shriberg], PhD-Thesis, University of California at Berkley; http://www.systranbox.com/systran/box, [Systran]; Takezawa, T., Sumita, E., Sugaya, F., Yamamoto, H., Yamamoto, S., Toward a broad-coverage bilingual corpus for speech translation of travel conversations in the real world (2002) Proc. of LREC 2002, pp. 147-152. , [Takezawa], Las Palmas, Canary Islands, Spain, May; Vogel, S., Zhang, Y., Huang, F., Tribble, A., Venugopal, A., Zhao, B., Waibel, A., The CMU statistical translation system (2003) Proc. of the MT Summit IX, , [Vogel]. New Orleans, LA. September; Wang, Y., Waibel, A., Decoding algorithm in statistical machine translation (1997) Proc. of the 35th Annual Meeting of the ACL, , [Wang]; http://www.cogsci.princeton.edu/~wn/, [WordNet]},
sponsors={Institute of Electrical and Electronics Engineers,},
address={Montreal, Que},
issn={15206149},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jin2004597,
author={Jin, Q. and Schultz, T.},
title={Speaker segmentation and clustering in meetings},
journal={8th International Conference on Spoken Language Processing, ICSLP 2004},
year={2004},
pages={597-600},
note={cited By 33; Conference of 8th International Conference on Spoken Language Processing, ICSLP 2004 ; Conference Date: 4 October 2004 Through 8 October 2004;  Conference Code:124335},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009080849&partnerID=40&md5=7e4ef3190315df81547187990bf0872a},
affiliation={Interactive Systems Laboratories, Carnegie Mellon University, United States},
abstract={This paper describes the automatic speaker segmentation and clustering system for natural, multi-speaker meeting conversations based on multiple distant microphones. The system was evaluated in the NTST RT-04S Meeting Recognition Evaluation on the speaker diarization task and achieved speaker diarization performance of 28.17%. This system also aims to provide automatic speech segments and speaker grouping information for speech recognition, a necessary prerequisite for subsequent audio processing. A 44.5% word error rate was achieved for speech recognition.},
keywords={Audio processing;  Automatic speech;  Meeting recognition;  nocv2;  Speaker diarization;  Speaker segmentation and clustering;  Word error rate, Speech recognition},
references={Morgan, N., Baron, D., Edwards, J., Ellis, D., Gelbart, D., Janin, A., Pfau, T., Stolcke, A., The meeting project at ICSI (2001) HLT, , San Diego, March; Siegler, M., Jain, U., Raj, B., Stern, R., Automatic segmentation, classification and clustering of broadcast news audio (1997) Proc. DARPA Speech Recognition Workshop, , Chantilly, Virginia; Chen, S.S., Gopalakrishnan, P.S., Clustering via the Bayesian information criterion with applications in speech recognition (1998) ICASSP; Delacourt, P., Wellekens, C.J., DISTBIC: A speaker-based segmentation for audio data indexing (2000) Speech Communications, 32, pp. 111-126; Strassel, S., Glenn, M., Shared linguistic resources for human language technology in the meeting domain (2004) Proc. MST Meeting Recognition Workshop, , Montreal, Canada; Burger, S., Sloane, Z., The ISL meeting corpus: Categorical features of communicative group interactions (2004) NIST Meeting Recognition Workshop, , Montreal, Canada; Janin, A., Ang, J., Bhagat, S., Dhillon, R., Edwards, J., Morgan, N., Peskin, B., Wrede, B., The ICSI meeting project: Resources and research (2004) NIST Meeting Recognition Workshop, , Montreal, Canada; Stanford, V., Garofolo, J., Beyond close-talk - Issues in distant speech acquisition, conditioning classification, and recognition (2004) NIST Meeting Recognition Workshop, , Montreal, Canada; Vandecatseye, A., Martens, J., A fast, accurate stream-based speaker segmentation and clustering algorithm (2003) Eurospeech, , Geneva, Switzerland; (2003) The Rich Transcription Spring 2003 Evaluation Plan, , http://www.nist.gov/speech/tests/rt/rt2003/spring/docs/rt03-spring-eval-plan-v4.pdf; Metze, F., Jin, Q., Fügen, C., Laskowski, K., Pan, Y., Schultz, T., The ISL meeting transcription system (2004) NIST Meeting Recognition Workshop, , Montreal, Canada},
sponsors={International Speech Communication Association (ISCA)},
publisher={International Speech Communication Association},
language={English},
abbrev_source_title={Int. Conf. Spok. Lang. Process., ICSLP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Laskowski2004973,
author={Laskowski, K. and Jin, Q. and Schultz, T.},
title={Crosscorrelation-based multispeaker speech activity detection},
journal={8th International Conference on Spoken Language Processing, ICSLP 2004},
year={2004},
pages={973-976},
note={cited By 20; Conference of 8th International Conference on Spoken Language Processing, ICSLP 2004 ; Conference Date: 4 October 2004 Through 8 October 2004;  Conference Code:124335},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009097062&partnerID=40&md5=232df5c252ca22a05909b67f45fe7b44},
affiliation={Interactive Systems Laboratories, Carnegie Mellon University, United States},
abstract={We propose an algorithm for segmenting multispeaker meeting audio, recorded with personal channel microphones, into speech and non-speech intervals for each microphone's wearer. An algorithm of this type turns out to be necessary prior to subsequent audio processing because, in spite of close-talking microphones, the channels exhibit a high degree of crosstalk due to unbalanced calibration and small inter-speaker distance. The proposed algorithm is based on the short-time crosscorrelation of all channel pairs. It requires no prior training and executes in one fifth real time on modern architectures. Using meeting audio collected at several sites, we present error rates for the segmentation task which do not appear correlated with microphone type or number of speakers. We also present the resulting improvement in speech recognition accuracy when segmentation is provided by this algorithm.},
keywords={Microphones, Audio processing;  Cross correlations;  Error rate;  Modern architectures;  Non speech;  Real time;  Recognition accuracy;  Speech activity detections, Speech recognition},
references={Rich Transcription 2004 Spring Meeting Recognition Evaluation, , http://www.itl.nist.gov/iad/894.01/tests/rt/rt2004/spring/; Burger, S., MacLaren, V., Yu, H., The ISL meeting corpus: The impact of meeting type on speech style Proc. ICSLP 2002, , Denver, USA; Shriberg, E., Stolcke, A., Baron, D., Observations on overlap: Findings and implications for automatic processing of multi-party conversation Proc. Eurospeech 2001, , Aalborg, Denmark; Pfau, T., Ellis, D.P.W., Stolcke, A., Multispeaker speech activity detection for the ICSI meeting recognizer Proc. ASRU 2001, , Madonna di Campiglio, Italy; Burger, S., Sloane, Z., The ISL meeting corpus: Categorical features of communicative group interactions Proc. ICASSP 2004 Meeting Recognition Workshop, , Montreal, Canada; Janin, A., Ang, J., Bhagat, S., Dhillon, R., Edwards, J., Morgan, N., Peskin, B., Wrede, B., The ICSI meeting project: Resources and research Proc. ICASSP 2004 Meeting Recognition Workshop, , Montreal, Canada; Strassel, S., Glenn, M., Shared linguistic resources for human language technology in the meeting domain Proc. ICASSP 2004 Meeting Recognition Workshop, , Montreal, Canada; Stanford, V., Garofolo, J., Beyond close-talk-issues in distant speech acquisition, conditioning classification, and recognition Proc. ICASSP 2004 Meeting Recognition Workshop, , Montreal, Canada; Moore, B.C.J., (1997) An Introduction to the Psychology of Hearing, , Academic Press; Metze, F., Jin, Q., Fügen, C., Laskowski, K., Pan, Y., Schultz, T., Issues in meeting transcription - The ISL meeting transcription system Proc. ICSLP 2004, , Jeju Island, Korea},
sponsors={International Speech Communication Association (ISCA)},
publisher={International Speech Communication Association},
language={English},
abbrev_source_title={Int. Conf. Spok. Lang. Process., ICSLP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jou20041493,
author={Jou, S.-C. and Schultz, T. and Waibel, A.},
title={Adaptation for soft whisper recognition using a throat microphone},
journal={8th International Conference on Spoken Language Processing, ICSLP 2004},
year={2004},
pages={1493-1496},
note={cited By 37; Conference of 8th International Conference on Spoken Language Processing, ICSLP 2004 ; Conference Date: 4 October 2004 Through 8 October 2004;  Conference Code:124335},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890487256&partnerID=40&md5=0dc3ef50f8c3623a2637ae529e411053},
affiliation={Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA, United States},
abstract={This paper describes various adaptation methods applied to recognizing soft whisper recorded with a throat microphone. Since the amount of adaptation data is small and the testing data is very different from the training data, a series of adaptation methods is necessary. The adaptation methods include: maximum likelihood linear regression, feature-space adaptation, and re-training with downsampling, sigmoidal low-pass filter, or linear multivariate regression. With these adaptation methods, the word error rate improves from 99.3% to 32.9%.},
keywords={Low pass filters;  Maximum likelihood;  Microphones;  Regression analysis, Adaptation methods;  Feature space;  Maximum likelihood linear regression;  Multivariate regression;  Testing data;  Throat microphones;  Training data;  Word error rate, Speech recognition},
references={Nakajima, Y., Kashioka, H., Shikano, K., Campbell, N., Non-audible murmur recognition input interface using stethoscopic microphone attached to the skin (2003) Proc. ICASSP, pp. 708-711. , Hong Kong; Heracleous, P., Nakajima, Y., Lee, A., Saruwatari, H., Shikano, K., Accurate hidden Markov models for non-audible murmur (NAM) recognition based on iterative supervised adaptation (2003) Proc. 3SRU, pp. 73-76. , St. Thomas, U.S. Virgin Islands; Zheng, Y., Liu, Z., Zhang, Z., Sinclair, M., Droppo, J., Deng, L., Acero, A., Huang, X., Air- and bone-conductive integrated microphones for robust speech detection and enhancement (2003) Proc. ASRU, pp. 249-254. , St. Thomas, U.S. Virgin Islands; http://voicetouch.myweb.hinet.net/english/prod01.htm; Yu, H., Waibel, A., Streaming the front-end of a speech recognizer (2000) Proc. ICSLP, , Beijing, China; Leggetter, C.J., Woodland, P.C., Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models (1995) Computer Speech and Language, 9, pp. 171-185; Valbret, H., Moulines, E., Tubach, J.P., Voice transformation using PSOLA technique (1992) Speech Communication, 11, pp. 175-187; Gales, M.J.F., Maximum likelihood linear transformations for HMM-based speech recognition (1998) Computer Speech and Language, 12, pp. 75-98},
sponsors={International Speech Communication Association (ISCA)},
publisher={International Speech Communication Association},
language={English},
abbrev_source_title={Int. Conf. Spok. Lang. Process., ICSLP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Katzenmaier2004144,
author={Katzenmaier, M. and Stiefelhagen, B. and Schultz, T.},
title={Identifying the addressee in human-human-robot interactions based on head pose and speech},
journal={ICMI'04 - Sixth International Conference on Multimodal Interfaces},
year={2004},
pages={144-151},
doi={10.1145/1027933.1027959},
note={cited By 89; Conference of ICMI'04 - Sixth International Conference on Multimodal Interfaces ; Conference Date: 14 October 2004 Through 15 October 2004;  Conference Code:64413},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-14944370978&doi=10.1145%2f1027933.1027959&partnerID=40&md5=47da225f02990d1b732c85fd8de32ca6},
affiliation={Interactive Systems Labs., Universität Karlsruhe (TH), Karlsruhe, Germany; Interactive Systems Labs., Carnegie Mellon University, Pittsburgh, PA, United States},
abstract={In this work we investigate the power of acoustic and visual cues, and their combination, to identify the addressee in a human-human-robot interaction. Based on eighteen audio-visual recordings of two human beings and a (simulated) robot we discriminate the interaction of the two humans from the interaction of one human with the robot. The paper compares the result of three approaches. The first approach uses purely acoustic cues to find the addressees. Low level, feature based cues as well as higher-level cues are examined. In the second approach we test whether the human's head pose is a suitable cue. Our results show that visually estimated head pose is a more reliable cue for the identification of the addressee in the human-human-robot interaction. In the third approach we combine the acoustic and visual cues which results in significant improvements. Copyright 2004 ACM.},
author_keywords={Attentive interfaces;  Focus of attention;  Head pose estimation;  Human-robot interaction;  Multimodal interfaces;  Speech recognition},
keywords={Error analysis;  Interfaces (computer);  Probability;  Robots;  Sound recording;  Speech recognition, Attentive interfaces;  Focus of attention;  Head pose estimation;  Human-robot interaction;  Multimodal interfaces, Human computer interaction},
references={Special issue on human-friendly robots (1998) Journal of the Robotics Society of Japan, 16; (2003) Proceedings of the Third IEEE International Conference on Humanoid Robots - Humanoids 2003, , IEEE, Karlsruhe, Germany; Argyle, M., (1969) Social Interaction, , Methuen, London; Bakx, I., Van Turnhout, K., Terken, J., Facial orientation during multi-party interaction with information kiosks (2003) Proceedings of the Interact 2003, , Zurich, Switzerland; Tankard, J.W., Effects of eye position on person perception (1970) Perc. Mot. Skills, (31), pp. 883-893; Katzenmaier, M., Determining the adressee in spoken human robot interaction, studienarbeit (2003) Technical Report, , Fakultät für Informatik, Universität Karlsruhe (TH); Kleinke, C.L., Bustos, A.A., Meeker, F.B., Staneski, R.A., Effects of self-attributed and other-attributed gaze in interpersonal evaluations between males and females (1973) Journal of Experimental Social Psychology, (9), pp. 154-163; Maglio, P.P., Matlock, T., Campbell, C.S., Zhai, S., Smith, B.A., Gaze and speech in attentive user interfaces (2000) LNCS, 1948. , Proceedings of the International Conference on Multimodal Interfaces, Springer; Ruusuvuori, J., Looking means listening: Coordinating displays of engagement in doctor-patient interaction (2001) Social Science & Medicine, 52, pp. 1093-1108; Stiefelhagen, R., Tracking focus of attention in meetings (2002) International Conference on Multimodal Interfaces, pp. 273-280. , Pittsburgh, PA, October IEEE; Stiefelhagen, R., Zhu, J., Head orientation and gaze direction in meetings (2002) Conference on Human Factors in Computing Systems (CHI2002), , Minneapolis, April; Vertegaal, R., Slagter, R., Van Der Veer, G., Nijholt, A., Eye gaze patterns in conversations: There is more to conversational agents than meets the eyes (2001) SIGCHI'01, , Seattle, March ACM},
correspondence_address1={Katzenmaier, M.; Interactive Systems Labs., Universität Karlsruhe (TH), Karlsruhe, Germany; email: stiefel@ira.uka.de},
sponsors={ACM SIGCHI},
publisher={Association for Computing Machinery (ACM)},
isbn={1581139543; 9781581139549},
language={English},
abbrev_source_title={ICMI Sixth Int. Conf. Multimodal Interfaces},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Saleem200441,
author={Saleem, S. and Jou, S.-C. and Vogel, S. and Schultz, T.},
title={Using word lattice information for a tighter coupling in speech translation systems},
journal={8th International Conference on Spoken Language Processing, ICSLP 2004},
year={2004},
pages={41-44},
note={cited By 25; Conference of 8th International Conference on Spoken Language Processing, ICSLP 2004 ; Conference Date: 4 October 2004 Through 8 October 2004;  Conference Code:124335},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009064092&partnerID=40&md5=c1d75b8df0a36a1b85368c274a0189a8},
affiliation={Interactive Systems Lab., Language Technologies Institute, Carnegie Mellon University, United States},
abstract={In this paper we present first experiments towards a tighter coupling between Automatic Speech Recognition (ASR) and Statistical Machine Translation (SMT) to improve the overall performance of our speech translation system. In coventional speech translation systems, the recognizer outputs a single hypothesis which is then translated by the SMT system. This approach has the limitation of being largely dependent on the word error rate of the first best hypothesis. The word error rate is typically lowered by generating many alternative hypotheses in the form of a word lattice. The information in the word lattice and the scores from the recognizer can be used by the translation system to obtain better performance. In our experiments, by switching from the single best hypotheses to word lattices as the interface between ASR and SMT, and by introducing weighted acoustic scores in the translation system, the overall performance was increased by 16.22%.},
keywords={Computer aided language translation;  Speech transmission;  Translation (languages), Alternative hypothesis;  Automatic speech recognition;  SMT systems;  Speech translation systems;  Statistical machine translation;  Translation systems;  Word error rate, Speech recognition},
references={Ringger, E.K., A robust loose coupling for speech recognition and natural language understanding (1995) Technical Report 592., University of Rochester Computer Science Department; Liu, F.H., Gu, L., Gao, Y., Picheny, M., Use of statistical N-gram models in natural language generation for machine translation (2003) Proceedings of ICASSP, pp. 636-639. , April; Black, A.W., Brown, R.D., Frederking, R., Lenzo, K., Moody, J., Rudnicky, A., Singh, R., Steinbrecher, E., Rapid development of speech-to-speech translation systems (2002) Proceedings of ICSLP, pp. 1709-1712. , Sept; Takezawa, T., Morimoto, T., Sagisaka, Y., Campbell, N., Iida, H., Sugaya, F., Yokoo, A., Yamamoto, S., A Japanese-to-English speech translation system:Atr-MATRIX (1998) Proceedings of ICSLP, , pp: 2779-2782, Dec; Pastor, M., Sanchis, A., Casacuberta, F., Vidal, E., EuTrans: A speech-to-speech translator prototype (2001) Proceedings of Eurospeech, , pp: 2385-2389, Sept; Bangalore, S., Riccardi, G., A finite-state approach to machine translation (2001) Proceedings of NAACL, , May; Llorens, D., Casacuberta, F., Segarra, E., Sánchez, J.A., Aibar, P., Acoustical and syntactical modeling in ATROS system (1999) Proceedings of ICASSP, pp. 641-644. , May; Woszczyna, M., Coccaro, N., Eisele, A., Lavie, A., McNair, A., Polzin, T., Rogina, I., Ward, W., Recent advances in JANUS: A speech translation system (1993) Proceedings of Eurospeech, , pp: 1295-1298, Sept; Metze, F., Langley, C., Lavie, A., McDonough, J., Soltau, H., Waibel, A., Burger, S., Taddei, L., The NESPOLE! Speech-to-speech translation system (2003) Proceedings of HLT; Ney, H., Speech translation: Coupling of recognition and translation (1999) Proceedings of ICASSP, pp. 517-520. , May; Woodland, P., Leggetter, C., Odell, J., Valtchev, V., Young, S.J., The development of the 1994 HTK large vocabulary speech recognition system (1995) Proceedings of the ARPA Spoken Language Systems Technology Workshop. ARPA, , ARPA, January; Soltau, H., Metze, F., Fgen, C., Waibel, A., A one passdecoder based on polymorphic linguistic context assignment (2001) Proceedings of ASRU, , Dec; Vogel, S., Zhang, Y., Huang, F., Tribble, A., Venugopal, A., Zhao, B., Waibel, A., The CMU statistical machine translation system (2003) Proceedings of MT-Summit IX, , LA. Sep; Papineni, K., Roukos, S., Ward, T., Zhu, W., BLEU: A method for automatic evaluation of machine translation (2002) Proceedings of ACL, pp. 311-318. , July},
sponsors={International Speech Communication Association (ISCA)},
publisher={International Speech Communication Association},
language={English},
abbrev_source_title={Int. Conf. Spok. Lang. Process., ICSLP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Metze20041709,
author={Metze, F. and Jin, Q. and Fügen, C. and Laskowski, K. and Pan, Y. and Schultz, T.},
title={Issues in meeting transcription - The ISL meeting transcription system},
journal={8th International Conference on Spoken Language Processing, ICSLP 2004},
year={2004},
pages={1709-1712},
note={cited By 17; Conference of 8th International Conference on Spoken Language Processing, ICSLP 2004 ; Conference Date: 4 October 2004 Through 8 October 2004;  Conference Code:124335},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887145372&partnerID=40&md5=170b92d78b8d3202d2f72469d00660d6},
affiliation={Interactive Systems Labs, Universität Karlsruhe (TH), Carnegie Mellon University, United States},
abstract={This paper describes the Interactive Systems Lab's Meeting transcription system, which performs segmentation, speaker clustering as well as transcriptions of conversational meeting speech. The system described here was evaluated in NIST's RT-04S "Meeting" speech evaluation and reached the lowest word error rates for the distant microphone conditions. Also, we compare the performance of our Broadcast News and the most recent Switchboard system on the Meeting data and compare both with the newly-trained meeting recognizer. Furthermore, we investigate the effects of automatic segmentation on adaptation. Our best meeting system achieves a WER of 44.5% on the "MDM" condition in NIST's RT-04S evaluation.},
keywords={Speech recognition, Automatic segmentations;  Broadcast news;  Interactive system;  Speaker clustering;  Word error rate, Transcription},
references={Waibel, A., Yue, H., Soltau, H., Schultz, T., Schaaf, T., Pan, Y., Metze, F., Bett, M., Advances in meeting recognition (2001) Proc. HLT-2001, p. 3. , San Diego, CA: ISCA; Burger, S., MacLaren, V., Yu, H., The ISL meeting corpus: The impact of meeting type on speech style (2002) Proc. ICSLP-2002, p. 9. , Denver, CO: ISCA; Burger, S., Sloan, Z., The ISL meeting corpus: Categorical features of communicative group interactions (2004) Proc. ICASSP-2004 Meeting Recognition Workshop, p. 5. , Montreal; Canada: NIST; Janin, A., Ang, J., Bhagat, S., Dhillon, R., Edwards, J., Morgan, N., Peskin, B., Wrede, B., The ICSI meeting project: Resources and research (2004) Proc. ICASSP-2004 Meeting Recognition Workshop, p. 5. , Montreal; Canada: NIST; Strassel, S., Glenn, M., Shared linguistic resources for human language technology in the meeting domain (2004) Proc. ICASSP-2004 Meeting Recognition Workshop, p. 5. , Montreal; Canada: NIST; Stanford, V., Garofolo, J., Beyond close-talk - Issues in distant speech acquistion, conditioning classification, and recognition (2004) Proc. ICASSP-2004 Meeting Recognition Workshop, p. 5. , Montreal; Canada: NIST; Docio-Fernandez, L., Gelbart, D., Morgan, N., Far-field ASR on inexpensive microphones (2003) Proc. Eurospeech-2003, p. 9. , Geneva; Switzerland: ISCA; Finke, M., Geutner, P., Hild, H., Kemp, T., Ries, K., Westphal, M., The karlsruhe verbmobil speech recognition engine (1997) Proc. ICASSP 97, p. 4. , München; Germany: IEEE; Soltau, H., Metze, F., Fügen, C., Waibel, A., A one-pass decoder based on polymorphic linguistic context assignment (2001) Proc. ASRU 2001, p. 12. , Madonna di Campiglio, Italy: IEEE; Gales, M., Semi-tied covariance matrices for hidden Markov models (1999) IEEE Transactions on Speech and Audio Processing, 2. , May; Yu, H., Waibel, A., Streaming the front-end of a speech recognizer (2000) Proc. ICSLP-2000, p. 10. , Beijing; China: ISCA; Soltau, H., Yu, H., Metze, F., Fügen, C., Jin, Q., Jou, S.-C., The 2003 ISL rich transcription system for conversational telephony speech (2004) Proc. ICASSP 2004, , Montreal; Canada: IEEE; Gade, R.R., Gelbart, D., Pfau, T., Stolcke, A., Wooters, C., Experiments with meeting data (2002) Proc. RT02 Workshop, p. 5. , Vienne, VA: NIST; Siegler, M., Jain, U., Raj, B., Stern, R., Automatic segmentation, classification and clustering of broadcast news audio (1997) Proc. DARPA Speech Recognition Workshop; Jin, Q., Laskowski, K., Schultz, T., Waibel, A., Speaker segmentation and clustering in meetings (2004) Proc. ICASSP-2004 Meeting Recognition Workshop, p. 5. , Montreal; Canada: NIST; Laskowski, K., Jin, Q., Schultz, T., Cross-correlation-based multispeaker speech activity detection (2004) Subm. Proc. ICSLP-2004, p. 10. , Jeju; Korea: ISCA; Gales, M.J.F., (1997) Maximum Likelihood Linear Transformations for HMM-based Speech Recognition, , Cambridge University, Cambridge, UK, Tech. Rep; Mangu, L., Brill, E., Stolcke, A., Finding consensus in speech recognition: Word error minimization and other applications of confusion networks (2000) Computer, Speech and Language, 14 (4), pp. 373-400},
sponsors={International Speech Communication Association (ISCA)},
publisher={International Speech Communication Association},
language={English},
abbrev_source_title={Int. Conf. Spok. Lang. Process., ICSLP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2003540,
author={Wang, Z. and Schultz, T. and Waibel, A.},
title={Comparison of acoustic model adaptation techniques on non-native speech},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2003},
volume={1},
pages={540-543},
note={cited By 98; Conference of 2003 IEEE International Conference on Accoustics, Speech, and Signal Processing ; Conference Date: 6 April 2003 Through 10 April 2003;  Conference Code:61464},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0141814617&partnerID=40&md5=2c34906e737ccac2c10eb74f8a76d7df},
affiliation={Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={The performance of speech recognition systems is consistently poor on non-native speech. The challenge for non-native speech recognition is to maximize the recognition performance with small amount of non-native data available. In this paper we report on the acoustic modeling adaptation for the recognition of non-native speech. Using non-native data from German speakers, we investigate how bilingual models, speaker adaptation, acoustic model interpolation and Polyphone Decision Tree Specialization methods can help to improve the recognizer performance. Results obtained from the experiments demonstrate the feasibility of these methods.},
keywords={Non-native speech, Acoustics;  Data reduction;  Interpolation, Speech recognition},
references={Schultz, T., Waibel, A., Polyphone Decision Tree Specialization for Language Adaptation (2000) Proc. ICASSP, , Istanbul, Turkey, June; Fischer, V., Janke, E., Kunzmann, S., Likelihood Combination and Recognition Output Voting for the Decoding of Non-native Speech with Multilingual HMMs (2002) Proc. ICSLP; Wang, Z., Topkara, U., Schultz, T., Waibel, A., Towards Universal Speech Recognition (2002) Proc. ICMI 2002; Mayfield Tomokiyo, L., (2001) Recognizing Non-native Speech: Characterizing and Adapting to Non-native Usage in Speech Recognition, , Ph.D. thesis, Carnegie Mellon University; Zavaliagkos, G., Schwartz, R., Makhoul, J., Batch, Incremental and Instantaneous Adaptation Techniques for Speech Recognition (1995) Proc. ICASSP; Uebler, U., Boros, M., Recognition of Non-native German Speech with Multilingual Recognizers (1999) Proc. Eurospeech, 2, pp. 911-914. , Budapest; Finke, M., Geutner, P., Hild, H., Kemp, T., Ries, K., Westphal, M., The Karlsruhe-verbmobil Speech Recognition Engine (1997) ICASSP, , Munich; Soltau, H., Schaaf, T., Metze, F., Waibel, A., The ISL Evaluation System for Verbmobil II (2001) ICASSP 2001, , Salt Lake City, May},
correspondence_address1={Wang, Z.; Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: zhirong@cs.cmu.edu},
sponsors={The Institute of Electrical and Electronics Engineers Signal},
address={Hong Kong},
issn={15206149},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Stüker2003144,
author={Stüker, S. and Schultz, T. and Metze, F. and Waibel, A.},
title={Multilingual articulatory features},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2003},
volume={1},
pages={144-147},
note={cited By 50; Conference of 2003 IEEE International Conference on Accoustics, Speech, and Signal Processing ; Conference Date: 6 April 2003 Through 10 April 2003;  Conference Code:61464},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0141591550&partnerID=40&md5=77ca5f563c52c8e82cdcec4e9a7ee2be},
affiliation={Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA, United States; Inst. Logik, Komplexitat/D., Univ. Fridericiana zu Karlsruhe (TH), Karlsruhe, Germany},
abstract={Speech recognition systems based on or aided by articulatory features, such as place and manner of articulation, have been shown to be useful under varying circumstances. Recognizers based on features better compensate channel and noise variability. In this work we show that it is also possible to compensate for inter language variability using articulatory feature detectors. We come to the conclusion that articulatory features can be recognized across languages and that using detectors from many languages can improve the classification accuracy of the feature detectors on a single language. We further demonstrate how those multilingual and crosslingual detectors can support an HMM based recognizer and thereby significantly reduce the word error rate by up to 12.3% relative. We expect that with the use of multilingual articulatory features it is possible to support the rapid deployment of recognition systems for new target languages.},
keywords={Articulatory features, Feature extraction;  Markov processes;  Spurious signal noise, Speech recognition},
references={(1999) Handbook of the International Phonetic Association, , Cambridge University Press; Deng, L., Sun, D.X., A statistical approach to automatic speech recognition using the atomic speech units constructed from overlapping articulatory features (1994) Journal of the Acoustical Society of America, 95. , May; Kirchhoff, K., Combining Articulatory and Acoustic Information for Speech Recognition in Noisy and Reverberant Environments (1998) Proceedings of the ICSLP, , December; Eide, E., Distinctive Features For Use in an Automatic Speech Recognition System (2001) Proceedings of the 7th EUROSPEECH, , Aalborg, Denmark; Metze, F., Waibel, A., A Flexible Stream Architecture for ASR Using Articulatory Features (2002) Proceedings of the 7th ICSLP, , Denver, Colorado, USA, September; Schultz, T., Waibel, A., Language Independent and Language Adaptive Acoustic Modeling for Speech Recognition (2001) Speech Communication, 35. , August; Schultz, T., Globalphone: A Multilingual Speech and Text Database Developed at Karlsruhe University (2002) Proceedings of the 7th ICSLP, , Denver, Colorado, USA, September},
correspondence_address1={Stüker, S.; Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA, United States; email: stueker@ira.uka.de},
sponsors={The Institute of Electrical and Electronics Engineers Signal},
address={Hong Kong},
issn={15206149},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Waibel2003752,
author={Waibel, A. and Schultz, T. and Bett, M. and Denecke, M. and Malkin, R. and Rogina, I. and Stiefelhagen, R. and Yang, J.},
title={SMaRT: The smart meeting room task at ISL},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2003},
volume={4},
pages={752-755},
note={cited By 60; Conference of 2003 IEEE International Conference on Accoustics, Speech, and Signal Processing ; Conference Date: 6 April 2003 Through 10 April 2003;  Conference Code:61467},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0141521622&partnerID=40&md5=c633ba764418acfa88b25173eefa4113},
affiliation={Interactive Syst. Laboratories (ISL), Carnegie Mellon University, Karlsruhe University, Karlsruhe, Germany},
abstract={As computational and communications systems become increasingly smaller, faster, more powerful, and more integrated, the goal of interactive, integrated meeting support rooms is slowly becoming reality. It is already possible, for instance, to rapidly locate task-related information during a meeting, filter it, and share it with remote users. Unfortunately, the technologies that provide such capabilities are as obstructive as they are useful - they force humans to focus on the tool rather than the task. Thus the veneer of utility often hides the true costs of use, which are longer, less focused human interactions. To address this issue, we present our current research efforts towards SMaRT: the Smart Meeting Room Task. The goal of SMaRT is to provide meeting support services that do not require explicit human-computer interaction. Instead, by monitoring the activities in the meeting room using both video and audio analysis, the room will be able to react appropriately to users' needs and allow the users to focus on their own goals.},
keywords={Remote users, Human computer interaction;  Image analysis;  Remote consoles, Interactive computer systems},
references={Rogina, I., Schaaf, T., Lecture and Presentation Tracking in an Intelligent Meeting Room (2002) Proceedings of the ICMI 2002, , Pittsburgh, PA, October; Stiefelhagen, R., Zhu, J., Head Orientation and Gaze Direction in Meetings (2002) Conference on Human Factors in Computing Systems (CHI2002), , Minneapolis, April; Schultz, T., Jin, Q., Laskowski, K., Tribble, A., Waibel, A., Speaker, Accent, and Language Identification using Multilingual Phone Strings (2002) Proceedings of the Human Language Technology Meeting (HLT-2002), , San Diego, March; Burger, S., MacLaren, V., Yu, H., The ISL Meeting Corpus: The Impact of Meeting Type on Speech Style (2002) Proceedings of the ICSLP, , Denver CO, September; Stiefelhagen, R., Tracking Focus of Attention in Meetings (2002) IEEE International Conference on Multimodal Interfaces, , Pittsburgh, PA, USA, October 14-16; Focken, D., Stiefelhagen, R., Towards Vision-based 3-D People Tracking in a Smart Room (2002) IEEE International Conference on Multimodal Interfaces, , Pittsburgh, PA, USA, October 14-16; Schultz, T., Waibel, A., Bett, M., Metze, F., Pan, Y., Ries, K., Schaaf, T., Zechner, K., The ISL Meeting Room System (2001) Proceedings of the Workshop on Hands-Free Speech Communication (HSC-2001), , Kyoto Japan, April; Waibel, A., Bett, M., Finke, M., Stiefelhagen, R., Meeting browser: Tracking and summarizing meetings (1998) Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, pp. 281-286. , Lansdowne, Virginia, February 8-11; Yang, J., Zhu, X., Gross, R., Kominek, J., Pan, Y., Waibel, A., Multimodal People ID for a Multimedia Meeting Browser Proceedings of ACM Multimedia 99; Chen, X., Yang, J., Towards monitoring human activities using an omnidirectional camera (2002) Proceedings of ICMI 2002, , Pittsburgh, PA, October; Gross, R., Yang, J., Waibel, A., Face Recognition in a Meeting Room Proceedings of Fourth IEEE International Conference on Automatic Face and Gesture Recognition (FG' 2000)},
correspondence_address1={Waibel, A.; Interactive Syst. Laboratories (ISL), Carnegie Mellon University, Karlsruhe University, Karlsruhe, Germany},
sponsors={The Institute of Electrical and Electronics Engineers Signal},
address={Hong Kong},
issn={15206149},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang20031449,
author={Wang, Z. and Schultz, T.},
title={Non-native spontaneous speech recognition through polyphone decision tree specialization},
journal={EUROSPEECH 2003 - 8th European Conference on Speech Communication and Technology},
year={2003},
pages={1449-1452},
note={cited By 12; Conference of 8th European Conference on Speech Communication and Technology, EUROSPEECH 2003 ; Conference Date: 1 September 2003 Through 4 September 2003;  Conference Code:124334},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009216453&partnerID=40&md5=4f8576009cee5d113496a3d87ae2884f},
affiliation={Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA  15213, United States},
abstract={With more and more non-native speakers speaking in English, the fast and efficient adaptation to non-native English speech becomes a practical concern. The performance of speech recognition systems is consistently poor on non-native speech. The challenge for non-native speech recognition is to maximize the recognition performance with small amount of non-native data available. In this paper we report on the effectiveness of using polyphone decision tree specialization method for non-native speech adaptation and recognition. Several recognition results are presented by using non-native speech from German speakers. Results obtained from the experiments demonstrate the feasibility of this method.},
keywords={Decision trees;  Speech;  Speech communication, Non-native;  Non-native speakers;  Non-native speech;  Speech recognition systems;  Spontaneous speech recognition, Speech recognition},
references={Schultz, T., Waibel, A., Polyphone decision tree specialization for language adaptation (2000) Proc. ICASSP, , Istanbul, Turkey, June; Fischer, V., Janke, E., Kunzmann, S., Likelihood combination and recognition output voting for the decoding of non-native speech with multilingual hmms (2002) Proc. ICSLP; Wang, Z., Topkara, U., Schultz, T., Waibel, A., Towards universal speech recognition (2002) Proc. ICMI; Mayfield Tomokiyo, L., (2001) Recognizing Non-native Speech: Characterizing and Adapting to Non-native Usage in Speech Recognition, , Ph.D. thesis, Carnegie Mellon University; Zavaliagkos, G., Schwartz, R., Makhoul, J., Batch, incremental and instantaneous adaptation techniques for speech recognition (1995) Proc. ICASSP; Uebler, U., Boros, M., Recognition of non-native German speech with multilingual recognizers (1999) Proc. Eurospeech, 2, pp. 911-914. , Budapest; Finke, M., Geutner, P., Hild, H., Kemp, T., Ries, K., Westphal, M., The karlsruhe-verbmobil speech recognition engine (1997) ICASSP, , Munich; Soltau, H., Schaaf, T., Metze, F., Waibel, A., The isl evaluation system for verbmobil II (2001) ICASSP 2001, , Salt Lake City, May},
sponsors={et al.; Office of Naval Research International Field Office (ONRG); State of Geneva; State of Valais; Swiss National Science Foundation; UBS SA},
publisher={International Speech Communication Association},
language={English},
abbrev_source_title={EUROSPEECH - Euro. Conf. Speech Commun. Technol.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yu20031869,
author={Yu, H. and Schultz, T.},
title={Enhanced tree clustering with single pronunciation dictionary for conversational speech recognition},
journal={EUROSPEECH 2003 - 8th European Conference on Speech Communication and Technology},
year={2003},
pages={1869-1872},
note={cited By 8; Conference of 8th European Conference on Speech Communication and Technology, EUROSPEECH 2003 ; Conference Date: 1 September 2003 Through 4 September 2003;  Conference Code:124334},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009168839&partnerID=40&md5=f779eb7f997d16572f92b3db3bc5ad70},
affiliation={Interactive Systems Lab, Carnegie Mellon University, Pittsburgh, PA  15213, United States},
abstract={Modeling pronunciation variation is key for recognizing conversational speech. Rather than being limited to dictionary modeling, we argue that triphone clustering is an integral part of pronunciation modeling. We propose a new approach called enhanced tree clustering. This approach, in contrast to traditional decision tree based state tying, allows parameter sharing across phonemes. We show that accurate pronunciation modeling can be achieved through efficient parameter sharing in the acoustic model. Combined with a single pronunciation dictionary, a 1.8% absolute word error rate improvement is achieved on Switchboard, a large vocabulary conversational speech recognition task.},
keywords={Decision trees;  Speech;  Speech communication, Conversational speech;  Conversational speech recognition;  Decision tree based state tying;  Large vocabulary;  Parameter sharing;  Pronunciation modeling;  Pronunciation variation;  Single pronunciations, Speech recognition},
references={Saraclar, M., Nock, H., Khudanpur, S., Pronunciation modeling by sharing Gaussian densities across phonetic models (2000) Computer Speech and Language, 14 (2), pp. 137-160. , April; Young, S., Odell, J., Woodland, P., Tree-based state tying for high accuracy acoustic modelling (1994) Proc. ARPA HLT Workshop; Hain, T., Implicit pronunciation modelling in ASR (2002) ISCA Pronunciation Modeling Workshop; Finke, M., Fritsch, J., Geutner, P., Ries, K., Zeppenfeld, T., The janusrtk switchboard/callhome 1997 evaluation system (1997) Proceedings of LVCSR Hub5-e Workshop; Finke, M., Rogina, I., Wide context acoustic modeling in read vs. spontaneous speech (1997) Proc. ICASSP, pp. 1743-1746; Luo, X., Jelinek, F., Probablistic classification of hmm states for large vocabulary continuous speech recognition (1999) Proc. ICASSP; Soltau, H., Yu, H., Metze, F., Fügen, C., Pan, Y., Jou, S., ISL meeting recognition (2002) Rich Transcription Workshop, , Vienna, VA; Jurafsky, D., Ward, W., Zhang, J., Herold, K., Yu, X., Zhang, S., What kind of pronunciation variation is hard for triphones to model? (2001) Proc. ICASSP; Hain, T., (2001) Hidden Model Sequence Models for Automatic Speech Recognition, , Ph.D. thesis, Cambridge University},
sponsors={et al.; Office of Naval Research International Field Office (ONRG); State of Geneva; State of Valais; Swiss National Science Foundation; UBS SA},
publisher={International Speech Communication Association},
language={English},
abbrev_source_title={EUROSPEECH - Euro. Conf. Speech Commun. Technol.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Honal20032781,
author={Honal, M. and Schultz, T.},
title={Correction of disfluencies in spontaneous speech using a noisy-channel approach},
journal={EUROSPEECH 2003 - 8th European Conference on Speech Communication and Technology},
year={2003},
pages={2781-2784},
note={cited By 22; Conference of 8th European Conference on Speech Communication and Technology, EUROSPEECH 2003 ; Conference Date: 1 September 2003 Through 4 September 2003;  Conference Code:124334},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-56149102222&partnerID=40&md5=e1daa7c5e06626219bc651b893616b25},
affiliation={Interactive Systems Laboratories, University of Karlsruhe Germany, Carnegie Mellon University, United States},
abstract={In this paper we present a system which automatically corrects disfluencies such as repairs and restarts typically occurring in spontaneously spoken speech. The system is based on a noisy-channel model and its development requires no linguistic knowledge, but only annotated texts. Therefore, it has large potential for rapid deployment and the adaptation to new target languages. The experiments were conducted on spontaneously spoken dialogs from the English VERBMOBIL corpus where a recall of 77.2% and a precision of 90.2% was obtained. To demonstrate the feasibility of rapid adaptation additional experiments on the spontaneous Mandarin Chinese CallHome corpus were performed achieving 49.4% recall and 76.8% precision.},
keywords={Computer applications;  Computer simulation, Additional experiments;  Linguistic knowledge;  Mandarin Chinese;  nocv1;  Noisy channel models;  Rapid adaptation;  Rapid deployments;  Spontaneous speech;  Target language, Speech communication},
references={Bear, J., Dowding, J., Shriberg, E., Integrating multiple knowledge sources for detection and correction of repairs in human-computer dialog (1992) Proc. of the 30th Annual Meeting of the ACL; Heeman, P.A., (1997) Speech Repairs, Intonational Boundaries and Discourse Markers: Modeling Speakers' Utterances in Spoken Dialog, , PhD-Thesis, University of Rochester; Hindle, D., Deterministic parsing of syntactic nonfluencies (1983) Proc. of the 21th Annual ACL Meeting; Shriberg, E., (1994) Preliminaries to A Theory of Speech Disfluencies, , PhD-Thesis, University of California at Berkeley; Spilker, J., Klarner, M., Görz, G., Processing self-corrections in a speech-to-speech system (2000) Verbmobil: Foundations of Speech-to-Speech Translation, , Springer Verlag Berlin; Wang, Y., Waibel, A., Decoding algorithm in statistical machine translation (1997) Proc. of the 35th Annual Meeting of the ACL; Zechner, K., (2001) Automatic Summarization of Spoken Dialogues in Unrestricted Domains, , PhD-Thesis, Carnegie Mellon University},
sponsors={et al.; Office of Naval Research International Field Office (ONRG); State of Geneva; State of Valais; Swiss National Science Foundation; UBS SA},
publisher={International Speech Communication Association},
language={English},
abbrev_source_title={EUROSPEECH - Euro. Conf. Speech Commun. Technol.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Killer20033141,
author={Killer, M. and Stüker, S. and Schultz, T.},
title={Grapheme based speech recognition},
journal={EUROSPEECH 2003 - 8th European Conference on Speech Communication and Technology},
year={2003},
pages={3141-3144},
note={cited By 64; Conference of 8th European Conference on Speech Communication and Technology, EUROSPEECH 2003 ; Conference Date: 1 September 2003 Through 4 September 2003;  Conference Code:124334},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009230817&partnerID=40&md5=eb3d7ea739580f8c165a444e39d8efb0},
affiliation={Computer Engineering and Networks Laboratory, ETH Swiss Federal Institute of Technology Zürich, Switzerland; Institut für Logik, Komplexität und Deduktionssysteme, Karlsruhe Universität, Germany; Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA, United States},
abstract={Large vocabulary speech recognition systems traditionally represent words in terms of subword units, usually phonemes. This paper investigates the potential of graphemes acting as subunits. In order to develop context dependent grapheme based speech recognizers several decision tree based clustering procedures are performed and compared to each other. Grapheme based speech recognizers in three languages - English, German, and Spanish - are trained and compared to their phoneme based counterparts. The results show that for languages with a close grapheme-to-phoneme relation, grapheme based modeling is as good as the phoneme based one. Furthermore, multilingual grapheme based recognizers are designed to investigate whether grapheme based information can be successfully shared among languages. Finally, some bootstrapping experiments for Swedish were performed to test the potential for rapid language deployment.},
keywords={Decision trees;  Modeling languages;  Speech;  Speech communication, Context dependent;  Grapheme to phonemes;  Large vocabulary speech recognition;  Speech recognizer;  Sub-word units;  Swedishs;  Tree-based, Speech recognition},
references={Black, A., Lenzo, K., Pagel, V., Issues in building general letter to sound rules (1998) Proceedings of the ESCAWorkshop on Speech Synthesis, p. 7780. , Australia; Besling, S., Heuristical and statistical methods for grapheme-to-phoneme conversion (1994) Proceedings of Konvens, pp. 23-31. , Wien, Austria; Singh, R., Raj, B., Stern, R.M., Automatic generation of subword units for speech recognition systems (2002) IEEE Transactions on Speech and Audio Processing, 10, pp. 98-99; Kanthak, S., Ney, H., Context-dependent acoustic modeling using graphemes for large vocabulary speech recognition (2002) Proceedings of the ICASSP, pp. 845-848. , Orlando FL; Black, A., Font Llitjos, A., Unit selection without a phoneme set (2002) Proceedings of the IEEE TTS Workshop, , Santa Monica, CA; Schultz, T., Waibel, A., Language independent and language adaptive acoustic modeling for speech recognition (2001) Speech Communication, 35. , August; Schultz, T., Globalphone: A multilingual speech and text database developed at karlsruhe university (2002) Proceedings of the ICSLP, , Denver, CO; Finke, M., Geutner, P., Hild, H., Kemp, T., Ries, K., Westphal, M., The karlsruhe-verbmobil speech recognition engine (1997) Proceedings of the ICASSP, p. 8386. , Munich, Germany; Soltau, H., Metze, F., Fügen, C., Waibel, A., A one pass-decoder based on polymorphic linguistic context assignment (2001) Proceedings of the ASRU, Madonna di Campiglio Trento, , Italy, December; Beulen, K., Ney, H., Automatic question generation for decision tree based state tying (1998) Proceedings of the ICASSP, pp. 805-808. , Seattle, WA; Weingarten, R., (2003), http://www.ruediger-weingarten.de/Texte/Latinisierung.pdf, University of Osnabrück},
sponsors={et al.; Office of Naval Research International Field Office (ONRG); State of Geneva; State of Valais; Swiss National Science Foundation; UBS SA},
publisher={International Speech Communication Association},
language={English},
abbrev_source_title={EUROSPEECH - Euro. Conf. Speech Commun. Technol.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Stüker20031033,
author={Stüker, S. and Metze, F. and Schultz, T. and Waibel, A.},
title={Integrating multilingual articulatory features into speech recognition},
journal={EUROSPEECH 2003 - 8th European Conference on Speech Communication and Technology},
year={2003},
pages={1033-1036},
note={cited By 37; Conference of 8th European Conference on Speech Communication and Technology, EUROSPEECH 2003 ; Conference Date: 1 September 2003 Through 4 September 2003;  Conference Code:124334},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649264838&partnerID=40&md5=3eec6a0bc06785e30a6705a855c2ff03},
affiliation={Institut für Logik, Komplexität und Deduktionssysteme, Karlsruhe University, Germany; Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA, United States},
abstract={The use of articulatory features, such as place and manner of articulation, has been shown to reduce the word error rate of speech recognition systems under different conditions and in different settings. For example recognition systems based on features are more robust to noise and reverberation. In earlier work we showed that articulatory features can compensate for inter language variability and can be recognized across languages. In this paper we show that using cross- and multilingual detectors to support an HMM based speech recognition system significantly reduces the word error rate. By selecting and weighting the features in a discriminative way, we achieve an error rate reduction that lies in the same range as that seen when using language specific feature detectors. By combining feature detectors from many languages and training the weights discriminatively, we even outperform the case where only monolingual detectors are being used.},
keywords={Errors;  Feature extraction;  Reverberation;  Speech;  Speech communication, Articulatory features;  Error rate reduction;  Feature detector;  HMM-based;  Recognition systems;  Speech recognition systems;  Word error rate, Speech recognition},
references={Ostendorf, M., Moving beyond the 'beads-on-a-string' model of speech (1999) Proceedings of the ASRU, , Keystone, Colorado, USA, December; (1999) Handbook of the International Phonetic Association, , International Phonetic Association. Cambridge University Press; Deng, L., Sun, D.X., A statistical approach to automatic speech recognition using the atomic speech units constructed from overlapping articulatory features (1994) Journal of the Acoustical Society of America, 95. , May; Kirchhoff, K., Combining articulatory and acoustic information for speech recognition in noisy and reverberant environments (1998) Proceedings of the ICSLP, , December; Eide, E., Distinctive features for use in an automatic speech recognition system (2001) Proceedings of the EUROSPEECH, , Aalborg, Denmark; Metze, F., Waibel, A., A flexible stream architecture for ASR using articulatory features (2002) Proceedings of the ICSLP, , Denver, Colorado, USA, September; Stüker, S., Schultz, T., Metze, F., Waibel, A., Multilingual articulatory features (2003) Proceedings of the ICASSP, , Hong Kong, April; Schultz, T., Waibel, A., Language independent and language adaptive acoustic modeling for speech recognition (2001) Speech Communication, 35. , August; Beyerlein, P., Discriminative model combination (1998) Proceedings of the ICASSP, 1, pp. 481-484. , Seattle,Washington, USA, May; Soltau, H., Metze, F., Fügen, C., Waibel, A., A one pass-decoder based on polymorphic linguistic context assignment (2001) Proceedings of the ASRU, , Madonna di Campiglio Trento, Italy, December; Schultz, T., Globalphone: A multilingual speech and text database developed at Karlsruhe university (2002) Proceedings of the ICSLP, , Denver, Colorado, USA, September; Juang, B.H., Chou, W., Lee, C.H., (1995) Statistical and Discriminative Methods for Speech Recognition and Coding - New Advances and Trends, , Springer Verlag, Berlin- Heidelberg},
sponsors={et al.; Office of Naval Research International Field Office (ONRG); State of Geneva; State of Valais; Swiss National Science Foundation; UBS SA},
publisher={International Speech Communication Association},
language={English},
abbrev_source_title={EUROSPEECH - Euro. Conf. Speech Commun. Technol.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Fügen2003441,
author={Fügen, C. and Stüker, S. and Soltau, H. and Metze, F. and Schultz, T.},
title={Efficient handling of multilingual language models},
journal={2003 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2003},
year={2003},
pages={441-446},
doi={10.1109/ASRU.2003.1318481},
art_number={1318481},
note={cited By 20; Conference of IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2003 ; Conference Date: 30 November 2003 Through 4 December 2003;  Conference Code:114366},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947174416&doi=10.1109%2fASRU.2003.1318481&partnerID=40&md5=e8c961290c42075fd66413d99a701721},
affiliation={Interactive Systems Labs, University of Karlsruhe, Am Fasanengarten 5, Karlsruhe, 76131, Germany; Interactive Systems Labs, Carnegie Mellon University, School of Computer Science, Pittsburgh, PA  15221, United States},
abstract={In this paper we introduce techniques for building a multilingual speech recognizer. More specifically, we present a new language model method that allows for the combination of several monolingual into one multilingual language model. Furthermore, we extend our techniques to the concept of grammars. All linguistic knowledge sources share one common interface to the search engine. As a consequence, new language model types can be easily integrated into our Ibis decoder. Based on a multilingual acoustic model we compare multilingual statistical n-gram language models with multilingual grammars. Results are given in terms of recognition performance as well as resource requirements. They show that (a) n-gram LMs can be easily combined at the meta level without major loss in performance, (b) grammars are very suitable to model multilinguality, (c) language switches can be significantly reduced by using the introduced techniques, (d) the resource overhead for handling multiple languages in one language model is acceptable, and (e) language identification can be done implicitly during decoding. © 2003 IEEE.},
keywords={C (programming language);  Decoding;  Natural language processing systems;  Search engines;  Speech recognition, Common interfaces;  Language identification;  Linguistic knowledge;  Multilingual acoustic models;  Multiple languages;  N-gram language models;  Resource requirements;  Speech recognizer, Computational linguistics},
funding_details={Deutsche ForschungsgemeinschaftDeutsche Forschungsgemeinschaft, DFG},
funding_details={European CommissionEuropean Commission, EC, IST-2000-28323},
references={Hazen, T.J., Hetherington, I.L., Park, A., FST-based recognition techniques for multi-lingual and multi-domain spontaneous speech (2001) Proceedings of the Eurospeech 2001, , Aalborg, Denmark, September; Harbeck, S., Nöth, E., Niemann, H., Multilingual speech recognition (1997) Proceedings of the 2nd SQEL Workshop on Multi-lingual Information Retrieval Dialogs, , Pilzen, Czech Republic; Weng, F., Bratt, H., Neumeyer, L., Stolke, A., A study of multilingual speech recognition (1997) Proceedings of the Eurospeech, , Rhodes, Greece, September; Ward, T., Roukos, S., Neti, C., Epstein, M., Dharanipragada, S., Towards speech understanding across multiple languages (1998) Proceedings of the ICSLP, , Sydney, Australia, November; Wang, Z., Topkara, U., Schultz, T., Waibel, A., Towards universal speech recognition (2002) Proceedings of the ICMI, , Pittsburgh; Fügen, C., Westphal, M., Schneider, M., Schultz, T., Waibel, A., LingWear: A mobile tourist information system (2000) Proceedings of the Human Language Technology Meeting (HLT-2000), , San Diego, USA, March; Schultz, T., Waibel, A., Language independent and language adaptive acoustic modeling for speech recognition (2001) Speech Communication, 35. , August; Soltau, H., Metze, F., Fügen, C., Waibel, A., A one pass-decoder based on polymorphic linguistic context assignment (2001) Proceedings of the ASRU, , Madonna di Campiglio Trento, Italy, December},
sponsors={},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={0780379802; 9780780379800},
language={English},
abbrev_source_title={IEEE Workshop Autom. Speech Recognit. Underst., ASRU},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Waibel2003369,
author={Waibel, A. and Badran, A. and Black, A.W. and Frederking, R. and Gates, D. and Lavie, A. and Levin, L. and Lenzo, K. and Tomokiyo, L.M. and Reichert, J. and Schultz, T. and Wallace, D. and Woszczyna, M. and Zhang, J.},
title={Speechalator: Two-way speech-to-speech translation on a consumer PDA},
journal={EUROSPEECH 2003 - 8th European Conference on Speech Communication and Technology},
year={2003},
pages={369-372},
note={cited By 22; Conference of 8th European Conference on Speech Communication and Technology, EUROSPEECH 2003 ; Conference Date: 1 September 2003 Through 4 September 2003;  Conference Code:124334},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009164517&partnerID=40&md5=6adcf5f1b7d6167c4f871d37fe756e34},
affiliation={Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Cepstral, LLC, United States; Multimodal Technologies Inc., United States; Mobile Technologies Inc., United States},
abstract={This paper describes a working two-way speech-to-speech translation system that runs in near real-time on a consumer handheld computer. It can translate from English to Arabic and Arabic to English in the domain of medical interviews. We describe the general architecture and frameworks within which we developed each of the components: HMM-based recognition, interlingua translation (both rule and statistically based), and unit selection synthesis.},
keywords={Hand held computers;  Speech, General architectures;  HMM-based;  Near-real time;  Speech-to-speech translation;  Two ways;  Unit selection, Speech communication},
references={Lavie, A., Levin, L., Schultz, T., Waibel, A., Domain portability in speech-to-speech translation (2001) HLT2001, , San Diego, California; Black, A., Brown, R., Frederking, R., Singh, R., Moody, J., Steinbrecher, E., Tongues: Rapid development of a speech-to-speech translation system (2002) HLT2002, pp. 2051-2054. , San Diego, California; Schultz, T., Waibel, A., Language independent and language adaptive acoustic modeling for speech recognition (2001) Speech Communication, 35 (1-2), pp. 31-51; Kirchhoff, K., (2003) Novel Speech Recognition Models for Arabic, , Technical report, Johns Hopkins University; Billa, J., Noamany, M., Srivasta, A., Liu, D., Stone, R., Xu, J., Makhoul, J., Kubala, F., Indexing of Arabic broadcast news (2003) ICASSP, , Orlando, Florida; (1997) Callhome Egyptian Arabic Speech, , Linguistic Data Consortium; Lavie, A., A multi-perspective evaluation of the NESPOLE! speech-to-speech translation system (2002) Proceedings of ACL 2002 Workshop on Speech-to-speech Translation: Algorithms and Systems, , Philadelphia, PA; Levin, L., Gates, D., Wallace, D., Peterson, K., Lavie, A., Pianesi, F., Pianta, E., Mana, N., Balancing expressiveness and simplicity in an interlingua for task based dialogue (2002) Proceedings of ACL 2002 Workshop on Speech-to-speech Translation: Algorithms and Systems, , Philadelphia, PA; Black, A., Taylor, P., Caley, R., (1998) The Festival Speech Synthesis System, , http://festvox.org/festival; Black, A., Lenzo, K., Optimal data selection for unit selection synthesis (2001) 4th ESCA Workshop on Speech Synthesis, , Scotland; Logan, J., Greene, B., Pisoni, D., Segmental intelligibility of synthetic speech produced by rule (1989) Journal of the Acoustical Society of America, 86 (2), pp. 566-581; Sarich, A., (2001) Phraselator, One-way Speech Translation System, , http://www.sarich.com/translator},
sponsors={et al.; Office of Naval Research International Field Office (ONRG); State of Geneva; State of Valais; Swiss National Science Foundation; UBS SA},
publisher={International Speech Communication Association},
language={English},
abbrev_source_title={EUROSPEECH - Euro. Conf. Speech Commun. Technol.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhao2002,
author={Zhao, B. and Schultz, T.},
title={Toward robust parametric trajectory segmental model for vowel recognition},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2002},
volume={4},
pages={IV/4165},
note={cited By 4; Conference of 2002 IEEE International Conference on Acoustic, Speech, and Signal Processing ; Conference Date: 13 May 2002 Through 17 May 2002;  Conference Code:59257},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036288697&partnerID=40&md5=7af640af0f48890a0489cc6c562f2892},
affiliation={Carnegie Mellon University, Pittsburgh, PA, United States},
abstract={In this paper we present a robust and discriminative segmental trajectory modeling for vowel recognition. We proposed two new approaches. One is using weighted least square estimation for the parametric trajectory parameter, which gives a much more robust performance over traditional least square estimation approach. The other is a specifically designed transformation matrix proposed to reduce the possible mismatch between the Gaussian modeling assumption and the trajectory feature's nature. Our experiments on the vowel classification using the mobile phone data of SpeechDAT(II) MDB showed significant improvement over both standard HMM and traditional segmental modeling.},
keywords={Gaussian modeling assumption;  Least square estimation;  Parametric trajectory segmental model;  Vowel recognition, Computer simulation;  Feature extraction;  Least squares approximations;  Mathematical transformations;  Mobile telecommunication systems;  Parameter estimation;  Speech, Speech recognition},
correspondence_address1={Zhao, B.; Carnegie Mellon University, Pittsburgh, PA, United States},
sponsors={IEEE},
address={Orlando, FL},
issn={15206149},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schultz2002345,
author={Schultz, T.},
title={Globalphone: A multilingual speech and text database developed at Karlsruhe University},
journal={7th International Conference on Spoken Language Processing, ICSLP 2002},
year={2002},
pages={345-348},
note={cited By 86; Conference of 7th International Conference on Spoken Language Processing, ICSLP 2002 ; Conference Date: 16 September 2002 Through 20 September 2002;  Conference Code:124333},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009274666&partnerID=40&md5=ec71f50c61e1369bbae5c71ac950916d},
affiliation={Interactive Systems Laboratories, Karlsruhe University Germany, Carnegie Mellon University, United States},
abstract={This paper describes the design, collection, and current status of the multilingual database GlobalPhone, an ongoing project since 1995 at Karlsruhe University. GlobalPhone is a highquality read speech and text database in a large variety of languages which is suitable for the development of large vocabulary speech recognition systems in many languages. It has already been successfully applied to language independent and language adaptive speech recognition. GlobalPhone currently covers 15 languages Arabic, Chinese (Mandarin and Shanghai), Croatian, Czech, French, German, Japanese, Korean, Portuguese, Russian, Spanish, Swedish, Tamil, and Turkish. The corpus contains more than 300 hours of transcribed speech spoken by more than 1500 native, adult speakers and will soon be available from ELRA.},
keywords={Character recognition;  Database systems;  Speech, Croatians;  Current status;  High quality;  Language independents;  Large vocabulary speech recognition;  Multilingual database;  Swedishs;  Text database, Speech recognition},
references={Schultz, T., Waibel, A., Language independent and language adaptive acoustic modeling for speech recognition (2001) Speech Communication, 35 (1-2), pp. 31-51. , August; Vaufreydaz, D., Bergamini, C., Serignat, J.F., Besacier, L., Akbar, M., A new methodology for speech corpora definition from internet documents (2000) Proceedings of the LREC 2000, , Athens, Greece; European Language Resources Association (ELRA), , http://www.icp.grenet.fr/ELRA/home.html},
correspondence_address1={Schultz, T.; Interactive Systems Laboratories, Karlsruhe University Germany, Carnegie Mellon UniversityUnited States; email: tanja@cs.cmu.edu},
sponsors={Acoustical Society of America (ASA); Institute of Electrical and Electronics Engineers, Inc., (IEEE), Signal Processing Society; International Phonetic Association (IPA); International Speech Communications Association (ISCA); Linguistic Society of Americs (LSA)},
publisher={International Speech Communication Association},
language={English},
abbrev_source_title={Int. Conf. Spok. Lang. Process., ICSLP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jin20021345,
author={Jin, Q. and Schultz, T. and Waibel, A.},
title={Phonetic speaker identification},
journal={7th International Conference on Spoken Language Processing, ICSLP 2002},
year={2002},
pages={1345-1348},
note={cited By 16; Conference of 7th International Conference on Spoken Language Processing, ICSLP 2002 ; Conference Date: 16 September 2002 Through 20 September 2002;  Conference Code:124333},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009250217&partnerID=40&md5=0f31880de237dd960b4094656696ca99},
affiliation={Interactive Systems Laboratory, Carnegie Mellon University, United States},
abstract={This paper describes the exploration of text-independent speaker identification using novel approaches based on speakers' phonetic features instead of traditional acoustic features. Different phonetic speaker identification approaches are discussed in this paper and evaluated using two speaker identification systems: one multilingual system and one single language multiple-engine system. Furthermore, textindependent speaker identification experiments are carried out on a distant-microphone database as well as gender identification experiments are investigated on the NIST 1999 Speaker Recognition Evaluation dataset. The results show that phonetic features are powerful for speaker identification and gender identification.},
keywords={Linguistics;  Loudspeakers, Acoustic features;  Gender identification;  Multilingual system;  Phonetic features;  Speaker identification;  Speaker identification systems;  Speaker recognition evaluations;  Text-independent speaker identification, Speech recognition},
references={Campbell, J.P., Jr., Speaker recognition: A tutorial (1997) Proceeding of the IEEE, IEEE, 85 (9), pp. 1437-1462. , Sept; Kohler, M.A., Andrews, W.D., Campbell, J.P., Hernandez-Cordero, J., Phonetic refraction for speaker recognition (2001) Proceedings of Workshop on Multilingual Speech and Language Processing, , Aalborg, Denmark, September; Andrews, W.D., Kohler, M.A., Campbell, J.P., Godfrey, J.J., Phonetic, idiolectal, and acoustic speaker recognition (2001) Proceedings of Odyssey Workshop; Doddington, G., (2001) Some Experiments on Idiolectal Differences Among Speakers, , http://www.nist.gov/speech/tests/spk/2001/doc, January; Jin, Q., Schultz, T., Waibel, A., Speaker identification using multilingual phone strings (2002) Proceedings of IEEE ICASSP, , Orlando, Florida; Schultz, T., Jin, Q., Laskowski, K., Tribble, A., Waibel, A., Speaker, accent and language identification using multilingual phone strings (2002) Proceedings of Human Language Technologies Conference, , San Diego, California; Schultz, T., Waibel, A., Language independent and language adaptive acoustic modeling for speech recognition (2001) Speech Communication, 35 (1-2), pp. 31-51. , August},
sponsors={Acoustical Society of America (ASA); Institute of Electrical and Electronics Engineers, Inc., (IEEE), Signal Processing Society; International Phonetic Association (IPA); International Speech Communications Association (ISCA); Linguistic Society of Americs (LSA)},
publisher={International Speech Communication Association},
language={English},
abbrev_source_title={Int. Conf. Spok. Lang. Process., ICSLP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jin2002,
author={Jin, Q. and Schultz, T. and Waibel, A.},
title={Speaker identification using multilingual phone strings},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2002},
volume={1},
pages={I/145-I/148},
doi={10.1109/icassp.2002.5743675},
note={cited By 14; Conference of 2002 IEEE International Conference on Acustics, Speech, and Signal Processing ; Conference Date: 13 May 2002 Through 17 May 2002;  Conference Code:59254},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036296865&doi=10.1109%2ficassp.2002.5743675&partnerID=40&md5=139d416bca560af5fbc3e25386618032},
affiliation={Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
abstract={Far-field speaker identification is very challenging since varying recording conditions often result in un-matching training and testing situations. Although the widely used Gaussian Mixture Models (GMM) approach achieves reasonable good results when training and testing conditions match, its performance degrades dramatically under un-matching conditions. In this paper we propose a new approach for far-field speaker identification: the usage of multilingual phone strings derived from phone recognizers in eight different languages. The experiments are carried out on a database of 30 speakers recorded with eight different microphone distances. The results show that the multi-lingual phone string approach is robust against un-matching conditions and significantly outperforms the GMMs. On 10-second test chunks, the average closed-set identification performance achieves 96.7% on variable distance data.},
keywords={Acoustic signal processing;  Database systems;  Mathematical models;  Speech analysis;  Speech communication;  Telephone systems, Far-field speaker identification;  Gaussian mixture model;  Multilinqual phone strings;  Pronunciation, Speech recognition},
references={Campbell J.P., Jr., Speaker recognition: A tutorial (1997) Proceeding of the IEEE, IEEE, 85 (9), pp. 1437-1462. , Sept; Gish, H., Schmidt, M., Text-independent speaker identification (1994) IEEE Signal Processing Magazine, pp. 1437-1462. , IEEE; Oct; Reynolds, D.A., Rose, R.C., Robust text-independent speaker identification using Gaussian mixture speaker models (1995) IEEE Transactions on Speech and Audio Processing, 3 (1). , January; Zissman, M.A., Singer, E., Automatic language identification of telephone speech messages using phone recognition and N-gram modeling (1994) Proceedings of IEEE ICASSP, 1, pp. 305-308. , Minneapolis, USA; Zissman, M.A., Language identification using phone recognition and phonotactic language modeling (1995) Proceedings of ICASSP, 5, pp. 3503-3506. , Detroit, MI, May; Kohler, M.A., Andrews, W.D., Compbell, J.P., Hernandez-Cordero, J., Phonetic refraction for speaker recognition Proceedings of Workshop on Multilingual Speech and Language Processing, Aalborg, Denmark, September 2001; Schultz, T., Waibel, A., Language independent and language adaptive acoustic modeling for speech recognition (2001) Speech Communication, 35 (1-2), pp. 31-51. , August},
correspondence_address1={Jin, Q.; Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: qjin@cs.cmu.edu},
sponsors={IEEE},
publisher={Institute of Electrical and Electronics Engineers Inc.},
address={Orlando, FL},
issn={15206149},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2002247,
author={Wang, Z. and Topkara, U. and Schultz, T. and Waibel, A.},
title={Towards universal speech recognition},
journal={Proceedings - 4th IEEE International Conference on Multimodal Interfaces, ICMI 2002},
year={2002},
pages={247-252},
doi={10.1109/ICMI.2002.1167001},
art_number={1167001},
note={cited By 24; Conference of 4th IEEE International Conference on Multimodal Interfaces, ICMI 2002 ; Conference Date: 14 October 2002 Through 16 October 2002;  Conference Code:116170},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963837901&doi=10.1109%2fICMI.2002.1167001&partnerID=40&md5=8e566d4e3b8fa68893a7ef09c3d4f25e},
affiliation={Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA  15213, United States},
abstract={The increasing interest in multilingual applications like speech-to-speech translation systems is accompanied by the need for speech recognition front-ends in many languages that can also handle multiple input languages at the same time. We describe a universal speech recognition system that fulfills such needs. It is trained by sharing speech and text data across languages and thus reduces the number of parameters and overhead significantly at the cost of only slight accuracy loss. The final recognizer eases the burden of maintaining several monolingual engines, makes dedicated language identification obsolete and allows for code-switching within an utterance. To achieve these goals we developed new methods for constructing multilingual acoustic models and multilingual n-gram language models. © 2002 IEEE.},
author_keywords={data-driven;  IPA;  Multilingual acoustic modeling;  Multilingual n-gram language modeling},
keywords={Computational linguistics;  Interactive computer systems;  Modeling languages;  Natural language processing systems;  Speech, Code-switching;  Data driven;  Language identification;  Multilingual acoustic models;  Multiple inputs;  N-gram language models;  Speech recognition systems;  Speech-to-speech translation, Speech recognition},
references={Waibel, A., Soltau, H., Schultz, T., Schaaf, T., Metze, F., Multilingual Speech Recognition (2000) Verbmobil: Foundations of Speech-to-Speech Translation, , W. Wahlster (Ed.), Springer Verlag; Schultz, T., Waibel, A., Language Independent and Language Adaptive Acoustic Modeling (2001) Speech Communication, 35 (1-2), pp. 31-51. , August; Harbeck, S., Nöth, E., Niemann, H., Multilingual Speech Recognition (1997) SQEL, 2nd Workshop on Multi-Lingual Information Retrieval Dialogs, , Plzeň, Czech Republic, April; Weng, F., Bratt, H., Neumeyer, L., Stolke, A., A Study of Multilingual Speech Recognition (1997) EUROSPEECH, , Rhodos, Greece, September; Ward, T., Roukos, S., Neti, C., Epstein, M., Dharanipragada, S., Towards Speech Understandig across Multiple Languages (1998) ICSLP, , Sydney, Australia, November; The International Phonetic Association (revised to 1993) IPA Chart (1993) Journal of the International Phonetic Association, 23. , 1993},
sponsors={IEEE Computer Society Technical Committee on Wearable Information Systems (TCWIS)},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={0769518346; 9780769518343},
language={English},
abbrev_source_title={Proc. - IEEE Int. Conf. Multimodal Interfaces, ICMI},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Waibel2001597,
author={Waibel, A. and Bett, M. and Metze, F. and Ries, K. and Schaaf, T. and Schultz, T. and Soltau, H. and Yu, H. and Zechner, K.},
title={Advances in automatic meeting record creation and access},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2001},
volume={1},
pages={597-600},
note={cited By 80; Conference of 2001 IEEE Interntional Conference on Acoustics, Speech, and Signal Processing ; Conference Date: 7 May 2001 Through 11 May 2001;  Conference Code:58541},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034842455&partnerID=40&md5=218fe827005ea7c3bc60fba6e98e3070},
affiliation={Interactive Systems Labs, Carnegie Mellon University, NSH 2602, Pittsburgh, PA 15217, United States},
abstract={Oral communication is transient but many important decisions, social contracts and fact 'ndings are 'rst carried out in an oral setup, documented in written form and later retrieved. At Carnegie Mellons University s Interactive Systems Laboratories we have been experimenting with the documentation of meetings. This paper summarizes part of the progress that we have made in this test bed, speci'cally on the question of automatic transcription using LVCSR, information access using non-keyword based methods, summarization and user interfaces. The system is capable to automatically construct a searchable and browsable audiovisual database of meetings and provide access to these records.},
keywords={Automatic meeting record creation;  Automatic transcription;  Non keyword based methods;  Transforming oral communications, Database systems;  Information retrieval systems;  Speech coding;  Speech processing;  System program documentation, Speech communication},
references={Waibel, A., Bett, M., Finke, M., Meeting browser: Tracking and summarising meetings Proceedings of the DARPA Broadcast News Workshop, 1998; Bett, M., Gross, R., Yu, H., Zhu, X., Pan, Y., Yang, J., Waibel, A., Multimodal meeting tracker Proceedings of RIAO2000, Paris, France, April 2000; Stiefelhagen, R., Yang, J., Waibel, A., Simultaneous tracking of head poses in a panoramic view International Conference on Pattern Recognition (ICPR), Barcelona, Spain, September 2000; Garofolo, J., Auzanne, C., Voorhees, E., The TREC spoken document retrieval track: A success story (1999) Text Retrieval Conference (TREC) 8, pp. 16-19. , E. Voorhees, Ed., Gaithersburg, Maryland, USA; November; Kubala, F., Colbath, S., Liu, D., Makhoul, J., Rough n ready: A meeting recorder and browser (1999) ACM Computing Surveys, 31 (7). , September; Article No. 7; Choi, J., Hindle, D., Pereira, F., Singhal, A., Whittaker, S., Spoken content-based audio navigation (SCAN) Proceedings of the ICPhS-99, 1999; Wactlar, H.D., Auto summarization and visualization across multiple video documents and libraries http://www.informedia.cs.cmu.edu/dli2/; Abowd, G.D., Classroom 2000: An experiment with the instrumentation of a living educational environment (1999) IBM Systems Journal, Special Issue on Pervasive Computing, 38 (4), pp. 508-530. , October; Arons, B., Speechskimmer: A system for interactively skimming recorded speech (1997) ACM Transactions on Computer Human Interaction, 4 (1), pp. 3-28. , March; Yu, H., Finke, M., Waibel, A., Progress in automatic meeting transcription Proceedings of the EUROSPEECH, September 1999; Yu, H., Tomokiyo, T., Wang, Z., Waibel, A., New developments in automatic meeting transcription Proceedings of the ICSLP, Beijing, China, October 2000; Fritsch, J., Finke, M., Waibel, A., Effective structural adaptation of lvcsr systems to unseen domains using hierarchical connectionist acoustic models Proceedings of the ICSLP, Sydney, Australia, 1998; Schultz, T., Waibel, A., Polyphone decision tree specialization for language adaptation Proceedings of the ICASSP, Instanbul, Turkey, June 2000; Zeppenfeld, T., Finke, M., Ries, K., Waibel, A., Recognition of conversational telephone speech using the Janus speech engine Proceedings of the ICASSP'97, München, Germany, 1997; Waibel, A., Soltau, H., Schultz, T., Schaaf, T., Metze, F., Multilingual speech recognition (2000) Verbmobil: Foundations of Speech-to-Speech Translation, , Springer-Verlag; Bahktin, M.M., (1986) Speech Genres and Other Late Essays, , chapter Speech Genres, University of Texas Press, Austin; Ries, K., Towards the detection and description of textual meaning indicators in spontaneous conversations Proceedings of the Eurospeech, Budapest, Hungary, September 1999, 3, pp. 1415-1418; Ries, K., HMM and neural network based speech act classication Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing, Phoenix, AZ, March 1999, 1, pp. 497-500; Finke, M., Lapata, M., Lavie, A., Levin, L., Tomokiyo, L.M., Polzin, T., Ries, K., Zechner, K., Clarity: Automatic discourse and dialogue analysis for a speech and natural language processing system AAAI Spring Symposium on Applying Machine Learning to Discourse Processing, March 1998; Ries, K., Levin, L., Valle, L., Lavie, A., Waibel, A., Shallow discourse genre annotation in callhome spanish Proceedings of the International Conference on Language Resources and Evaluation (LREC-2000), Athens, Greece, May 2000; Polzin, T.S., Waibel, A., Detecting emotions in speech Proceedings of the CMC, 1998; Zechner, K., Waibel, A., DIASUMM: Flexible summarization of spontaneous dialogues in unrestricted domains Proceedings of COLING, Saarbrücken, Germany, 2000; Treebank-3: CD-ROM containing databases of dis'uency annotated switchboard transcripts (LDC99T42) (1999) Linguistic Data Consortium (LDC); Brill, E., Some advances in transformation-based part of speech tagging Proceedings of AAAI-94, 1994; Quinlan, J.R., (1992) C4.5: Programs for Machine Learning, , Morgan Kaufmann, San Mateo, CA; Carbonell, J., Goldstein, J., The use of MMR, diversity-based reranking for reordering documents and producing summaries Proceedings of the 21st ACM-SIGIR International Conference on Research and Development in Information Retrieval, Melbourne, Australia, 1998; Zechner, K., Waibel, A., Minimizing word error rate in textual summaries of spoken language (2000) Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics, NAACL-2000, Seattle, WA, April/May,, pp. 186-193; Hearst, M.A., Texttiling: Segmenting text into multi-paragraph subtopic passages (1997) Computational Linguistics, 23 (1), pp. 33-64. , March; Manke, S., Finke, M., Waibel, A., Npen++: A writer independent, large vocabulary on-line cursive hand-writing recognition system (1995) ICDAR},
correspondence_address1={Waibel, A.; Interactive Systems Labs, Carnegie Mellon University, NSH 2602, Pittsburgh, PA 15217, United States},
sponsors={IEEE},
address={Salt Lake, UT},
issn={15206149},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Schultz200131,
author={Schultz, T. and Waibel, A.},
title={Language-independent and language-adaptive acoustic modeling for speech recognition},
journal={Speech Communication},
year={2001},
volume={35},
number={1-2},
pages={31-51},
doi={10.1016/S0167-6393(00)00094-7},
note={cited By 248},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035426931&doi=10.1016%2fS0167-6393%2800%2900094-7&partnerID=40&md5=96591a67ff2632d2296e3e15dddced5a},
affiliation={Interactive Systems Laboratories, University of Karlsruhe, 76131 Karlsruhe, Germany; Interactive Systems Laboratories, Carnegie Mellon University, United States},
abstract={With the distribution of speech technology products all over the world, the portability to new target languages becomes a practical concern. As a consequence our research focuses on the question of how to port large vocabulary continuous speech recognition (LVCSR) systems in a fast and efficient way. More specifically we want to estimate acoustic models for a new target language using speech data from varied source languages, but only limited data from the target language. For this purpose, we introduce different methods for multilingual acoustic model combination and a polyphone decision tree specialization procedure. Recognition results using language-dependent, independent and language-adaptive acoustic models are presented and discussed in the framework of our GlobalPhone project which investigates LVCSR systems in 15 languages. © 2001 Published by Elsevier Science B.V.},
author_keywords={GlobalPhone;  Language portability;  Large vocabulary continuous speech recognition;  Multilingual acoustic models;  Polyphone decision tree specialization (PDTS)},
keywords={Computer simulation;  Decision theory;  Linguistics;  Speech analysis;  Trees (mathematics), Language probability;  Multilingual acoustic models;  Polyphone decision tree specialization, Speech recognition},
references={Andersen, O., Dalsgaard, P., Language identification based on cross-language acoustic models and optimised information combination (1997) Proceedings Eurospeech, , Rhodes, 1997; Andersen, O., Dalsgaard, P., Barry, W., Data-driven identification of poly- and mono-phonemes for four European languages (1993) Proceedings Eurospeech, pp. 759-762. , Berlin, 1993; Barnett, J., Corrada, A., Gao, G., Gillik, L., Ito, Y., Lowe, S., Manganaro, L., Peskin, B., Multilingual speech recognition at Dragon systems (1996) Proceedings ICSLP, pp. 2191-2194. , Philadelphia, 1996; Bonaventura, P., Gallocchio, F., Micca, G., Multilingual speech recognition for flexible vocabularies (1997) Proceedings Eurospeech, pp. 355-358. , Rhodes, 1997; Bub, U., Köhler, J., Imperl, B., In-service adaptation of multilingual Hidden-Markov-Models (1997) Proceedings ICASSP, pp. 1451-1454. , Munich, 1997; Billa, J., Ma, K., McDonough, J., Zavaliagkos, G., Miller, D.R., Ross, K.N., El-Jaroudi, A., Multilingual speech recognition: The 1996 Byblos Callhome system (1997) Proceedings Eurospeech, pp. 363-366. , Rhodes, 1997; Çarki, K., Geutner, P., Schultz, T., Turkish LVCSR: Towards better speech recognition for agglutinative languages (2000) Proceedings ICASSP, pp. 1563-1566. , Istanbul, 2000; Cohen, P., Dharanipragada, S., Gros, J., Monkowski, M., Neti, C., Roukos, S., Ward, T., Towards a universal speech recognizer for multiple languages (1997) Proceedings Automatic Speech Recognition and Understanding (ASRU), pp. 591-598. , St. Barbara, CA, 1997; Constantinescu, A., Chollet, G., On cross-language experiments and data-driven units for ALISP (1997) Proceedings Automatic Speech Recognition and Understanding (ASRU), pp. 606-613. , St. Barbara, CA, 1997; Corredor-Ardoy, C., Gauvain, J.L., Adda-Decker, M., Lamel, L., Language identification with language-independent acoustic models (1997) Proceedings Eurospeech, pp. 355-358. , Rhodes, 1997; Dugast, C., Aubert, X., Kneser, R., The Philips large-vocabulary recognition system for American English, French, and German (1995) Proceedings Eurospeech, pp. 197-200. , Madrid, 1995; http://www.icp.grenet.fr/ELRA/home.html; Finke, M., Rogina, I., Wide context acoustic modeling in read vs. spontaneous speech (1997) Proceedings ICASSP, pp. 1743-1746. , Munich, 1997; Finke, M., Geutner, P., Hild, H., Kemp, T., Ries, K., Westphal, M., The Karlsruhe-Verbmobil speech recognition engine (1997) Proceedings ICASSP, pp. 83-86. , Munich, 1997; Glass, J., Flammia, G., Goodine, D., Phillips, M., Polifroni, J., Sakai, S., Seneff, S., Zue, V., Multilingual spoken language understanding in the MIT Voyager system (1995) Speech Communication, 17, pp. 1-18; Gokcen, S., Gokcen, J., A multilingual phoneme and model set: Towards a universal base for automatic speech recognition (1997) Proceedings Automatic Speech Recognition and Understanding (ASRU), pp. 599-603. , St. Barbara, CA, 1997; Hieronymus, J.L., ASCII phonetic symbols for the world's languages Worldbet (1993) J. Int. Phonetic Assoc., 23; The International Phonetic Association (revised to 1993) - IPA Chart J. Int. Phonetic Assoc., 23; Kiecza, D., Schultz, T., Waibel, A., Data-driven determination of appropriate dictionary units for Korean LVCSR (1999) Proceedings International Conference on Speech Processing (ICSP), pp. 323-327. , Seoul, 1999; Köhler, J., Language adaptation of multilingual phone models for vocabulary independent speech recognition tasks (1998) Proceedings ICASSP, pp. 417-420. , Seattle, 1998; Lamel, L., Adda-Decker, M., Gauvain, J.L., Issues in large vocabulary multilingual speech recognition (1995) Proceedings Eurospeech, pp. 185-189. , Madrid, 1995; Leggetter, C., Woodland, P., Maximum likelihood linear regression for speaker adaptation of continuous density Hidden Markov Models (1995) Comput. Speech Language, 9, pp. 171-185; (2000), http://www.ldc.upenn.edu; Osterholtz, L., Augustine, C., McNair, A., Rogina, I., Saito, H., Sloboda, T., Tebelskis, J., Woszczyna, M., Testing generality in JANUS: A multi-lingual speech translation system (1992) Proceedings ICASSP, , San Francisco, 1992; Reichert, J., Schultz, T., Waibel, A., Mandarin large vocabulary speech recognition using the GlobalPhone database (1999) Proceedings Eurospeech, pp. 815-818. , Budapest, 1999; Schultz, T., (2000) Multilinguale Spracherkennung: Kombination akustischer Modelle zur Portierung auf neue Sprachen, , Dissertation, Universität Karlsruhe, Institut für Logik, Komplexität und Deduktionssysteme, 2000; Schultz, T., Waibel, A., Fast bootstrapping of LVCSR Systems with multilingual phoneme sets (1997) Proceedings Eurospeech, pp. 371-374. , Rhodes, 1997; Schultz, T., Waibel, A., Multilingual and crosslingual speech recognition (1998) Proceedings DARPA Workshop on Broadcast News Transcription and Understanding, pp. 259-262. , Lansdowne, VA, 1998; Schultz, T., Waibel, A., Adaptation of pronunciation dictionaries for recognition of unseen languages (1998) Proceedings SPIIRAS International Workshop on Speech and Computer, pp. 207-210. , St. Petersburg, 1998; Schultz, T., Waibel, A., Language independent and language adaptive LVCSR (1998) Proceedings ICSLP, pp. 1819-1822. , Sydney, 1998; Schultz, T., Westphal, M., Waibel, A., The GlobalPhone project: Multilingual LVCSR with Janus-3 (1997) Proceedings SQEL, Second Workshop on Multi-lingual Information Retrieval Dialogs, pp. 20-27. , Plzeň, 1997; (1992) New Encyclopedic Dictionary, , Black, Dog & Leventhal; Wells, C.J., Computer-coded phonemic notation of individual languages of the European community (1989) J. Int. Phonetic Assoc., 19, pp. 32-54; Wheatley, B., Kondo, K., Anderson, W., Muthusamy, Y., An evaluation of cross-language adaptation for rapid HMM development in a new language (1994) Proceedings ICASSP, pp. 237-240. , Adelaide, 1994; Young, S.J., Adda-Decker, M., Aubert, X., Dugast, C., Gauvain, J.L., Kershaw, D.J., Lamel, L., Woodland, P.C., Multilingual large vocabulary speech recognition: The European SQALE project (1997) Comput. Speech Language, 11, pp. 73-89},
correspondence_address1={Schultz, T.; Interactive Systems Laboratories, University of Karlsruhe, 76131 Karlsruhe, Germany; email: tanja@ira.uka.de},
issn={01676393},
coden={SCOMD},
language={English},
abbrev_source_title={Speech Commun},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Schultz20012721,
author={Schultz, T. and Waibel, A.},
title={Experiments on cross-language acoustic modeling},
journal={EUROSPEECH 2001 - SCANDINAVIA - 7th European Conference on Speech Communication and Technology},
year={2001},
pages={2721-2724},
note={cited By 46; Conference of 7th European Conference on Speech Communication and Technology - Scandinavia, EUROSPEECH  2001 ; Conference Date: 3 September 2001 Through 7 September 2001;  Conference Code:124332},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009101138&partnerID=40&md5=f0c08b14ab06525f3c52e8fb12c13fc9},
affiliation={Interactive Systems Laboratories, Carnegie Mellon University, United States; Interactive Systems Laboratories, University of Karlsruhe, Germany},
abstract={With the distribution of speech products all over the world, the portability to new target languages becomes a practical concern. As a consequence our research focuses on rapid transfer of LVCSR systems to other languages. In former studies we evaluated the performance if limited adaptation data is available. Particularly for very time constrained tasks and minority languages, it is even reasonable that no training data is available at all. In this paper we examine what performance can be expected in this scenario. All experiments are run in the framework of the GlobalPhone project which investigates LVCSR systems in 15 languages.},
keywords={Speech communication, Acoustic model;  Constrained tasks;  Cross languages;  Minority languages;  Research focus;  Target language;  Training data, Modeling languages},
references={Constantinescu, A., Chollet, G., On cross- language experiments and data-driven units for ALISP (1997) Proc. ASRU, pp. 606-613. , St. Barbara, CA; Gokcen, S., Gokcen, J.M., A multilingual phoneme and model set: Towards a universal base for automatic speech recognition (1997) Proc. ASRU, pp. 599-603. , St. Barbara, CA; Kohler, J., Language adaptation of multilingual phone models for vocabulary independent speech recognition tasks (1998) Proc. ICASSP, pp. 417-420. , Seattle; Osterholtz, L., Augustine, C., McNair, A., Rogina, I., Saito, H., Sloboda, T., Tebelskis, J., Woszczyna, M., Testing generality in JANUS: A multilingual speech translation system (1992) Proc. ICASSP, , San Francisco, CA; Schultz, T., Waibel, A., Fast bootstrapping of LVCSR Systems with multilingual phoneme sets (1997) Proc. Eurospeech, pp. 371-374. , Rhodes; Schultz, T., Waibel, A., Language independent and language adaptive LVCSR (1998) Proc. ICSLP, pp. 1819-1822. , Sydney; Schultz, T., Waibel, A., Polyphone decision tree specialization for language adaptation (2000) Proc. ICASSP, , Istanbul; Wheatley, B., Kondo, K., Anderson, W., Muthusamy, Y., An evaluation of cross-language adaptation for Rapid HMM Development in a new language (1994) Proc. ICASSP, pp. 237-240. , Adelaide},
editor={Lindberg B., Benner H., Dalsgaard P., Tan Z.-H.},
sponsors={Aalborg City Council; Aalborg University; Det Obelske Familiefond; ELRA/ELDA; ELSNET; et al.},
publisher={International Speech Communication Association},
isbn={8790834100; 9788790834104},
language={English},
abbrev_source_title={EUROSPEECH - SCANDINAVIA - Euro. Conf. Speech Commun. Technol.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schultz20001707,
author={Schultz, T. and Waibel, A.},
title={Polyphone decision tree specialization for language adaptation},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2000},
volume={3},
pages={1707-1710},
doi={10.1109/ICASSP.2000.862080},
art_number={862080},
note={cited By 24; Conference of 25th IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2000 ; Conference Date: 5 June 2000 Through 9 June 2000;  Conference Code:126254},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033676746&doi=10.1109%2fICASSP.2000.862080&partnerID=40&md5=532302ac3b0a0551355173a1eab5c488},
affiliation={Interactive Systems Laboratories, University of Karlsruhe (Germany), Carnegie Mellon University (USA), United States},
abstract={With the distribution of speech technology products all over the world, the fast and efficient portability to new target languages becomes a practical concern. The authors explore the relative effectiveness of adapting multilingual LVCSR systems to a new target language with limited adaptation data. For this purpose they introduce a polyphone decision tree specialization method. Several recognition results are presented based on mono- and multilingual recognizers. These recognizers are developed in the framework of the project GlobalPhone. In this project we investigate speech recognition in 15 languages: Arabic, Mandarin and Shanghai Chinese, Croatian, English, French, German, Japanese, Korean, Portuguese, Russian, Spanish, Swedish, Tamil, and Turkish. © 2000 IEEE.},
keywords={Decision trees;  Signal processing;  Speech processing;  Decision theory;  Decoding;  Markov processes;  Maximum likelihood estimation;  Pattern recognition systems;  Speech coding, Croatians;  Speech technology;  Swedishs;  Target language;  Turkishs, Speech recognition;  Continuous speech recognition, Language adaptation;  Polyphone decision tree specialization;  Word error rate},
references={Constantinescu, A., On cross-language experiments and data-driven units for ALISP (1997) Proc. ASRU, pp. 606-613. , St. Barbara, CA; Bub, U., In-service adaptation of multilingual hidden-Markov-models (1997) Proc. ICASSP, pp. 1451-1454. , Munich; Schultz, T., Multilingual and crosslingual speech recognition (1998) Proc. DARPA Workshop on Broadcast News Transcription and Understanding, pp. 259-262. , Lansdowne, VA; Glass, J., Multi-lingual spoken language understanding in the mit voyager system (1995) Speech Communication, (17), pp. 1-18; Wheatley, B., An evaluation of cross-language adaptation for rapid HMM development in a new language (1994) Proc. ICASSP, pp. 237-240. , Adelaide; Kohler, J., Language adaptation of multilingualphone models for vocabulary independent speech recognition tasks (1998) Proc. ICASSP, pp. 417-420. , Seattle; Schultz, T., Fast bootstrapping of LVCSR systems with multilingual phoneme sets (1997) Proc. Eurospeech, pp. 371-374. , Rhodes; Schultz, T., Language independent and language adaptive LVCSR (1998) Proc. ICSLP, pp. 1819-1822. , Sydney; Cohen, P., Towards a universal speech recognizer for multiple languages (1997) Proc. ASRU, pp. 591-598. , St. Barbara CA; Lee, K.-F., (1988) Large-vocabulary Speaker-independent Continuous Speech Recognition: The SPHINX System, , PhD Thesis, Carnegie Mellon University; Finke, M., Wide context acoustic modeling in read vs. Spontaneous speech (1997) Proc. ICASSP, pp. 1743-1746. , Munich},
sponsors={IEEE Signal Processing Society; The Institute of Electrical and Electronics Engineers (IEEE)},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15206149},
isbn={0780362934},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Çarki20001563,
author={Çarki, K. and Geutner, P. and Schultz, T.},
title={Turkish LVCSR: Towards better speech recognition for agglutinative languages},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2000},
volume={3},
pages={1563-1566},
doi={10.1109/ICASSP.2000.861971},
art_number={861971},
note={cited By 38; Conference of 25th IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2000 ; Conference Date: 5 June 2000 Through 9 June 2000;  Conference Code:126254},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033692891&doi=10.1109%2fICASSP.2000.861971&partnerID=40&md5=81d607acd803992e0e13c16df8d3d51c},
affiliation={Interactive Systems Laboratories, University of Karlsruhe, Germany},
abstract={The Turkish language belongs to the Turkic family. All members of this family are close to one another in terms of linguistic structure. Typological similarities are vowel harmony, verb-final word order and agglutinative morphology. This latter property causes a very fast vocabulary growth resulting in a large number of out-of-vocabulary words. In this paper we describe our first experiments in a speaker independent LVCSR engine for Modern Standard Turkish. First results on our Turkish speech recognition system are presented. The currently best system shows very promising results achieving 16.9% word error rate. To overcome the OOV-problem we propose a morphem-based and the Hypothesis Driven Lexical Adaptation approach. The final Turkish system is integrated into the multilingual recognition engine of the GlobalPhone project. © 2000 IEEE.},
keywords={Deep neural networks;  Engines;  Linguistics;  Natural language processing systems;  Signal processing;  Speech processing;  Adaptive systems;  Database systems;  Linguistics;  Mathematical models;  Pattern recognition systems;  Speech analysis, Agglutinative language;  Agglutinative morphology;  Lexical adaptation;  Linguistic structure;  Out of vocabulary words;  Recognition engines;  Speaker independents;  Speech recognition systems, Speech recognition;  Continuous speech recognition, Large vocabulary continuous speech recognition;  Out of vocabulary words;  Speech database;  Turkish language},
references={Young, S.J., Multilingual large vocabulary speech recognition: The European SQALE project (1997) Computer Speech and Language, 11, pp. 73-89; Oflazar, K., Göçmen, E., Bozşahin, C., (1994) An Outline of Turkish Morphology, , Report on Turkish Natural Language Processing Initiative Project; Schultz, T., Waibel, A., Language independent and language adaptive LVCSR (1998) Proc. ICSLP, pp. 1819-1822. , Sydney; Kornfilt, J., (1990) Turkish and the Turkic Languages, pp. 619-645. , B. Comrie Editor: The Worlds Major Languages; Schultz, T., Waibel, A., Fast bootstrapping of LVCSR systems with multilingual phoneme sets (1997) Proc. Eurospeech, pp. 371-374. , Rhodes; Oflazar, K., Tür, G., Combining hand-crafted rules and unsupervised learning in constrained-based morphological disambiguation (1996) Proc. ACL, , Philadelphia; Kiecza, D., Schultz, T., Waibel, A., Data-driven determination of appropriate dictionary units for Korean LVCSR (1999) Proc. ICSP, , Seoul; Geutner, P., (1999) Adaptive Vocabularies in Large Conversational Speech Recognition, , PhD Thesis, University of Karlsruhe, Germany; Geutner, P., Finke, M., Waibel, A., Selection criteria for hypothesis driven lexical adaptation (1999) Proc. ICASSP'99, , Phoenix},
sponsors={IEEE Signal Processing Society; The Institute of Electrical and Electronics Engineers (IEEE)},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15206149},
isbn={0780362934},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Metze20001827,
author={Metze, F. and Kemp, T. and Schaaf, T. and Schultz, T. and Soltau, H.},
title={Confidence measure based language identification},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2000},
volume={3},
pages={1827-1830},
doi={10.1109/ICASSP.2000.862110},
art_number={862110},
note={cited By 10; Conference of 25th IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2000 ; Conference Date: 5 June 2000 Through 9 June 2000;  Conference Code:126254},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033676893&doi=10.1109%2fICASSP.2000.862110&partnerID=40&md5=1e54bf9cd20b5f14a4dca53c1f6bc3c8},
affiliation={Interactive Systems Laboratories, University of Karlsruhe, Germany},
abstract={In this paper we present a new application for confidence measures in spoken language processing. In today's computerized dialogue systems, language identification (LID) is typically achieved via dedicated modules. In our approach, LID is integrated into the speech recognizer, therefore profiting from high-level linguistic knowledge at very little extra cost. Our new approach is based on a word lattice based confidence measure (Kemp and Schaaf, 1997), which was originally devised for unsupervised training. In this work, we show that the confidence based language identification algorithm outperforms conventional score based methods. Also, this method is less dependent on the acoustic characteristics of the transmission channel than score based methods. By introducing additional parameters, unknown languages can be rejected. The proposed method is compared to a score based approach on the Verbmobil database, a three language task. © 2000 IEEE.},
keywords={Natural language processing systems;  Signal processing;  Speech processing;  Algorithms;  Computational linguistics;  Database systems;  Markov processes;  Mathematical models;  Probability density function;  Speech analysis;  Speech processing, Acoustic characteristic;  Confidence Measure;  Language identification;  Linguistic knowledge;  Speech recognizer;  Spoken language processing;  Transmission channels;  Unsupervised training, Speech recognition;  Speech recognition, Confidence based language identification algorithm;  Confidence measure;  Hidden Markov model},
references={Zissman, M.A., Berkling, K.M., Automatic language identification (1999) Proc. Multilingual Interoperability in Speech Technology, pp. 93-101. , Leusden; Schaaf, T., Kemp, T., Confidence measures for spontaneous speech (1997) Proc. ICASSP-97, 2, p. 875. , Munich, Germany; Kemp, T., Schaaf, T., Estimating confidence using word lattices (1997) Proc. Eurospeech 97, 2, p. 827. , Rhodes, Greece; Schultz, T., LVCSR-based language identification (1996) Proc. ICASSP, pp. 781-784. , Altlanta; Hazen, T.J., Zue, V.W., Automatic language identification using a segment-based approach (1993) Proc. Eurospeech, pp. 1303-1306; Reyes, A.A., Seino, T., Nakagawa, S., Three language identification methods based on HMMs (1994) Proc. ICSLP, pp. 1895-1898; Lamel, L.F., Gauvain, J., Identifying non-linguistic speech features (1993) Proc Eurospeech, 1, pp. 23-30; Muthusamy, Y., Berkling, K., Arai, T., Cole, R.A., Barnard, E., Comparison of approaches to automatic language identification using telephone speech (1993) Proc Eurospeech, pp. 1307-1310; Zissmann, M.A., Singer, E., Automatic language identification of telephone speech messages using phoneme recognition and n-gram modeling (1993) Proc ICASSP, 2, pp. 309-402; Berkling, K.M., Arai, T., Barnard, E., Analysis of phoneme-based features for language identification (1994) Proc ICASSP, 1, pp. 289-292; Mendoza, S., Automatic language identification using large vocabulary continuous speech recognition (1996) Proc ICASSP, pp. 785-788. , Atlanta; Hieronymus, J.L., Kadambe, S., Robust spoken language identification using large vocabulary speech recognition (1997) Proc ICASSP, pp. 1111-1114; Karger, R., Wahlster, W., (1997) Multilinguale Verarbeitung von Spontansprache, , KI 4/, in German},
sponsors={IEEE Signal Processing Society; The Institute of Electrical and Electronics Engineers (IEEE)},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15206149},
isbn={0780362934},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Waibel20001297,
author={Waibel, A. and Geutner, P. and Tomokiyo, L.M. and Schultz, T. and Woszczyna, M.},
title={Multilinguality in speech and spoken language systems},
journal={Proceedings of the IEEE},
year={2000},
volume={88},
number={8},
pages={1297-1313},
doi={10.1109/5.880085},
note={cited By 58},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012327341&doi=10.1109%2f5.880085&partnerID=40&md5=7fc89cd5075a4fc0ea8e6769a1d70b5b},
affiliation={Interactive Systems Laboratories, Karlsruhe University, Karlsruhe 76131, Germany; Interactive Systems Laboratories of Karlsruhe University, Karlsruhe 76131, Germany; Carneeie-Mellon Universitv, Pittsburgh, PA 15213, United States},
abstract={Building modern speech and language systems currently requires large data resources such as texts, voice recordings, pronunciation lexicons, morphological decomposition information, and parsing grammars. Based on a study of the most important differences between language groups, we introduce approaches to efficiently deal with the enormous task of covering even a small percentage of the world's languages. For speech recognition, we have reduced the resource requirements by applying acoustic model combination, bootstrapping, and adaption techniques. Similar algorithms have been applied to improve the recognition of foreign accents. Segmenting language into appropriate units reduces the amount of data required to robusth estimate statistical models. The underlying morphological principles are also used to automatically adapt the coverage of our speech recognition dictionaries M itli the Hypothesis Driven Lexical Adaptation (HDLA) algorithm. This reduces the out-of-vocabulaiy problems encountered in agglutinative languages. Speech Recognition results are reported for the read GlobalPhone Database and some broadcast news data. For speech translation, using a task-oriented Interlingua allows to build a system with N languages with linear rather than quadratic effort. We have introduced a modular grammar design to maximize reusability and portability. End-to-end translation results are reported on a travel-domain task in the framework ofC-STAR. © 2000 IEEE Publisher Item Identifier S 0018-9219(00)08100-7.},
author_keywords={Acoustic model combination;  C-STAR;  Context-dependent models;  Croatian;  Cross-language bootstrapping, cross-language transfer;  Dynamic;  English;  Foreign-accented speech;  French;  German;  GlobalPhone;  Hypothesis driven lexical adaptation (HDLA);  Lexical adaptation},
references={Allberg, S., Correa, N., Locktionov, V., Molitor, R., Rothenberg, M., The accent coach: An English pronunciation training system for Japanese speakers (1998) Proc. Speech Technology in Language Learning (STILL); Barnctt, J., Corrada, A., Gao, G., Gillik, L., Ito, Y., Lowe, S., Manganaro, L., Peskin, B., Multilingual speech recognition at Dragon Systems/' (1996) Proc. ICSLP, pp. 2191-2194. , Philadelphia, PA; Beebe, L.M., Myths about intcrlanguage phonology (1980) Inlerlanguage Phonology: the Acquisition of U Second Language Sound System, , G. loup and S. H. Weinberger, Eds. New York: Newbury House; Bub, U., Kohler, J., Imped, B., In-service adaptation of multilingual hidden-Markov-models, in (1997) Proc. ICASSP, Munich, Germany, pp. 1451-1454; Constantinescit, A., Chollet, G., On cross-language experiments and data-driven units for ALISP, in (1997) Proc. ASRU, pp. 606-613. , Santa Barbara, C A; Cohen, P., Dharanipragada, S., Gros, J., Monkowski, M., Neti, C., Roukos, S., Ward, T., Toward a universal speech recognizer for multiple languages, in (1997) Proc: ASRU, pp. 591-598. , Santa Barbara, CA; Cutler, A., The comparative perspective on spoken-language processing (1997) Speech Commun., 21, pp. 3-15; Dugast, C., Aubert, X., Kneser, R., The Philips large-vocabulary recognition system for American English, French, and German (1995) Five. Eitmspeech, pp. 197-200. , Madrid. Spain; Finke, M., Rogina, I., Wide context acoustic modeling in read vs. spontaneous speech, in (1997) Proc. ICASSP, Munich, Germany, pp. 1743-1746; Fukada, T., Koli, D., Waibel, A., Tanigaki, K., Probabilistic dialogue act extraction for concept based multilingual translation systems, in (1998) Proc. 1CSI..P, , Sydney, Australia; Gavalda, M., Waibel, A., Growing semantic grammars, in (1998) Proc. COLING.'ACL, Montrai, Quebec, Canada; Geumer, P., Finke, M., Scheytt, P., Waibel, A., Wactlar, H., Transcribing multilingual broadcast news using hypothesis driven lexical adaptation. in (1998) Proc. DARPA Workshop on Broadens! Mews Transcription Und Uiiilemtfinilinf;. Lansdovvne, VA, Feb.; Geutner, P., Finke, M., Scheytt, P., Adaptive vocabularies for transcribing multilingual broadcast news, in (1998) Proc. ICASSP, , Seattle, WA. May; Geutner, P., Finke, M., Waibel, A., Phonetic-distance-based hypothesis driven lexical adaptation for transcribing multilingual broadcast news, in (1998) Proc. 1CSLP, , Sydney, Australia, Dec; Geutner, P., (1999) Adaptive Vocabularies in Large Vocabulary Conversational Speech Recognition", , Ph.D. dissertation, Univ. Karlsruhe, Germany, Feb; Geutner, P., Finke, M., Waibel, A., Selection criteria for hypothesis driven lexical adaptation, in (1999) Proc. ICASSP, , Phoenix, AZ, Mar; Glass, J., Flammia, G., Goodine, D., Phillips, M., Polifroni, J., Sakai, S., Seneff, S., Zue, V., Multi-lingual spoken language understanding in the MIT voyager system (1995) Speech Commun., 17, pp. 1-18; Gokcen, S., Gokcen, J., A multilingual phoneme and model set: Toward a universal base for automatic speech recognition, in (1997) Proc. ASRU, Santa Barbara, CA, pp. 599-603; Humphries, J.J., Woodland, P.C., The use of accent-specific pronunciation dictionaries in acoustic model training, in (1998) Proc. ICASSP, Seattle, WA; Kawai, G., (1999) Spoken Language Processing Applied to Nonnative Language Pronunciation Learning, , Ph.D. dissertation, Univ. Tokyo; Köhler, J., Language adaptation of multilingual phone models for vocabulary independent speech recognition tasks (1998) M Proc.. ICASSP, Seattle, WA, pp. 417-420; Lamel, L., Adda-Decker, M., Gauvain, J.L., Issues in large vocabulary multilingual speech recognition, in (1995) Proc. Euraspeeck, Madrid, Spain, pp. 185-189; Mayfieldtomokiyo, L., Burger, S., Eliciting natural speech from nonnative users: Collecting speech data for LVCSR, in (1999) Proc. ACL 'Workshop in Computer-Mediated Language Assessment and Evaluation M Natural Language Processing., , College Park, MD; Munk, M., Shallow statistical parsing for machine translation (1999) Dipl. Thesis, Carnegic-Mellon Univ., Pittsburgh, PA, May; N'Irenburg, S., The Patigloss Mark III machine translation system (1996) CMU Tech. Rep. CMU-CMT-95-145; Oflazar, K., Gocmen, E., Bozsahin, C., An outline of Turkish morphology (1994) Tech. Rep. Turkish Natural Language Processing Initiative Project; Osterholtz, L., Augustine, C., McNair, A., Rogir, A.I., Saito, H., Sloboda, T., Tebelskis, J., Woszczyna, M., Testing generality in JANUS: A multi-lingual speech translation system (1992) Proc. ICASSP, 1; Ronen, O., Neumeyer, L., Franco, H., Automatic detection of mispronunciation for language instruction (1997) Presented at the P Roc. Eurospccch, , Rhodes, Greece; Schnitz, T., Waihcl, A., Language independent and language adaptive LVCSR (1998) Proc. /CSZ., pp. 1819-1822. , Sydney, Australia; Fast bootstrapping of LVCSR systems with mulliliugual phoneme sets (1997) Proc. Eum.tpeerh, Rhodes, Greece, pp. 371-374; Adaptation of pronunciation dictionaries for recognition of unseen languages. in (1998) Proc. Spccom, pp. 207-210. , St. Petersburg, Russia; Multilingual and crosslingual speech recognition (1998) Proc. DARPA Workshop on Broadcast T\'ews Transcription and Understanding, , Lansdowne, VA; Language adaptation through polyphone decision tree specialization, in (1999) Proc. Multilingual Interoperability Speech Technology 'Workshop, Leusden, the Netherlands; Schwarz, R., Jin, H., Kubala, F., Modeling those F-conditions-or not (1997) Proc. DARPA Speech Recognition Workshop; Shriberg, E., Stolcke, A., Word predictability after hesitations (1996) InPmc.ICSLP; Wheatley, B., Kondo, K., Anderson, W., Muthusamy, Y., An evaluation of cross-language adaptation for rapid HMM development in a new language (1994) Proc. ICASSP, pp. 237-240. , Adelaide, Australia; Witt, S., Young, S., Language learning based on nonnative speech recognition, presented at the (1997) Proc. Eitrospeech, , Rhodes, Greece; Off-line acoustic modeling of nonnative accents, in (1999) Proc. Euwspeech, , Budapest, Hungary; Woszczyna, M., Broadhead, M., Gates, D., Gavaldà, M., Lavie, A., Levin, L., Waibel, A., A modular approach to spoken language translation for large domains, in (1998) Proc. AMTA; Proc. C-STAR Workshop. [Online] Available: Www.cstar.org; Palmer, D., A trainable rule-based algorithm for word segmentation, presented at the (1997) Proc. Istli Ai'iin. Mit. ACL, , Madrid, Spain; Mayfield Tomokiyo, L., Ries, K., An automatic method for learning a Japanese lexicon for recognition of spontaneous speech, presented at the (1998) Proc. ICASSP-9k, , Seattle, WA, May},
correspondence_address1={Waibel, A.; Interactive Systems Laboratories, Karlsruhe University, Karlsruhe 76131, Germany},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={00189219},
coden={IEEPA},
language={English},
abbrev_source_title={Proc. IEEE},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kurematsu2000,
author={Kurematsu, A. and Akegami, Y. and Burger, S. and Jekat, S. and Lause, B. and Maclaren, V.L. and Oppermann, D. and Schultz, T.},
title={VERBMOBIL dialogues: Multifaced analysis},
journal={6th International Conference on Spoken Language Processing, ICSLP 2000},
year={2000},
note={cited By 3; Conference of 6th International Conference on Spoken Language Processing, ICSLP 2000 ; Conference Date: 16 October 2000 Through 20 October 2000;  Conference Code:124331},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009115428&partnerID=40&md5=23492410aa0c543001f0466e3cdcb71c},
affiliation={University of Electro-Communications, Japan; ATR International, Japan; Carnegie Mellon University, United States; University of Hamburg, Germany; University of Muenchen, Germany; University of Kahlsruhe, Germany},
abstract={This paper describes the outline of collecting and transcribing spontaneous spoken dialogues for VERBMOBIL, the German research project on m ult ilingual processing of spontaneous speech. The method and conditions of data collection performed using the same scenario and the transliteration convention of spontaneous speech were described. The characteristics of VERBMOBIL corpus were presented in terms of the size of dialogues, turns, sentences, words, perplexity based on the linguistic analysis.},
keywords={Data collection;  Linguistic analysis;  nocv2;  Spoken dialogue;  Spontaneous speech, Linguistics},
references={Burger, S., Transliteration of spontaneous speech data Manual of transliteration conventions VERBMOBIL 2 (1997) English Version Release 2, , August; Weilhammer, K., Burger, S., File names, formats and structures in VERBMOBIL VERBMOBIL MEMO-131; Kurematsu, A., Akegami, Y., Schultz, T., Burger, S., Data collection and transliteration of Japanese spontaneous database in the travel arrangement task domain (1999) Oriental COCOSDA'99, pp. 125-128; Matsumura, A., (1985) Daijirin Dictionary, , Sanseido Publishing; Matsumoto, Y., Japanese morphological analysis system: CHASEN (1997) Information Science Technical Report NAIST-IS-TR97007, , Nara Inst itute of Science and Technology; Jekat, S., Lause, B., (1999) VMII Szenario A und B: Instruktionen fur Alle Sprachstellungen, 71. , Universitat Hamburg, Friedrich-Alexander Universität Erlangen-Nürnberg, VM-Techdoc, August},
sponsors={},
publisher={International Speech Communication Association},
isbn={7801501144; 9787801501141},
language={English},
abbrev_source_title={Int. Conf. Spok. Lang. Process., ICSLP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Soltau19981137,
author={Soltau, H. and Schultz, T. and Westphal, M. and Waibel, A.},
title={Recognition of music types},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={1998},
volume={2},
pages={1137-1140},
doi={10.1109/ICASSP.1998.675470},
art_number={675470},
note={cited By 68; Conference of 1998 23rd IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 1998 ; Conference Date: 12 May 1998 Through 15 May 1998;  Conference Code:101940},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892149562&doi=10.1109%2fICASSP.1998.675470&partnerID=40&md5=ec288ba29181997991dc5e656d0c2248},
affiliation={Interactive Systems Laboratories, University of Kxlsruhe, Germany; Carnegie Mellon University, University of Kxlsruhe, United States},
abstract={This paper describes a music type recognition system that can be used to index and search in multimedia databases. A new approach to temporal structure modeling is supposed. The so called ETM-NN (explicit time modelling with neural network) method uses abstraction of acoustical events to the hidden units of a neural network. This new set of abstract features representing temporal structures, can be then learned via a traditional neural networks to discriminate between different types of music. The experiments show that this method outperforms HMMs significantly. © 1998 IEEE.},
keywords={Hidden units;  Multimedia database;  New approaches;  Recognition systems;  Temporal structures, Signal processing, Neural networks},
references={Dixon, S., Multiphonicnote identification (1996) Proc. of the 19th Austral. Conference, Melborne; El Man, J.L., Finding structures in time (1990) Cognitive Science, 14; Gallinari, P., Thiria, S., Badran, F., Fogelman-Soulie, F., On the relations between discriminant analysis and multilayer perceptron (1991) Neural Networks, 4; Rabiner, L.R., Juang, B.H., An introduction to hidden markov models (1986) IEEE ASSP Magazine, , Jan; Saunders, J., Real-time discrimination of broadcast speech-music (1996) Proc. ICASSP; Soltau, H., (1997) Erkennung von Musikstilen, , Diplomarbeit, Universitat Karlsruhe; Webb, A., Loewe, D., The optimized internal representation of multilayer classifier networks performs nonlinear discriminant analysis (1990) Neural Networks, 3},
correspondence_address1={Interactive Systems Laboratories, University of KxlsruheGermany},
sponsors={The Institute of Electrical and Electronic; Engineers (IEEE) Signal Processing Society},
address={Seattle, WA},
issn={15206149},
isbn={0780344286; 9780780344280},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schultz1996781,
author={Schultz, T. and Rogina, I. and Waibel, A.},
title={LVCSR-based language identification},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={1996},
volume={2},
pages={781-784},
note={cited By 33; Conference of Proceedings of the 1996 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP. Part 1 (of 6) ; Conference Date: 7 May 1996 Through 10 May 1996;  Conference Code:45447},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029725380&partnerID=40&md5=4944fad991559879501b518bf49dfe7a},
affiliation={Univ of Karlsruhe, Karlsruhe, Germany},
abstract={Automatic language identification is an important problem in building multilingual speech recognition and understanding systems. Building a language identification module for four languages we studied the influence of applying different levels of knowledge sources on a large vocabulary continuous speech recognition (LVCSR) approach, i.e. the phonetic, phonotactic, lexical, and syntactic-semantic knowledge. The resulting language identification (LID) module can identify spontaneous speech input and can be used as a frontend for our multilingual speech-to-speech translation system JANUS-II. A comparison of five LID systems showed that the incorporation of lexical and linguistic knowledge reduces the language identification error for the 2-language tests up to 50%. Based on these results we build a LID module for German, English, Spanish, and Japanese which yields 84% identification rate on the Spontaneous Scheduling Task (SST).},
keywords={Classification (of information);  Computational linguistics;  Database systems;  Natural language processing systems;  Scheduling;  Speech communication;  Speech recognition, Knowledge sources;  Language identification;  Large vocabulary continuous speech recognition;  Multilingual speech to speech translation system;  Spontaneous scheduling task, Speech processing},
correspondence_address1={Schultz, T.; Univ of Karlsruhe, Karlsruhe, Germany},
sponsors={IEEE},
publisher={IEEE, Piscataway, NJ, United States},
address={Atlanta, GA, USA},
issn={07367791},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schultz1995293,
author={Schultz, T. and Rogina, I.},
title={Acoustic and language modeling of human and nonhuman noises for human-to-human spontaneous speech recognition},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={1995},
volume={1},
pages={293-296},
note={cited By 11; Conference of Proceedings of the 1995 20th International Conference on Acoustics, Speech, and Signal Processing. Part 1 (of 5) ; Conference Date: 9 May 1995 Through 12 May 1995;  Conference Code:43127},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028996909&partnerID=40&md5=6aa6cd42fd5848d0199b938325697446},
affiliation={Univ of Karlsruhe, Karlsruhe, Germany},
abstract={In this paper several improvements of our speech-to-speech translation system JANUS on spontaneous human-to-human dialogs are presented. Common phenomena in spontaneous speech are described, followed by a classification of different types of noises. To handle the variety of spontaneous effects in human-to-human dialogs, special noise models are introduced representing both human and nonhuman noises, as well as word fragments. It will be shown that both the acoustic and the language modeling of these noises increase the recognition performance significantly. In the experiments, a clustering of the noise classes is performed and the resulting cluster variants are compared, thus allowing to determine the best tradeoff between sensitivity and trainability of the models.},
keywords={Acoustic noise;  Computer aided language translation;  Computer simulation;  Database systems;  Errors;  Human engineering;  Sensitivity analysis;  Mathematical models;  Performance;  Speech;  Speech processing, Human noises;  Human to human dialogs;  Nonhuman noises;  Speech to speech translation system;  Trainability;  Acoustic modeling;  Clustering;  Human-to-human dialogs;  Language modeling;  Speech-to-speech translation system, Speech recognition},
correspondence_address1={Schultz, T.; Univ of Karlsruhe, Karlsruhe, Germany},
sponsors={IEEE},
publisher={IEEE, Piscataway, NJ, United States},
address={Detroit, MI, USA},
issn={07367791},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}
